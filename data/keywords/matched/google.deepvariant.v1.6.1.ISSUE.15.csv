id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/google/deepvariant/issues/530:915,modifiability,servic,service,915,"Image /mnt overriding my machine's /mnt causing errors; I ran this using singularity. I tried to tell the system to read and write files to a folder in my machine's /mnt/ folder. I keep getting an error. After inspecting, it looks like this image has an empty /mnt/ directory that is not writable. This is a problem for us and many users because it is very common to store large amounts of data in the /mnt/ folder on servers that access shared space from a common storage device. Please tell your Dockerfile to ""RUN rm -rf /mnt/"" or something (I'm not a docker expert by any means). The deepvariant docker container clearly does not need /mnt/. **Setup**. - Centos 7. - deepvariant 1.3.0. - Singularity run pulling from here: docker://google/deepvariant:""1.3.0"". - quickstart example. **Steps to reproduce:**. ...please note that /mnt/share is an NFS mount. My server mounts a drive on another machine running nfs.service . mkdir -p /mnt/share/jasontest. cd /mnt/share/jasontest. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. BIN_VERSION=""1.3.0"". OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". singularity",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530
https://github.com/google/deepvariant/issues/530:2563,modifiability,interm,intermediate,2563,"19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. BIN_VERSION=""1.3.0"". OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" --num_shards=1. . stack trace:. I0317 09:40:21.184321 140398386173760 run_deepvariant.py:341] Creating a directory for intermediate results in /mnt/share/jasontest/quickstart-output/intermediate_results_dir. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 493, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 460, in main. intermediate_results_dir = check_or_create_intermediate_results_dir(. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 343, in check_or_create_intermediate_results_dir. os.makedirs(intermediate_results_dir). File ""/usr/lib/python3.8/os.py"", line 213, in makedirs. makedirs(head, exist_ok=exist_ok). File ""/usr/lib/python3.8/os.py"", line 213, in makedirs. makedirs(head, exist_ok=exist_ok). File ""/usr/lib/python3.8/os.py"", line 213, in makedirs. makedirs(head, exist_ok=exist_ok). [Previous line repeated 1 more time]. ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530
https://github.com/google/deepvariant/issues/530:2750,modifiability,modul,module,2750,"${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. BIN_VERSION=""1.3.0"". OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" --num_shards=1. . stack trace:. I0317 09:40:21.184321 140398386173760 run_deepvariant.py:341] Creating a directory for intermediate results in /mnt/share/jasontest/quickstart-output/intermediate_results_dir. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 493, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 460, in main. intermediate_results_dir = check_or_create_intermediate_results_dir(. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 343, in check_or_create_intermediate_results_dir. os.makedirs(intermediate_results_dir). File ""/usr/lib/python3.8/os.py"", line 213, in makedirs. makedirs(head, exist_ok=exist_ok). File ""/usr/lib/python3.8/os.py"", line 213, in makedirs. makedirs(head, exist_ok=exist_ok). File ""/usr/lib/python3.8/os.py"", line 213, in makedirs. makedirs(head, exist_ok=exist_ok). [Previous line repeated 1 more time]. File ""/usr/lib/python3.8/os.py"", line 223, in makedirs. mkdir(name, mode). OSError: [Errno 30] Read-only file system: '/mnt/share'. **Does the quick start test work on your system?**. ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530
https://github.com/google/deepvariant/issues/530:2810,modifiability,pac,packages,2810,"a.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. BIN_VERSION=""1.3.0"". OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" --num_shards=1. . stack trace:. I0317 09:40:21.184321 140398386173760 run_deepvariant.py:341] Creating a directory for intermediate results in /mnt/share/jasontest/quickstart-output/intermediate_results_dir. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 493, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 460, in main. intermediate_results_dir = check_or_create_intermediate_results_dir(. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 343, in check_or_create_intermediate_results_dir. os.makedirs(intermediate_results_dir). File ""/usr/lib/python3.8/os.py"", line 213, in makedirs. makedirs(head, exist_ok=exist_ok). File ""/usr/lib/python3.8/os.py"", line 213, in makedirs. makedirs(head, exist_ok=exist_ok). File ""/usr/lib/python3.8/os.py"", line 213, in makedirs. makedirs(head, exist_ok=exist_ok). [Previous line repeated 1 more time]. File ""/usr/lib/python3.8/os.py"", line 223, in makedirs. mkdir(name, mode). OSError: [Errno 30] Read-only file system: '/mnt/share'. **Does the quick start test work on your system?**. The quick test works on my system as long as my data is not i",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530
https://github.com/google/deepvariant/issues/530:2910,modifiability,pac,packages,2910,"""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. BIN_VERSION=""1.3.0"". OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" --num_shards=1. . stack trace:. I0317 09:40:21.184321 140398386173760 run_deepvariant.py:341] Creating a directory for intermediate results in /mnt/share/jasontest/quickstart-output/intermediate_results_dir. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 493, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 460, in main. intermediate_results_dir = check_or_create_intermediate_results_dir(. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 343, in check_or_create_intermediate_results_dir. os.makedirs(intermediate_results_dir). File ""/usr/lib/python3.8/os.py"", line 213, in makedirs. makedirs(head, exist_ok=exist_ok). File ""/usr/lib/python3.8/os.py"", line 213, in makedirs. makedirs(head, exist_ok=exist_ok). File ""/usr/lib/python3.8/os.py"", line 213, in makedirs. makedirs(head, exist_ok=exist_ok). [Previous line repeated 1 more time]. File ""/usr/lib/python3.8/os.py"", line 223, in makedirs. mkdir(name, mode). OSError: [Errno 30] Read-only file system: '/mnt/share'. **Does the quick start test work on your system?**. The quick test works on my system as long as my data is not in the /mnt/ folder. **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530
https://github.com/google/deepvariant/issues/530:48,performance,error,errors,48,"Image /mnt overriding my machine's /mnt causing errors; I ran this using singularity. I tried to tell the system to read and write files to a folder in my machine's /mnt/ folder. I keep getting an error. After inspecting, it looks like this image has an empty /mnt/ directory that is not writable. This is a problem for us and many users because it is very common to store large amounts of data in the /mnt/ folder on servers that access shared space from a common storage device. Please tell your Dockerfile to ""RUN rm -rf /mnt/"" or something (I'm not a docker expert by any means). The deepvariant docker container clearly does not need /mnt/. **Setup**. - Centos 7. - deepvariant 1.3.0. - Singularity run pulling from here: docker://google/deepvariant:""1.3.0"". - quickstart example. **Steps to reproduce:**. ...please note that /mnt/share is an NFS mount. My server mounts a drive on another machine running nfs.service . mkdir -p /mnt/share/jasontest. cd /mnt/share/jasontest. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. BIN_VERSION=""1.3.0"". OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". singularity",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530
https://github.com/google/deepvariant/issues/530:197,performance,error,error,197,"Image /mnt overriding my machine's /mnt causing errors; I ran this using singularity. I tried to tell the system to read and write files to a folder in my machine's /mnt/ folder. I keep getting an error. After inspecting, it looks like this image has an empty /mnt/ directory that is not writable. This is a problem for us and many users because it is very common to store large amounts of data in the /mnt/ folder on servers that access shared space from a common storage device. Please tell your Dockerfile to ""RUN rm -rf /mnt/"" or something (I'm not a docker expert by any means). The deepvariant docker container clearly does not need /mnt/. **Setup**. - Centos 7. - deepvariant 1.3.0. - Singularity run pulling from here: docker://google/deepvariant:""1.3.0"". - quickstart example. **Steps to reproduce:**. ...please note that /mnt/share is an NFS mount. My server mounts a drive on another machine running nfs.service . mkdir -p /mnt/share/jasontest. cd /mnt/share/jasontest. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. BIN_VERSION=""1.3.0"". OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". singularity",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530
https://github.com/google/deepvariant/issues/530:3562,performance,time,time,3562,"""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. BIN_VERSION=""1.3.0"". OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" --num_shards=1. . stack trace:. I0317 09:40:21.184321 140398386173760 run_deepvariant.py:341] Creating a directory for intermediate results in /mnt/share/jasontest/quickstart-output/intermediate_results_dir. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 493, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 460, in main. intermediate_results_dir = check_or_create_intermediate_results_dir(. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 343, in check_or_create_intermediate_results_dir. os.makedirs(intermediate_results_dir). File ""/usr/lib/python3.8/os.py"", line 213, in makedirs. makedirs(head, exist_ok=exist_ok). File ""/usr/lib/python3.8/os.py"", line 213, in makedirs. makedirs(head, exist_ok=exist_ok). File ""/usr/lib/python3.8/os.py"", line 213, in makedirs. makedirs(head, exist_ok=exist_ok). [Previous line repeated 1 more time]. File ""/usr/lib/python3.8/os.py"", line 223, in makedirs. mkdir(name, mode). OSError: [Errno 30] Read-only file system: '/mnt/share'. **Does the quick start test work on your system?**. The quick test works on my system as long as my data is not in the /mnt/ folder. **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530
https://github.com/google/deepvariant/issues/530:625,reliability,doe,does,625,"Image /mnt overriding my machine's /mnt causing errors; I ran this using singularity. I tried to tell the system to read and write files to a folder in my machine's /mnt/ folder. I keep getting an error. After inspecting, it looks like this image has an empty /mnt/ directory that is not writable. This is a problem for us and many users because it is very common to store large amounts of data in the /mnt/ folder on servers that access shared space from a common storage device. Please tell your Dockerfile to ""RUN rm -rf /mnt/"" or something (I'm not a docker expert by any means). The deepvariant docker container clearly does not need /mnt/. **Setup**. - Centos 7. - deepvariant 1.3.0. - Singularity run pulling from here: docker://google/deepvariant:""1.3.0"". - quickstart example. **Steps to reproduce:**. ...please note that /mnt/share is an NFS mount. My server mounts a drive on another machine running nfs.service . mkdir -p /mnt/share/jasontest. cd /mnt/share/jasontest. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. BIN_VERSION=""1.3.0"". OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". singularity",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530
https://github.com/google/deepvariant/issues/530:3703,reliability,Doe,Does,3703,"""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. BIN_VERSION=""1.3.0"". OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" --num_shards=1. . stack trace:. I0317 09:40:21.184321 140398386173760 run_deepvariant.py:341] Creating a directory for intermediate results in /mnt/share/jasontest/quickstart-output/intermediate_results_dir. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 493, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 460, in main. intermediate_results_dir = check_or_create_intermediate_results_dir(. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 343, in check_or_create_intermediate_results_dir. os.makedirs(intermediate_results_dir). File ""/usr/lib/python3.8/os.py"", line 213, in makedirs. makedirs(head, exist_ok=exist_ok). File ""/usr/lib/python3.8/os.py"", line 213, in makedirs. makedirs(head, exist_ok=exist_ok). File ""/usr/lib/python3.8/os.py"", line 213, in makedirs. makedirs(head, exist_ok=exist_ok). [Previous line repeated 1 more time]. File ""/usr/lib/python3.8/os.py"", line 223, in makedirs. mkdir(name, mode). OSError: [Errno 30] Read-only file system: '/mnt/share'. **Does the quick start test work on your system?**. The quick test works on my system as long as my data is not in the /mnt/ folder. **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530
https://github.com/google/deepvariant/issues/530:48,safety,error,errors,48,"Image /mnt overriding my machine's /mnt causing errors; I ran this using singularity. I tried to tell the system to read and write files to a folder in my machine's /mnt/ folder. I keep getting an error. After inspecting, it looks like this image has an empty /mnt/ directory that is not writable. This is a problem for us and many users because it is very common to store large amounts of data in the /mnt/ folder on servers that access shared space from a common storage device. Please tell your Dockerfile to ""RUN rm -rf /mnt/"" or something (I'm not a docker expert by any means). The deepvariant docker container clearly does not need /mnt/. **Setup**. - Centos 7. - deepvariant 1.3.0. - Singularity run pulling from here: docker://google/deepvariant:""1.3.0"". - quickstart example. **Steps to reproduce:**. ...please note that /mnt/share is an NFS mount. My server mounts a drive on another machine running nfs.service . mkdir -p /mnt/share/jasontest. cd /mnt/share/jasontest. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. BIN_VERSION=""1.3.0"". OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". singularity",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530
https://github.com/google/deepvariant/issues/530:197,safety,error,error,197,"Image /mnt overriding my machine's /mnt causing errors; I ran this using singularity. I tried to tell the system to read and write files to a folder in my machine's /mnt/ folder. I keep getting an error. After inspecting, it looks like this image has an empty /mnt/ directory that is not writable. This is a problem for us and many users because it is very common to store large amounts of data in the /mnt/ folder on servers that access shared space from a common storage device. Please tell your Dockerfile to ""RUN rm -rf /mnt/"" or something (I'm not a docker expert by any means). The deepvariant docker container clearly does not need /mnt/. **Setup**. - Centos 7. - deepvariant 1.3.0. - Singularity run pulling from here: docker://google/deepvariant:""1.3.0"". - quickstart example. **Steps to reproduce:**. ...please note that /mnt/share is an NFS mount. My server mounts a drive on another machine running nfs.service . mkdir -p /mnt/share/jasontest. cd /mnt/share/jasontest. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. BIN_VERSION=""1.3.0"". OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". singularity",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530
https://github.com/google/deepvariant/issues/530:1010,safety,test,testdata,1010,"rriding my machine's /mnt causing errors; I ran this using singularity. I tried to tell the system to read and write files to a folder in my machine's /mnt/ folder. I keep getting an error. After inspecting, it looks like this image has an empty /mnt/ directory that is not writable. This is a problem for us and many users because it is very common to store large amounts of data in the /mnt/ folder on servers that access shared space from a common storage device. Please tell your Dockerfile to ""RUN rm -rf /mnt/"" or something (I'm not a docker expert by any means). The deepvariant docker container clearly does not need /mnt/. **Setup**. - Centos 7. - deepvariant 1.3.0. - Singularity run pulling from here: docker://google/deepvariant:""1.3.0"". - quickstart example. **Steps to reproduce:**. ...please note that /mnt/share is an NFS mount. My server mounts a drive on another machine running nfs.service . mkdir -p /mnt/share/jasontest. cd /mnt/share/jasontest. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. BIN_VERSION=""1.3.0"". OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". singularity run -B /usr/l",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530
https://github.com/google/deepvariant/issues/530:1090,safety,test,testdata,1090,"to tell the system to read and write files to a folder in my machine's /mnt/ folder. I keep getting an error. After inspecting, it looks like this image has an empty /mnt/ directory that is not writable. This is a problem for us and many users because it is very common to store large amounts of data in the /mnt/ folder on servers that access shared space from a common storage device. Please tell your Dockerfile to ""RUN rm -rf /mnt/"" or something (I'm not a docker expert by any means). The deepvariant docker container clearly does not need /mnt/. **Setup**. - Centos 7. - deepvariant 1.3.0. - Singularity run pulling from here: docker://google/deepvariant:""1.3.0"". - quickstart example. **Steps to reproduce:**. ...please note that /mnt/share is an NFS mount. My server mounts a drive on another machine running nfs.service . mkdir -p /mnt/share/jasontest. cd /mnt/share/jasontest. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. BIN_VERSION=""1.3.0"". OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}"" /opt/de",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530
https://github.com/google/deepvariant/issues/530:2750,safety,modul,module,2750,"${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. BIN_VERSION=""1.3.0"". OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" --num_shards=1. . stack trace:. I0317 09:40:21.184321 140398386173760 run_deepvariant.py:341] Creating a directory for intermediate results in /mnt/share/jasontest/quickstart-output/intermediate_results_dir. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 493, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 460, in main. intermediate_results_dir = check_or_create_intermediate_results_dir(. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 343, in check_or_create_intermediate_results_dir. os.makedirs(intermediate_results_dir). File ""/usr/lib/python3.8/os.py"", line 213, in makedirs. makedirs(head, exist_ok=exist_ok). File ""/usr/lib/python3.8/os.py"", line 213, in makedirs. makedirs(head, exist_ok=exist_ok). File ""/usr/lib/python3.8/os.py"", line 213, in makedirs. makedirs(head, exist_ok=exist_ok). [Previous line repeated 1 more time]. File ""/usr/lib/python3.8/os.py"", line 223, in makedirs. mkdir(name, mode). OSError: [Errno 30] Read-only file system: '/mnt/share'. **Does the quick start test work on your system?**. ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530
https://github.com/google/deepvariant/issues/530:3724,safety,test,test,3724,"""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. BIN_VERSION=""1.3.0"". OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" --num_shards=1. . stack trace:. I0317 09:40:21.184321 140398386173760 run_deepvariant.py:341] Creating a directory for intermediate results in /mnt/share/jasontest/quickstart-output/intermediate_results_dir. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 493, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 460, in main. intermediate_results_dir = check_or_create_intermediate_results_dir(. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 343, in check_or_create_intermediate_results_dir. os.makedirs(intermediate_results_dir). File ""/usr/lib/python3.8/os.py"", line 213, in makedirs. makedirs(head, exist_ok=exist_ok). File ""/usr/lib/python3.8/os.py"", line 213, in makedirs. makedirs(head, exist_ok=exist_ok). File ""/usr/lib/python3.8/os.py"", line 213, in makedirs. makedirs(head, exist_ok=exist_ok). [Previous line repeated 1 more time]. File ""/usr/lib/python3.8/os.py"", line 223, in makedirs. mkdir(name, mode). OSError: [Errno 30] Read-only file system: '/mnt/share'. **Does the quick start test work on your system?**. The quick test works on my system as long as my data is not in the /mnt/ folder. **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530
https://github.com/google/deepvariant/issues/530:3763,safety,test,test,3763,"""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. BIN_VERSION=""1.3.0"". OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" --num_shards=1. . stack trace:. I0317 09:40:21.184321 140398386173760 run_deepvariant.py:341] Creating a directory for intermediate results in /mnt/share/jasontest/quickstart-output/intermediate_results_dir. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 493, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 460, in main. intermediate_results_dir = check_or_create_intermediate_results_dir(. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 343, in check_or_create_intermediate_results_dir. os.makedirs(intermediate_results_dir). File ""/usr/lib/python3.8/os.py"", line 213, in makedirs. makedirs(head, exist_ok=exist_ok). File ""/usr/lib/python3.8/os.py"", line 213, in makedirs. makedirs(head, exist_ok=exist_ok). File ""/usr/lib/python3.8/os.py"", line 213, in makedirs. makedirs(head, exist_ok=exist_ok). [Previous line repeated 1 more time]. File ""/usr/lib/python3.8/os.py"", line 223, in makedirs. mkdir(name, mode). OSError: [Errno 30] Read-only file system: '/mnt/share'. **Does the quick start test work on your system?**. The quick test works on my system as long as my data is not in the /mnt/ folder. **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530
https://github.com/google/deepvariant/issues/530:431,security,access,access,431,"Image /mnt overriding my machine's /mnt causing errors; I ran this using singularity. I tried to tell the system to read and write files to a folder in my machine's /mnt/ folder. I keep getting an error. After inspecting, it looks like this image has an empty /mnt/ directory that is not writable. This is a problem for us and many users because it is very common to store large amounts of data in the /mnt/ folder on servers that access shared space from a common storage device. Please tell your Dockerfile to ""RUN rm -rf /mnt/"" or something (I'm not a docker expert by any means). The deepvariant docker container clearly does not need /mnt/. **Setup**. - Centos 7. - deepvariant 1.3.0. - Singularity run pulling from here: docker://google/deepvariant:""1.3.0"". - quickstart example. **Steps to reproduce:**. ...please note that /mnt/share is an NFS mount. My server mounts a drive on another machine running nfs.service . mkdir -p /mnt/share/jasontest. cd /mnt/share/jasontest. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. BIN_VERSION=""1.3.0"". OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". singularity",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530
https://github.com/google/deepvariant/issues/530:1010,testability,test,testdata,1010,"rriding my machine's /mnt causing errors; I ran this using singularity. I tried to tell the system to read and write files to a folder in my machine's /mnt/ folder. I keep getting an error. After inspecting, it looks like this image has an empty /mnt/ directory that is not writable. This is a problem for us and many users because it is very common to store large amounts of data in the /mnt/ folder on servers that access shared space from a common storage device. Please tell your Dockerfile to ""RUN rm -rf /mnt/"" or something (I'm not a docker expert by any means). The deepvariant docker container clearly does not need /mnt/. **Setup**. - Centos 7. - deepvariant 1.3.0. - Singularity run pulling from here: docker://google/deepvariant:""1.3.0"". - quickstart example. **Steps to reproduce:**. ...please note that /mnt/share is an NFS mount. My server mounts a drive on another machine running nfs.service . mkdir -p /mnt/share/jasontest. cd /mnt/share/jasontest. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. BIN_VERSION=""1.3.0"". OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". singularity run -B /usr/l",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530
https://github.com/google/deepvariant/issues/530:1090,testability,test,testdata,1090,"to tell the system to read and write files to a folder in my machine's /mnt/ folder. I keep getting an error. After inspecting, it looks like this image has an empty /mnt/ directory that is not writable. This is a problem for us and many users because it is very common to store large amounts of data in the /mnt/ folder on servers that access shared space from a common storage device. Please tell your Dockerfile to ""RUN rm -rf /mnt/"" or something (I'm not a docker expert by any means). The deepvariant docker container clearly does not need /mnt/. **Setup**. - Centos 7. - deepvariant 1.3.0. - Singularity run pulling from here: docker://google/deepvariant:""1.3.0"". - quickstart example. **Steps to reproduce:**. ...please note that /mnt/share is an NFS mount. My server mounts a drive on another machine running nfs.service . mkdir -p /mnt/share/jasontest. cd /mnt/share/jasontest. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. BIN_VERSION=""1.3.0"". OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}"" /opt/de",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530
https://github.com/google/deepvariant/issues/530:1578,testability,unit,unittest,1578,". The deepvariant docker container clearly does not need /mnt/. **Setup**. - Centos 7. - deepvariant 1.3.0. - Singularity run pulling from here: docker://google/deepvariant:""1.3.0"". - quickstart example. **Steps to reproduce:**. ...please note that /mnt/share is an NFS mount. My server mounts a drive on another machine running nfs.service . mkdir -p /mnt/share/jasontest. cd /mnt/share/jasontest. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. BIN_VERSION=""1.3.0"". OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" --num_shards=1. . stack trace:. I0317 09:40:21.184321 140398386173760 run_deepvariant.py:341] Creating a directory for intermediate result",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530
https://github.com/google/deepvariant/issues/530:1650,testability,unit,unittest,1650,"*. - Centos 7. - deepvariant 1.3.0. - Singularity run pulling from here: docker://google/deepvariant:""1.3.0"". - quickstart example. **Steps to reproduce:**. ...please note that /mnt/share is an NFS mount. My server mounts a drive on another machine running nfs.service . mkdir -p /mnt/share/jasontest. cd /mnt/share/jasontest. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. BIN_VERSION=""1.3.0"". OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" --num_shards=1. . stack trace:. I0317 09:40:21.184321 140398386173760 run_deepvariant.py:341] Creating a directory for intermediate results in /mnt/share/jasontest/quickstart-output/intermediate_results_dir. Tr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530
https://github.com/google/deepvariant/issues/530:1726,testability,unit,unittest,1726,"ker://google/deepvariant:""1.3.0"". - quickstart example. **Steps to reproduce:**. ...please note that /mnt/share is an NFS mount. My server mounts a drive on another machine running nfs.service . mkdir -p /mnt/share/jasontest. cd /mnt/share/jasontest. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. BIN_VERSION=""1.3.0"". OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" --num_shards=1. . stack trace:. I0317 09:40:21.184321 140398386173760 run_deepvariant.py:341] Creating a directory for intermediate results in /mnt/share/jasontest/quickstart-output/intermediate_results_dir. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530
https://github.com/google/deepvariant/issues/530:1801,testability,unit,unittest,1801,"e:**. ...please note that /mnt/share is an NFS mount. My server mounts a drive on another machine running nfs.service . mkdir -p /mnt/share/jasontest. cd /mnt/share/jasontest. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. BIN_VERSION=""1.3.0"". OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" --num_shards=1. . stack trace:. I0317 09:40:21.184321 140398386173760 run_deepvariant.py:341] Creating a directory for intermediate results in /mnt/share/jasontest/quickstart-output/intermediate_results_dir. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 493, in <module>. app.run(main). File ""/usr/local/lib/python3.8/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530
https://github.com/google/deepvariant/issues/530:1880,testability,unit,unittest,1880,"on another machine running nfs.service . mkdir -p /mnt/share/jasontest. cd /mnt/share/jasontest. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. BIN_VERSION=""1.3.0"". OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" --num_shards=1. . stack trace:. I0317 09:40:21.184321 140398386173760 run_deepvariant.py:341] Creating a directory for intermediate results in /mnt/share/jasontest/quickstart-output/intermediate_results_dir. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 493, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530
https://github.com/google/deepvariant/issues/530:2178,testability,unit,unittest,2178,"0_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. BIN_VERSION=""1.3.0"". OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" --num_shards=1. . stack trace:. I0317 09:40:21.184321 140398386173760 run_deepvariant.py:341] Creating a directory for intermediate results in /mnt/share/jasontest/quickstart-output/intermediate_results_dir. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 493, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 460, in main. intermediate_results_dir = check_or_create_intermediate_results_dir(. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 343, in check",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530
https://github.com/google/deepvariant/issues/530:2468,testability,trace,trace,2468,"R}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. BIN_VERSION=""1.3.0"". OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" --num_shards=1. . stack trace:. I0317 09:40:21.184321 140398386173760 run_deepvariant.py:341] Creating a directory for intermediate results in /mnt/share/jasontest/quickstart-output/intermediate_results_dir. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 493, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 460, in main. intermediate_results_dir = check_or_create_intermediate_results_dir(. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 343, in check_or_create_intermediate_results_dir. os.makedirs(intermediate_results_dir). File ""/usr/lib/python3.8/os.py"", line 213, in makedirs. makedirs(head, exist_ok=exist_ok). File ""/usr/lib/python3.8/os.py"", line 213, in makedirs. makedirs(head, exist_ok=exist_ok). File ""/usr/lib/python3.8/os.py""",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530
https://github.com/google/deepvariant/issues/530:2652,testability,Trace,Traceback,2652,"t.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. BIN_VERSION=""1.3.0"". OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" --num_shards=1. . stack trace:. I0317 09:40:21.184321 140398386173760 run_deepvariant.py:341] Creating a directory for intermediate results in /mnt/share/jasontest/quickstart-output/intermediate_results_dir. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 493, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 460, in main. intermediate_results_dir = check_or_create_intermediate_results_dir(. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 343, in check_or_create_intermediate_results_dir. os.makedirs(intermediate_results_dir). File ""/usr/lib/python3.8/os.py"", line 213, in makedirs. makedirs(head, exist_ok=exist_ok). File ""/usr/lib/python3.8/os.py"", line 213, in makedirs. makedirs(head, exist_ok=exist_ok). File ""/usr/lib/python3.8/os.py"", line 213, in makedirs. makedirs(head, exist_ok=exist_ok). [Previous line repeated 1 more time]. File ""/usr/lib/python3.8/os.py"", line 223, in makedirs. mkdir(name, mode). OSError: [Err",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530
https://github.com/google/deepvariant/issues/530:3724,testability,test,test,3724,"""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. BIN_VERSION=""1.3.0"". OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" --num_shards=1. . stack trace:. I0317 09:40:21.184321 140398386173760 run_deepvariant.py:341] Creating a directory for intermediate results in /mnt/share/jasontest/quickstart-output/intermediate_results_dir. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 493, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 460, in main. intermediate_results_dir = check_or_create_intermediate_results_dir(. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 343, in check_or_create_intermediate_results_dir. os.makedirs(intermediate_results_dir). File ""/usr/lib/python3.8/os.py"", line 213, in makedirs. makedirs(head, exist_ok=exist_ok). File ""/usr/lib/python3.8/os.py"", line 213, in makedirs. makedirs(head, exist_ok=exist_ok). File ""/usr/lib/python3.8/os.py"", line 213, in makedirs. makedirs(head, exist_ok=exist_ok). [Previous line repeated 1 more time]. File ""/usr/lib/python3.8/os.py"", line 223, in makedirs. mkdir(name, mode). OSError: [Errno 30] Read-only file system: '/mnt/share'. **Does the quick start test work on your system?**. The quick test works on my system as long as my data is not in the /mnt/ folder. **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530
https://github.com/google/deepvariant/issues/530:3763,testability,test,test,3763,"""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. BIN_VERSION=""1.3.0"". OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" --num_shards=1. . stack trace:. I0317 09:40:21.184321 140398386173760 run_deepvariant.py:341] Creating a directory for intermediate results in /mnt/share/jasontest/quickstart-output/intermediate_results_dir. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 493, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 460, in main. intermediate_results_dir = check_or_create_intermediate_results_dir(. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 343, in check_or_create_intermediate_results_dir. os.makedirs(intermediate_results_dir). File ""/usr/lib/python3.8/os.py"", line 213, in makedirs. makedirs(head, exist_ok=exist_ok). File ""/usr/lib/python3.8/os.py"", line 213, in makedirs. makedirs(head, exist_ok=exist_ok). File ""/usr/lib/python3.8/os.py"", line 213, in makedirs. makedirs(head, exist_ok=exist_ok). [Previous line repeated 1 more time]. File ""/usr/lib/python3.8/os.py"", line 223, in makedirs. mkdir(name, mode). OSError: [Errno 30] Read-only file system: '/mnt/share'. **Does the quick start test work on your system?**. The quick test works on my system as long as my data is not in the /mnt/ folder. **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530
https://github.com/google/deepvariant/issues/530:3851,testability,context,context,3851,"""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. BIN_VERSION=""1.3.0"". OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" --num_shards=1. . stack trace:. I0317 09:40:21.184321 140398386173760 run_deepvariant.py:341] Creating a directory for intermediate results in /mnt/share/jasontest/quickstart-output/intermediate_results_dir. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 493, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 460, in main. intermediate_results_dir = check_or_create_intermediate_results_dir(. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 343, in check_or_create_intermediate_results_dir. os.makedirs(intermediate_results_dir). File ""/usr/lib/python3.8/os.py"", line 213, in makedirs. makedirs(head, exist_ok=exist_ok). File ""/usr/lib/python3.8/os.py"", line 213, in makedirs. makedirs(head, exist_ok=exist_ok). File ""/usr/lib/python3.8/os.py"", line 213, in makedirs. makedirs(head, exist_ok=exist_ok). [Previous line repeated 1 more time]. File ""/usr/lib/python3.8/os.py"", line 223, in makedirs. mkdir(name, mode). OSError: [Errno 30] Read-only file system: '/mnt/share'. **Does the quick start test work on your system?**. The quick test works on my system as long as my data is not in the /mnt/ folder. **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530
https://github.com/google/deepvariant/issues/530:48,usability,error,errors,48,"Image /mnt overriding my machine's /mnt causing errors; I ran this using singularity. I tried to tell the system to read and write files to a folder in my machine's /mnt/ folder. I keep getting an error. After inspecting, it looks like this image has an empty /mnt/ directory that is not writable. This is a problem for us and many users because it is very common to store large amounts of data in the /mnt/ folder on servers that access shared space from a common storage device. Please tell your Dockerfile to ""RUN rm -rf /mnt/"" or something (I'm not a docker expert by any means). The deepvariant docker container clearly does not need /mnt/. **Setup**. - Centos 7. - deepvariant 1.3.0. - Singularity run pulling from here: docker://google/deepvariant:""1.3.0"". - quickstart example. **Steps to reproduce:**. ...please note that /mnt/share is an NFS mount. My server mounts a drive on another machine running nfs.service . mkdir -p /mnt/share/jasontest. cd /mnt/share/jasontest. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. BIN_VERSION=""1.3.0"". OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". singularity",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530
https://github.com/google/deepvariant/issues/530:197,usability,error,error,197,"Image /mnt overriding my machine's /mnt causing errors; I ran this using singularity. I tried to tell the system to read and write files to a folder in my machine's /mnt/ folder. I keep getting an error. After inspecting, it looks like this image has an empty /mnt/ directory that is not writable. This is a problem for us and many users because it is very common to store large amounts of data in the /mnt/ folder on servers that access shared space from a common storage device. Please tell your Dockerfile to ""RUN rm -rf /mnt/"" or something (I'm not a docker expert by any means). The deepvariant docker container clearly does not need /mnt/. **Setup**. - Centos 7. - deepvariant 1.3.0. - Singularity run pulling from here: docker://google/deepvariant:""1.3.0"". - quickstart example. **Steps to reproduce:**. ...please note that /mnt/share is an NFS mount. My server mounts a drive on another machine running nfs.service . mkdir -p /mnt/share/jasontest. cd /mnt/share/jasontest. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. BIN_VERSION=""1.3.0"". OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". singularity",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530
https://github.com/google/deepvariant/issues/530:332,usability,user,users,332,"Image /mnt overriding my machine's /mnt causing errors; I ran this using singularity. I tried to tell the system to read and write files to a folder in my machine's /mnt/ folder. I keep getting an error. After inspecting, it looks like this image has an empty /mnt/ directory that is not writable. This is a problem for us and many users because it is very common to store large amounts of data in the /mnt/ folder on servers that access shared space from a common storage device. Please tell your Dockerfile to ""RUN rm -rf /mnt/"" or something (I'm not a docker expert by any means). The deepvariant docker container clearly does not need /mnt/. **Setup**. - Centos 7. - deepvariant 1.3.0. - Singularity run pulling from here: docker://google/deepvariant:""1.3.0"". - quickstart example. **Steps to reproduce:**. ...please note that /mnt/share is an NFS mount. My server mounts a drive on another machine running nfs.service . mkdir -p /mnt/share/jasontest. cd /mnt/share/jasontest. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. BIN_VERSION=""1.3.0"". OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". singularity",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530
https://github.com/google/deepvariant/issues/530:617,usability,clear,clearly,617,"Image /mnt overriding my machine's /mnt causing errors; I ran this using singularity. I tried to tell the system to read and write files to a folder in my machine's /mnt/ folder. I keep getting an error. After inspecting, it looks like this image has an empty /mnt/ directory that is not writable. This is a problem for us and many users because it is very common to store large amounts of data in the /mnt/ folder on servers that access shared space from a common storage device. Please tell your Dockerfile to ""RUN rm -rf /mnt/"" or something (I'm not a docker expert by any means). The deepvariant docker container clearly does not need /mnt/. **Setup**. - Centos 7. - deepvariant 1.3.0. - Singularity run pulling from here: docker://google/deepvariant:""1.3.0"". - quickstart example. **Steps to reproduce:**. ...please note that /mnt/share is an NFS mount. My server mounts a drive on another machine running nfs.service . mkdir -p /mnt/share/jasontest. cd /mnt/share/jasontest. INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. BIN_VERSION=""1.3.0"". OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". singularity",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530
https://github.com/google/deepvariant/issues/531:393,energy efficiency,reduc,reduce,393,"Documentation on quality scores; Hi,. I have been looking for documentation about the quality scores and how to interpret those. I am working on WGS data for human samples derived from patients affected by rare inherited disorders. I am looking to establish a filtering strategy on the raw VCF data, specifically for `RefCall` variants. I assume that simply getting rid of all those calls may reduce sensitivity. Therefore, I would like to figure out a quality score to use to filter these variants to maximize both sensitivity and specificity. Any suggestion?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/531
https://github.com/google/deepvariant/issues/531:260,integrability,filter,filtering,260,"Documentation on quality scores; Hi,. I have been looking for documentation about the quality scores and how to interpret those. I am working on WGS data for human samples derived from patients affected by rare inherited disorders. I am looking to establish a filtering strategy on the raw VCF data, specifically for `RefCall` variants. I assume that simply getting rid of all those calls may reduce sensitivity. Therefore, I would like to figure out a quality score to use to filter these variants to maximize both sensitivity and specificity. Any suggestion?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/531
https://github.com/google/deepvariant/issues/531:477,integrability,filter,filter,477,"Documentation on quality scores; Hi,. I have been looking for documentation about the quality scores and how to interpret those. I am working on WGS data for human samples derived from patients affected by rare inherited disorders. I am looking to establish a filtering strategy on the raw VCF data, specifically for `RefCall` variants. I assume that simply getting rid of all those calls may reduce sensitivity. Therefore, I would like to figure out a quality score to use to filter these variants to maximize both sensitivity and specificity. Any suggestion?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/531
https://github.com/google/deepvariant/issues/531:300,interoperability,specif,specifically,300,"Documentation on quality scores; Hi,. I have been looking for documentation about the quality scores and how to interpret those. I am working on WGS data for human samples derived from patients affected by rare inherited disorders. I am looking to establish a filtering strategy on the raw VCF data, specifically for `RefCall` variants. I assume that simply getting rid of all those calls may reduce sensitivity. Therefore, I would like to figure out a quality score to use to filter these variants to maximize both sensitivity and specificity. Any suggestion?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/531
https://github.com/google/deepvariant/issues/531:532,interoperability,specif,specificity,532,"Documentation on quality scores; Hi,. I have been looking for documentation about the quality scores and how to interpret those. I am working on WGS data for human samples derived from patients affected by rare inherited disorders. I am looking to establish a filtering strategy on the raw VCF data, specifically for `RefCall` variants. I assume that simply getting rid of all those calls may reduce sensitivity. Therefore, I would like to figure out a quality score to use to filter these variants to maximize both sensitivity and specificity. Any suggestion?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/531
https://github.com/google/deepvariant/issues/531:211,modifiability,inherit,inherited,211,"Documentation on quality scores; Hi,. I have been looking for documentation about the quality scores and how to interpret those. I am working on WGS data for human samples derived from patients affected by rare inherited disorders. I am looking to establish a filtering strategy on the raw VCF data, specifically for `RefCall` variants. I assume that simply getting rid of all those calls may reduce sensitivity. Therefore, I would like to figure out a quality score to use to filter these variants to maximize both sensitivity and specificity. Any suggestion?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/531
https://github.com/google/deepvariant/issues/531:351,testability,simpl,simply,351,"Documentation on quality scores; Hi,. I have been looking for documentation about the quality scores and how to interpret those. I am working on WGS data for human samples derived from patients affected by rare inherited disorders. I am looking to establish a filtering strategy on the raw VCF data, specifically for `RefCall` variants. I assume that simply getting rid of all those calls may reduce sensitivity. Therefore, I would like to figure out a quality score to use to filter these variants to maximize both sensitivity and specificity. Any suggestion?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/531
https://github.com/google/deepvariant/issues/531:0,usability,Document,Documentation,0,"Documentation on quality scores; Hi,. I have been looking for documentation about the quality scores and how to interpret those. I am working on WGS data for human samples derived from patients affected by rare inherited disorders. I am looking to establish a filtering strategy on the raw VCF data, specifically for `RefCall` variants. I assume that simply getting rid of all those calls may reduce sensitivity. Therefore, I would like to figure out a quality score to use to filter these variants to maximize both sensitivity and specificity. Any suggestion?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/531
https://github.com/google/deepvariant/issues/531:62,usability,document,documentation,62,"Documentation on quality scores; Hi,. I have been looking for documentation about the quality scores and how to interpret those. I am working on WGS data for human samples derived from patients affected by rare inherited disorders. I am looking to establish a filtering strategy on the raw VCF data, specifically for `RefCall` variants. I assume that simply getting rid of all those calls may reduce sensitivity. Therefore, I would like to figure out a quality score to use to filter these variants to maximize both sensitivity and specificity. Any suggestion?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/531
https://github.com/google/deepvariant/issues/531:351,usability,simpl,simply,351,"Documentation on quality scores; Hi,. I have been looking for documentation about the quality scores and how to interpret those. I am working on WGS data for human samples derived from patients affected by rare inherited disorders. I am looking to establish a filtering strategy on the raw VCF data, specifically for `RefCall` variants. I assume that simply getting rid of all those calls may reduce sensitivity. Therefore, I would like to figure out a quality score to use to filter these variants to maximize both sensitivity and specificity. Any suggestion?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/531
https://github.com/google/deepvariant/issues/532:332,availability,Operat,Operating,332,"Training non-human model of deep-trio ; **Describe the issue:**. Hi! I am trying to use deep-trio to call variants of drosophila (PACBIO data). I have noticed you have provide guides for training CNN model of deep variant, but I have no idea of training model of deep trio. Can I train a drosophila model of deep trio? **Setup**. - Operating system: Cent OS. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) pacbio sequencing data.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/532
https://github.com/google/deepvariant/issues/532:373,deployability,version,version,373,"Training non-human model of deep-trio ; **Describe the issue:**. Hi! I am trying to use deep-trio to call variants of drosophila (PACBIO data). I have noticed you have provide guides for training CNN model of deep variant, but I have no idea of training model of deep trio. Can I train a drosophila model of deep trio? **Setup**. - Operating system: Cent OS. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) pacbio sequencing data.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/532
https://github.com/google/deepvariant/issues/532:391,deployability,Instal,Installation,391,"Training non-human model of deep-trio ; **Describe the issue:**. Hi! I am trying to use deep-trio to call variants of drosophila (PACBIO data). I have noticed you have provide guides for training CNN model of deep variant, but I have no idea of training model of deep trio. Can I train a drosophila model of deep trio? **Setup**. - Operating system: Cent OS. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) pacbio sequencing data.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/532
https://github.com/google/deepvariant/issues/532:19,energy efficiency,model,model,19,"Training non-human model of deep-trio ; **Describe the issue:**. Hi! I am trying to use deep-trio to call variants of drosophila (PACBIO data). I have noticed you have provide guides for training CNN model of deep variant, but I have no idea of training model of deep trio. Can I train a drosophila model of deep trio? **Setup**. - Operating system: Cent OS. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) pacbio sequencing data.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/532
https://github.com/google/deepvariant/issues/532:200,energy efficiency,model,model,200,"Training non-human model of deep-trio ; **Describe the issue:**. Hi! I am trying to use deep-trio to call variants of drosophila (PACBIO data). I have noticed you have provide guides for training CNN model of deep variant, but I have no idea of training model of deep trio. Can I train a drosophila model of deep trio? **Setup**. - Operating system: Cent OS. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) pacbio sequencing data.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/532
https://github.com/google/deepvariant/issues/532:254,energy efficiency,model,model,254,"Training non-human model of deep-trio ; **Describe the issue:**. Hi! I am trying to use deep-trio to call variants of drosophila (PACBIO data). I have noticed you have provide guides for training CNN model of deep variant, but I have no idea of training model of deep trio. Can I train a drosophila model of deep trio? **Setup**. - Operating system: Cent OS. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) pacbio sequencing data.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/532
https://github.com/google/deepvariant/issues/532:299,energy efficiency,model,model,299,"Training non-human model of deep-trio ; **Describe the issue:**. Hi! I am trying to use deep-trio to call variants of drosophila (PACBIO data). I have noticed you have provide guides for training CNN model of deep variant, but I have no idea of training model of deep trio. Can I train a drosophila model of deep trio? **Setup**. - Operating system: Cent OS. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) pacbio sequencing data.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/532
https://github.com/google/deepvariant/issues/532:373,integrability,version,version,373,"Training non-human model of deep-trio ; **Describe the issue:**. Hi! I am trying to use deep-trio to call variants of drosophila (PACBIO data). I have noticed you have provide guides for training CNN model of deep variant, but I have no idea of training model of deep trio. Can I train a drosophila model of deep trio? **Setup**. - Operating system: Cent OS. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) pacbio sequencing data.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/532
https://github.com/google/deepvariant/issues/532:130,modifiability,PAC,PACBIO,130,"Training non-human model of deep-trio ; **Describe the issue:**. Hi! I am trying to use deep-trio to call variants of drosophila (PACBIO data). I have noticed you have provide guides for training CNN model of deep variant, but I have no idea of training model of deep trio. Can I train a drosophila model of deep trio? **Setup**. - Operating system: Cent OS. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) pacbio sequencing data.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/532
https://github.com/google/deepvariant/issues/532:373,modifiability,version,version,373,"Training non-human model of deep-trio ; **Describe the issue:**. Hi! I am trying to use deep-trio to call variants of drosophila (PACBIO data). I have noticed you have provide guides for training CNN model of deep variant, but I have no idea of training model of deep trio. Can I train a drosophila model of deep trio? **Setup**. - Operating system: Cent OS. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) pacbio sequencing data.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/532
https://github.com/google/deepvariant/issues/532:568,modifiability,pac,pacbio,568,"Training non-human model of deep-trio ; **Describe the issue:**. Hi! I am trying to use deep-trio to call variants of drosophila (PACBIO data). I have noticed you have provide guides for training CNN model of deep variant, but I have no idea of training model of deep trio. Can I train a drosophila model of deep trio? **Setup**. - Operating system: Cent OS. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) pacbio sequencing data.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/532
https://github.com/google/deepvariant/issues/532:19,security,model,model,19,"Training non-human model of deep-trio ; **Describe the issue:**. Hi! I am trying to use deep-trio to call variants of drosophila (PACBIO data). I have noticed you have provide guides for training CNN model of deep variant, but I have no idea of training model of deep trio. Can I train a drosophila model of deep trio? **Setup**. - Operating system: Cent OS. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) pacbio sequencing data.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/532
https://github.com/google/deepvariant/issues/532:200,security,model,model,200,"Training non-human model of deep-trio ; **Describe the issue:**. Hi! I am trying to use deep-trio to call variants of drosophila (PACBIO data). I have noticed you have provide guides for training CNN model of deep variant, but I have no idea of training model of deep trio. Can I train a drosophila model of deep trio? **Setup**. - Operating system: Cent OS. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) pacbio sequencing data.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/532
https://github.com/google/deepvariant/issues/532:254,security,model,model,254,"Training non-human model of deep-trio ; **Describe the issue:**. Hi! I am trying to use deep-trio to call variants of drosophila (PACBIO data). I have noticed you have provide guides for training CNN model of deep variant, but I have no idea of training model of deep trio. Can I train a drosophila model of deep trio? **Setup**. - Operating system: Cent OS. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) pacbio sequencing data.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/532
https://github.com/google/deepvariant/issues/532:299,security,model,model,299,"Training non-human model of deep-trio ; **Describe the issue:**. Hi! I am trying to use deep-trio to call variants of drosophila (PACBIO data). I have noticed you have provide guides for training CNN model of deep variant, but I have no idea of training model of deep trio. Can I train a drosophila model of deep trio? **Setup**. - Operating system: Cent OS. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) pacbio sequencing data.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/532
https://github.com/google/deepvariant/issues/532:487,testability,instrument,instrument,487,"Training non-human model of deep-trio ; **Describe the issue:**. Hi! I am trying to use deep-trio to call variants of drosophila (PACBIO data). I have noticed you have provide guides for training CNN model of deep variant, but I have no idea of training model of deep trio. Can I train a drosophila model of deep trio? **Setup**. - Operating system: Cent OS. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) pacbio sequencing data.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/532
https://github.com/google/deepvariant/issues/532:176,usability,guid,guides,176,"Training non-human model of deep-trio ; **Describe the issue:**. Hi! I am trying to use deep-trio to call variants of drosophila (PACBIO data). I have noticed you have provide guides for training CNN model of deep variant, but I have no idea of training model of deep trio. Can I train a drosophila model of deep trio? **Setup**. - Operating system: Cent OS. - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) pacbio sequencing data.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/532
https://github.com/google/deepvariant/issues/533:28,availability,error,error,28,"Singularity tempfile/TMPDIR error: Tutorials fail; **Describe the issue:**. I am following the tutorial for [PacBio HiFi data](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md). When I reach the step for calling singularity to `run_deepvariant` ([this step](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments)), I receive an error which appears to be associated with the tempfile/TMPDIR path:. Command: . ```. BIN_VERSION=""1.3.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20. ```. **Error 1**. ```. INFO: Using cached SIF image. I0403 10:34:56.987876 23171167450944 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:453,availability,error,error,453,"Singularity tempfile/TMPDIR error: Tutorials fail; **Describe the issue:**. I am following the tutorial for [PacBio HiFi data](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md). When I reach the step for calling singularity to `run_deepvariant` ([this step](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments)), I receive an error which appears to be associated with the tempfile/TMPDIR path:. Command: . ```. BIN_VERSION=""1.3.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20. ```. **Error 1**. ```. INFO: Using cached SIF image. I0403 10:34:56.987876 23171167450944 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:932,availability,Error,Error,932,"Singularity tempfile/TMPDIR error: Tutorials fail; **Describe the issue:**. I am following the tutorial for [PacBio HiFi data](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md). When I reach the step for calling singularity to `run_deepvariant` ([this step](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments)), I receive an error which appears to be associated with the tempfile/TMPDIR path:. Command: . ```. BIN_VERSION=""1.3.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20. ```. **Error 1**. ```. INFO: Using cached SIF image. I0403 10:34:56.987876 23171167450944 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:1748,availability,Error,Error,1748,"GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20. ```. **Error 1**. ```. INFO: Using cached SIF image. I0403 10:34:56.987876 23171167450944 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only to receive a different error stating that it cannot find any of the files that are downloaded in the previous steps of the tutorial. . **Error 2**. ```. INFO: Using cached SIF image. I0404 16:29:50.730109 22987118802752 run_deepvariant.py:345] Re-using the directory for intermediate results in ./tmpkj84jstw. ***** Intermediate results will be written to ./tmpkj84jstw in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""./tmpkj84jstw/make_examples.tfrecord@16.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_au",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:1990,availability,error,error,1990,"987876 23171167450944 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only to receive a different error stating that it cannot find any of the files that are downloaded in the previous steps of the tutorial. . **Error 2**. ```. INFO: Using cached SIF image. I0404 16:29:50.730109 22987118802752 run_deepvariant.py:345] Re-using the directory for intermediate results in ./tmpkj84jstw. ***** Intermediate results will be written to ./tmpkj84jstw in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""./tmpkj84jstw/make_examples.tfrecord@16.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. [E::hts_open_format] Failed to open file ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" : No such file or ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:2024,availability,error,error,2024,"ant.py:345] Re-using the directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only to receive a different error stating that it cannot find any of the files that are downloaded in the previous steps of the tutorial. . **Error 2**. ```. INFO: Using cached SIF image. I0404 16:29:50.730109 22987118802752 run_deepvariant.py:345] Re-using the directory for intermediate results in ./tmpkj84jstw. ***** Intermediate results will be written to ./tmpkj84jstw in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""./tmpkj84jstw/make_examples.tfrecord@16.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. [E::hts_open_format] Failed to open file ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" : No such file or directory. Traceback (most recent ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:2084,availability,down,downloaded,2084," /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only to receive a different error stating that it cannot find any of the files that are downloaded in the previous steps of the tutorial. . **Error 2**. ```. INFO: Using cached SIF image. I0404 16:29:50.730109 22987118802752 run_deepvariant.py:345] Re-using the directory for intermediate results in ./tmpkj84jstw. ***** Intermediate results will be written to ./tmpkj84jstw in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""./tmpkj84jstw/make_examples.tfrecord@16.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. [E::hts_open_format] Failed to open file ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" : No such file or directory. Traceback (most recent call last):. File ""./Bazel.runfiles_bpldxvlm/runfiles/com_goog",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:2138,availability,Error,Error,2138,"e written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only to receive a different error stating that it cannot find any of the files that are downloaded in the previous steps of the tutorial. . **Error 2**. ```. INFO: Using cached SIF image. I0404 16:29:50.730109 22987118802752 run_deepvariant.py:345] Re-using the directory for intermediate results in ./tmpkj84jstw. ***** Intermediate results will be written to ./tmpkj84jstw in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""./tmpkj84jstw/make_examples.tfrecord@16.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. [E::hts_open_format] Failed to open file ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" : No such file or directory. Traceback (most recent call last):. File ""./Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:4829,availability,ERROR,ERROR,4829,"unfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 131, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open input/HG003.GRCh38.chr20.pFDA_truthv2.bam. ...REPEAT ABOVE ERROR {NPROC} TIMES... parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref reference/GRCh38_no_alt_analysis_set.fasta --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --examples ./tmpkj84jstw/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --pileup_image_width 199 --norealign_reads --regions chr20 --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 12. real	0m4.843s. user	0m3.036s. sys	0m0.866s. ```. **Setup**. - Operating system: CentOS Linux release 8.2.2004 (Core). - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity/Docker. - Type of data: [Tutorial Data]((https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md)). **Does the quick start test work on your system?**. The same error occurs in the quick start test with `Error in tempfile() using template...` as above.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:5346,availability,Operat,Operating,5346,"unfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 131, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open input/HG003.GRCh38.chr20.pFDA_truthv2.bam. ...REPEAT ABOVE ERROR {NPROC} TIMES... parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref reference/GRCh38_no_alt_analysis_set.fasta --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --examples ./tmpkj84jstw/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --pileup_image_width 199 --norealign_reads --regions chr20 --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 12. real	0m4.843s. user	0m3.036s. sys	0m0.866s. ```. **Setup**. - Operating system: CentOS Linux release 8.2.2004 (Core). - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity/Docker. - Type of data: [Tutorial Data]((https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md)). **Does the quick start test work on your system?**. The same error occurs in the quick start test with `Error in tempfile() using template...` as above.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:5698,availability,error,error,5698,"unfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 131, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open input/HG003.GRCh38.chr20.pFDA_truthv2.bam. ...REPEAT ABOVE ERROR {NPROC} TIMES... parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref reference/GRCh38_no_alt_analysis_set.fasta --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --examples ./tmpkj84jstw/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --pileup_image_width 199 --norealign_reads --regions chr20 --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 12. real	0m4.843s. user	0m3.036s. sys	0m0.866s. ```. **Setup**. - Operating system: CentOS Linux release 8.2.2004 (Core). - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity/Docker. - Type of data: [Tutorial Data]((https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md)). **Does the quick start test work on your system?**. The same error occurs in the quick start test with `Error in tempfile() using template...` as above.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:5741,availability,Error,Error,5741,"unfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 131, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open input/HG003.GRCh38.chr20.pFDA_truthv2.bam. ...REPEAT ABOVE ERROR {NPROC} TIMES... parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref reference/GRCh38_no_alt_analysis_set.fasta --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --examples ./tmpkj84jstw/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --pileup_image_width 199 --norealign_reads --regions chr20 --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 12. real	0m4.843s. user	0m3.036s. sys	0m0.866s. ```. **Setup**. - Operating system: CentOS Linux release 8.2.2004 (Core). - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity/Docker. - Type of data: [Tutorial Data]((https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md)). **Does the quick start test work on your system?**. The same error occurs in the quick start test with `Error in tempfile() using template...` as above.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:45,deployability,fail,fail,45,"Singularity tempfile/TMPDIR error: Tutorials fail; **Describe the issue:**. I am following the tutorial for [PacBio HiFi data](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md). When I reach the step for calling singularity to `run_deepvariant` ([this step](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments)), I receive an error which appears to be associated with the tempfile/TMPDIR path:. Command: . ```. BIN_VERSION=""1.3.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20. ```. **Error 1**. ```. INFO: Using cached SIF image. I0403 10:34:56.987876 23171167450944 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:2911,deployability,Fail,Failed,2911,"llel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only to receive a different error stating that it cannot find any of the files that are downloaded in the previous steps of the tutorial. . **Error 2**. ```. INFO: Using cached SIF image. I0404 16:29:50.730109 22987118802752 run_deepvariant.py:345] Re-using the directory for intermediate results in ./tmpkj84jstw. ***** Intermediate results will be written to ./tmpkj84jstw in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""./tmpkj84jstw/make_examples.tfrecord@16.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. [E::hts_open_format] Failed to open file ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" : No such file or directory. Traceback (most recent call last):. File ""./Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""./Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 166, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""./Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 128, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""./Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_nam",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:3149,deployability,modul,module,3149,". INFO: Using cached SIF image. I0404 16:29:50.730109 22987118802752 run_deepvariant.py:345] Re-using the directory for intermediate results in ./tmpkj84jstw. ***** Intermediate results will be written to ./tmpkj84jstw in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""./tmpkj84jstw/make_examples.tfrecord@16.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. [E::hts_open_format] Failed to open file ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" : No such file or directory. Traceback (most recent call last):. File ""./Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""./Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 166, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""./Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 128, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""./Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 131, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0])",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:4871,deployability,fail,failed,4871,"unfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 131, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open input/HG003.GRCh38.chr20.pFDA_truthv2.bam. ...REPEAT ABOVE ERROR {NPROC} TIMES... parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref reference/GRCh38_no_alt_analysis_set.fasta --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --examples ./tmpkj84jstw/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --pileup_image_width 199 --norealign_reads --regions chr20 --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 12. real	0m4.843s. user	0m3.036s. sys	0m0.866s. ```. **Setup**. - Operating system: CentOS Linux release 8.2.2004 (Core). - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity/Docker. - Type of data: [Tutorial Data]((https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md)). **Does the quick start test work on your system?**. The same error occurs in the quick start test with `Error in tempfile() using template...` as above.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:5377,deployability,releas,release,5377,"unfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 131, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open input/HG003.GRCh38.chr20.pFDA_truthv2.bam. ...REPEAT ABOVE ERROR {NPROC} TIMES... parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref reference/GRCh38_no_alt_analysis_set.fasta --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --examples ./tmpkj84jstw/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --pileup_image_width 199 --norealign_reads --regions chr20 --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 12. real	0m4.843s. user	0m3.036s. sys	0m0.866s. ```. **Setup**. - Operating system: CentOS Linux release 8.2.2004 (Core). - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity/Docker. - Type of data: [Tutorial Data]((https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md)). **Does the quick start test work on your system?**. The same error occurs in the quick start test with `Error in tempfile() using template...` as above.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:5416,deployability,version,version,5416,"unfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 131, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open input/HG003.GRCh38.chr20.pFDA_truthv2.bam. ...REPEAT ABOVE ERROR {NPROC} TIMES... parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref reference/GRCh38_no_alt_analysis_set.fasta --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --examples ./tmpkj84jstw/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --pileup_image_width 199 --norealign_reads --regions chr20 --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 12. real	0m4.843s. user	0m3.036s. sys	0m0.866s. ```. **Setup**. - Operating system: CentOS Linux release 8.2.2004 (Core). - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity/Docker. - Type of data: [Tutorial Data]((https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md)). **Does the quick start test work on your system?**. The same error occurs in the quick start test with `Error in tempfile() using template...` as above.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:5434,deployability,Instal,Installation,5434,"unfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 131, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open input/HG003.GRCh38.chr20.pFDA_truthv2.bam. ...REPEAT ABOVE ERROR {NPROC} TIMES... parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref reference/GRCh38_no_alt_analysis_set.fasta --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --examples ./tmpkj84jstw/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --pileup_image_width 199 --norealign_reads --regions chr20 --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 12. real	0m4.843s. user	0m3.036s. sys	0m0.866s. ```. **Setup**. - Operating system: CentOS Linux release 8.2.2004 (Core). - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity/Docker. - Type of data: [Tutorial Data]((https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md)). **Does the quick start test work on your system?**. The same error occurs in the quick start test with `Error in tempfile() using template...` as above.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:199,energy efficiency,model,model-case-study,199,"Singularity tempfile/TMPDIR error: Tutorials fail; **Describe the issue:**. I am following the tutorial for [PacBio HiFi data](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md). When I reach the step for calling singularity to `run_deepvariant` ([this step](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments)), I receive an error which appears to be associated with the tempfile/TMPDIR path:. Command: . ```. BIN_VERSION=""1.3.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20. ```. **Error 1**. ```. INFO: Using cached SIF image. I0403 10:34:56.987876 23171167450944 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:373,energy efficiency,model,model-case-study,373,"Singularity tempfile/TMPDIR error: Tutorials fail; **Describe the issue:**. I am following the tutorial for [PacBio HiFi data](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md). When I reach the step for calling singularity to `run_deepvariant` ([this step](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments)), I receive an error which appears to be associated with the tempfile/TMPDIR path:. Command: . ```. BIN_VERSION=""1.3.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20. ```. **Error 1**. ```. INFO: Using cached SIF image. I0403 10:34:56.987876 23171167450944 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:5395,energy efficiency,Core,Core,5395,"unfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 131, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open input/HG003.GRCh38.chr20.pFDA_truthv2.bam. ...REPEAT ABOVE ERROR {NPROC} TIMES... parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref reference/GRCh38_no_alt_analysis_set.fasta --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --examples ./tmpkj84jstw/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --pileup_image_width 199 --norealign_reads --regions chr20 --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 12. real	0m4.843s. user	0m3.036s. sys	0m0.866s. ```. **Setup**. - Operating system: CentOS Linux release 8.2.2004 (Core). - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity/Docker. - Type of data: [Tutorial Data]((https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md)). **Does the quick start test work on your system?**. The same error occurs in the quick start test with `Error in tempfile() using template...` as above.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:5614,energy efficiency,model,model-case-study,5614,"unfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 131, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open input/HG003.GRCh38.chr20.pFDA_truthv2.bam. ...REPEAT ABOVE ERROR {NPROC} TIMES... parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref reference/GRCh38_no_alt_analysis_set.fasta --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --examples ./tmpkj84jstw/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --pileup_image_width 199 --norealign_reads --regions chr20 --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 12. real	0m4.843s. user	0m3.036s. sys	0m0.866s. ```. **Setup**. - Operating system: CentOS Linux release 8.2.2004 (Core). - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity/Docker. - Type of data: [Tutorial Data]((https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md)). **Does the quick start test work on your system?**. The same error occurs in the quick start test with `Error in tempfile() using template...` as above.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:1265,integrability,buffer,buffer,1265,"o `run_deepvariant` ([this step](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments)), I receive an error which appears to be associated with the tempfile/TMPDIR path:. Command: . ```. BIN_VERSION=""1.3.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20. ```. **Error 1**. ```. INFO: Using cached SIF image. I0403 10:34:56.987876 23171167450944 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only to receive a different error stating that it cannot find any of the files that are downloaded in the previous steps of the tutorial. . **Error 2**. ```. INFO: Using cached SIF image. I0404 16:29:50.730109 22987118802752 run_deepvariant.py:345] Re-using the directory ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:2465,integrability,buffer,buffer,2465,".tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only to receive a different error stating that it cannot find any of the files that are downloaded in the previous steps of the tutorial. . **Error 2**. ```. INFO: Using cached SIF image. I0404 16:29:50.730109 22987118802752 run_deepvariant.py:345] Re-using the directory for intermediate results in ./tmpkj84jstw. ***** Intermediate results will be written to ./tmpkj84jstw in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""./tmpkj84jstw/make_examples.tfrecord@16.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. [E::hts_open_format] Failed to open file ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" : No such file or directory. Traceback (most recent call last):. File ""./Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""./Bazel.runfiles_bpldxvlm/runfiles/com_google_deepva",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:5416,integrability,version,version,5416,"unfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 131, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open input/HG003.GRCh38.chr20.pFDA_truthv2.bam. ...REPEAT ABOVE ERROR {NPROC} TIMES... parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref reference/GRCh38_no_alt_analysis_set.fasta --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --examples ./tmpkj84jstw/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --pileup_image_width 199 --norealign_reads --regions chr20 --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 12. real	0m4.843s. user	0m3.036s. sys	0m0.866s. ```. **Setup**. - Operating system: CentOS Linux release 8.2.2004 (Core). - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity/Docker. - Type of data: [Tutorial Data]((https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md)). **Does the quick start test work on your system?**. The same error occurs in the quick start test with `Error in tempfile() using template...` as above.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:601,interoperability,bind,bind,601,"Singularity tempfile/TMPDIR error: Tutorials fail; **Describe the issue:**. I am following the tutorial for [PacBio HiFi data](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md). When I reach the step for calling singularity to `run_deepvariant` ([this step](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments)), I receive an error which appears to be associated with the tempfile/TMPDIR path:. Command: . ```. BIN_VERSION=""1.3.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20. ```. **Error 1**. ```. INFO: Using cached SIF image. I0403 10:34:56.987876 23171167450944 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:109,modifiability,Pac,PacBio,109,"Singularity tempfile/TMPDIR error: Tutorials fail; **Describe the issue:**. I am following the tutorial for [PacBio HiFi data](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md). When I reach the step for calling singularity to `run_deepvariant` ([this step](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments)), I receive an error which appears to be associated with the tempfile/TMPDIR path:. Command: . ```. BIN_VERSION=""1.3.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20. ```. **Error 1**. ```. INFO: Using cached SIF image. I0403 10:34:56.987876 23171167450944 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:192,modifiability,pac,pacbio-model-case-study,192,"Singularity tempfile/TMPDIR error: Tutorials fail; **Describe the issue:**. I am following the tutorial for [PacBio HiFi data](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md). When I reach the step for calling singularity to `run_deepvariant` ([this step](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments)), I receive an error which appears to be associated with the tempfile/TMPDIR path:. Command: . ```. BIN_VERSION=""1.3.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20. ```. **Error 1**. ```. INFO: Using cached SIF image. I0403 10:34:56.987876 23171167450944 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:366,modifiability,pac,pacbio-model-case-study,366,"Singularity tempfile/TMPDIR error: Tutorials fail; **Describe the issue:**. I am following the tutorial for [PacBio HiFi data](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md). When I reach the step for calling singularity to `run_deepvariant` ([this step](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments)), I receive an error which appears to be associated with the tempfile/TMPDIR path:. Command: . ```. BIN_VERSION=""1.3.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20. ```. **Error 1**. ```. INFO: Using cached SIF image. I0403 10:34:56.987876 23171167450944 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:601,modifiability,bind,bind,601,"Singularity tempfile/TMPDIR error: Tutorials fail; **Describe the issue:**. I am following the tutorial for [PacBio HiFi data](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md). When I reach the step for calling singularity to `run_deepvariant` ([this step](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments)), I receive an error which appears to be associated with the tempfile/TMPDIR path:. Command: . ```. BIN_VERSION=""1.3.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20. ```. **Error 1**. ```. INFO: Using cached SIF image. I0403 10:34:56.987876 23171167450944 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:725,modifiability,PAC,PACBIO,725,"Singularity tempfile/TMPDIR error: Tutorials fail; **Describe the issue:**. I am following the tutorial for [PacBio HiFi data](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md). When I reach the step for calling singularity to `run_deepvariant` ([this step](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments)), I receive an error which appears to be associated with the tempfile/TMPDIR path:. Command: . ```. BIN_VERSION=""1.3.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20. ```. **Error 1**. ```. INFO: Using cached SIF image. I0403 10:34:56.987876 23171167450944 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:1066,modifiability,interm,intermediate,1066,"**. I am following the tutorial for [PacBio HiFi data](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md). When I reach the step for calling singularity to `run_deepvariant` ([this step](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments)), I receive an error which appears to be associated with the tempfile/TMPDIR path:. Command: . ```. BIN_VERSION=""1.3.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20. ```. **Error 1**. ```. INFO: Using cached SIF image. I0403 10:34:56.987876 23171167450944 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only to receive a different error stating that it cannot find any of the fil",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:1114,modifiability,Interm,Intermediate,1114," data](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md). When I reach the step for calling singularity to `run_deepvariant` ([this step](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments)), I receive an error which appears to be associated with the tempfile/TMPDIR path:. Command: . ```. BIN_VERSION=""1.3.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20. ```. **Error 1**. ```. INFO: Using cached SIF image. I0403 10:34:56.987876 23171167450944 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only to receive a different error stating that it cannot find any of the files that are downloaded in the previous steps of ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:2272,modifiability,interm,intermediate,2272,"eepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only to receive a different error stating that it cannot find any of the files that are downloaded in the previous steps of the tutorial. . **Error 2**. ```. INFO: Using cached SIF image. I0404 16:29:50.730109 22987118802752 run_deepvariant.py:345] Re-using the directory for intermediate results in ./tmpkj84jstw. ***** Intermediate results will be written to ./tmpkj84jstw in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""./tmpkj84jstw/make_examples.tfrecord@16.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. [E::hts_open_format] Failed to open file ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" : No such file or directory. Traceback (most recent call last):. File ""./Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(ma",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:2317,modifiability,Interm,Intermediate,2317,"-ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only to receive a different error stating that it cannot find any of the files that are downloaded in the previous steps of the tutorial. . **Error 2**. ```. INFO: Using cached SIF image. I0404 16:29:50.730109 22987118802752 run_deepvariant.py:345] Re-using the directory for intermediate results in ./tmpkj84jstw. ***** Intermediate results will be written to ./tmpkj84jstw in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""./tmpkj84jstw/make_examples.tfrecord@16.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. [E::hts_open_format] Failed to open file ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" : No such file or directory. Traceback (most recent call last):. File ""./Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/home/rrautsa/Bazel.runfiles",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:3149,modifiability,modul,module,3149,". INFO: Using cached SIF image. I0404 16:29:50.730109 22987118802752 run_deepvariant.py:345] Re-using the directory for intermediate results in ./tmpkj84jstw. ***** Intermediate results will be written to ./tmpkj84jstw in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""./tmpkj84jstw/make_examples.tfrecord@16.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. [E::hts_open_format] Failed to open file ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" : No such file or directory. Traceback (most recent call last):. File ""./Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""./Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 166, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""./Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 128, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""./Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 131, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0])",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:5416,modifiability,version,version,5416,"unfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 131, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open input/HG003.GRCh38.chr20.pFDA_truthv2.bam. ...REPEAT ABOVE ERROR {NPROC} TIMES... parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref reference/GRCh38_no_alt_analysis_set.fasta --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --examples ./tmpkj84jstw/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --pileup_image_width 199 --norealign_reads --regions chr20 --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 12. real	0m4.843s. user	0m3.036s. sys	0m0.866s. ```. **Setup**. - Operating system: CentOS Linux release 8.2.2004 (Core). - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity/Docker. - Type of data: [Tutorial Data]((https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md)). **Does the quick start test work on your system?**. The same error occurs in the quick start test with `Error in tempfile() using template...` as above.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:5607,modifiability,pac,pacbio-model-case-study,5607,"unfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 131, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open input/HG003.GRCh38.chr20.pFDA_truthv2.bam. ...REPEAT ABOVE ERROR {NPROC} TIMES... parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref reference/GRCh38_no_alt_analysis_set.fasta --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --examples ./tmpkj84jstw/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --pileup_image_width 199 --norealign_reads --regions chr20 --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 12. real	0m4.843s. user	0m3.036s. sys	0m0.866s. ```. **Setup**. - Operating system: CentOS Linux release 8.2.2004 (Core). - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity/Docker. - Type of data: [Tutorial Data]((https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md)). **Does the quick start test work on your system?**. The same error occurs in the quick start test with `Error in tempfile() using template...` as above.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:28,performance,error,error,28,"Singularity tempfile/TMPDIR error: Tutorials fail; **Describe the issue:**. I am following the tutorial for [PacBio HiFi data](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md). When I reach the step for calling singularity to `run_deepvariant` ([this step](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments)), I receive an error which appears to be associated with the tempfile/TMPDIR path:. Command: . ```. BIN_VERSION=""1.3.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20. ```. **Error 1**. ```. INFO: Using cached SIF image. I0403 10:34:56.987876 23171167450944 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:453,performance,error,error,453,"Singularity tempfile/TMPDIR error: Tutorials fail; **Describe the issue:**. I am following the tutorial for [PacBio HiFi data](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md). When I reach the step for calling singularity to `run_deepvariant` ([this step](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments)), I receive an error which appears to be associated with the tempfile/TMPDIR path:. Command: . ```. BIN_VERSION=""1.3.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20. ```. **Error 1**. ```. INFO: Using cached SIF image. I0403 10:34:56.987876 23171167450944 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:932,performance,Error,Error,932,"Singularity tempfile/TMPDIR error: Tutorials fail; **Describe the issue:**. I am following the tutorial for [PacBio HiFi data](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md). When I reach the step for calling singularity to `run_deepvariant` ([this step](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments)), I receive an error which appears to be associated with the tempfile/TMPDIR path:. Command: . ```. BIN_VERSION=""1.3.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20. ```. **Error 1**. ```. INFO: Using cached SIF image. I0403 10:34:56.987876 23171167450944 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:960,performance,cach,cached,960,"Singularity tempfile/TMPDIR error: Tutorials fail; **Describe the issue:**. I am following the tutorial for [PacBio HiFi data](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md). When I reach the step for calling singularity to `run_deepvariant` ([this step](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments)), I receive an error which appears to be associated with the tempfile/TMPDIR path:. Command: . ```. BIN_VERSION=""1.3.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20. ```. **Error 1**. ```. INFO: Using cached SIF image. I0403 10:34:56.987876 23171167450944 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:1221,performance,time,time,1221,"en I reach the step for calling singularity to `run_deepvariant` ([this step](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments)), I receive an error which appears to be associated with the tempfile/TMPDIR path:. Command: . ```. BIN_VERSION=""1.3.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20. ```. **Error 1**. ```. INFO: Using cached SIF image. I0403 10:34:56.987876 23171167450944 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only to receive a different error stating that it cannot find any of the files that are downloaded in the previous steps of the tutorial. . **Error 2**. ```. INFO: Using cached SIF image. I0404 16:29:50.730109 22987118802752 ru",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:1237,performance,parallel,parallel,1237,"p for calling singularity to `run_deepvariant` ([this step](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments)), I receive an error which appears to be associated with the tempfile/TMPDIR path:. Command: . ```. BIN_VERSION=""1.3.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20. ```. **Error 1**. ```. INFO: Using cached SIF image. I0403 10:34:56.987876 23171167450944 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only to receive a different error stating that it cannot find any of the files that are downloaded in the previous steps of the tutorial. . **Error 2**. ```. INFO: Using cached SIF image. I0404 16:29:50.730109 22987118802752 run_deepvariant.py:3",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:1748,performance,Error,Error,1748,"GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20. ```. **Error 1**. ```. INFO: Using cached SIF image. I0403 10:34:56.987876 23171167450944 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only to receive a different error stating that it cannot find any of the files that are downloaded in the previous steps of the tutorial. . **Error 2**. ```. INFO: Using cached SIF image. I0404 16:29:50.730109 22987118802752 run_deepvariant.py:345] Re-using the directory for intermediate results in ./tmpkj84jstw. ***** Intermediate results will be written to ./tmpkj84jstw in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""./tmpkj84jstw/make_examples.tfrecord@16.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_au",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:1910,performance,parallel,parallel,1910,"ons chr20. ```. **Error 1**. ```. INFO: Using cached SIF image. I0403 10:34:56.987876 23171167450944 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only to receive a different error stating that it cannot find any of the files that are downloaded in the previous steps of the tutorial. . **Error 2**. ```. INFO: Using cached SIF image. I0404 16:29:50.730109 22987118802752 run_deepvariant.py:345] Re-using the directory for intermediate results in ./tmpkj84jstw. ***** Intermediate results will be written to ./tmpkj84jstw in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""./tmpkj84jstw/make_examples.tfrecord@16.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. [E::hts_open_format] Fai",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:1990,performance,error,error,1990,"987876 23171167450944 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only to receive a different error stating that it cannot find any of the files that are downloaded in the previous steps of the tutorial. . **Error 2**. ```. INFO: Using cached SIF image. I0404 16:29:50.730109 22987118802752 run_deepvariant.py:345] Re-using the directory for intermediate results in ./tmpkj84jstw. ***** Intermediate results will be written to ./tmpkj84jstw in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""./tmpkj84jstw/make_examples.tfrecord@16.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. [E::hts_open_format] Failed to open file ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" : No such file or ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:2024,performance,error,error,2024,"ant.py:345] Re-using the directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only to receive a different error stating that it cannot find any of the files that are downloaded in the previous steps of the tutorial. . **Error 2**. ```. INFO: Using cached SIF image. I0404 16:29:50.730109 22987118802752 run_deepvariant.py:345] Re-using the directory for intermediate results in ./tmpkj84jstw. ***** Intermediate results will be written to ./tmpkj84jstw in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""./tmpkj84jstw/make_examples.tfrecord@16.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. [E::hts_open_format] Failed to open file ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" : No such file or directory. Traceback (most recent ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:2138,performance,Error,Error,2138,"e written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only to receive a different error stating that it cannot find any of the files that are downloaded in the previous steps of the tutorial. . **Error 2**. ```. INFO: Using cached SIF image. I0404 16:29:50.730109 22987118802752 run_deepvariant.py:345] Re-using the directory for intermediate results in ./tmpkj84jstw. ***** Intermediate results will be written to ./tmpkj84jstw in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""./tmpkj84jstw/make_examples.tfrecord@16.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. [E::hts_open_format] Failed to open file ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" : No such file or directory. Traceback (most recent call last):. File ""./Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:2166,performance,cach,cached,2166,"h in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only to receive a different error stating that it cannot find any of the files that are downloaded in the previous steps of the tutorial. . **Error 2**. ```. INFO: Using cached SIF image. I0404 16:29:50.730109 22987118802752 run_deepvariant.py:345] Re-using the directory for intermediate results in ./tmpkj84jstw. ***** Intermediate results will be written to ./tmpkj84jstw in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""./tmpkj84jstw/make_examples.tfrecord@16.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. [E::hts_open_format] Failed to open file ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" : No such file or directory. Traceback (most recent call last):. File ""./Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(mai",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:2421,performance,time,time,2421,""" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only to receive a different error stating that it cannot find any of the files that are downloaded in the previous steps of the tutorial. . **Error 2**. ```. INFO: Using cached SIF image. I0404 16:29:50.730109 22987118802752 run_deepvariant.py:345] Re-using the directory for intermediate results in ./tmpkj84jstw. ***** Intermediate results will be written to ./tmpkj84jstw in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""./tmpkj84jstw/make_examples.tfrecord@16.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. [E::hts_open_format] Failed to open file ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" : No such file or directory. Traceback (most recent call last):. File ""./Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""./Bazel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:2437,performance,parallel,parallel,2437,"p/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only to receive a different error stating that it cannot find any of the files that are downloaded in the previous steps of the tutorial. . **Error 2**. ```. INFO: Using cached SIF image. I0404 16:29:50.730109 22987118802752 run_deepvariant.py:345] Re-using the directory for intermediate results in ./tmpkj84jstw. ***** Intermediate results will be written to ./tmpkj84jstw in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""./tmpkj84jstw/make_examples.tfrecord@16.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. [E::hts_open_format] Failed to open file ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" : No such file or directory. Traceback (most recent call last):. File ""./Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""./Bazel.runfiles_bpldxvlm",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:4829,performance,ERROR,ERROR,4829,"unfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 131, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open input/HG003.GRCh38.chr20.pFDA_truthv2.bam. ...REPEAT ABOVE ERROR {NPROC} TIMES... parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref reference/GRCh38_no_alt_analysis_set.fasta --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --examples ./tmpkj84jstw/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --pileup_image_width 199 --norealign_reads --regions chr20 --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 12. real	0m4.843s. user	0m3.036s. sys	0m0.866s. ```. **Setup**. - Operating system: CentOS Linux release 8.2.2004 (Core). - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity/Docker. - Type of data: [Tutorial Data]((https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md)). **Does the quick start test work on your system?**. The same error occurs in the quick start test with `Error in tempfile() using template...` as above.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:4843,performance,TIME,TIMES,4843,"unfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 131, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open input/HG003.GRCh38.chr20.pFDA_truthv2.bam. ...REPEAT ABOVE ERROR {NPROC} TIMES... parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref reference/GRCh38_no_alt_analysis_set.fasta --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --examples ./tmpkj84jstw/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --pileup_image_width 199 --norealign_reads --regions chr20 --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 12. real	0m4.843s. user	0m3.036s. sys	0m0.866s. ```. **Setup**. - Operating system: CentOS Linux release 8.2.2004 (Core). - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity/Docker. - Type of data: [Tutorial Data]((https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md)). **Does the quick start test work on your system?**. The same error occurs in the quick start test with `Error in tempfile() using template...` as above.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:4852,performance,parallel,parallel,4852,"unfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 131, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open input/HG003.GRCh38.chr20.pFDA_truthv2.bam. ...REPEAT ABOVE ERROR {NPROC} TIMES... parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref reference/GRCh38_no_alt_analysis_set.fasta --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --examples ./tmpkj84jstw/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --pileup_image_width 199 --norealign_reads --regions chr20 --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 12. real	0m4.843s. user	0m3.036s. sys	0m0.866s. ```. **Setup**. - Operating system: CentOS Linux release 8.2.2004 (Core). - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity/Docker. - Type of data: [Tutorial Data]((https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md)). **Does the quick start test work on your system?**. The same error occurs in the quick start test with `Error in tempfile() using template...` as above.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:5698,performance,error,error,5698,"unfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 131, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open input/HG003.GRCh38.chr20.pFDA_truthv2.bam. ...REPEAT ABOVE ERROR {NPROC} TIMES... parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref reference/GRCh38_no_alt_analysis_set.fasta --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --examples ./tmpkj84jstw/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --pileup_image_width 199 --norealign_reads --regions chr20 --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 12. real	0m4.843s. user	0m3.036s. sys	0m0.866s. ```. **Setup**. - Operating system: CentOS Linux release 8.2.2004 (Core). - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity/Docker. - Type of data: [Tutorial Data]((https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md)). **Does the quick start test work on your system?**. The same error occurs in the quick start test with `Error in tempfile() using template...` as above.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:5741,performance,Error,Error,5741,"unfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 131, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open input/HG003.GRCh38.chr20.pFDA_truthv2.bam. ...REPEAT ABOVE ERROR {NPROC} TIMES... parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref reference/GRCh38_no_alt_analysis_set.fasta --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --examples ./tmpkj84jstw/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --pileup_image_width 199 --norealign_reads --regions chr20 --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 12. real	0m4.843s. user	0m3.036s. sys	0m0.866s. ```. **Setup**. - Operating system: CentOS Linux release 8.2.2004 (Core). - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity/Docker. - Type of data: [Tutorial Data]((https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md)). **Does the quick start test work on your system?**. The same error occurs in the quick start test with `Error in tempfile() using template...` as above.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:45,reliability,fail,fail,45,"Singularity tempfile/TMPDIR error: Tutorials fail; **Describe the issue:**. I am following the tutorial for [PacBio HiFi data](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md). When I reach the step for calling singularity to `run_deepvariant` ([this step](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments)), I receive an error which appears to be associated with the tempfile/TMPDIR path:. Command: . ```. BIN_VERSION=""1.3.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20. ```. **Error 1**. ```. INFO: Using cached SIF image. I0403 10:34:56.987876 23171167450944 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:1883,reliability,doe,does,1883,"num_shards $(nproc) \. --regions chr20. ```. **Error 1**. ```. INFO: Using cached SIF image. I0403 10:34:56.987876 23171167450944 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only to receive a different error stating that it cannot find any of the files that are downloaded in the previous steps of the tutorial. . **Error 2**. ```. INFO: Using cached SIF image. I0404 16:29:50.730109 22987118802752 run_deepvariant.py:345] Re-using the directory for intermediate results in ./tmpkj84jstw. ***** Intermediate results will be written to ./tmpkj84jstw in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""./tmpkj84jstw/make_examples.tfrecord@16.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:2911,reliability,Fail,Failed,2911,"llel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only to receive a different error stating that it cannot find any of the files that are downloaded in the previous steps of the tutorial. . **Error 2**. ```. INFO: Using cached SIF image. I0404 16:29:50.730109 22987118802752 run_deepvariant.py:345] Re-using the directory for intermediate results in ./tmpkj84jstw. ***** Intermediate results will be written to ./tmpkj84jstw in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""./tmpkj84jstw/make_examples.tfrecord@16.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. [E::hts_open_format] Failed to open file ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" : No such file or directory. Traceback (most recent call last):. File ""./Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""./Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 166, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""./Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 128, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""./Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_nam",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:4871,reliability,fail,failed,4871,"unfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 131, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open input/HG003.GRCh38.chr20.pFDA_truthv2.bam. ...REPEAT ABOVE ERROR {NPROC} TIMES... parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref reference/GRCh38_no_alt_analysis_set.fasta --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --examples ./tmpkj84jstw/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --pileup_image_width 199 --norealign_reads --regions chr20 --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 12. real	0m4.843s. user	0m3.036s. sys	0m0.866s. ```. **Setup**. - Operating system: CentOS Linux release 8.2.2004 (Core). - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity/Docker. - Type of data: [Tutorial Data]((https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md)). **Does the quick start test work on your system?**. The same error occurs in the quick start test with `Error in tempfile() using template...` as above.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:5639,reliability,Doe,Does,5639,"unfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 131, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open input/HG003.GRCh38.chr20.pFDA_truthv2.bam. ...REPEAT ABOVE ERROR {NPROC} TIMES... parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref reference/GRCh38_no_alt_analysis_set.fasta --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --examples ./tmpkj84jstw/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --pileup_image_width 199 --norealign_reads --regions chr20 --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 12. real	0m4.843s. user	0m3.036s. sys	0m0.866s. ```. **Setup**. - Operating system: CentOS Linux release 8.2.2004 (Core). - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity/Docker. - Type of data: [Tutorial Data]((https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md)). **Does the quick start test work on your system?**. The same error occurs in the quick start test with `Error in tempfile() using template...` as above.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:28,safety,error,error,28,"Singularity tempfile/TMPDIR error: Tutorials fail; **Describe the issue:**. I am following the tutorial for [PacBio HiFi data](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md). When I reach the step for calling singularity to `run_deepvariant` ([this step](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments)), I receive an error which appears to be associated with the tempfile/TMPDIR path:. Command: . ```. BIN_VERSION=""1.3.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20. ```. **Error 1**. ```. INFO: Using cached SIF image. I0403 10:34:56.987876 23171167450944 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:453,safety,error,error,453,"Singularity tempfile/TMPDIR error: Tutorials fail; **Describe the issue:**. I am following the tutorial for [PacBio HiFi data](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md). When I reach the step for calling singularity to `run_deepvariant` ([this step](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments)), I receive an error which appears to be associated with the tempfile/TMPDIR path:. Command: . ```. BIN_VERSION=""1.3.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20. ```. **Error 1**. ```. INFO: Using cached SIF image. I0403 10:34:56.987876 23171167450944 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:795,safety,input,input,795,"Singularity tempfile/TMPDIR error: Tutorials fail; **Describe the issue:**. I am following the tutorial for [PacBio HiFi data](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md). When I reach the step for calling singularity to `run_deepvariant` ([this step](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments)), I receive an error which appears to be associated with the tempfile/TMPDIR path:. Command: . ```. BIN_VERSION=""1.3.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20. ```. **Error 1**. ```. INFO: Using cached SIF image. I0403 10:34:56.987876 23171167450944 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:932,safety,Error,Error,932,"Singularity tempfile/TMPDIR error: Tutorials fail; **Describe the issue:**. I am following the tutorial for [PacBio HiFi data](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md). When I reach the step for calling singularity to `run_deepvariant` ([this step](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments)), I receive an error which appears to be associated with the tempfile/TMPDIR path:. Command: . ```. BIN_VERSION=""1.3.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20. ```. **Error 1**. ```. INFO: Using cached SIF image. I0403 10:34:56.987876 23171167450944 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:1382,safety,input,input,1382,"tudy.md#run-deepvariant-on-chromosome-20-alignments)), I receive an error which appears to be associated with the tempfile/TMPDIR path:. Command: . ```. BIN_VERSION=""1.3.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20. ```. **Error 1**. ```. INFO: Using cached SIF image. I0403 10:34:56.987876 23171167450944 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only to receive a different error stating that it cannot find any of the files that are downloaded in the previous steps of the tutorial. . **Error 2**. ```. INFO: Using cached SIF image. I0404 16:29:50.730109 22987118802752 run_deepvariant.py:345] Re-using the directory for intermediate results in ./tmpkj84jstw. ***** Intermediate results will be written to ./tmpkj84jstw in docker. ***",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:1748,safety,Error,Error,1748,"GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20. ```. **Error 1**. ```. INFO: Using cached SIF image. I0403 10:34:56.987876 23171167450944 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only to receive a different error stating that it cannot find any of the files that are downloaded in the previous steps of the tutorial. . **Error 2**. ```. INFO: Using cached SIF image. I0404 16:29:50.730109 22987118802752 run_deepvariant.py:345] Re-using the directory for intermediate results in ./tmpkj84jstw. ***** Intermediate results will be written to ./tmpkj84jstw in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""./tmpkj84jstw/make_examples.tfrecord@16.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_au",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:1990,safety,error,error,1990,"987876 23171167450944 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only to receive a different error stating that it cannot find any of the files that are downloaded in the previous steps of the tutorial. . **Error 2**. ```. INFO: Using cached SIF image. I0404 16:29:50.730109 22987118802752 run_deepvariant.py:345] Re-using the directory for intermediate results in ./tmpkj84jstw. ***** Intermediate results will be written to ./tmpkj84jstw in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""./tmpkj84jstw/make_examples.tfrecord@16.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. [E::hts_open_format] Failed to open file ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" : No such file or ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:2024,safety,error,error,2024,"ant.py:345] Re-using the directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only to receive a different error stating that it cannot find any of the files that are downloaded in the previous steps of the tutorial. . **Error 2**. ```. INFO: Using cached SIF image. I0404 16:29:50.730109 22987118802752 run_deepvariant.py:345] Re-using the directory for intermediate results in ./tmpkj84jstw. ***** Intermediate results will be written to ./tmpkj84jstw in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""./tmpkj84jstw/make_examples.tfrecord@16.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. [E::hts_open_format] Failed to open file ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" : No such file or directory. Traceback (most recent ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:2138,safety,Error,Error,2138,"e written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only to receive a different error stating that it cannot find any of the files that are downloaded in the previous steps of the tutorial. . **Error 2**. ```. INFO: Using cached SIF image. I0404 16:29:50.730109 22987118802752 run_deepvariant.py:345] Re-using the directory for intermediate results in ./tmpkj84jstw. ***** Intermediate results will be written to ./tmpkj84jstw in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""./tmpkj84jstw/make_examples.tfrecord@16.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. [E::hts_open_format] Failed to open file ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" : No such file or directory. Traceback (most recent call last):. File ""./Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:2582,safety,input,input,2582,"annels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only to receive a different error stating that it cannot find any of the files that are downloaded in the previous steps of the tutorial. . **Error 2**. ```. INFO: Using cached SIF image. I0404 16:29:50.730109 22987118802752 run_deepvariant.py:345] Re-using the directory for intermediate results in ./tmpkj84jstw. ***** Intermediate results will be written to ./tmpkj84jstw in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""./tmpkj84jstw/make_examples.tfrecord@16.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. [E::hts_open_format] Failed to open file ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" : No such file or directory. Traceback (most recent call last):. File ""./Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""./Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 166, in main. options = default_options(add_flags=True, flags_obj=FLAGS). F",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:2932,safety,input,input,2932,"I can set `export TMPDIR = "".""` and this bypasses this error only to receive a different error stating that it cannot find any of the files that are downloaded in the previous steps of the tutorial. . **Error 2**. ```. INFO: Using cached SIF image. I0404 16:29:50.730109 22987118802752 run_deepvariant.py:345] Re-using the directory for intermediate results in ./tmpkj84jstw. ***** Intermediate results will be written to ./tmpkj84jstw in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""./tmpkj84jstw/make_examples.tfrecord@16.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. [E::hts_open_format] Failed to open file ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" : No such file or directory. Traceback (most recent call last):. File ""./Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""./Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 166, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""./Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 128, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""./Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_cor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:3149,safety,modul,module,3149,". INFO: Using cached SIF image. I0404 16:29:50.730109 22987118802752 run_deepvariant.py:345] Re-using the directory for intermediate results in ./tmpkj84jstw. ***** Intermediate results will be written to ./tmpkj84jstw in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""./tmpkj84jstw/make_examples.tfrecord@16.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. [E::hts_open_format] Failed to open file ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" : No such file or directory. Traceback (most recent call last):. File ""./Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""./Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 166, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""./Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 128, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""./Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 131, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0])",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:4770,safety,input,input,4770," File ""./Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 131, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open input/HG003.GRCh38.chr20.pFDA_truthv2.bam. ...REPEAT ABOVE ERROR {NPROC} TIMES... parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref reference/GRCh38_no_alt_analysis_set.fasta --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --examples ./tmpkj84jstw/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --pileup_image_width 199 --norealign_reads --regions chr20 --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 12. real	0m4.843s. user	0m3.036s. sys	0m0.866s. ```. **Setup**. - Operating system: CentOS Linux release 8.2.2004 (Core). - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity/Docker. - Type of data: [Tutorial Data]((https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md)). **Does the quick start test work on your system?**. The same error occurs in the quick start test with `Error in tempfile() using templa",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:4829,safety,ERROR,ERROR,4829,"unfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 131, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open input/HG003.GRCh38.chr20.pFDA_truthv2.bam. ...REPEAT ABOVE ERROR {NPROC} TIMES... parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref reference/GRCh38_no_alt_analysis_set.fasta --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --examples ./tmpkj84jstw/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --pileup_image_width 199 --norealign_reads --regions chr20 --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 12. real	0m4.843s. user	0m3.036s. sys	0m0.866s. ```. **Setup**. - Operating system: CentOS Linux release 8.2.2004 (Core). - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity/Docker. - Type of data: [Tutorial Data]((https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md)). **Does the quick start test work on your system?**. The same error occurs in the quick start test with `Error in tempfile() using template...` as above.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:4987,safety,input,input,4987,"unfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 131, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open input/HG003.GRCh38.chr20.pFDA_truthv2.bam. ...REPEAT ABOVE ERROR {NPROC} TIMES... parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref reference/GRCh38_no_alt_analysis_set.fasta --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --examples ./tmpkj84jstw/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --pileup_image_width 199 --norealign_reads --regions chr20 --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 12. real	0m4.843s. user	0m3.036s. sys	0m0.866s. ```. **Setup**. - Operating system: CentOS Linux release 8.2.2004 (Core). - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity/Docker. - Type of data: [Tutorial Data]((https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md)). **Does the quick start test work on your system?**. The same error occurs in the quick start test with `Error in tempfile() using template...` as above.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:5660,safety,test,test,5660,"unfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 131, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open input/HG003.GRCh38.chr20.pFDA_truthv2.bam. ...REPEAT ABOVE ERROR {NPROC} TIMES... parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref reference/GRCh38_no_alt_analysis_set.fasta --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --examples ./tmpkj84jstw/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --pileup_image_width 199 --norealign_reads --regions chr20 --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 12. real	0m4.843s. user	0m3.036s. sys	0m0.866s. ```. **Setup**. - Operating system: CentOS Linux release 8.2.2004 (Core). - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity/Docker. - Type of data: [Tutorial Data]((https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md)). **Does the quick start test work on your system?**. The same error occurs in the quick start test with `Error in tempfile() using template...` as above.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:5698,safety,error,error,5698,"unfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 131, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open input/HG003.GRCh38.chr20.pFDA_truthv2.bam. ...REPEAT ABOVE ERROR {NPROC} TIMES... parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref reference/GRCh38_no_alt_analysis_set.fasta --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --examples ./tmpkj84jstw/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --pileup_image_width 199 --norealign_reads --regions chr20 --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 12. real	0m4.843s. user	0m3.036s. sys	0m0.866s. ```. **Setup**. - Operating system: CentOS Linux release 8.2.2004 (Core). - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity/Docker. - Type of data: [Tutorial Data]((https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md)). **Does the quick start test work on your system?**. The same error occurs in the quick start test with `Error in tempfile() using template...` as above.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:5730,safety,test,test,5730,"unfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 131, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open input/HG003.GRCh38.chr20.pFDA_truthv2.bam. ...REPEAT ABOVE ERROR {NPROC} TIMES... parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref reference/GRCh38_no_alt_analysis_set.fasta --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --examples ./tmpkj84jstw/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --pileup_image_width 199 --norealign_reads --regions chr20 --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 12. real	0m4.843s. user	0m3.036s. sys	0m0.866s. ```. **Setup**. - Operating system: CentOS Linux release 8.2.2004 (Core). - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity/Docker. - Type of data: [Tutorial Data]((https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md)). **Does the quick start test work on your system?**. The same error occurs in the quick start test with `Error in tempfile() using template...` as above.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:5741,safety,Error,Error,5741,"unfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 131, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open input/HG003.GRCh38.chr20.pFDA_truthv2.bam. ...REPEAT ABOVE ERROR {NPROC} TIMES... parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref reference/GRCh38_no_alt_analysis_set.fasta --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --examples ./tmpkj84jstw/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --pileup_image_width 199 --norealign_reads --regions chr20 --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 12. real	0m4.843s. user	0m3.036s. sys	0m0.866s. ```. **Setup**. - Operating system: CentOS Linux release 8.2.2004 (Core). - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity/Docker. - Type of data: [Tutorial Data]((https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md)). **Does the quick start test work on your system?**. The same error occurs in the quick start test with `Error in tempfile() using template...` as above.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:199,security,model,model-case-study,199,"Singularity tempfile/TMPDIR error: Tutorials fail; **Describe the issue:**. I am following the tutorial for [PacBio HiFi data](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md). When I reach the step for calling singularity to `run_deepvariant` ([this step](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments)), I receive an error which appears to be associated with the tempfile/TMPDIR path:. Command: . ```. BIN_VERSION=""1.3.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20. ```. **Error 1**. ```. INFO: Using cached SIF image. I0403 10:34:56.987876 23171167450944 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:373,security,model,model-case-study,373,"Singularity tempfile/TMPDIR error: Tutorials fail; **Describe the issue:**. I am following the tutorial for [PacBio HiFi data](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md). When I reach the step for calling singularity to `run_deepvariant` ([this step](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments)), I receive an error which appears to be associated with the tempfile/TMPDIR path:. Command: . ```. BIN_VERSION=""1.3.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20. ```. **Error 1**. ```. INFO: Using cached SIF image. I0403 10:34:56.987876 23171167450944 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:5614,security,model,model-case-study,5614,"unfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 131, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open input/HG003.GRCh38.chr20.pFDA_truthv2.bam. ...REPEAT ABOVE ERROR {NPROC} TIMES... parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref reference/GRCh38_no_alt_analysis_set.fasta --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --examples ./tmpkj84jstw/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --pileup_image_width 199 --norealign_reads --regions chr20 --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 12. real	0m4.843s. user	0m3.036s. sys	0m0.866s. ```. **Setup**. - Operating system: CentOS Linux release 8.2.2004 (Core). - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity/Docker. - Type of data: [Tutorial Data]((https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md)). **Does the quick start test work on your system?**. The same error occurs in the quick start test with `Error in tempfile() using template...` as above.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:3004,testability,Trace,Traceback,3004,"ve a different error stating that it cannot find any of the files that are downloaded in the previous steps of the tutorial. . **Error 2**. ```. INFO: Using cached SIF image. I0404 16:29:50.730109 22987118802752 run_deepvariant.py:345] Re-using the directory for intermediate results in ./tmpkj84jstw. ***** Intermediate results will be written to ./tmpkj84jstw in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""./tmpkj84jstw/make_examples.tfrecord@16.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. [E::hts_open_format] Failed to open file ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" : No such file or directory. Traceback (most recent call last):. File ""./Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""./Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 166, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""./Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 128, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""./Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfile",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:5660,testability,test,test,5660,"unfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 131, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open input/HG003.GRCh38.chr20.pFDA_truthv2.bam. ...REPEAT ABOVE ERROR {NPROC} TIMES... parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref reference/GRCh38_no_alt_analysis_set.fasta --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --examples ./tmpkj84jstw/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --pileup_image_width 199 --norealign_reads --regions chr20 --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 12. real	0m4.843s. user	0m3.036s. sys	0m0.866s. ```. **Setup**. - Operating system: CentOS Linux release 8.2.2004 (Core). - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity/Docker. - Type of data: [Tutorial Data]((https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md)). **Does the quick start test work on your system?**. The same error occurs in the quick start test with `Error in tempfile() using template...` as above.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:5730,testability,test,test,5730,"unfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 131, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open input/HG003.GRCh38.chr20.pFDA_truthv2.bam. ...REPEAT ABOVE ERROR {NPROC} TIMES... parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref reference/GRCh38_no_alt_analysis_set.fasta --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --examples ./tmpkj84jstw/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --pileup_image_width 199 --norealign_reads --regions chr20 --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 12. real	0m4.843s. user	0m3.036s. sys	0m0.866s. ```. **Setup**. - Operating system: CentOS Linux release 8.2.2004 (Core). - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity/Docker. - Type of data: [Tutorial Data]((https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md)). **Does the quick start test work on your system?**. The same error occurs in the quick start test with `Error in tempfile() using template...` as above.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:28,usability,error,error,28,"Singularity tempfile/TMPDIR error: Tutorials fail; **Describe the issue:**. I am following the tutorial for [PacBio HiFi data](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md). When I reach the step for calling singularity to `run_deepvariant` ([this step](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments)), I receive an error which appears to be associated with the tempfile/TMPDIR path:. Command: . ```. BIN_VERSION=""1.3.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20. ```. **Error 1**. ```. INFO: Using cached SIF image. I0403 10:34:56.987876 23171167450944 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:453,usability,error,error,453,"Singularity tempfile/TMPDIR error: Tutorials fail; **Describe the issue:**. I am following the tutorial for [PacBio HiFi data](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md). When I reach the step for calling singularity to `run_deepvariant` ([this step](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments)), I receive an error which appears to be associated with the tempfile/TMPDIR path:. Command: . ```. BIN_VERSION=""1.3.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20. ```. **Error 1**. ```. INFO: Using cached SIF image. I0403 10:34:56.987876 23171167450944 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:522,usability,Command,Command,522,"Singularity tempfile/TMPDIR error: Tutorials fail; **Describe the issue:**. I am following the tutorial for [PacBio HiFi data](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md). When I reach the step for calling singularity to `run_deepvariant` ([this step](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments)), I receive an error which appears to be associated with the tempfile/TMPDIR path:. Command: . ```. BIN_VERSION=""1.3.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20. ```. **Error 1**. ```. INFO: Using cached SIF image. I0403 10:34:56.987876 23171167450944 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:795,usability,input,input,795,"Singularity tempfile/TMPDIR error: Tutorials fail; **Describe the issue:**. I am following the tutorial for [PacBio HiFi data](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md). When I reach the step for calling singularity to `run_deepvariant` ([this step](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments)), I receive an error which appears to be associated with the tempfile/TMPDIR path:. Command: . ```. BIN_VERSION=""1.3.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20. ```. **Error 1**. ```. INFO: Using cached SIF image. I0403 10:34:56.987876 23171167450944 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:932,usability,Error,Error,932,"Singularity tempfile/TMPDIR error: Tutorials fail; **Describe the issue:**. I am following the tutorial for [PacBio HiFi data](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md). When I reach the step for calling singularity to `run_deepvariant` ([this step](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments)), I receive an error which appears to be associated with the tempfile/TMPDIR path:. Command: . ```. BIN_VERSION=""1.3.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20. ```. **Error 1**. ```. INFO: Using cached SIF image. I0403 10:34:56.987876 23171167450944 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:1206,usability,command,command,1206,"study.md). When I reach the step for calling singularity to `run_deepvariant` ([this step](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments)), I receive an error which appears to be associated with the tempfile/TMPDIR path:. Command: . ```. BIN_VERSION=""1.3.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20. ```. **Error 1**. ```. INFO: Using cached SIF image. I0403 10:34:56.987876 23171167450944 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only to receive a different error stating that it cannot find any of the files that are downloaded in the previous steps of the tutorial. . **Error 2**. ```. INFO: Using cached SIF image. I0404 16:29:50.730109 2298",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:1382,usability,input,input,1382,"tudy.md#run-deepvariant-on-chromosome-20-alignments)), I receive an error which appears to be associated with the tempfile/TMPDIR path:. Command: . ```. BIN_VERSION=""1.3.0"". mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref reference/GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20. ```. **Error 1**. ```. INFO: Using cached SIF image. I0403 10:34:56.987876 23171167450944 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only to receive a different error stating that it cannot find any of the files that are downloaded in the previous steps of the tutorial. . **Error 2**. ```. INFO: Using cached SIF image. I0404 16:29:50.730109 22987118802752 run_deepvariant.py:345] Re-using the directory for intermediate results in ./tmpkj84jstw. ***** Intermediate results will be written to ./tmpkj84jstw in docker. ***",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:1748,usability,Error,Error,1748,"GRCh38_no_alt_analysis_set.fasta \. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \. --output_vcf deepvariant1/output.vcf.gz \. --num_shards $(nproc) \. --regions chr20. ```. **Error 1**. ```. INFO: Using cached SIF image. I0403 10:34:56.987876 23171167450944 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only to receive a different error stating that it cannot find any of the files that are downloaded in the previous steps of the tutorial. . **Error 2**. ```. INFO: Using cached SIF image. I0404 16:29:50.730109 22987118802752 run_deepvariant.py:345] Re-using the directory for intermediate results in ./tmpkj84jstw. ***** Intermediate results will be written to ./tmpkj84jstw in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""./tmpkj84jstw/make_examples.tfrecord@16.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_au",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:1990,usability,error,error,1990,"987876 23171167450944 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only to receive a different error stating that it cannot find any of the files that are downloaded in the previous steps of the tutorial. . **Error 2**. ```. INFO: Using cached SIF image. I0404 16:29:50.730109 22987118802752 run_deepvariant.py:345] Re-using the directory for intermediate results in ./tmpkj84jstw. ***** Intermediate results will be written to ./tmpkj84jstw in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""./tmpkj84jstw/make_examples.tfrecord@16.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. [E::hts_open_format] Failed to open file ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" : No such file or ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:2024,usability,error,error,2024,"ant.py:345] Re-using the directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only to receive a different error stating that it cannot find any of the files that are downloaded in the previous steps of the tutorial. . **Error 2**. ```. INFO: Using cached SIF image. I0404 16:29:50.730109 22987118802752 run_deepvariant.py:345] Re-using the directory for intermediate results in ./tmpkj84jstw. ***** Intermediate results will be written to ./tmpkj84jstw in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""./tmpkj84jstw/make_examples.tfrecord@16.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. [E::hts_open_format] Failed to open file ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" : No such file or directory. Traceback (most recent ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:2138,usability,Error,Error,2138,"e written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only to receive a different error stating that it cannot find any of the files that are downloaded in the previous steps of the tutorial. . **Error 2**. ```. INFO: Using cached SIF image. I0404 16:29:50.730109 22987118802752 run_deepvariant.py:345] Re-using the directory for intermediate results in ./tmpkj84jstw. ***** Intermediate results will be written to ./tmpkj84jstw in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""./tmpkj84jstw/make_examples.tfrecord@16.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. [E::hts_open_format] Failed to open file ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" : No such file or directory. Traceback (most recent call last):. File ""./Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:2406,usability,command,command,2406,"A_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only to receive a different error stating that it cannot find any of the files that are downloaded in the previous steps of the tutorial. . **Error 2**. ```. INFO: Using cached SIF image. I0404 16:29:50.730109 22987118802752 run_deepvariant.py:345] Re-using the directory for intermediate results in ./tmpkj84jstw. ***** Intermediate results will be written to ./tmpkj84jstw in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""./tmpkj84jstw/make_examples.tfrecord@16.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. [E::hts_open_format] Failed to open file ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" : No such file or directory. Traceback (most recent call last):. File ""./Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:2582,usability,input,input,2582,"annels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889. ```. I can set `export TMPDIR = "".""` and this bypasses this error only to receive a different error stating that it cannot find any of the files that are downloaded in the previous steps of the tutorial. . **Error 2**. ```. INFO: Using cached SIF image. I0404 16:29:50.730109 22987118802752 run_deepvariant.py:345] Re-using the directory for intermediate results in ./tmpkj84jstw. ***** Intermediate results will be written to ./tmpkj84jstw in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""./tmpkj84jstw/make_examples.tfrecord@16.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. [E::hts_open_format] Failed to open file ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" : No such file or directory. Traceback (most recent call last):. File ""./Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""./Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 166, in main. options = default_options(add_flags=True, flags_obj=FLAGS). F",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:2932,usability,input,input,2932,"I can set `export TMPDIR = "".""` and this bypasses this error only to receive a different error stating that it cannot find any of the files that are downloaded in the previous steps of the tutorial. . **Error 2**. ```. INFO: Using cached SIF image. I0404 16:29:50.730109 22987118802752 run_deepvariant.py:345] Re-using the directory for intermediate results in ./tmpkj84jstw. ***** Intermediate results will be written to ./tmpkj84jstw in docker. ****. ***** Running the command:*****. time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""./tmpkj84jstw/make_examples.tfrecord@16.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. [E::hts_open_format] Failed to open file ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" : No such file or directory. Traceback (most recent call last):. File ""./Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""./Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 166, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""./Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 128, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""./Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_cor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:4770,usability,input,input,4770," File ""./Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 131, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open input/HG003.GRCh38.chr20.pFDA_truthv2.bam. ...REPEAT ABOVE ERROR {NPROC} TIMES... parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref reference/GRCh38_no_alt_analysis_set.fasta --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --examples ./tmpkj84jstw/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --pileup_image_width 199 --norealign_reads --regions chr20 --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 12. real	0m4.843s. user	0m3.036s. sys	0m0.866s. ```. **Setup**. - Operating system: CentOS Linux release 8.2.2004 (Core). - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity/Docker. - Type of data: [Tutorial Data]((https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md)). **Does the quick start test work on your system?**. The same error occurs in the quick start test with `Error in tempfile() using templa",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:4829,usability,ERROR,ERROR,4829,"unfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 131, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open input/HG003.GRCh38.chr20.pFDA_truthv2.bam. ...REPEAT ABOVE ERROR {NPROC} TIMES... parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref reference/GRCh38_no_alt_analysis_set.fasta --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --examples ./tmpkj84jstw/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --pileup_image_width 199 --norealign_reads --regions chr20 --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 12. real	0m4.843s. user	0m3.036s. sys	0m0.866s. ```. **Setup**. - Operating system: CentOS Linux release 8.2.2004 (Core). - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity/Docker. - Type of data: [Tutorial Data]((https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md)). **Does the quick start test work on your system?**. The same error occurs in the quick start test with `Error in tempfile() using template...` as above.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:4987,usability,input,input,4987,"unfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 131, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open input/HG003.GRCh38.chr20.pFDA_truthv2.bam. ...REPEAT ABOVE ERROR {NPROC} TIMES... parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref reference/GRCh38_no_alt_analysis_set.fasta --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --examples ./tmpkj84jstw/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --pileup_image_width 199 --norealign_reads --regions chr20 --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 12. real	0m4.843s. user	0m3.036s. sys	0m0.866s. ```. **Setup**. - Operating system: CentOS Linux release 8.2.2004 (Core). - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity/Docker. - Type of data: [Tutorial Data]((https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md)). **Does the quick start test work on your system?**. The same error occurs in the quick start test with `Error in tempfile() using template...` as above.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:5299,usability,user,user,5299,"unfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 131, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open input/HG003.GRCh38.chr20.pFDA_truthv2.bam. ...REPEAT ABOVE ERROR {NPROC} TIMES... parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref reference/GRCh38_no_alt_analysis_set.fasta --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --examples ./tmpkj84jstw/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --pileup_image_width 199 --norealign_reads --regions chr20 --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 12. real	0m4.843s. user	0m3.036s. sys	0m0.866s. ```. **Setup**. - Operating system: CentOS Linux release 8.2.2004 (Core). - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity/Docker. - Type of data: [Tutorial Data]((https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md)). **Does the quick start test work on your system?**. The same error occurs in the quick start test with `Error in tempfile() using template...` as above.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:5698,usability,error,error,5698,"unfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 131, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open input/HG003.GRCh38.chr20.pFDA_truthv2.bam. ...REPEAT ABOVE ERROR {NPROC} TIMES... parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref reference/GRCh38_no_alt_analysis_set.fasta --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --examples ./tmpkj84jstw/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --pileup_image_width 199 --norealign_reads --regions chr20 --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 12. real	0m4.843s. user	0m3.036s. sys	0m0.866s. ```. **Setup**. - Operating system: CentOS Linux release 8.2.2004 (Core). - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity/Docker. - Type of data: [Tutorial Data]((https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md)). **Does the quick start test work on your system?**. The same error occurs in the quick start test with `Error in tempfile() using template...` as above.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/533:5741,usability,Error,Error,5741,"unfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 131, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: Not found: Could not open input/HG003.GRCh38.chr20.pFDA_truthv2.bam. ...REPEAT ABOVE ERROR {NPROC} TIMES... parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref reference/GRCh38_no_alt_analysis_set.fasta --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --examples ./tmpkj84jstw/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --pileup_image_width 199 --norealign_reads --regions chr20 --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 12. real	0m4.843s. user	0m3.036s. sys	0m0.866s. ```. **Setup**. - Operating system: CentOS Linux release 8.2.2004 (Core). - DeepVariant version: 1.3.0. - Installation method (Docker, built from source, etc.): Singularity/Docker. - Type of data: [Tutorial Data]((https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md)). **Does the quick start test work on your system?**. The same error occurs in the quick start test with `Error in tempfile() using template...` as above.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533
https://github.com/google/deepvariant/issues/534:225,deployability,releas,released,225,"Should DV be re-trained with new human T2T reference; **Describe the issue:**. I apologize as this is a question rather than a problem. So this ticket isn't using any predefined template. Here's my question:. given the newly released human T2T reference (v2.0), should DV be re-trained against that reference? I must admit I don't understand DV deep enough to ponder with what the potential benefits would be, so am curious about your thoughts. Thanks! Steve. p.s. the data modes most relevant for us are CCS, ONT.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/534
https://github.com/google/deepvariant/issues/534:331,testability,understand,understand,331,"Should DV be re-trained with new human T2T reference; **Describe the issue:**. I apologize as this is a question rather than a problem. So this ticket isn't using any predefined template. Here's my question:. given the newly released human T2T reference (v2.0), should DV be re-trained against that reference? I must admit I don't understand DV deep enough to ponder with what the potential benefits would be, so am curious about your thoughts. Thanks! Steve. p.s. the data modes most relevant for us are CCS, ONT.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/534
https://github.com/google/deepvariant/issues/535:144,deployability,probe,probes,144,"Type of data; Can I use DeepVariant on NGS (Illumina) panel data(some genes)?panel data such as amplicon data ,target enrichment (hybridization probes) type of data. Thanks! lily",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/535
https://github.com/google/deepvariant/issues/536:262,energy efficiency,measur,measure,262,"representing depth; ##  Questions & Help. Apologies for the naive question: . In [visualizing_examples](https://github.com/google/deepvariant/blob/r0.6/docs/visualizing_examples.ipynb), I get that the reconstructed image has bases on the x-axis and y-axis is a measure of sequencing depth, but why have so many rows empty if none of the examples reach that depth (there's so much space between the last aligned read and the end of the ""image"").",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/536
https://github.com/google/deepvariant/issues/536:37,usability,Help,Help,37,"representing depth; ##  Questions & Help. Apologies for the naive question: . In [visualizing_examples](https://github.com/google/deepvariant/blob/r0.6/docs/visualizing_examples.ipynb), I get that the reconstructed image has bases on the x-axis and y-axis is a measure of sequencing depth, but why have so many rows empty if none of the examples reach that depth (there's so much space between the last aligned read and the end of the ""image"").",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/536
https://github.com/google/deepvariant/issues/537:728,availability,error,error,728,"Issue with TPU Node v3-8 in call variants step; I am trying to use deepvariant to call variants using a TPU Node v3-8, but I am running into a persistent issue. Here is the command I am using:. ```bash. docker run \. -v `pwd`:`pwd` -w `pwd` \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --call_variants_extra_args use_tpu=true,tpu_name=""variantcaller-node1"",tpu_zone=""europe-west4-a"" \. --model_type=WGS \. --ref=""input/data/${REF}"" \. --reads=""input/data/${BAM}"" \. --output_vcf=""output/${OUTPUT_VCF}"" \. --output_gvcf=""output/${OUTPUT_GVCF}"" \. --regions chr20 \. --num_shards=$(nproc) \. --intermediate_results_dir /output/intermediate_results_dir. ```. However, I am seeing the following error in the call variants step. ```bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@96.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --openvino_model_dir ""/output/intermediate_results_dir"" --tpu_name ""variantcaller-node1"" --tpu_zone ""europe-west4-a"" --use_tpu. I0524 21:18:26.485428 140032543119168 transport.py:157] Attempting refresh to obtain initial access_token. I0524 21:18:26.576728 140032543119168 call_variants.py:336] Shape of input examples: [100, 221, 6]. I0524 21:18:26.579230 140032543119168 call_variants.py:361] /opt/models/wgs/model.ckpt.input_shape has the correct shape: [100, 221, 6]. 2022-05-24 21:18:26.581705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-05-24 21:18:26.586196: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:998,availability,checkpoint,checkpoint,998,"ue with TPU Node v3-8 in call variants step; I am trying to use deepvariant to call variants using a TPU Node v3-8, but I am running into a persistent issue. Here is the command I am using:. ```bash. docker run \. -v `pwd`:`pwd` -w `pwd` \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --call_variants_extra_args use_tpu=true,tpu_name=""variantcaller-node1"",tpu_zone=""europe-west4-a"" \. --model_type=WGS \. --ref=""input/data/${REF}"" \. --reads=""input/data/${BAM}"" \. --output_vcf=""output/${OUTPUT_VCF}"" \. --output_gvcf=""output/${OUTPUT_GVCF}"" \. --regions chr20 \. --num_shards=$(nproc) \. --intermediate_results_dir /output/intermediate_results_dir. ```. However, I am seeing the following error in the call variants step. ```bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@96.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --openvino_model_dir ""/output/intermediate_results_dir"" --tpu_name ""variantcaller-node1"" --tpu_zone ""europe-west4-a"" --use_tpu. I0524 21:18:26.485428 140032543119168 transport.py:157] Attempting refresh to obtain initial access_token. I0524 21:18:26.576728 140032543119168 call_variants.py:336] Shape of input examples: [100, 221, 6]. I0524 21:18:26.579230 140032543119168 call_variants.py:361] /opt/models/wgs/model.ckpt.input_shape has the correct shape: [100, 221, 6]. 2022-05-24 21:18:26.581705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-05-24 21:18:26.586196: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tu",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:1738,availability,operat,operations,1738,"ll variants step. ```bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@96.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --openvino_model_dir ""/output/intermediate_results_dir"" --tpu_name ""variantcaller-node1"" --tpu_zone ""europe-west4-a"" --use_tpu. I0524 21:18:26.485428 140032543119168 transport.py:157] Attempting refresh to obtain initial access_token. I0524 21:18:26.576728 140032543119168 call_variants.py:336] Shape of input examples: [100, 221, 6]. I0524 21:18:26.579230 140032543119168 call_variants.py:361] /opt/models/wgs/model.ckpt.input_shape has the correct shape: [100, 221, 6]. 2022-05-24 21:18:26.581705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-05-24 21:18:26.586196: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2022-05-24 21:18:26.587127: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2000160000 Hz. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp_f348kd0. W0524 21:18:26.619681 140032543119168 estimator.py:1846] Using temporary folder as model directory: /tmp/tmp_f348kd0. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp_f348kd0', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_ho",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:1792,availability,operat,operations,1792,"****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@96.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --openvino_model_dir ""/output/intermediate_results_dir"" --tpu_name ""variantcaller-node1"" --tpu_zone ""europe-west4-a"" --use_tpu. I0524 21:18:26.485428 140032543119168 transport.py:157] Attempting refresh to obtain initial access_token. I0524 21:18:26.576728 140032543119168 call_variants.py:336] Shape of input examples: [100, 221, 6]. I0524 21:18:26.579230 140032543119168 call_variants.py:361] /opt/models/wgs/model.ckpt.input_shape has the correct shape: [100, 221, 6]. 2022-05-24 21:18:26.581705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-05-24 21:18:26.586196: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2022-05-24 21:18:26.587127: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2000160000 Hz. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp_f348kd0. W0524 21:18:26.619681 140032543119168 estimator.py:1846] Using temporary folder as model directory: /tmp/tmp_f348kd0. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp_f348kd0', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_di",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:3069,availability,Cluster,ClusterSpec,3069,"1:18:26.587127: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2000160000 Hz. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp_f348kd0. W0524 21:18:26.619681 140032543119168 estimator.py:1846] Using temporary folder as model directory: /tmp/tmp_f348kd0. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp_f348kd0', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.73.74.226:8470', '_evaluation_master': 'grpc://10.73.74.226:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=100, num_shards=None, num_cores_per_replica=None, per_host_input_for_training=2, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1, experimental_allow_per_host_v2_parallel_get_next=False, experimental_feed_hook=None), '_cluster': None}. I0524 21:18:26.620151 140032543119168 estimator.py:191] Using config: {'_model_dir': '/tmp/tmp_f348kd0', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_keep_checkpoint_max': 100000, '",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:4428,availability,Cluster,ClusterSpec,4428,"2, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1, experimental_allow_per_host_v2_parallel_get_next=False, experimental_feed_hook=None), '_cluster': None}. I0524 21:18:26.620151 140032543119168 estimator.py:191] Using config: {'_model_dir': '/tmp/tmp_f348kd0', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.73.74.226:8470', '_evaluation_master': 'grpc://10.73.74.226:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=100, num_shards=None, num_cores_per_replica=None, per_host_input_for_training=2, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1, experimental_allow_per_host_v2_parallel_get_next=False, experimental_feed_hook=None), '_cluster': None}. INFO:tensorflow:_TPUContext: eval_on_tpu True. I0524 21:18:26.620373 140032543119168 tpu_context.py:271] _TPUContext: eval_on_tpu True. I0524 21:18:26.620768 140032543119168 call_variants.py:426] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz. INFO:tensorflow:Querying Tensorflow master (grpc://10.73.74.226:8470) for TPU system metadata. I05",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:6348,availability,Avail,Available,6348,"uerying Tensorflow master (grpc://10.73.74.226:8470) for TPU system metadata. I0524 21:18:26.625535 140032543119168 tpu_system_metadata.py:90] Querying Tensorflow master (grpc://10.73.74.226:8470) for TPU system metadata. 2022-05-24 21:18:26.626490: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:373] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created. INFO:tensorflow:Found TPU system:. I0524 21:18:26.631762 140032543119168 tpu_system_metadata.py:159] Found TPU system:. INFO:tensorflow:*** Num TPU Cores: 8. I0524 21:18:26.631872 140032543119168 tpu_system_metadata.py:160] *** Num TPU Cores: 8. INFO:tensorflow:*** Num TPU Workers: 1. I0524 21:18:26.631940 140032543119168 tpu_system_metadata.py:161] *** Num TPU Workers: 1. INFO:tensorflow:*** Num TPU Cores Per Worker: 8. I0524 21:18:26.631998 140032543119168 tpu_system_metadata.py:162] *** Num TPU Cores Per Worker: 8. INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available D",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:6400,availability,replic,replica,6400,") for TPU system metadata. I0524 21:18:26.625535 140032543119168 tpu_system_metadata.py:90] Querying Tensorflow master (grpc://10.73.74.226:8470) for TPU system metadata. 2022-05-24 21:18:26.626490: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:373] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created. INFO:tensorflow:Found TPU system:. I0524 21:18:26.631762 140032543119168 tpu_system_metadata.py:159] Found TPU system:. INFO:tensorflow:*** Num TPU Cores: 8. I0524 21:18:26.631872 140032543119168 tpu_system_metadata.py:160] *** Num TPU Cores: 8. INFO:tensorflow:*** Num TPU Workers: 1. I0524 21:18:26.631940 140032543119168 tpu_system_metadata.py:161] *** Num TPU Workers: 1. INFO:tensorflow:*** Num TPU Cores Per Worker: 8. I0524 21:18:26.631998 140032543119168 tpu_system_metadata.py:162] *** Num TPU Cores Per Worker: 8. INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:6532,availability,Avail,Available,6532,"74.226:8470) for TPU system metadata. 2022-05-24 21:18:26.626490: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:373] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created. INFO:tensorflow:Found TPU system:. I0524 21:18:26.631762 140032543119168 tpu_system_metadata.py:159] Found TPU system:. INFO:tensorflow:*** Num TPU Cores: 8. I0524 21:18:26.631872 140032543119168 tpu_system_metadata.py:160] *** Num TPU Cores: 8. INFO:tensorflow:*** Num TPU Workers: 1. I0524 21:18:26.631940 140032543119168 tpu_system_metadata.py:161] *** Num TPU Workers: 1. INFO:tensorflow:*** Num TPU Cores Per Worker: 8. I0524 21:18:26.631998 140032543119168 tpu_system_metadata.py:162] *** Num TPU Cores Per Worker: 8. INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** A",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:6584,availability,replic,replica,6584,":18:26.626490: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:373] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created. INFO:tensorflow:Found TPU system:. I0524 21:18:26.631762 140032543119168 tpu_system_metadata.py:159] Found TPU system:. INFO:tensorflow:*** Num TPU Cores: 8. I0524 21:18:26.631872 140032543119168 tpu_system_metadata.py:160] *** Num TPU Cores: 8. INFO:tensorflow:*** Num TPU Workers: 1. I0524 21:18:26.631940 140032543119168 tpu_system_metadata.py:161] *** Num TPU Workers: 1. INFO:tensorflow:*** Num TPU Cores Per Worker: 8. I0524 21:18:26.631998 140032543119168 tpu_system_metadata.py:162] *** Num TPU Cores Per Worker: 8. INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:6666,availability,Avail,Available,6666,"ession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created. INFO:tensorflow:Found TPU system:. I0524 21:18:26.631762 140032543119168 tpu_system_metadata.py:159] Found TPU system:. INFO:tensorflow:*** Num TPU Cores: 8. I0524 21:18:26.631872 140032543119168 tpu_system_metadata.py:160] *** Num TPU Cores: 8. INFO:tensorflow:*** Num TPU Workers: 1. I0524 21:18:26.631940 140032543119168 tpu_system_metadata.py:161] *** Num TPU Workers: 1. INFO:tensorflow:*** Num TPU Cores Per Worker: 8. I0524 21:18:26.631998 140032543119168 tpu_system_metadata.py:162] *** Num TPU Cores Per Worker: 8. INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:6718,availability,replic,replica,6718,"h an empty graph and other defaults because the session has not yet been created. INFO:tensorflow:Found TPU system:. I0524 21:18:26.631762 140032543119168 tpu_system_metadata.py:159] Found TPU system:. INFO:tensorflow:*** Num TPU Cores: 8. I0524 21:18:26.631872 140032543119168 tpu_system_metadata.py:160] *** Num TPU Cores: 8. INFO:tensorflow:*** Num TPU Workers: 1. I0524 21:18:26.631940 140032543119168 tpu_system_metadata.py:161] *** Num TPU Workers: 1. INFO:tensorflow:*** Num TPU Cores Per Worker: 8. I0524 21:18:26.631998 140032543119168 tpu_system_metadata.py:162] *** Num TPU Cores Per Worker: 8. INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:6860,availability,Avail,Available,6860,"32543119168 tpu_system_metadata.py:159] Found TPU system:. INFO:tensorflow:*** Num TPU Cores: 8. I0524 21:18:26.631872 140032543119168 tpu_system_metadata.py:160] *** Num TPU Cores: 8. INFO:tensorflow:*** Num TPU Workers: 1. I0524 21:18:26.631940 140032543119168 tpu_system_metadata.py:161] *** Num TPU Workers: 1. INFO:tensorflow:*** Num TPU Cores Per Worker: 8. I0524 21:18:26.631998 140032543119168 tpu_system_metadata.py:162] *** Num TPU Cores Per Worker: 8. INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:6912,availability,replic,replica,6912,"ystem:. INFO:tensorflow:*** Num TPU Cores: 8. I0524 21:18:26.631872 140032543119168 tpu_system_metadata.py:160] *** Num TPU Cores: 8. INFO:tensorflow:*** Num TPU Workers: 1. I0524 21:18:26.631940 140032543119168 tpu_system_metadata.py:161] *** Num TPU Workers: 1. INFO:tensorflow:*** Num TPU Cores Per Worker: 8. I0524 21:18:26.631998 140032543119168 tpu_system_metadata.py:162] *** Num TPU Cores Per Worker: 8. INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:7004,availability,Avail,Available,7004,"m_metadata.py:160] *** Num TPU Cores: 8. INFO:tensorflow:*** Num TPU Workers: 1. I0524 21:18:26.631940 140032543119168 tpu_system_metadata.py:161] *** Num TPU Workers: 1. INFO:tensorflow:*** Num TPU Cores Per Worker: 8. I0524 21:18:26.631998 140032543119168 tpu_system_metadata.py:162] *** Num TPU Cores Per Worker: 8. INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:7056,availability,replic,replica,7056,"rflow:*** Num TPU Workers: 1. I0524 21:18:26.631940 140032543119168 tpu_system_metadata.py:161] *** Num TPU Workers: 1. INFO:tensorflow:*** Num TPU Cores Per Worker: 8. I0524 21:18:26.631998 140032543119168 tpu_system_metadata.py:162] *** Num TPU Cores Per Worker: 8. INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:7198,availability,Avail,Available,7198," TPU Cores Per Worker: 8. I0524 21:18:26.631998 140032543119168 tpu_system_metadata.py:162] *** Num TPU Cores Per Worker: 8. INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:1",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:7250,availability,replic,replica,7250,"032543119168 tpu_system_metadata.py:162] *** Num TPU Cores Per Worker: 8. INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tp",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:7342,availability,Avail,Available,7342," Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:7394,availability,replic,replica,7394,"r/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:7536,availability,Avail,Available,7536," Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:16",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:7588,availability,replic,replica,7588,"0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:7680,availability,Avail,Available,7680,"DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:7732,availability,replic,replica,7732,"evice:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:7874,availability,Avail,Available,7874,"DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:7926,availability,replic,replica,7926,"evice:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:8018,availability,Avail,Available,8018,"DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:8070,availability,replic,replica,8070,"evice:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_w",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:8211,availability,Avail,Available,8211,"_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:8263,availability,replic,replica,8263,"device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:8354,availability,Avail,Available,8354," _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:8406,availability,replic,replica,8406,"/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:8548,availability,Avail,Available,8548," _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_m",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:8600,availability,replic,replica,8600,"/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttrib",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:8692,availability,Avail,Available,8692," _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 229118620",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:8744,availability,replic,replica,8744,"/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). INFO:tensorflow:*** Available Device: ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:8885,availability,Avail,Available,8885,": _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). I0524 21:18:26.632941 1400325",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:8937,availability,replic,replica,8937,"0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). I0524 21:18:26.632941 140032543119168 tpu_system_metadata.py:165] *** Available ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:9028,availability,Avail,Available,9028,"e: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). I0524 21:18:26.632941 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 171798",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:9080,availability,replic,replica,9080,":0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). I0524 21:18:26.632941 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). INFO:tensorflow:Callin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:9222,availability,Avail,Available,9222,": _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). I0524 21:18:26.632941 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). INFO:tensorflow:Calling model_fn. I0524 21:18:26.633588 140032543119168 estimator.py:1162] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:9274,availability,replic,replica,9274,"0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). I0524 21:18:26.632941 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). INFO:tensorflow:Calling model_fn. I0524 21:18:26.633588 140032543119168 estimator.py:1162] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:9366,availability,Avail,Available,9366," _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). I0524 21:18:26.632941 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). INFO:tensorflow:Calling model_fn. I0524 21:18:26.633588 140032543119168 estimator.py:1162] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:9418,availability,replic,replica,9418,"/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). I0524 21:18:26.632941 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). INFO:tensorflow:Calling model_fn. I0524 21:18:26.633588 140032543119168 estimator.py:1162] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:9573,availability,Avail,Available,9573,"butes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). I0524 21:18:26.632941 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). INFO:tensorflow:Calling model_fn. I0524 21:18:26.633588 140032543119168 estimator.py:1162] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn. INFO:tensorflow:TPU",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:9625,availability,replic,replica,9625,", TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). I0524 21:18:26.632941 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). INFO:tensorflow:Calling model_fn. I0524 21:18:26.633588 140032543119168 estimator.py:1162] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn. INFO:tensorflow:TPU job name tpu_worker. I0524 21:18:33.019782 1400325",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:9730,availability,Avail,Available,9730,"u_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). I0524 21:18:26.632941 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). INFO:tensorflow:Calling model_fn. I0524 21:18:26.633588 140032543119168 estimator.py:1162] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn. INFO:tensorflow:TPU job name tpu_worker. I0524 21:18:33.019782 140032543119168 tpu_estimator.py:514] TPU job name tpu_worker. INFO:tensorflow:Graph was finalized. I0524 21:18:3",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:9782,availability,replic,replica,9782,"69184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). I0524 21:18:26.632941 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). INFO:tensorflow:Calling model_fn. I0524 21:18:26.633588 140032543119168 estimator.py:1162] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn. INFO:tensorflow:TPU job name tpu_worker. I0524 21:18:33.019782 140032543119168 tpu_estimator.py:514] TPU job name tpu_worker. INFO:tensorflow:Graph was finalized. I0524 21:18:33.525068 140032543119168 monitored_session.py:247] ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:9931,availability,Avail,Available,9931,"/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). I0524 21:18:26.632941 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). INFO:tensorflow:Calling model_fn. I0524 21:18:26.633588 140032543119168 estimator.py:1162] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn. INFO:tensorflow:TPU job name tpu_worker. I0524 21:18:33.019782 140032543119168 tpu_estimator.py:514] TPU job name tpu_worker. INFO:tensorflow:Graph was finalized. I0524 21:18:33.525068 140032543119168 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0524 21:18:33.525994 140032543119168 saver.py:1298] Restor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:9983,availability,replic,replica,9983,"158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). I0524 21:18:26.632941 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). INFO:tensorflow:Calling model_fn. I0524 21:18:26.633588 140032543119168 estimator.py:1162] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn. INFO:tensorflow:TPU job name tpu_worker. I0524 21:18:33.019782 140032543119168 tpu_estimator.py:514] TPU job name tpu_worker. INFO:tensorflow:Graph was finalized. I0524 21:18:33.525068 140032543119168 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0524 21:18:33.525994 140032543119168 saver.py:1298] Restoring parameters from /opt/models/wgs/model.ckpt. INF",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:10823,availability,Restor,Restoring,10823,"179869184, 7884439564287565365). I0524 21:18:26.632941 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). INFO:tensorflow:Calling model_fn. I0524 21:18:26.633588 140032543119168 estimator.py:1162] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn. INFO:tensorflow:TPU job name tpu_worker. I0524 21:18:33.019782 140032543119168 tpu_estimator.py:514] TPU job name tpu_worker. INFO:tensorflow:Graph was finalized. I0524 21:18:33.525068 140032543119168 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0524 21:18:33.525994 140032543119168 saver.py:1298] Restoring parameters from /opt/models/wgs/model.ckpt. INFO:tensorflow:prediction_loop marked as finished. I0524 21:18:34.251420 140032543119168 error_handling.py:115] prediction_loop marked as finished. WARNING:tensorflow:Reraising captured error. W0524 21:18:34.251592 140032543119168 error_handling.py:149] Reraising captured error. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1375, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_im",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:10930,availability,Restor,Restoring,10930,"lable Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). INFO:tensorflow:Calling model_fn. I0524 21:18:26.633588 140032543119168 estimator.py:1162] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn. INFO:tensorflow:TPU job name tpu_worker. I0524 21:18:33.019782 140032543119168 tpu_estimator.py:514] TPU job name tpu_worker. INFO:tensorflow:Graph was finalized. I0524 21:18:33.525068 140032543119168 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0524 21:18:33.525994 140032543119168 saver.py:1298] Restoring parameters from /opt/models/wgs/model.ckpt. INFO:tensorflow:prediction_loop marked as finished. I0524 21:18:34.251420 140032543119168 error_handling.py:115] prediction_loop marked as finished. WARNING:tensorflow:Reraising captured error. W0524 21:18:34.251592 140032543119168 error_handling.py:149] Reraising captured error. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1375, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:11171,availability,error,error,11171,"r/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn. INFO:tensorflow:TPU job name tpu_worker. I0524 21:18:33.019782 140032543119168 tpu_estimator.py:514] TPU job name tpu_worker. INFO:tensorflow:Graph was finalized. I0524 21:18:33.525068 140032543119168 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0524 21:18:33.525994 140032543119168 saver.py:1298] Restoring parameters from /opt/models/wgs/model.ckpt. INFO:tensorflow:prediction_loop marked as finished. I0524 21:18:34.251420 140032543119168 error_handling.py:115] prediction_loop marked as finished. WARNING:tensorflow:Reraising captured error. W0524 21:18:34.251592 140032543119168 error_handling.py:149] Reraising captured error. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1375, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:11258,availability,error,error,11258,"2: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn. INFO:tensorflow:TPU job name tpu_worker. I0524 21:18:33.019782 140032543119168 tpu_estimator.py:514] TPU job name tpu_worker. INFO:tensorflow:Graph was finalized. I0524 21:18:33.525068 140032543119168 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0524 21:18:33.525994 140032543119168 saver.py:1298] Restoring parameters from /opt/models/wgs/model.ckpt. INFO:tensorflow:prediction_loop marked as finished. I0524 21:18:34.251420 140032543119168 error_handling.py:115] prediction_loop marked as finished. WARNING:tensorflow:Reraising captured error. W0524 21:18:34.251592 140032543119168 error_handling.py:149] Reraising captured error. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1375, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.8",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:11874,availability,replic,replica,11874,"0524 21:18:33.525994 140032543119168 saver.py:1298] Restoring parameters from /opt/models/wgs/model.ckpt. INFO:tensorflow:prediction_loop marked as finished. I0524 21:18:34.251420 140032543119168 error_handling.py:115] prediction_loop marked as finished. WARNING:tensorflow:Reraising captured error. W0524 21:18:34.251592 140032543119168 error_handling.py:149] Reraising captured error. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1375, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1303, in restore. sess.run(self.saver_def.restore_op_name,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 967, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1190, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1368, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. Fil",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:12328,availability,restor,restore,12328,"dist-packages/tensorflow/python/client/session.py"", line 1375, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1303, in restore. sess.run(self.saver_def.restore_op_name,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 967, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1190, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1368, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1394, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node s",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:13103,availability,replic,replica,13103,"e_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1303, in restore. sess.run(self.saver_def.restore_op_name,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 967, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1190, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1368, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1394, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvaria",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:14642,availability,Monitor,MonitoredSession,14642,"le ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._sc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:14785,availability,Monitor,MonitoredSession,14785,"lags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in finalize. self._s",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:17183,availability,sli,slices,17183,".8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 876, in _build. self.saver_def = self._builder._build_internal( # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal. restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps. self._AddRestoreOps(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 335, in _AddRestoreOps. all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1490, in restore_v2. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 748, in _apply_op_helper. op = g._create_op_internal(op_type_name, inputs, dtypes=None,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3557, in _create_op_internal. ret = Operation(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 2045, in __init__. self._traceback = tf_stack.extract_stack_for_node(self._c_op). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", li",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:17673,availability,Operat,Operation,17673,"sor, per_device,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps. self._AddRestoreOps(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 335, in _AddRestoreOps. all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1490, in restore_v2. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 748, in _apply_op_helper. op = g._create_op_internal(op_type_name, inputs, dtypes=None,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3557, in _create_op_internal. ret = Operation(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 2045, in __init__. self._traceback = tf_stack.extract_stack_for_node(self._c_op). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:19530,availability,Monitor,MonitoredSession,19530,"google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict. rendezvous.raise_errors(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors. six.reraise(typ, value, traceback). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/six_archive/six.py"", line 703, in reraise. raise value. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 662, in create_session. re",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:19674,availability,Monitor,MonitoredSession,19674,"variant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict. rendezvous.raise_errors(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors. six.reraise(typ, value, traceback). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/six_archive/six.py"", line 703, in reraise. raise value. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 662, in create_session. return self._get_session_manager().prepare_session(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:21132,availability,restor,restore,21132,"onitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 662, in create_session. return self._get_session_manager().prepare_session(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 314, in prepare_session. sess, is_loaded_from_checkpoint = self._restore_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 233, in _restore_checkpoint. _restore_checkpoint_and_maybe_run_saved_model_initializers(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers. saver.restore(sess, path). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1339, in restore. raise _wrap_restore_error_with_msg(. tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:21250,availability,restor,restore,21250,"python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 662, in create_session. return self._get_session_manager().prepare_session(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 314, in prepare_session. sess, is_loaded_from_checkpoint = self._restore_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 233, in _restore_checkpoint. _restore_checkpoint_and_maybe_run_saved_model_initializers(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers. saver.restore(sess, path). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1339, in restore. raise _wrap_restore_error_with_msg(. tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:21358,availability,Restor,Restoring,21358,"f_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 662, in create_session. return self._get_session_manager().prepare_session(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 314, in prepare_session. sess, is_loaded_from_checkpoint = self._restore_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 233, in _restore_checkpoint. _restore_checkpoint_and_maybe_run_saved_model_initializers(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers. saver.restore(sess, path). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1339, in restore. raise _wrap_restore_error_with_msg(. tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runf",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:21373,availability,checkpoint,checkpoint,21373,"session_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 662, in create_session. return self._get_session_manager().prepare_session(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 314, in prepare_session. sess, is_loaded_from_checkpoint = self._restore_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 233, in _restore_checkpoint. _restore_checkpoint_and_maybe_run_saved_model_initializers(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers. saver.restore(sess, path). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1339, in restore. raise _wrap_restore_error_with_msg(. tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/ab",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:21479,availability,checkpoint,checkpoint,21479,"monitored_session.py"", line 662, in create_session. return self._get_session_manager().prepare_session(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 314, in prepare_session. sess, is_loaded_from_checkpoint = self._restore_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 233, in _restore_checkpoint. _restore_checkpoint_and_maybe_run_saved_model_initializers(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers. saver.restore(sess, path). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1339, in restore. raise _wrap_restore_error_with_msg(. tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/ab",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:21563,availability,checkpoint,checkpoint,21563,"().prepare_session(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 314, in prepare_session. sess, is_loaded_from_checkpoint = self._restore_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 233, in _restore_checkpoint. _restore_checkpoint_and_maybe_run_saved_model_initializers(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers. saver.restore(sess, path). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1339, in restore. raise _wrap_restore_error_with_msg(. tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:21584,availability,error,error,21584,". File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 314, in prepare_session. sess, is_loaded_from_checkpoint = self._restore_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 233, in _restore_checkpoint. _restore_checkpoint_and_maybe_run_saved_model_initializers(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers. saver.restore(sess, path). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1339, in restore. raise _wrap_restore_error_with_msg(. tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/co",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:21613,availability,replic,replica,21613,".8/dist-packages/tensorflow/python/training/session_manager.py"", line 314, in prepare_session. sess, is_loaded_from_checkpoint = self._restore_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 233, in _restore_checkpoint. _restore_checkpoint_and_maybe_run_saved_model_initializers(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers. saver.restore(sess, path). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1339, in restore. raise _wrap_restore_error_with_msg(. tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvaria",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:23152,availability,Monitor,MonitoredSession,23152,"le ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._sc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:23295,availability,Monitor,MonitoredSession,23295,"lags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in finalize. self._s",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:25693,availability,sli,slices,25693,"tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 876, in _build. self.saver_def = self._builder._build_internal( # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal. restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps. self._AddRestoreOps(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 335, in _AddRestoreOps. all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1490, in restore_v2. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 748, in _apply_op_helper. op = g._create_op_internal(op_type_name, inputs, dtypes=None,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3557, in _create_op_internal. ret = Operation(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 2045, in __init__. self._traceback = tf_stack.extract_stack_for_node(self._c_op). real 0m10.757s. user 0m13.496s. sys 0m5.144s. ```. This same command works fine without using TPUs on this system, and it looks like the TPU node is being recognized by deepvariant. Is there something I'm missing for call_variants?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:26183,availability,Operat,Operation,26183,"tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 876, in _build. self.saver_def = self._builder._build_internal( # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal. restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps. self._AddRestoreOps(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 335, in _AddRestoreOps. all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1490, in restore_v2. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 748, in _apply_op_helper. op = g._create_op_internal(op_type_name, inputs, dtypes=None,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3557, in _create_op_internal. ret = Operation(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 2045, in __init__. self._traceback = tf_stack.extract_stack_for_node(self._c_op). real 0m10.757s. user 0m13.496s. sys 0m5.144s. ```. This same command works fine without using TPUs on this system, and it looks like the TPU node is being recognized by deepvariant. Is there something I'm missing for call_variants?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:3069,deployability,Cluster,ClusterSpec,3069,"1:18:26.587127: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2000160000 Hz. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp_f348kd0. W0524 21:18:26.619681 140032543119168 estimator.py:1846] Using temporary folder as model directory: /tmp/tmp_f348kd0. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp_f348kd0', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.73.74.226:8470', '_evaluation_master': 'grpc://10.73.74.226:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=100, num_shards=None, num_cores_per_replica=None, per_host_input_for_training=2, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1, experimental_allow_per_host_v2_parallel_get_next=False, experimental_feed_hook=None), '_cluster': None}. I0524 21:18:26.620151 140032543119168 estimator.py:191] Using config: {'_model_dir': '/tmp/tmp_f348kd0', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_keep_checkpoint_max': 100000, '",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:4428,deployability,Cluster,ClusterSpec,4428,"2, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1, experimental_allow_per_host_v2_parallel_get_next=False, experimental_feed_hook=None), '_cluster': None}. I0524 21:18:26.620151 140032543119168 estimator.py:191] Using config: {'_model_dir': '/tmp/tmp_f348kd0', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.73.74.226:8470', '_evaluation_master': 'grpc://10.73.74.226:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=100, num_shards=None, num_cores_per_replica=None, per_host_input_for_training=2, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1, experimental_allow_per_host_v2_parallel_get_next=False, experimental_feed_hook=None), '_cluster': None}. INFO:tensorflow:_TPUContext: eval_on_tpu True. I0524 21:18:26.620373 140032543119168 tpu_context.py:271] _TPUContext: eval_on_tpu True. I0524 21:18:26.620768 140032543119168 call_variants.py:426] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz. INFO:tensorflow:Querying Tensorflow master (grpc://10.73.74.226:8470) for TPU system metadata. I05",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:10337,deployability,version,version,10337,"78). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). I0524 21:18:26.632941 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). INFO:tensorflow:Calling model_fn. I0524 21:18:26.633588 140032543119168 estimator.py:1162] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn. INFO:tensorflow:TPU job name tpu_worker. I0524 21:18:33.019782 140032543119168 tpu_estimator.py:514] TPU job name tpu_worker. INFO:tensorflow:Graph was finalized. I0524 21:18:33.525068 140032543119168 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0524 21:18:33.525994 140032543119168 saver.py:1298] Restoring parameters from /opt/models/wgs/model.ckpt. INFO:tensorflow:prediction_loop marked as finished. I0524 21:18:34.251420 140032543119168 error_handling.py:115] prediction_loop marked as finished. WARNING:tensorflow:Reraising captured error. W0524 21:18:34.251592 140032543119168 error_handling.py:149] Reraising captured error. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-pack",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:11937,deployability,Fail,Failed,11937,"parameters from /opt/models/wgs/model.ckpt. INFO:tensorflow:prediction_loop marked as finished. I0524 21:18:34.251420 140032543119168 error_handling.py:115] prediction_loop marked as finished. WARNING:tensorflow:Reraising captured error. W0524 21:18:34.251592 140032543119168 error_handling.py:149] Reraising captured error. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1375, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1303, in restore. sess.run(self.saver_def.restore_op_name,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 967, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1190, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1368, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/cl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:13166,deployability,Fail,Failed,13166,"ther exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1303, in restore. sess.run(self.saver_def.restore_op_name,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 967, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1190, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1368, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1394, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:13467,deployability,stack,stack,13467,", in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1190, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1368, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1394, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(T",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:13615,deployability,modul,module,13615,"y"", line 1190, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1368, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1394, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:14642,deployability,Monitor,MonitoredSession,14642,"le ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._sc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:14785,deployability,Monitor,MonitoredSession,14785,"lags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in finalize. self._s",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:16150,deployability,build,build,16150,"create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 876, in _build. self.saver_def = self._builder._build_internal( # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal. restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps. self._AddRestoreOps(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 335, in _AddRestoreOps. all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.resto",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:16254,deployability,build,build,16254,"ion.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 876, in _build. self.saver_def = self._builder._build_internal( # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal. restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps. self._AddRestoreOps(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 335, in _AddRestoreOps. all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.8/dist-packages/tensorflow/py",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:18070,deployability,modul,module,18070,"orflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1490, in restore_v2. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 748, in _apply_op_helper. op = g._create_op_internal(op_type_name, inputs, dtypes=None,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3557, in _create_op_internal. ret = Operation(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 2045, in __init__. self._traceback = tf_stack.extract_stack_for_node(self._c_op). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict. rendezvous.raise_errors(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors. si",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:19530,deployability,Monitor,MonitoredSession,19530,"google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict. rendezvous.raise_errors(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors. six.reraise(typ, value, traceback). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/six_archive/six.py"", line 703, in reraise. raise value. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 662, in create_session. re",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:19674,deployability,Monitor,MonitoredSession,19674,"variant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict. rendezvous.raise_errors(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors. six.reraise(typ, value, traceback). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/six_archive/six.py"", line 703, in reraise. raise value. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 662, in create_session. return self._get_session_manager().prepare_session(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:21384,deployability,fail,failed,21384,"reator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 662, in create_session. return self._get_session_manager().prepare_session(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 314, in prepare_session. sess, is_loaded_from_checkpoint = self._restore_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 233, in _restore_checkpoint. _restore_checkpoint_and_maybe_run_saved_model_initializers(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers. saver.restore(sess, path). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1339, in restore. raise _wrap_restore_error_with_msg(. tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:21676,deployability,Fail,Failed,21676,""", line 314, in prepare_session. sess, is_loaded_from_checkpoint = self._restore_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 233, in _restore_checkpoint. _restore_checkpoint_and_maybe_run_saved_model_initializers(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers. saver.restore(sess, path). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1339, in restore. raise _wrap_restore_error_with_msg(. tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:21977,deployability,stack,stack,21977,"b/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers. saver.restore(sess, path). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1339, in restore. raise _wrap_restore_error_with_msg(. tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(T",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:22125,deployability,modul,module,22125,"ver.restore(sess, path). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1339, in restore. raise _wrap_restore_error_with_msg(. tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:23152,deployability,Monitor,MonitoredSession,23152,"le ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._sc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:23295,deployability,Monitor,MonitoredSession,23295,"lags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in finalize. self._s",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:24660,deployability,build,build,24660,"create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 876, in _build. self.saver_def = self._builder._build_internal( # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal. restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps. self._AddRestoreOps(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 335, in _AddRestoreOps. all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.resto",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:24764,deployability,build,build,24764,"ion.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 876, in _build. self.saver_def = self._builder._build_internal( # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal. restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps. self._AddRestoreOps(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 335, in _AddRestoreOps. all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.8/dist-packages/tensorflow/py",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:1015,energy efficiency,model,models,1015,"e v3-8 in call variants step; I am trying to use deepvariant to call variants using a TPU Node v3-8, but I am running into a persistent issue. Here is the command I am using:. ```bash. docker run \. -v `pwd`:`pwd` -w `pwd` \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --call_variants_extra_args use_tpu=true,tpu_name=""variantcaller-node1"",tpu_zone=""europe-west4-a"" \. --model_type=WGS \. --ref=""input/data/${REF}"" \. --reads=""input/data/${BAM}"" \. --output_vcf=""output/${OUTPUT_VCF}"" \. --output_gvcf=""output/${OUTPUT_GVCF}"" \. --regions chr20 \. --num_shards=$(nproc) \. --intermediate_results_dir /output/intermediate_results_dir. ```. However, I am seeing the following error in the call variants step. ```bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@96.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --openvino_model_dir ""/output/intermediate_results_dir"" --tpu_name ""variantcaller-node1"" --tpu_zone ""europe-west4-a"" --use_tpu. I0524 21:18:26.485428 140032543119168 transport.py:157] Attempting refresh to obtain initial access_token. I0524 21:18:26.576728 140032543119168 call_variants.py:336] Shape of input examples: [100, 221, 6]. I0524 21:18:26.579230 140032543119168 call_variants.py:361] /opt/models/wgs/model.ckpt.input_shape has the correct shape: [100, 221, 6]. 2022-05-24 21:18:26.581705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-05-24 21:18:26.586196: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:1026,energy efficiency,model,model,1026,"all variants step; I am trying to use deepvariant to call variants using a TPU Node v3-8, but I am running into a persistent issue. Here is the command I am using:. ```bash. docker run \. -v `pwd`:`pwd` -w `pwd` \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --call_variants_extra_args use_tpu=true,tpu_name=""variantcaller-node1"",tpu_zone=""europe-west4-a"" \. --model_type=WGS \. --ref=""input/data/${REF}"" \. --reads=""input/data/${BAM}"" \. --output_vcf=""output/${OUTPUT_VCF}"" \. --output_gvcf=""output/${OUTPUT_GVCF}"" \. --regions chr20 \. --num_shards=$(nproc) \. --intermediate_results_dir /output/intermediate_results_dir. ```. However, I am seeing the following error in the call variants step. ```bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@96.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --openvino_model_dir ""/output/intermediate_results_dir"" --tpu_name ""variantcaller-node1"" --tpu_zone ""europe-west4-a"" --use_tpu. I0524 21:18:26.485428 140032543119168 transport.py:157] Attempting refresh to obtain initial access_token. I0524 21:18:26.576728 140032543119168 call_variants.py:336] Shape of input examples: [100, 221, 6]. I0524 21:18:26.579230 140032543119168 call_variants.py:361] /opt/models/wgs/model.ckpt.input_shape has the correct shape: [100, 221, 6]. 2022-05-24 21:18:26.581705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-05-24 21:18:26.586196: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:1438,energy efficiency,model,models,1438,". --ref=""input/data/${REF}"" \. --reads=""input/data/${BAM}"" \. --output_vcf=""output/${OUTPUT_VCF}"" \. --output_gvcf=""output/${OUTPUT_GVCF}"" \. --regions chr20 \. --num_shards=$(nproc) \. --intermediate_results_dir /output/intermediate_results_dir. ```. However, I am seeing the following error in the call variants step. ```bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@96.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --openvino_model_dir ""/output/intermediate_results_dir"" --tpu_name ""variantcaller-node1"" --tpu_zone ""europe-west4-a"" --use_tpu. I0524 21:18:26.485428 140032543119168 transport.py:157] Attempting refresh to obtain initial access_token. I0524 21:18:26.576728 140032543119168 call_variants.py:336] Shape of input examples: [100, 221, 6]. I0524 21:18:26.579230 140032543119168 call_variants.py:361] /opt/models/wgs/model.ckpt.input_shape has the correct shape: [100, 221, 6]. 2022-05-24 21:18:26.581705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-05-24 21:18:26.586196: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2022-05-24 21:18:26.587127: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2000160000 Hz. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp_f348kd0. W0524 21:18:26.619681 140032543119168 estimator.py:1846] Using temporary folder as model directory: /tmp/tmp_f348kd0. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp_f348kd0'",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:1449,energy efficiency,model,model,1449,"put/data/${REF}"" \. --reads=""input/data/${BAM}"" \. --output_vcf=""output/${OUTPUT_VCF}"" \. --output_gvcf=""output/${OUTPUT_GVCF}"" \. --regions chr20 \. --num_shards=$(nproc) \. --intermediate_results_dir /output/intermediate_results_dir. ```. However, I am seeing the following error in the call variants step. ```bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@96.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --openvino_model_dir ""/output/intermediate_results_dir"" --tpu_name ""variantcaller-node1"" --tpu_zone ""europe-west4-a"" --use_tpu. I0524 21:18:26.485428 140032543119168 transport.py:157] Attempting refresh to obtain initial access_token. I0524 21:18:26.576728 140032543119168 call_variants.py:336] Shape of input examples: [100, 221, 6]. I0524 21:18:26.579230 140032543119168 call_variants.py:361] /opt/models/wgs/model.ckpt.input_shape has the correct shape: [100, 221, 6]. 2022-05-24 21:18:26.581705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-05-24 21:18:26.586196: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2022-05-24 21:18:26.587127: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2000160000 Hz. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp_f348kd0. W0524 21:18:26.619681 140032543119168 estimator.py:1846] Using temporary folder as model directory: /tmp/tmp_f348kd0. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp_f348kd0', '_tf_rand",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:1551,energy efficiency,core,core,1551,"cf=""output/${OUTPUT_GVCF}"" \. --regions chr20 \. --num_shards=$(nproc) \. --intermediate_results_dir /output/intermediate_results_dir. ```. However, I am seeing the following error in the call variants step. ```bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@96.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --openvino_model_dir ""/output/intermediate_results_dir"" --tpu_name ""variantcaller-node1"" --tpu_zone ""europe-west4-a"" --use_tpu. I0524 21:18:26.485428 140032543119168 transport.py:157] Attempting refresh to obtain initial access_token. I0524 21:18:26.576728 140032543119168 call_variants.py:336] Shape of input examples: [100, 221, 6]. I0524 21:18:26.579230 140032543119168 call_variants.py:361] /opt/models/wgs/model.ckpt.input_shape has the correct shape: [100, 221, 6]. 2022-05-24 21:18:26.581705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-05-24 21:18:26.586196: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2022-05-24 21:18:26.587127: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2000160000 Hz. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp_f348kd0. W0524 21:18:26.619681 140032543119168 estimator.py:1846] Using temporary folder as model directory: /tmp/tmp_f348kd0. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp_f348kd0', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs'",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:1617,energy efficiency,optim,optimized,1617,") \. --intermediate_results_dir /output/intermediate_results_dir. ```. However, I am seeing the following error in the call variants step. ```bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@96.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --openvino_model_dir ""/output/intermediate_results_dir"" --tpu_name ""variantcaller-node1"" --tpu_zone ""europe-west4-a"" --use_tpu. I0524 21:18:26.485428 140032543119168 transport.py:157] Attempting refresh to obtain initial access_token. I0524 21:18:26.576728 140032543119168 call_variants.py:336] Shape of input examples: [100, 221, 6]. I0524 21:18:26.579230 140032543119168 call_variants.py:361] /opt/models/wgs/model.ckpt.input_shape has the correct shape: [100, 221, 6]. 2022-05-24 21:18:26.581705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-05-24 21:18:26.586196: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2022-05-24 21:18:26.587127: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2000160000 Hz. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp_f348kd0. W0524 21:18:26.619681 140032543119168 estimator.py:1846] Using temporary folder as model directory: /tmp/tmp_f348kd0. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp_f348kd0', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true. graph_options {",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:1697,energy efficiency,CPU,CPU,1697,"r, I am seeing the following error in the call variants step. ```bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@96.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --openvino_model_dir ""/output/intermediate_results_dir"" --tpu_name ""variantcaller-node1"" --tpu_zone ""europe-west4-a"" --use_tpu. I0524 21:18:26.485428 140032543119168 transport.py:157] Attempting refresh to obtain initial access_token. I0524 21:18:26.576728 140032543119168 call_variants.py:336] Shape of input examples: [100, 221, 6]. I0524 21:18:26.579230 140032543119168 call_variants.py:361] /opt/models/wgs/model.ckpt.input_shape has the correct shape: [100, 221, 6]. 2022-05-24 21:18:26.581705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-05-24 21:18:26.586196: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2022-05-24 21:18:26.587127: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2000160000 Hz. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp_f348kd0. W0524 21:18:26.619681 140032543119168 estimator.py:1846] Using temporary folder as model directory: /tmp/tmp_f348kd0. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp_f348kd0', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_keep_checkpoin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:1901,energy efficiency,core,core,1901,"tput.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@96.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --openvino_model_dir ""/output/intermediate_results_dir"" --tpu_name ""variantcaller-node1"" --tpu_zone ""europe-west4-a"" --use_tpu. I0524 21:18:26.485428 140032543119168 transport.py:157] Attempting refresh to obtain initial access_token. I0524 21:18:26.576728 140032543119168 call_variants.py:336] Shape of input examples: [100, 221, 6]. I0524 21:18:26.579230 140032543119168 call_variants.py:361] /opt/models/wgs/model.ckpt.input_shape has the correct shape: [100, 221, 6]. 2022-05-24 21:18:26.581705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-05-24 21:18:26.586196: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2022-05-24 21:18:26.587127: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2000160000 Hz. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp_f348kd0. W0524 21:18:26.619681 140032543119168 estimator.py:1846] Using temporary folder as model directory: /tmp/tmp_f348kd0. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp_f348kd0', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribut",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:2104,energy efficiency,core,core,2104,"variantcaller-node1"" --tpu_zone ""europe-west4-a"" --use_tpu. I0524 21:18:26.485428 140032543119168 transport.py:157] Attempting refresh to obtain initial access_token. I0524 21:18:26.576728 140032543119168 call_variants.py:336] Shape of input examples: [100, 221, 6]. I0524 21:18:26.579230 140032543119168 call_variants.py:361] /opt/models/wgs/model.ckpt.input_shape has the correct shape: [100, 221, 6]. 2022-05-24 21:18:26.581705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-05-24 21:18:26.586196: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2022-05-24 21:18:26.587127: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2000160000 Hz. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp_f348kd0. W0524 21:18:26.619681 140032543119168 estimator.py:1846] Using temporary folder as model directory: /tmp/tmp_f348kd0. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp_f348kd0', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worke",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:2150,energy efficiency,CPU,CPU,2150,"a"" --use_tpu. I0524 21:18:26.485428 140032543119168 transport.py:157] Attempting refresh to obtain initial access_token. I0524 21:18:26.576728 140032543119168 call_variants.py:336] Shape of input examples: [100, 221, 6]. I0524 21:18:26.579230 140032543119168 call_variants.py:361] /opt/models/wgs/model.ckpt.input_shape has the correct shape: [100, 221, 6]. 2022-05-24 21:18:26.581705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-05-24 21:18:26.586196: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2022-05-24 21:18:26.587127: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2000160000 Hz. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp_f348kd0. W0524 21:18:26.619681 140032543119168 estimator.py:1846] Using temporary folder as model directory: /tmp/tmp_f348kd0. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp_f348kd0', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:2154,energy efficiency,Frequenc,Frequency,2154,"e_tpu. I0524 21:18:26.485428 140032543119168 transport.py:157] Attempting refresh to obtain initial access_token. I0524 21:18:26.576728 140032543119168 call_variants.py:336] Shape of input examples: [100, 221, 6]. I0524 21:18:26.579230 140032543119168 call_variants.py:361] /opt/models/wgs/model.ckpt.input_shape has the correct shape: [100, 221, 6]. 2022-05-24 21:18:26.581705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-05-24 21:18:26.586196: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2022-05-24 21:18:26.587127: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2000160000 Hz. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp_f348kd0. W0524 21:18:26.619681 140032543119168 estimator.py:1846] Using temporary folder as model directory: /tmp/tmp_f348kd0. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp_f348kd0', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_mast",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:2225,energy efficiency,model,model,2225,"ting refresh to obtain initial access_token. I0524 21:18:26.576728 140032543119168 call_variants.py:336] Shape of input examples: [100, 221, 6]. I0524 21:18:26.579230 140032543119168 call_variants.py:361] /opt/models/wgs/model.ckpt.input_shape has the correct shape: [100, 221, 6]. 2022-05-24 21:18:26.581705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-05-24 21:18:26.586196: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2022-05-24 21:18:26.587127: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2000160000 Hz. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp_f348kd0. W0524 21:18:26.619681 140032543119168 estimator.py:1846] Using temporary folder as model directory: /tmp/tmp_f348kd0. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp_f348kd0', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.73.74.226:8470', '_evaluation_master': 'grpc://10.73.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:2298,energy efficiency,estimat,estimator,2298,"3119168 call_variants.py:336] Shape of input examples: [100, 221, 6]. I0524 21:18:26.579230 140032543119168 call_variants.py:361] /opt/models/wgs/model.ckpt.input_shape has the correct shape: [100, 221, 6]. 2022-05-24 21:18:26.581705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-05-24 21:18:26.586196: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2022-05-24 21:18:26.587127: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2000160000 Hz. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp_f348kd0. W0524 21:18:26.619681 140032543119168 estimator.py:1846] Using temporary folder as model directory: /tmp/tmp_f348kd0. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp_f348kd0', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.73.74.226:8470', '_evaluation_master': 'grpc://10.73.74.226:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replic",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:2343,energy efficiency,model,model,2343,"t examples: [100, 221, 6]. I0524 21:18:26.579230 140032543119168 call_variants.py:361] /opt/models/wgs/model.ckpt.input_shape has the correct shape: [100, 221, 6]. 2022-05-24 21:18:26.581705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-05-24 21:18:26.586196: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2022-05-24 21:18:26.587127: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2000160000 Hz. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp_f348kd0. W0524 21:18:26.619681 140032543119168 estimator.py:1846] Using temporary folder as model directory: /tmp/tmp_f348kd0. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp_f348kd0', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.73.74.226:8470', '_evaluation_master': 'grpc://10.73.74.226:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:3735,energy efficiency,estimat,estimator,3735,"_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.73.74.226:8470', '_evaluation_master': 'grpc://10.73.74.226:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=100, num_shards=None, num_cores_per_replica=None, per_host_input_for_training=2, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1, experimental_allow_per_host_v2_parallel_get_next=False, experimental_feed_hook=None), '_cluster': None}. I0524 21:18:26.620151 140032543119168 estimator.py:191] Using config: {'_model_dir': '/tmp/tmp_f348kd0', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.73.74.226:8470', '_evaluation_master': 'grpc://10.73.74.226:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=100, num_shards=None, num",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:5616,energy efficiency,core,core,5616,", '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=100, num_shards=None, num_cores_per_replica=None, per_host_input_for_training=2, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1, experimental_allow_per_host_v2_parallel_get_next=False, experimental_feed_hook=None), '_cluster': None}. INFO:tensorflow:_TPUContext: eval_on_tpu True. I0524 21:18:26.620373 140032543119168 tpu_context.py:271] _TPUContext: eval_on_tpu True. I0524 21:18:26.620768 140032543119168 call_variants.py:426] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz. INFO:tensorflow:Querying Tensorflow master (grpc://10.73.74.226:8470) for TPU system metadata. I0524 21:18:26.625535 140032543119168 tpu_system_metadata.py:90] Querying Tensorflow master (grpc://10.73.74.226:8470) for TPU system metadata. 2022-05-24 21:18:26.626490: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:373] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created. INFO:tensorflow:Found TPU system:. I0524 21:18:26.631762 140032543119168 tpu_system_metadata.py:159] Found TPU system:. INFO:tensorflow:*** Num TPU Cores: 8. I0524 21:18:26.631872 140032543119168 tpu_system_metadata.py:160] *** Num TPU Cores: 8. INFO:tensorflow:*** Num TPU Workers: 1. I0524 21:18:26.631940 140032543119168 tpu_system_metadata.py:161] *** Num TPU Workers: 1. INFO:tensorflow:*** Num TPU Cores Per Worker: 8. I0524 21:18:26.631998 140032543119168 tpu_system_metadata.py:162] *** Num TPU Cores Per Worker: 8. INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:5952,energy efficiency,Core,Cores,5952,"rimental_allow_per_host_v2_parallel_get_next=False, experimental_feed_hook=None), '_cluster': None}. INFO:tensorflow:_TPUContext: eval_on_tpu True. I0524 21:18:26.620373 140032543119168 tpu_context.py:271] _TPUContext: eval_on_tpu True. I0524 21:18:26.620768 140032543119168 call_variants.py:426] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz. INFO:tensorflow:Querying Tensorflow master (grpc://10.73.74.226:8470) for TPU system metadata. I0524 21:18:26.625535 140032543119168 tpu_system_metadata.py:90] Querying Tensorflow master (grpc://10.73.74.226:8470) for TPU system metadata. 2022-05-24 21:18:26.626490: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:373] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created. INFO:tensorflow:Found TPU system:. I0524 21:18:26.631762 140032543119168 tpu_system_metadata.py:159] Found TPU system:. INFO:tensorflow:*** Num TPU Cores: 8. I0524 21:18:26.631872 140032543119168 tpu_system_metadata.py:160] *** Num TPU Cores: 8. INFO:tensorflow:*** Num TPU Workers: 1. I0524 21:18:26.631940 140032543119168 tpu_system_metadata.py:161] *** Num TPU Workers: 1. INFO:tensorflow:*** Num TPU Cores Per Worker: 8. I0524 21:18:26.631998 140032543119168 tpu_system_metadata.py:162] *** Num TPU Cores Per Worker: 8. INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 1717986",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:6040,energy efficiency,Core,Cores,6040,"ter': None}. INFO:tensorflow:_TPUContext: eval_on_tpu True. I0524 21:18:26.620373 140032543119168 tpu_context.py:271] _TPUContext: eval_on_tpu True. I0524 21:18:26.620768 140032543119168 call_variants.py:426] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz. INFO:tensorflow:Querying Tensorflow master (grpc://10.73.74.226:8470) for TPU system metadata. I0524 21:18:26.625535 140032543119168 tpu_system_metadata.py:90] Querying Tensorflow master (grpc://10.73.74.226:8470) for TPU system metadata. 2022-05-24 21:18:26.626490: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:373] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created. INFO:tensorflow:Found TPU system:. I0524 21:18:26.631762 140032543119168 tpu_system_metadata.py:159] Found TPU system:. INFO:tensorflow:*** Num TPU Cores: 8. I0524 21:18:26.631872 140032543119168 tpu_system_metadata.py:160] *** Num TPU Cores: 8. INFO:tensorflow:*** Num TPU Workers: 1. I0524 21:18:26.631940 140032543119168 tpu_system_metadata.py:161] *** Num TPU Workers: 1. INFO:tensorflow:*** Num TPU Cores Per Worker: 8. I0524 21:18:26.631998 140032543119168 tpu_system_metadata.py:162] *** Num TPU Cores Per Worker: 8. INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/jo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:6208,energy efficiency,Core,Cores,6208,"68 140032543119168 call_variants.py:426] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz. INFO:tensorflow:Querying Tensorflow master (grpc://10.73.74.226:8470) for TPU system metadata. I0524 21:18:26.625535 140032543119168 tpu_system_metadata.py:90] Querying Tensorflow master (grpc://10.73.74.226:8470) for TPU system metadata. 2022-05-24 21:18:26.626490: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:373] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created. INFO:tensorflow:Found TPU system:. I0524 21:18:26.631762 140032543119168 tpu_system_metadata.py:159] Found TPU system:. INFO:tensorflow:*** Num TPU Cores: 8. I0524 21:18:26.631872 140032543119168 tpu_system_metadata.py:160] *** Num TPU Cores: 8. INFO:tensorflow:*** Num TPU Workers: 1. I0524 21:18:26.631940 140032543119168 tpu_system_metadata.py:161] *** Num TPU Workers: 1. INFO:tensorflow:*** Num TPU Cores Per Worker: 8. I0524 21:18:26.631998 140032543119168 tpu_system_metadata.py:162] *** Num TPU Cores Per Worker: 8. INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Dev",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:6307,energy efficiency,Core,Cores,6307,"iants_output.tfrecord.gz. INFO:tensorflow:Querying Tensorflow master (grpc://10.73.74.226:8470) for TPU system metadata. I0524 21:18:26.625535 140032543119168 tpu_system_metadata.py:90] Querying Tensorflow master (grpc://10.73.74.226:8470) for TPU system metadata. 2022-05-24 21:18:26.626490: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:373] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created. INFO:tensorflow:Found TPU system:. I0524 21:18:26.631762 140032543119168 tpu_system_metadata.py:159] Found TPU system:. INFO:tensorflow:*** Num TPU Cores: 8. I0524 21:18:26.631872 140032543119168 tpu_system_metadata.py:160] *** Num TPU Cores: 8. INFO:tensorflow:*** Num TPU Workers: 1. I0524 21:18:26.631940 140032543119168 tpu_system_metadata.py:161] *** Num TPU Workers: 1. INFO:tensorflow:*** Num TPU Cores Per Worker: 8. I0524 21:18:26.631998 140032543119168 tpu_system_metadata.py:162] *** Num TPU Cores Per Worker: 8. INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:6424,energy efficiency,CPU,CPU,6424,"ata. I0524 21:18:26.625535 140032543119168 tpu_system_metadata.py:90] Querying Tensorflow master (grpc://10.73.74.226:8470) for TPU system metadata. 2022-05-24 21:18:26.626490: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:373] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created. INFO:tensorflow:Found TPU system:. I0524 21:18:26.631762 140032543119168 tpu_system_metadata.py:159] Found TPU system:. INFO:tensorflow:*** Num TPU Cores: 8. I0524 21:18:26.631872 140032543119168 tpu_system_metadata.py:160] *** Num TPU Cores: 8. INFO:tensorflow:*** Num TPU Workers: 1. I0524 21:18:26.631940 140032543119168 tpu_system_metadata.py:161] *** Num TPU Workers: 1. INFO:tensorflow:*** Num TPU Cores Per Worker: 8. I0524 21:18:26.631998 140032543119168 tpu_system_metadata.py:162] *** Num TPU Cores Per Worker: 8. INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, T",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:6431,energy efficiency,CPU,CPU,6431,"524 21:18:26.625535 140032543119168 tpu_system_metadata.py:90] Querying Tensorflow master (grpc://10.73.74.226:8470) for TPU system metadata. 2022-05-24 21:18:26.626490: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:373] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created. INFO:tensorflow:Found TPU system:. I0524 21:18:26.631762 140032543119168 tpu_system_metadata.py:159] Found TPU system:. INFO:tensorflow:*** Num TPU Cores: 8. I0524 21:18:26.631872 140032543119168 tpu_system_metadata.py:160] *** Num TPU Cores: 8. INFO:tensorflow:*** Num TPU Workers: 1. I0524 21:18:26.631940 140032543119168 tpu_system_metadata.py:161] *** Num TPU Workers: 1. INFO:tensorflow:*** Num TPU Cores Per Worker: 8. I0524 21:18:26.631998 140032543119168 tpu_system_metadata.py:162] *** Num TPU Cores Per Worker: 8. INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 171",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:6608,energy efficiency,CPU,CPU,6608,"rflow/core/distributed_runtime/rpc/grpc_session.cc:373] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created. INFO:tensorflow:Found TPU system:. I0524 21:18:26.631762 140032543119168 tpu_system_metadata.py:159] Found TPU system:. INFO:tensorflow:*** Num TPU Cores: 8. I0524 21:18:26.631872 140032543119168 tpu_system_metadata.py:160] *** Num TPU Cores: 8. INFO:tensorflow:*** Num TPU Workers: 1. I0524 21:18:26.631940 140032543119168 tpu_system_metadata.py:161] *** Num TPU Workers: 1. INFO:tensorflow:*** Num TPU Cores Per Worker: 8. I0524 21:18:26.631998 140032543119168 tpu_system_metadata.py:162] *** Num TPU Cores Per Worker: 8. INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/devic",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:6615,energy efficiency,CPU,CPU,6615,"ore/distributed_runtime/rpc/grpc_session.cc:373] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created. INFO:tensorflow:Found TPU system:. I0524 21:18:26.631762 140032543119168 tpu_system_metadata.py:159] Found TPU system:. INFO:tensorflow:*** Num TPU Cores: 8. I0524 21:18:26.631872 140032543119168 tpu_system_metadata.py:160] *** Num TPU Cores: 8. INFO:tensorflow:*** Num TPU Workers: 1. I0524 21:18:26.631940 140032543119168 tpu_system_metadata.py:161] *** Num TPU Workers: 1. INFO:tensorflow:*** Num TPU Cores Per Worker: 8. I0524 21:18:26.631998 140032543119168 tpu_system_metadata.py:162] *** Num TPU Cores Per Worker: 8. INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:10134,energy efficiency,estimat,estimator,10134,"3137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). I0524 21:18:26.632941 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). INFO:tensorflow:Calling model_fn. I0524 21:18:26.633588 140032543119168 estimator.py:1162] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn. INFO:tensorflow:TPU job name tpu_worker. I0524 21:18:33.019782 140032543119168 tpu_estimator.py:514] TPU job name tpu_worker. INFO:tensorflow:Graph was finalized. I0524 21:18:33.525068 140032543119168 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0524 21:18:33.525994 140032543119168 saver.py:1298] Restoring parameters from /opt/models/wgs/model.ckpt. INFO:tensorflow:prediction_loop marked as finished. I0524 21:18:34.251420 140032543119168 error_handling.py:115] prediction_loop marked as finished. WARNIN",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:10517,energy efficiency,estimat,estimator,10517,"60 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). I0524 21:18:26.632941 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). INFO:tensorflow:Calling model_fn. I0524 21:18:26.633588 140032543119168 estimator.py:1162] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn. INFO:tensorflow:TPU job name tpu_worker. I0524 21:18:33.019782 140032543119168 tpu_estimator.py:514] TPU job name tpu_worker. INFO:tensorflow:Graph was finalized. I0524 21:18:33.525068 140032543119168 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0524 21:18:33.525994 140032543119168 saver.py:1298] Restoring parameters from /opt/models/wgs/model.ckpt. INFO:tensorflow:prediction_loop marked as finished. I0524 21:18:34.251420 140032543119168 error_handling.py:115] prediction_loop marked as finished. WARNING:tensorflow:Reraising captured error. W0524 21:18:34.251592 140032543119168 error_handling.py:149] Reraising captured error. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1375, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1359, in",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:10854,energy efficiency,model,models,10854,"5). I0524 21:18:26.632941 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). INFO:tensorflow:Calling model_fn. I0524 21:18:26.633588 140032543119168 estimator.py:1162] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn. INFO:tensorflow:TPU job name tpu_worker. I0524 21:18:33.019782 140032543119168 tpu_estimator.py:514] TPU job name tpu_worker. INFO:tensorflow:Graph was finalized. I0524 21:18:33.525068 140032543119168 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0524 21:18:33.525994 140032543119168 saver.py:1298] Restoring parameters from /opt/models/wgs/model.ckpt. INFO:tensorflow:prediction_loop marked as finished. I0524 21:18:34.251420 140032543119168 error_handling.py:115] prediction_loop marked as finished. WARNING:tensorflow:Reraising captured error. W0524 21:18:34.251592 140032543119168 error_handling.py:149] Reraising captured error. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1375, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.InvalidArgumentError: From",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:10865,energy efficiency,model,model,10865,"1:18:26.632941 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). INFO:tensorflow:Calling model_fn. I0524 21:18:26.633588 140032543119168 estimator.py:1162] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn. INFO:tensorflow:TPU job name tpu_worker. I0524 21:18:33.019782 140032543119168 tpu_estimator.py:514] TPU job name tpu_worker. INFO:tensorflow:Graph was finalized. I0524 21:18:33.525068 140032543119168 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0524 21:18:33.525994 140032543119168 saver.py:1298] Restoring parameters from /opt/models/wgs/model.ckpt. INFO:tensorflow:prediction_loop marked as finished. I0524 21:18:34.251420 140032543119168 error_handling.py:115] prediction_loop marked as finished. WARNING:tensorflow:Reraising captured error. W0524 21:18:34.251592 140032543119168 error_handling.py:149] Reraising captured error. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1375, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_w",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:10961,energy efficiency,model,models,10961,"es(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). INFO:tensorflow:Calling model_fn. I0524 21:18:26.633588 140032543119168 estimator.py:1162] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn. INFO:tensorflow:TPU job name tpu_worker. I0524 21:18:33.019782 140032543119168 tpu_estimator.py:514] TPU job name tpu_worker. INFO:tensorflow:Graph was finalized. I0524 21:18:33.525068 140032543119168 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0524 21:18:33.525994 140032543119168 saver.py:1298] Restoring parameters from /opt/models/wgs/model.ckpt. INFO:tensorflow:prediction_loop marked as finished. I0524 21:18:34.251420 140032543119168 error_handling.py:115] prediction_loop marked as finished. WARNING:tensorflow:Reraising captured error. W0524 21:18:34.251592 140032543119168 error_handling.py:149] Reraising captured error. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1375, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching file",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:10972,energy efficiency,model,model,10972,"_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). INFO:tensorflow:Calling model_fn. I0524 21:18:26.633588 140032543119168 estimator.py:1162] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn. INFO:tensorflow:TPU job name tpu_worker. I0524 21:18:33.019782 140032543119168 tpu_estimator.py:514] TPU job name tpu_worker. INFO:tensorflow:Graph was finalized. I0524 21:18:33.525068 140032543119168 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0524 21:18:33.525994 140032543119168 saver.py:1298] Restoring parameters from /opt/models/wgs/model.ckpt. INFO:tensorflow:prediction_loop marked as finished. I0524 21:18:34.251420 140032543119168 error_handling.py:115] prediction_loop marked as finished. WARNING:tensorflow:Reraising captured error. W0524 21:18:34.251592 140032543119168 error_handling.py:149] Reraising captured error. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1375, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/m",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:11974,energy efficiency,model,models,11974,".ckpt. INFO:tensorflow:prediction_loop marked as finished. I0524 21:18:34.251420 140032543119168 error_handling.py:115] prediction_loop marked as finished. WARNING:tensorflow:Reraising captured error. W0524 21:18:34.251592 140032543119168 error_handling.py:149] Reraising captured error. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1375, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1303, in restore. sess.run(self.saver_def.restore_op_name,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 967, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1190, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1368, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1394, in _do_c",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:11985,energy efficiency,model,model,11985,":tensorflow:prediction_loop marked as finished. I0524 21:18:34.251420 140032543119168 error_handling.py:115] prediction_loop marked as finished. WARNING:tensorflow:Reraising captured error. W0524 21:18:34.251592 140032543119168 error_handling.py:149] Reraising captured error. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1375, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1303, in restore. sess.run(self.saver_def.restore_op_name,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 967, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1190, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1368, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1394, in _do_call. raise ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:12070,energy efficiency,model,models,12070," error_handling.py:115] prediction_loop marked as finished. WARNING:tensorflow:Reraising captured error. W0524 21:18:34.251592 140032543119168 error_handling.py:149] Reraising captured error. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1375, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1303, in restore. sess.run(self.saver_def.restore_op_name,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 967, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1190, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1368, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1394, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.InvalidArgume",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:12081,energy efficiency,model,model,12081,"ling.py:115] prediction_loop marked as finished. WARNING:tensorflow:Reraising captured error. W0524 21:18:34.251592 140032543119168 error_handling.py:149] Reraising captured error. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1375, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1303, in restore. sess.run(self.saver_def.restore_op_name,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 967, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1190, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1368, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1394, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.InvalidArgumentError: Fr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:13203,energy efficiency,model,models,13203,"most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1303, in restore. sess.run(self.saver_def.restore_op_name,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 967, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1190, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1368, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1394, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:13214,energy efficiency,model,model,13214," call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1303, in restore. sess.run(self.saver_def.restore_op_name,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 967, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1190, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1368, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1394, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:13299,energy efficiency,model,models,13299,"/saver.py"", line 1303, in restore. sess.run(self.saver_def.restore_op_name,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 967, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1190, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1368, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1394, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:13310,energy efficiency,model,model,13310," line 1303, in restore. sess.run(self.saver_def.restore_op_name,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 967, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1190, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1368, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1394, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predic",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:13426,energy efficiency,estimat,estimator,13426,"low/python/client/session.py"", line 967, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1190, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1368, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1394, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:13436,energy efficiency,estimat,estimator,13436,"/client/session.py"", line 967, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1190, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1368, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1394, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in p",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:14289,energy efficiency,predict,prediction,14289,"/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_sessio",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:14307,energy efficiency,predict,predictions,14307,"el.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._ses",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:14393,energy efficiency,estimat,estimator,14393,"tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/py",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:14440,energy efficiency,predict,predict,14440,"r.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:14489,energy efficiency,predict,predict,14489,"storeV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:14571,energy efficiency,estimat,estimator,14571,"riant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/py",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:14581,energy efficiency,estimat,estimator,14581,"_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/train",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:14609,energy efficiency,predict,predict,14609," <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:14642,energy efficiency,Monitor,MonitoredSession,14642,"le ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._sc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:14785,energy efficiency,Monitor,MonitoredSession,14785,"lags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in finalize. self._s",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:18749,energy efficiency,predict,prediction,18749,"rk/ops.py"", line 2045, in __init__. self._traceback = tf_stack.extract_stack_for_node(self._c_op). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict. rendezvous.raise_errors(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors. six.reraise(typ, value, traceback). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/six_archive/six.py"", line 703, in reraise. raise value. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""/usr/local/lib/python3.8/dist-packages/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:18767,energy efficiency,predict,predictions,18767,"45, in __init__. self._traceback = tf_stack.extract_stack_for_node(self._c_op). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict. rendezvous.raise_errors(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors. six.reraise(typ, value, traceback). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/six_archive/six.py"", line 703, in reraise. raise value. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:18854,energy efficiency,estimat,estimator,18854," handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict. rendezvous.raise_errors(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors. six.reraise(typ, value, traceback). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/six_archive/six.py"", line 703, in reraise. raise value. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:18901,energy efficiency,predict,predict,18901,"ption occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict. rendezvous.raise_errors(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors. six.reraise(typ, value, traceback). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/six_archive/six.py"", line 703, in reraise. raise value. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:19010,energy efficiency,estimat,estimator,19010,"epvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict. rendezvous.raise_errors(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors. six.reraise(typ, value, traceback). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/six_archive/six.py"", line 703, in reraise. raise value. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _Wrapped",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:19280,energy efficiency,estimat,estimator,19280,"el.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict. rendezvous.raise_errors(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors. six.reraise(typ, value, traceback). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/six_archive/six.py"", line 703, in reraise. raise value. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorf",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:19327,energy efficiency,predict,predict,19327,".py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict. rendezvous.raise_errors(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors. six.reraise(typ, value, traceback). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/six_archive/six.py"", line 703, in reraise. raise value. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", lin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:19376,energy efficiency,predict,predict,19376,"le ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict. rendezvous.raise_errors(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors. six.reraise(typ, value, traceback). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/six_archive/six.py"", line 703, in reraise. raise value. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._se",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:19459,energy efficiency,estimat,estimator,19459,"in. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict. rendezvous.raise_errors(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors. six.reraise(typ, value, traceback). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/six_archive/six.py"", line 703, in reraise. raise value. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorf",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:19469,energy efficiency,estimat,estimator,19469,"it(main(argv)). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict. rendezvous.raise_errors(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors. six.reraise(typ, value, traceback). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/six_archive/six.py"", line 703, in reraise. raise value. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:19497,energy efficiency,predict,predict,19497,"Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict. rendezvous.raise_errors(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors. six.reraise(typ, value, traceback). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/six_archive/six.py"", line 703, in reraise. raise value. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:19530,energy efficiency,Monitor,MonitoredSession,19530,"google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict. rendezvous.raise_errors(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors. six.reraise(typ, value, traceback). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/six_archive/six.py"", line 703, in reraise. raise value. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 662, in create_session. re",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:19674,energy efficiency,Monitor,MonitoredSession,19674,"variant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict. rendezvous.raise_errors(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors. six.reraise(typ, value, traceback). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/six_archive/six.py"", line 703, in reraise. raise value. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 662, in create_session. return self._get_session_manager().prepare_session(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:21442,energy efficiency,current,current,21442,"t-packages/tensorflow/python/training/monitored_session.py"", line 662, in create_session. return self._get_session_manager().prepare_session(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 314, in prepare_session. sess, is_loaded_from_checkpoint = self._restore_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 233, in _restore_checkpoint. _restore_checkpoint_and_maybe_run_saved_model_initializers(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers. saver.restore(sess, path). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1339, in restore. raise _wrap_restore_error_with_msg(. tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:21713,energy efficiency,model,models,21713,", is_loaded_from_checkpoint = self._restore_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 233, in _restore_checkpoint. _restore_checkpoint_and_maybe_run_saved_model_initializers(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers. saver.restore(sess, path). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1339, in restore. raise _wrap_restore_error_with_msg(. tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:21724,energy efficiency,model,model,21724,"_from_checkpoint = self._restore_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 233, in _restore_checkpoint. _restore_checkpoint_and_maybe_run_saved_model_initializers(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers. saver.restore(sess, path). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1339, in restore. raise _wrap_restore_error_with_msg(. tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:21809,energy efficiency,model,models,21809,"kages/tensorflow/python/training/session_manager.py"", line 233, in _restore_checkpoint. _restore_checkpoint_and_maybe_run_saved_model_initializers(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers. saver.restore(sess, path). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1339, in restore. raise _wrap_restore_error_with_msg(. tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:21820,energy efficiency,model,model,21820,"rflow/python/training/session_manager.py"", line 233, in _restore_checkpoint. _restore_checkpoint_and_maybe_run_saved_model_initializers(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers. saver.restore(sess, path). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1339, in restore. raise _wrap_restore_error_with_msg(. tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predic",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:21936,energy efficiency,estimat,estimator,21936,"odel_initializers(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers. saver.restore(sess, path). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1339, in restore. raise _wrap_restore_error_with_msg(. tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:21946,energy efficiency,estimat,estimator,21946,"alizers(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers. saver.restore(sess, path). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1339, in restore. raise _wrap_restore_error_with_msg(. tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in p",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:22799,energy efficiency,predict,prediction,22799,"/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_sessio",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:22817,energy efficiency,predict,predictions,22817,"el.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._ses",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:22903,energy efficiency,estimat,estimator,22903,"tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/py",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:22950,energy efficiency,predict,predict,22950,"r.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:22999,energy efficiency,predict,predict,22999,"storeV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:23081,energy efficiency,estimat,estimator,23081,"riant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/py",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:23091,energy efficiency,estimat,estimator,23091,"_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/train",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:23119,energy efficiency,predict,predict,23119," <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:23152,energy efficiency,Monitor,MonitoredSession,23152,"le ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._sc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:23295,energy efficiency,Monitor,MonitoredSession,23295,"lags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in finalize. self._s",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:10337,integrability,version,version,10337,"78). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). I0524 21:18:26.632941 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). INFO:tensorflow:Calling model_fn. I0524 21:18:26.633588 140032543119168 estimator.py:1162] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn. INFO:tensorflow:TPU job name tpu_worker. I0524 21:18:33.019782 140032543119168 tpu_estimator.py:514] TPU job name tpu_worker. INFO:tensorflow:Graph was finalized. I0524 21:18:33.525068 140032543119168 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0524 21:18:33.525994 140032543119168 saver.py:1298] Restoring parameters from /opt/models/wgs/model.ckpt. INFO:tensorflow:prediction_loop marked as finished. I0524 21:18:34.251420 140032543119168 error_handling.py:115] prediction_loop marked as finished. WARNING:tensorflow:Reraising captured error. W0524 21:18:34.251592 140032543119168 error_handling.py:149] Reraising captured error. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-pack",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:13010,integrability,messag,message,13010,"le system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1303, in restore. sess.run(self.saver_def.restore_op_name,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 967, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1190, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1368, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1394, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:1556,interoperability,platform,platform,1556,"put/${OUTPUT_GVCF}"" \. --regions chr20 \. --num_shards=$(nproc) \. --intermediate_results_dir /output/intermediate_results_dir. ```. However, I am seeing the following error in the call variants step. ```bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@96.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --openvino_model_dir ""/output/intermediate_results_dir"" --tpu_name ""variantcaller-node1"" --tpu_zone ""europe-west4-a"" --use_tpu. I0524 21:18:26.485428 140032543119168 transport.py:157] Attempting refresh to obtain initial access_token. I0524 21:18:26.576728 140032543119168 call_variants.py:336] Shape of input examples: [100, 221, 6]. I0524 21:18:26.579230 140032543119168 call_variants.py:361] /opt/models/wgs/model.ckpt.input_shape has the correct shape: [100, 221, 6]. 2022-05-24 21:18:26.581705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-05-24 21:18:26.586196: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2022-05-24 21:18:26.587127: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2000160000 Hz. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp_f348kd0. W0524 21:18:26.619681 140032543119168 estimator.py:1846] Using temporary folder as model directory: /tmp/tmp_f348kd0. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp_f348kd0', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:2109,interoperability,platform,platform,2109,"caller-node1"" --tpu_zone ""europe-west4-a"" --use_tpu. I0524 21:18:26.485428 140032543119168 transport.py:157] Attempting refresh to obtain initial access_token. I0524 21:18:26.576728 140032543119168 call_variants.py:336] Shape of input examples: [100, 221, 6]. I0524 21:18:26.579230 140032543119168 call_variants.py:361] /opt/models/wgs/model.ckpt.input_shape has the correct shape: [100, 221, 6]. 2022-05-24 21:18:26.581705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-05-24 21:18:26.586196: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2022-05-24 21:18:26.587127: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2000160000 Hz. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp_f348kd0. W0524 21:18:26.619681 140032543119168 estimator.py:1846] Using temporary folder as model directory: /tmp/tmp_f348kd0. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp_f348kd0', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:5641,interoperability,rpc,rpc,5641,"'_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=100, num_shards=None, num_cores_per_replica=None, per_host_input_for_training=2, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1, experimental_allow_per_host_v2_parallel_get_next=False, experimental_feed_hook=None), '_cluster': None}. INFO:tensorflow:_TPUContext: eval_on_tpu True. I0524 21:18:26.620373 140032543119168 tpu_context.py:271] _TPUContext: eval_on_tpu True. I0524 21:18:26.620768 140032543119168 call_variants.py:426] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz. INFO:tensorflow:Querying Tensorflow master (grpc://10.73.74.226:8470) for TPU system metadata. I0524 21:18:26.625535 140032543119168 tpu_system_metadata.py:90] Querying Tensorflow master (grpc://10.73.74.226:8470) for TPU system metadata. 2022-05-24 21:18:26.626490: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:373] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created. INFO:tensorflow:Found TPU system:. I0524 21:18:26.631762 140032543119168 tpu_system_metadata.py:159] Found TPU system:. INFO:tensorflow:*** Num TPU Cores: 8. I0524 21:18:26.631872 140032543119168 tpu_system_metadata.py:160] *** Num TPU Cores: 8. INFO:tensorflow:*** Num TPU Workers: 1. I0524 21:18:26.631940 140032543119168 tpu_system_metadata.py:161] *** Num TPU Workers: 1. INFO:tensorflow:*** Num TPU Cores Per Worker: 8. I0524 21:18:26.631998 140032543119168 tpu_system_metadata.py:162] *** Num TPU Cores Per Worker: 8. INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:13010,interoperability,messag,message,13010,"le system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1303, in restore. sess.run(self.saver_def.restore_op_name,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 967, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1190, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1368, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1394, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:13710,interoperability,platform,platform,13710,"/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1368, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1394, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:18166,interoperability,platform,platform,18166,"nsor, names, slices, dtypes). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1490, in restore_v2. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 748, in _apply_op_helper. op = g._create_op_internal(op_type_name, inputs, dtypes=None,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3557, in _create_op_internal. ret = Operation(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 2045, in __init__. self._traceback = tf_stack.extract_stack_for_node(self._c_op). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict. rendezvous.raise_errors(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors. six.reraise(typ, value, traceback). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/six_archive/six.py""",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:21421,interoperability,mismatch,mismatch,21421,"cal/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 662, in create_session. return self._get_session_manager().prepare_session(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 314, in prepare_session. sess, is_loaded_from_checkpoint = self._restore_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 233, in _restore_checkpoint. _restore_checkpoint_and_maybe_run_saved_model_initializers(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers. saver.restore(sess, path). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1339, in restore. raise _wrap_restore_error_with_msg(. tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, a",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:22220,interoperability,platform,platform,22220,"/saver.py"", line 1339, in restore. raise _wrap_restore_error_with_msg(. tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:10201,modifiability,pac,packages,10201,"data.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). I0524 21:18:26.632941 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). INFO:tensorflow:Calling model_fn. I0524 21:18:26.633588 140032543119168 estimator.py:1162] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn. INFO:tensorflow:TPU job name tpu_worker. I0524 21:18:33.019782 140032543119168 tpu_estimator.py:514] TPU job name tpu_worker. INFO:tensorflow:Graph was finalized. I0524 21:18:33.525068 140032543119168 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0524 21:18:33.525994 140032543119168 saver.py:1298] Restoring parameters from /opt/models/wgs/model.ckpt. INFO:tensorflow:prediction_loop marked as finished. I0524 21:18:34.251420 140032543119168 error_handling.py:115] prediction_loop marked as finished. WARNING:tensorflow:Reraising captured error. W0524 21:18:34.251592 14003",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:10278,modifiability,layer,layer,10278,":0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). I0524 21:18:26.632941 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). INFO:tensorflow:Calling model_fn. I0524 21:18:26.633588 140032543119168 estimator.py:1162] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn. INFO:tensorflow:TPU job name tpu_worker. I0524 21:18:33.019782 140032543119168 tpu_estimator.py:514] TPU job name tpu_worker. INFO:tensorflow:Graph was finalized. I0524 21:18:33.525068 140032543119168 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0524 21:18:33.525994 140032543119168 saver.py:1298] Restoring parameters from /opt/models/wgs/model.ckpt. INFO:tensorflow:prediction_loop marked as finished. I0524 21:18:34.251420 140032543119168 error_handling.py:115] prediction_loop marked as finished. WARNING:tensorflow:Reraising captured error. W0524 21:18:34.251592 140032543119168 error_handling.py:149] Reraising captured error. Traceback (most ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:10337,modifiability,version,version,10337,"78). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). I0524 21:18:26.632941 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). INFO:tensorflow:Calling model_fn. I0524 21:18:26.633588 140032543119168 estimator.py:1162] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn. INFO:tensorflow:TPU job name tpu_worker. I0524 21:18:33.019782 140032543119168 tpu_estimator.py:514] TPU job name tpu_worker. INFO:tensorflow:Graph was finalized. I0524 21:18:33.525068 140032543119168 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0524 21:18:33.525994 140032543119168 saver.py:1298] Restoring parameters from /opt/models/wgs/model.ckpt. INFO:tensorflow:prediction_loop marked as finished. I0524 21:18:34.251420 140032543119168 error_handling.py:115] prediction_loop marked as finished. WARNING:tensorflow:Reraising captured error. W0524 21:18:34.251592 140032543119168 error_handling.py:149] Reraising captured error. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-pack",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:10358,modifiability,layer,layer,10358,":*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). I0524 21:18:26.632941 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). INFO:tensorflow:Calling model_fn. I0524 21:18:26.633588 140032543119168 estimator.py:1162] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn. INFO:tensorflow:TPU job name tpu_worker. I0524 21:18:33.019782 140032543119168 tpu_estimator.py:514] TPU job name tpu_worker. INFO:tensorflow:Graph was finalized. I0524 21:18:33.525068 140032543119168 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0524 21:18:33.525994 140032543119168 saver.py:1298] Restoring parameters from /opt/models/wgs/model.ckpt. INFO:tensorflow:prediction_loop marked as finished. I0524 21:18:34.251420 140032543119168 error_handling.py:115] prediction_loop marked as finished. WARNING:tensorflow:Reraising captured error. W0524 21:18:34.251592 140032543119168 error_handling.py:149] Reraising captured error. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/pyth",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:10406,modifiability,layer,layer,10406,"u_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). I0524 21:18:26.632941 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). INFO:tensorflow:Calling model_fn. I0524 21:18:26.633588 140032543119168 estimator.py:1162] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn. INFO:tensorflow:TPU job name tpu_worker. I0524 21:18:33.019782 140032543119168 tpu_estimator.py:514] TPU job name tpu_worker. INFO:tensorflow:Graph was finalized. I0524 21:18:33.525068 140032543119168 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0524 21:18:33.525994 140032543119168 saver.py:1298] Restoring parameters from /opt/models/wgs/model.ckpt. INFO:tensorflow:prediction_loop marked as finished. I0524 21:18:34.251420 140032543119168 error_handling.py:115] prediction_loop marked as finished. WARNING:tensorflow:Reraising captured error. W0524 21:18:34.251592 140032543119168 error_handling.py:149] Reraising captured error. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1375, in _do_call. r",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:10833,modifiability,paramet,parameters,10833," 7884439564287565365). I0524 21:18:26.632941 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). INFO:tensorflow:Calling model_fn. I0524 21:18:26.633588 140032543119168 estimator.py:1162] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn. INFO:tensorflow:TPU job name tpu_worker. I0524 21:18:33.019782 140032543119168 tpu_estimator.py:514] TPU job name tpu_worker. INFO:tensorflow:Graph was finalized. I0524 21:18:33.525068 140032543119168 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0524 21:18:33.525994 140032543119168 saver.py:1298] Restoring parameters from /opt/models/wgs/model.ckpt. INFO:tensorflow:prediction_loop marked as finished. I0524 21:18:34.251420 140032543119168 error_handling.py:115] prediction_loop marked as finished. WARNING:tensorflow:Reraising captured error. W0524 21:18:34.251592 140032543119168 error_handling.py:149] Reraising captured error. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1375, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.Invalid",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:10940,modifiability,paramet,parameters,10940,"ce: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). INFO:tensorflow:Calling model_fn. I0524 21:18:26.633588 140032543119168 estimator.py:1162] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn. INFO:tensorflow:TPU job name tpu_worker. I0524 21:18:33.019782 140032543119168 tpu_estimator.py:514] TPU job name tpu_worker. INFO:tensorflow:Graph was finalized. I0524 21:18:33.525068 140032543119168 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0524 21:18:33.525994 140032543119168 saver.py:1298] Restoring parameters from /opt/models/wgs/model.ckpt. INFO:tensorflow:prediction_loop marked as finished. I0524 21:18:34.251420 140032543119168 error_handling.py:115] prediction_loop marked as finished. WARNING:tensorflow:Reraising captured error. W0524 21:18:34.251592 140032543119168 error_handling.py:149] Reraising captured error. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1375, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:11337,modifiability,pac,packages,11337,"ion. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn. INFO:tensorflow:TPU job name tpu_worker. I0524 21:18:33.019782 140032543119168 tpu_estimator.py:514] TPU job name tpu_worker. INFO:tensorflow:Graph was finalized. I0524 21:18:33.525068 140032543119168 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0524 21:18:33.525994 140032543119168 saver.py:1298] Restoring parameters from /opt/models/wgs/model.ckpt. INFO:tensorflow:prediction_loop marked as finished. I0524 21:18:34.251420 140032543119168 error_handling.py:115] prediction_loop marked as finished. WARNING:tensorflow:Reraising captured error. W0524 21:18:34.251592 140032543119168 error_handling.py:149] Reraising captured error. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1375, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1303, in restore. sess",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:11462,modifiability,pac,packages,11462,"ng model_fn. I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn. INFO:tensorflow:TPU job name tpu_worker. I0524 21:18:33.019782 140032543119168 tpu_estimator.py:514] TPU job name tpu_worker. INFO:tensorflow:Graph was finalized. I0524 21:18:33.525068 140032543119168 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0524 21:18:33.525994 140032543119168 saver.py:1298] Restoring parameters from /opt/models/wgs/model.ckpt. INFO:tensorflow:prediction_loop marked as finished. I0524 21:18:34.251420 140032543119168 error_handling.py:115] prediction_loop marked as finished. WARNING:tensorflow:Reraising captured error. W0524 21:18:34.251592 140032543119168 error_handling.py:149] Reraising captured error. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1375, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1303, in restore. sess.run(self.saver_def.restore_op_name,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:11633,modifiability,pac,packages,11633," tpu_estimator.py:514] TPU job name tpu_worker. INFO:tensorflow:Graph was finalized. I0524 21:18:33.525068 140032543119168 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0524 21:18:33.525994 140032543119168 saver.py:1298] Restoring parameters from /opt/models/wgs/model.ckpt. INFO:tensorflow:prediction_loop marked as finished. I0524 21:18:34.251420 140032543119168 error_handling.py:115] prediction_loop marked as finished. WARNING:tensorflow:Reraising captured error. W0524 21:18:34.251592 140032543119168 error_handling.py:149] Reraising captured error. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1375, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1303, in restore. sess.run(self.saver_def.restore_op_name,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 967, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1190, in _",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:12267,modifiability,pac,packages,12267,"ack (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1375, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1303, in restore. sess.run(self.saver_def.restore_op_name,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 967, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1190, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1368, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1394, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' n",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:12415,modifiability,pac,packages,12415,"args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1303, in restore. sess.run(self.saver_def.restore_op_name,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 967, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1190, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1368, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1394, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:12575,modifiability,pac,packages,12575,"ict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1303, in restore. sess.run(self.saver_def.restore_op_name,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 967, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1190, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1368, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1394, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvaria",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:12740,modifiability,pac,packages,12740,"un_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1303, in restore. sess.run(self.saver_def.restore_op_name,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 967, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1190, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1368, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1394, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:12911,modifiability,pac,packages,12911,"ceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1303, in restore. sess.run(self.saver_def.restore_op_name,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 967, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1190, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1368, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1394, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, a",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:13389,modifiability,pac,packages,13389,"al/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 967, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1190, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1368, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1394, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:13615,modifiability,modul,module,13615,"y"", line 1190, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1368, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1394, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:13683,modifiability,pac,packages,13683," final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1368, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1394, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:14356,modifiability,pac,packages,14356,"usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:14534,modifiability,pac,packages,14534,"runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:14696,modifiability,pac,packages,14696,"ow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.8/d",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:14855,modifiability,pac,packages,14855,"bsl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=pr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:15033,modifiability,pac,packages,15033,"""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:15207,modifiability,pac,packages,15207,"google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.8/dist-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:15376,modifiability,pac,packages,15376,"mator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.8/dist-packages/tensorflo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:15554,modifiability,pac,packages,15554,"mator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 876, in _build. self.saver_def = self._builder._build_internal( # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:15704,modifiability,pac,packages,15704,"sorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 876, in _build. self.saver_def = self._builder._build_internal( # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal. restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,. File ""usr/lo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:15910,modifiability,pac,packages,15910,"line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 876, in _build. self.saver_def = self._builder._build_internal( # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal. restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps. self._AddRestoreOps(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.p",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:16075,modifiability,pac,packages,16075,"session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 876, in _build. self.saver_def = self._builder._build_internal( # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal. restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps. self._AddRestoreOps(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 335, in _AddRestoreOps. all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,. File ""usr/local/lib/python3.8/dist-packages/tensorflow",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:16194,modifiability,pac,packages,16194,"3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 876, in _build. self.saver_def = self._builder._build_internal( # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal. restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps. self._AddRestoreOps(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 335, in _AddRestoreOps. all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:16362,modifiability,pac,packages,16362,"n3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 876, in _build. self.saver_def = self._builder._build_internal( # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal. restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps. self._AddRestoreOps(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 335, in _AddRestoreOps. all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1490, in restore_v2. _, _, _op, _outputs = _op_def_library._apply_op_helper(. F",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:16549,modifiability,pac,packages,16549,"-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 876, in _build. self.saver_def = self._builder._build_internal( # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal. restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps. self._AddRestoreOps(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 335, in _AddRestoreOps. all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1490, in restore_v2. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 748, in _apply_op_helper. op = g._create_op_internal(op_type_name, inputs, dtypes=None,. Fi",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:16731,modifiability,pac,packages,16731,"ning/monitored_session.py"", line 236, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 876, in _build. self.saver_def = self._builder._build_internal( # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal. restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps. self._AddRestoreOps(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 335, in _AddRestoreOps. all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1490, in restore_v2. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 748, in _apply_op_helper. op = g._create_op_internal(op_type_name, inputs, dtypes=None,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3557, in _create_op_internal. ret = Operation(. File ""usr/local/lib/python3.8/dist-packages/tensor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:16871,modifiability,pac,packages,16871,"File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 876, in _build. self.saver_def = self._builder._build_internal( # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal. restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps. self._AddRestoreOps(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 335, in _AddRestoreOps. all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1490, in restore_v2. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 748, in _apply_op_helper. op = g._create_op_internal(op_type_name, inputs, dtypes=None,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3557, in _create_op_internal. ret = Operation(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 2045, in __init__. self._traceback = tf_stack.extract_stack_for_node(self._c_op). During handling of the",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:17060,modifiability,pac,packages,17060,"on3.8/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 876, in _build. self.saver_def = self._builder._build_internal( # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal. restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps. self._AddRestoreOps(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 335, in _AddRestoreOps. all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1490, in restore_v2. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 748, in _apply_op_helper. op = g._create_op_internal(op_type_name, inputs, dtypes=None,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3557, in _create_op_internal. ret = Operation(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 2045, in __init__. self._traceback = tf_stack.extract_stack_for_node(self._c_op). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:17235,modifiability,pac,packages,17235,", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 876, in _build. self.saver_def = self._builder._build_internal( # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal. restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps. self._AddRestoreOps(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 335, in _AddRestoreOps. all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1490, in restore_v2. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 748, in _apply_op_helper. op = g._create_op_internal(op_type_name, inputs, dtypes=None,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3557, in _create_op_internal. ret = Operation(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 2045, in __init__. self._traceback = tf_stack.extract_stack_for_node(self._c_op). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parse",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:17400,modifiability,pac,packages,17400,"py"", line 876, in _build. self.saver_def = self._builder._build_internal( # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal. restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps. self._AddRestoreOps(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 335, in _AddRestoreOps. all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1490, in restore_v2. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 748, in _apply_op_helper. op = g._create_op_internal(op_type_name, inputs, dtypes=None,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3557, in _create_op_internal. ret = Operation(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 2045, in __init__. self._traceback = tf_stack.extract_stack_for_node(self._c_op). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:17586,modifiability,pac,packages,17586,".py"", line 509, in _build_internal. restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps. self._AddRestoreOps(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 335, in _AddRestoreOps. all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1490, in restore_v2. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 748, in _apply_op_helper. op = g._create_op_internal(op_type_name, inputs, dtypes=None,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3557, in _create_op_internal. ret = Operation(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 2045, in __init__. self._traceback = tf_stack.extract_stack_for_node(self._c_op). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", li",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:17720,modifiability,pac,packages,17720,"8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps. self._AddRestoreOps(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 335, in _AddRestoreOps. all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1490, in restore_v2. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 748, in _apply_op_helper. op = g._create_op_internal(op_type_name, inputs, dtypes=None,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3557, in _create_op_internal. ret = Operation(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 2045, in __init__. self._traceback = tf_stack.extract_stack_for_node(self._c_op). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", lin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:18070,modifiability,modul,module,18070,"orflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1490, in restore_v2. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 748, in _apply_op_helper. op = g._create_op_internal(op_type_name, inputs, dtypes=None,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3557, in _create_op_internal. ret = Operation(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 2045, in __init__. self._traceback = tf_stack.extract_stack_for_node(self._c_op). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict. rendezvous.raise_errors(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors. si",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:18139,modifiability,pac,packages,18139,"_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1490, in restore_v2. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 748, in _apply_op_helper. op = g._create_op_internal(op_type_name, inputs, dtypes=None,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3557, in _create_op_internal. ret = Operation(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 2045, in __init__. self._traceback = tf_stack.extract_stack_for_node(self._c_op). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict. rendezvous.raise_errors(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors. six.reraise(typ, value, traceback). File ""/tmp/Bazel.runfiles_o0nxhusg/r",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:18817,modifiability,pac,packages,18817,"act_stack_for_node(self._c_op). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict. rendezvous.raise_errors(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors. six.reraise(typ, value, traceback). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/six_archive/six.py"", line 703, in reraise. raise value. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __in",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:18973,modifiability,pac,packages,18973,"nfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict. rendezvous.raise_errors(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors. six.reraise(typ, value, traceback). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/six_archive/six.py"", line 703, in reraise. raise value. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:19243,modifiability,pac,packages,19243,"_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict. rendezvous.raise_errors(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors. six.reraise(typ, value, traceback). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/six_archive/six.py"", line 703, in reraise. raise value. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""/usr/loc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:19422,modifiability,pac,packages,19422,"_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict. rendezvous.raise_errors(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors. six.reraise(typ, value, traceback). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/six_archive/six.py"", line 703, in reraise. raise value. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/loc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:19585,modifiability,pac,packages,19585,"ine 474, in main. call_variants(. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict. rendezvous.raise_errors(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors. six.reraise(typ, value, traceback). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/six_archive/six.py"", line 703, in reraise. raise value. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 662, in create_session. return self._get_session_manager().prepare_session(. ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:19745,modifiability,pac,packages,19745,"prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict. rendezvous.raise_errors(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors. six.reraise(typ, value, traceback). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/six_archive/six.py"", line 703, in reraise. raise value. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 662, in create_session. return self._get_session_manager().prepare_session(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 314, in prepare_session. sess, is_loaded_from_checkpoint = sel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:19924,modifiability,pac,packages,19924,"rrors(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors. six.reraise(typ, value, traceback). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/six_archive/six.py"", line 703, in reraise. raise value. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 662, in create_session. return self._get_session_manager().prepare_session(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 314, in prepare_session. sess, is_loaded_from_checkpoint = self._restore_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 233, in _restore_checkpoint. _restore_checkpoint_and_mayb",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:20099,modifiability,pac,packages,20099,"k). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/six_archive/six.py"", line 703, in reraise. raise value. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 662, in create_session. return self._get_session_manager().prepare_session(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 314, in prepare_session. sess, is_loaded_from_checkpoint = self._restore_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 233, in _restore_checkpoint. _restore_checkpoint_and_maybe_run_saved_model_initializers(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_sav",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:20269,modifiability,pac,packages,20269,"python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 662, in create_session. return self._get_session_manager().prepare_session(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 314, in prepare_session. sess, is_loaded_from_checkpoint = self._restore_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 233, in _restore_checkpoint. _restore_checkpoint_and_maybe_run_saved_model_initializers(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers. saver.restore(sess, path). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1339, in restore. raise _wrap_re",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:20448,modifiability,pac,packages,20448,"python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 662, in create_session. return self._get_session_manager().prepare_session(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 314, in prepare_session. sess, is_loaded_from_checkpoint = self._restore_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 233, in _restore_checkpoint. _restore_checkpoint_and_maybe_run_saved_model_initializers(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers. saver.restore(sess, path). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1339, in restore. raise _wrap_restore_error_with_msg(. tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current gr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:20625,modifiability,pac,packages,20625,"d_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 662, in create_session. return self._get_session_manager().prepare_session(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 314, in prepare_session. sess, is_loaded_from_checkpoint = self._restore_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 233, in _restore_checkpoint. _restore_checkpoint_and_maybe_run_saved_model_initializers(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers. saver.restore(sess, path). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1339, in restore. raise _wrap_restore_error_with_msg(. tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:20809,modifiability,pac,packages,20809," in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 662, in create_session. return self._get_session_manager().prepare_session(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 314, in prepare_session. sess, is_loaded_from_checkpoint = self._restore_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 233, in _restore_checkpoint. _restore_checkpoint_and_maybe_run_saved_model_initializers(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers. saver.restore(sess, path). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1339, in restore. raise _wrap_restore_error_with_msg(. tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/mode",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:20997,modifiability,pac,packages,20997,"it__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 662, in create_session. return self._get_session_manager().prepare_session(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 314, in prepare_session. sess, is_loaded_from_checkpoint = self._restore_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 233, in _restore_checkpoint. _restore_checkpoint_and_maybe_run_saved_model_initializers(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers. saver.restore(sess, path). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1339, in restore. raise _wrap_restore_error_with_msg(. tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:21189,modifiability,pac,packages,21189,"rn self._sess_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 662, in create_session. return self._get_session_manager().prepare_session(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 314, in prepare_session. sess, is_loaded_from_checkpoint = self._restore_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 233, in _restore_checkpoint. _restore_checkpoint_and_maybe_run_saved_model_initializers(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers. saver.restore(sess, path). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1339, in restore. raise _wrap_restore_error_with_msg(. tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:21899,modifiability,pac,packages,21899,"store_checkpoint_and_maybe_run_saved_model_initializers(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers. saver.restore(sess, path). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1339, in restore. raise _wrap_restore_error_with_msg(. tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:22125,modifiability,modul,module,22125,"ver.restore(sess, path). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1339, in restore. raise _wrap_restore_error_with_msg(. tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:22193,modifiability,pac,packages,22193,"/tensorflow/python/training/saver.py"", line 1339, in restore. raise _wrap_restore_error_with_msg(. tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:22866,modifiability,pac,packages,22866,"usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:23044,modifiability,pac,packages,23044,"runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:23206,modifiability,pac,packages,23206,"ow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.8/d",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:23365,modifiability,pac,packages,23365,"bsl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=pr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:23543,modifiability,pac,packages,23543,"""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:23717,modifiability,pac,packages,23717,"google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.8/dist-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:23886,modifiability,pac,packages,23886,"mator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.8/dist-packages/tensorflo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:24064,modifiability,pac,packages,24064,"mator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 876, in _build. self.saver_def = self._builder._build_internal( # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:24214,modifiability,pac,packages,24214,"sorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 876, in _build. self.saver_def = self._builder._build_internal( # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal. restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,. File ""usr/lo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:24420,modifiability,pac,packages,24420,"line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 876, in _build. self.saver_def = self._builder._build_internal( # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal. restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps. self._AddRestoreOps(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.p",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:24585,modifiability,pac,packages,24585,"session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 876, in _build. self.saver_def = self._builder._build_internal( # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal. restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps. self._AddRestoreOps(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 335, in _AddRestoreOps. all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,. File ""usr/local/lib/python3.8/dist-packages/tensorflow",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:24704,modifiability,pac,packages,24704,"3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 876, in _build. self.saver_def = self._builder._build_internal( # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal. restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps. self._AddRestoreOps(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 335, in _AddRestoreOps. all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:24872,modifiability,pac,packages,24872,"n3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 876, in _build. self.saver_def = self._builder._build_internal( # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal. restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps. self._AddRestoreOps(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 335, in _AddRestoreOps. all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1490, in restore_v2. _, _, _op, _outputs = _op_def_library._apply_op_helper(. F",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:25059,modifiability,pac,packages,25059,"-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 876, in _build. self.saver_def = self._builder._build_internal( # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal. restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps. self._AddRestoreOps(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 335, in _AddRestoreOps. all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1490, in restore_v2. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 748, in _apply_op_helper. op = g._create_op_internal(op_type_name, inputs, dtypes=None,. Fi",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:25241,modifiability,pac,packages,25241,"ning/monitored_session.py"", line 236, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 876, in _build. self.saver_def = self._builder._build_internal( # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal. restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps. self._AddRestoreOps(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 335, in _AddRestoreOps. all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1490, in restore_v2. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 748, in _apply_op_helper. op = g._create_op_internal(op_type_name, inputs, dtypes=None,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3557, in _create_op_internal. ret = Operation(. File ""usr/local/lib/python3.8/dist-packages/tensor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:25381,modifiability,pac,packages,25381,"File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 876, in _build. self.saver_def = self._builder._build_internal( # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal. restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps. self._AddRestoreOps(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 335, in _AddRestoreOps. all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1490, in restore_v2. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 748, in _apply_op_helper. op = g._create_op_internal(op_type_name, inputs, dtypes=None,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3557, in _create_op_internal. ret = Operation(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 2045, in __init__. self._traceback = tf_stack.extract_stack_for_node(self._c_op). real 0m10.757s. user 0",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:25570,modifiability,pac,packages,25570,"on3.8/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 876, in _build. self.saver_def = self._builder._build_internal( # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal. restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps. self._AddRestoreOps(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 335, in _AddRestoreOps. all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1490, in restore_v2. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 748, in _apply_op_helper. op = g._create_op_internal(op_type_name, inputs, dtypes=None,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3557, in _create_op_internal. ret = Operation(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 2045, in __init__. self._traceback = tf_stack.extract_stack_for_node(self._c_op). real 0m10.757s. user 0m13.496s. sys 0m5.144s. ```. This same command works fine without using TPUs on this system, and it looks like the TPU node is being recognized by deepvariant. Is there something I'm missin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:25745,modifiability,pac,packages,25745,"tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 876, in _build. self.saver_def = self._builder._build_internal( # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal. restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps. self._AddRestoreOps(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 335, in _AddRestoreOps. all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1490, in restore_v2. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 748, in _apply_op_helper. op = g._create_op_internal(op_type_name, inputs, dtypes=None,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3557, in _create_op_internal. ret = Operation(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 2045, in __init__. self._traceback = tf_stack.extract_stack_for_node(self._c_op). real 0m10.757s. user 0m13.496s. sys 0m5.144s. ```. This same command works fine without using TPUs on this system, and it looks like the TPU node is being recognized by deepvariant. Is there something I'm missing for call_variants?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:25910,modifiability,pac,packages,25910,"tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 876, in _build. self.saver_def = self._builder._build_internal( # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal. restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps. self._AddRestoreOps(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 335, in _AddRestoreOps. all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1490, in restore_v2. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 748, in _apply_op_helper. op = g._create_op_internal(op_type_name, inputs, dtypes=None,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3557, in _create_op_internal. ret = Operation(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 2045, in __init__. self._traceback = tf_stack.extract_stack_for_node(self._c_op). real 0m10.757s. user 0m13.496s. sys 0m5.144s. ```. This same command works fine without using TPUs on this system, and it looks like the TPU node is being recognized by deepvariant. Is there something I'm missing for call_variants?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:26096,modifiability,pac,packages,26096,"tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 876, in _build. self.saver_def = self._builder._build_internal( # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal. restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps. self._AddRestoreOps(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 335, in _AddRestoreOps. all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1490, in restore_v2. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 748, in _apply_op_helper. op = g._create_op_internal(op_type_name, inputs, dtypes=None,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3557, in _create_op_internal. ret = Operation(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 2045, in __init__. self._traceback = tf_stack.extract_stack_for_node(self._c_op). real 0m10.757s. user 0m13.496s. sys 0m5.144s. ```. This same command works fine without using TPUs on this system, and it looks like the TPU node is being recognized by deepvariant. Is there something I'm missing for call_variants?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:26230,modifiability,pac,packages,26230,"tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 876, in _build. self.saver_def = self._builder._build_internal( # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal. restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps. self._AddRestoreOps(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 335, in _AddRestoreOps. all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1490, in restore_v2. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 748, in _apply_op_helper. op = g._create_op_internal(op_type_name, inputs, dtypes=None,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3557, in _create_op_internal. ret = Operation(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 2045, in __init__. self._traceback = tf_stack.extract_stack_for_node(self._c_op). real 0m10.757s. user 0m13.496s. sys 0m5.144s. ```. This same command works fine without using TPUs on this system, and it looks like the TPU node is being recognized by deepvariant. Is there something I'm missing for call_variants?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:728,performance,error,error,728,"Issue with TPU Node v3-8 in call variants step; I am trying to use deepvariant to call variants using a TPU Node v3-8, but I am running into a persistent issue. Here is the command I am using:. ```bash. docker run \. -v `pwd`:`pwd` -w `pwd` \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --call_variants_extra_args use_tpu=true,tpu_name=""variantcaller-node1"",tpu_zone=""europe-west4-a"" \. --model_type=WGS \. --ref=""input/data/${REF}"" \. --reads=""input/data/${BAM}"" \. --output_vcf=""output/${OUTPUT_VCF}"" \. --output_gvcf=""output/${OUTPUT_GVCF}"" \. --regions chr20 \. --num_shards=$(nproc) \. --intermediate_results_dir /output/intermediate_results_dir. ```. However, I am seeing the following error in the call variants step. ```bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@96.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --openvino_model_dir ""/output/intermediate_results_dir"" --tpu_name ""variantcaller-node1"" --tpu_zone ""europe-west4-a"" --use_tpu. I0524 21:18:26.485428 140032543119168 transport.py:157] Attempting refresh to obtain initial access_token. I0524 21:18:26.576728 140032543119168 call_variants.py:336] Shape of input examples: [100, 221, 6]. I0524 21:18:26.579230 140032543119168 call_variants.py:361] /opt/models/wgs/model.ckpt.input_shape has the correct shape: [100, 221, 6]. 2022-05-24 21:18:26.581705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-05-24 21:18:26.586196: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:803,performance,time,time,803,"Issue with TPU Node v3-8 in call variants step; I am trying to use deepvariant to call variants using a TPU Node v3-8, but I am running into a persistent issue. Here is the command I am using:. ```bash. docker run \. -v `pwd`:`pwd` -w `pwd` \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --call_variants_extra_args use_tpu=true,tpu_name=""variantcaller-node1"",tpu_zone=""europe-west4-a"" \. --model_type=WGS \. --ref=""input/data/${REF}"" \. --reads=""input/data/${BAM}"" \. --output_vcf=""output/${OUTPUT_VCF}"" \. --output_gvcf=""output/${OUTPUT_GVCF}"" \. --regions chr20 \. --num_shards=$(nproc) \. --intermediate_results_dir /output/intermediate_results_dir. ```. However, I am seeing the following error in the call variants step. ```bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@96.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --openvino_model_dir ""/output/intermediate_results_dir"" --tpu_name ""variantcaller-node1"" --tpu_zone ""europe-west4-a"" --use_tpu. I0524 21:18:26.485428 140032543119168 transport.py:157] Attempting refresh to obtain initial access_token. I0524 21:18:26.576728 140032543119168 call_variants.py:336] Shape of input examples: [100, 221, 6]. I0524 21:18:26.579230 140032543119168 call_variants.py:361] /opt/models/wgs/model.ckpt.input_shape has the correct shape: [100, 221, 6]. 2022-05-24 21:18:26.581705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-05-24 21:18:26.586196: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:1617,performance,optimiz,optimized,1617,") \. --intermediate_results_dir /output/intermediate_results_dir. ```. However, I am seeing the following error in the call variants step. ```bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@96.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --openvino_model_dir ""/output/intermediate_results_dir"" --tpu_name ""variantcaller-node1"" --tpu_zone ""europe-west4-a"" --use_tpu. I0524 21:18:26.485428 140032543119168 transport.py:157] Attempting refresh to obtain initial access_token. I0524 21:18:26.576728 140032543119168 call_variants.py:336] Shape of input examples: [100, 221, 6]. I0524 21:18:26.579230 140032543119168 call_variants.py:361] /opt/models/wgs/model.ckpt.input_shape has the correct shape: [100, 221, 6]. 2022-05-24 21:18:26.581705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-05-24 21:18:26.586196: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2022-05-24 21:18:26.587127: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2000160000 Hz. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp_f348kd0. W0524 21:18:26.619681 140032543119168 estimator.py:1846] Using temporary folder as model directory: /tmp/tmp_f348kd0. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp_f348kd0', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true. graph_options {",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:1651,performance,Network,Network,1651,"output/intermediate_results_dir. ```. However, I am seeing the following error in the call variants step. ```bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@96.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --openvino_model_dir ""/output/intermediate_results_dir"" --tpu_name ""variantcaller-node1"" --tpu_zone ""europe-west4-a"" --use_tpu. I0524 21:18:26.485428 140032543119168 transport.py:157] Attempting refresh to obtain initial access_token. I0524 21:18:26.576728 140032543119168 call_variants.py:336] Shape of input examples: [100, 221, 6]. I0524 21:18:26.579230 140032543119168 call_variants.py:361] /opt/models/wgs/model.ckpt.input_shape has the correct shape: [100, 221, 6]. 2022-05-24 21:18:26.581705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-05-24 21:18:26.586196: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2022-05-24 21:18:26.587127: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2000160000 Hz. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp_f348kd0. W0524 21:18:26.619681 140032543119168 estimator.py:1846] Using temporary folder as model directory: /tmp/tmp_f348kd0. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp_f348kd0', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimiz",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:1697,performance,CPU,CPU,1697,"r, I am seeing the following error in the call variants step. ```bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@96.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --openvino_model_dir ""/output/intermediate_results_dir"" --tpu_name ""variantcaller-node1"" --tpu_zone ""europe-west4-a"" --use_tpu. I0524 21:18:26.485428 140032543119168 transport.py:157] Attempting refresh to obtain initial access_token. I0524 21:18:26.576728 140032543119168 call_variants.py:336] Shape of input examples: [100, 221, 6]. I0524 21:18:26.579230 140032543119168 call_variants.py:361] /opt/models/wgs/model.ckpt.input_shape has the correct shape: [100, 221, 6]. 2022-05-24 21:18:26.581705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-05-24 21:18:26.586196: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2022-05-24 21:18:26.587127: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2000160000 Hz. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp_f348kd0. W0524 21:18:26.619681 140032543119168 estimator.py:1846] Using temporary folder as model directory: /tmp/tmp_f348kd0. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp_f348kd0', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_keep_checkpoin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:1717,performance,perform,performance-critical,1717," error in the call variants step. ```bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@96.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --openvino_model_dir ""/output/intermediate_results_dir"" --tpu_name ""variantcaller-node1"" --tpu_zone ""europe-west4-a"" --use_tpu. I0524 21:18:26.485428 140032543119168 transport.py:157] Attempting refresh to obtain initial access_token. I0524 21:18:26.576728 140032543119168 call_variants.py:336] Shape of input examples: [100, 221, 6]. I0524 21:18:26.579230 140032543119168 call_variants.py:361] /opt/models/wgs/model.ckpt.input_shape has the correct shape: [100, 221, 6]. 2022-05-24 21:18:26.581705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-05-24 21:18:26.586196: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2022-05-24 21:18:26.587127: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2000160000 Hz. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp_f348kd0. W0524 21:18:26.619681 140032543119168 estimator.py:1846] Using temporary folder as model directory: /tmp/tmp_f348kd0. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp_f348kd0', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_keep_checkpoint_max': 100000, '_keep_check",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:2001,performance,Tune,Tune,2001,"point ""/opt/models/wgs/model.ckpt"" --openvino_model_dir ""/output/intermediate_results_dir"" --tpu_name ""variantcaller-node1"" --tpu_zone ""europe-west4-a"" --use_tpu. I0524 21:18:26.485428 140032543119168 transport.py:157] Attempting refresh to obtain initial access_token. I0524 21:18:26.576728 140032543119168 call_variants.py:336] Shape of input examples: [100, 221, 6]. I0524 21:18:26.579230 140032543119168 call_variants.py:361] /opt/models/wgs/model.ckpt.input_shape has the correct shape: [100, 221, 6]. 2022-05-24 21:18:26.581705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-05-24 21:18:26.586196: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2022-05-24 21:18:26.587127: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2000160000 Hz. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp_f348kd0. W0524 21:18:26.619681 140032543119168 estimator.py:1846] Using temporary folder as model directory: /tmp/tmp_f348kd0. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp_f348kd0', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_che",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:2050,performance,perform,performance,2050,"ir ""/output/intermediate_results_dir"" --tpu_name ""variantcaller-node1"" --tpu_zone ""europe-west4-a"" --use_tpu. I0524 21:18:26.485428 140032543119168 transport.py:157] Attempting refresh to obtain initial access_token. I0524 21:18:26.576728 140032543119168 call_variants.py:336] Shape of input examples: [100, 221, 6]. I0524 21:18:26.579230 140032543119168 call_variants.py:361] /opt/models/wgs/model.ckpt.input_shape has the correct shape: [100, 221, 6]. 2022-05-24 21:18:26.581705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-05-24 21:18:26.586196: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2022-05-24 21:18:26.587127: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2000160000 Hz. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp_f348kd0. W0524 21:18:26.619681 140032543119168 estimator.py:1846] Using temporary folder as model directory: /tmp/tmp_f348kd0. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp_f348kd0', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:2150,performance,CPU,CPU,2150,"a"" --use_tpu. I0524 21:18:26.485428 140032543119168 transport.py:157] Attempting refresh to obtain initial access_token. I0524 21:18:26.576728 140032543119168 call_variants.py:336] Shape of input examples: [100, 221, 6]. I0524 21:18:26.579230 140032543119168 call_variants.py:361] /opt/models/wgs/model.ckpt.input_shape has the correct shape: [100, 221, 6]. 2022-05-24 21:18:26.581705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-05-24 21:18:26.586196: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2022-05-24 21:18:26.587127: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2000160000 Hz. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp_f348kd0. W0524 21:18:26.619681 140032543119168 estimator.py:1846] Using temporary folder as model directory: /tmp/tmp_f348kd0. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp_f348kd0', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:6424,performance,CPU,CPU,6424,"ata. I0524 21:18:26.625535 140032543119168 tpu_system_metadata.py:90] Querying Tensorflow master (grpc://10.73.74.226:8470) for TPU system metadata. 2022-05-24 21:18:26.626490: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:373] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created. INFO:tensorflow:Found TPU system:. I0524 21:18:26.631762 140032543119168 tpu_system_metadata.py:159] Found TPU system:. INFO:tensorflow:*** Num TPU Cores: 8. I0524 21:18:26.631872 140032543119168 tpu_system_metadata.py:160] *** Num TPU Cores: 8. INFO:tensorflow:*** Num TPU Workers: 1. I0524 21:18:26.631940 140032543119168 tpu_system_metadata.py:161] *** Num TPU Workers: 1. INFO:tensorflow:*** Num TPU Cores Per Worker: 8. I0524 21:18:26.631998 140032543119168 tpu_system_metadata.py:162] *** Num TPU Cores Per Worker: 8. INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, T",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:6431,performance,CPU,CPU,6431,"524 21:18:26.625535 140032543119168 tpu_system_metadata.py:90] Querying Tensorflow master (grpc://10.73.74.226:8470) for TPU system metadata. 2022-05-24 21:18:26.626490: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:373] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created. INFO:tensorflow:Found TPU system:. I0524 21:18:26.631762 140032543119168 tpu_system_metadata.py:159] Found TPU system:. INFO:tensorflow:*** Num TPU Cores: 8. I0524 21:18:26.631872 140032543119168 tpu_system_metadata.py:160] *** Num TPU Cores: 8. INFO:tensorflow:*** Num TPU Workers: 1. I0524 21:18:26.631940 140032543119168 tpu_system_metadata.py:161] *** Num TPU Workers: 1. INFO:tensorflow:*** Num TPU Cores Per Worker: 8. I0524 21:18:26.631998 140032543119168 tpu_system_metadata.py:162] *** Num TPU Cores Per Worker: 8. INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 171",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:6608,performance,CPU,CPU,6608,"rflow/core/distributed_runtime/rpc/grpc_session.cc:373] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created. INFO:tensorflow:Found TPU system:. I0524 21:18:26.631762 140032543119168 tpu_system_metadata.py:159] Found TPU system:. INFO:tensorflow:*** Num TPU Cores: 8. I0524 21:18:26.631872 140032543119168 tpu_system_metadata.py:160] *** Num TPU Cores: 8. INFO:tensorflow:*** Num TPU Workers: 1. I0524 21:18:26.631940 140032543119168 tpu_system_metadata.py:161] *** Num TPU Workers: 1. INFO:tensorflow:*** Num TPU Cores Per Worker: 8. I0524 21:18:26.631998 140032543119168 tpu_system_metadata.py:162] *** Num TPU Cores Per Worker: 8. INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/devic",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:6615,performance,CPU,CPU,6615,"ore/distributed_runtime/rpc/grpc_session.cc:373] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created. INFO:tensorflow:Found TPU system:. I0524 21:18:26.631762 140032543119168 tpu_system_metadata.py:159] Found TPU system:. INFO:tensorflow:*** Num TPU Cores: 8. I0524 21:18:26.631872 140032543119168 tpu_system_metadata.py:160] *** Num TPU Cores: 8. INFO:tensorflow:*** Num TPU Workers: 1. I0524 21:18:26.631940 140032543119168 tpu_system_metadata.py:161] *** Num TPU Workers: 1. INFO:tensorflow:*** Num TPU Cores Per Worker: 8. I0524 21:18:26.631998 140032543119168 tpu_system_metadata.py:162] *** Num TPU Cores Per Worker: 8. INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:11171,performance,error,error,11171,"r/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn. INFO:tensorflow:TPU job name tpu_worker. I0524 21:18:33.019782 140032543119168 tpu_estimator.py:514] TPU job name tpu_worker. INFO:tensorflow:Graph was finalized. I0524 21:18:33.525068 140032543119168 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0524 21:18:33.525994 140032543119168 saver.py:1298] Restoring parameters from /opt/models/wgs/model.ckpt. INFO:tensorflow:prediction_loop marked as finished. I0524 21:18:34.251420 140032543119168 error_handling.py:115] prediction_loop marked as finished. WARNING:tensorflow:Reraising captured error. W0524 21:18:34.251592 140032543119168 error_handling.py:149] Reraising captured error. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1375, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:11258,performance,error,error,11258,"2: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn. INFO:tensorflow:TPU job name tpu_worker. I0524 21:18:33.019782 140032543119168 tpu_estimator.py:514] TPU job name tpu_worker. INFO:tensorflow:Graph was finalized. I0524 21:18:33.525068 140032543119168 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0524 21:18:33.525994 140032543119168 saver.py:1298] Restoring parameters from /opt/models/wgs/model.ckpt. INFO:tensorflow:prediction_loop marked as finished. I0524 21:18:34.251420 140032543119168 error_handling.py:115] prediction_loop marked as finished. WARNING:tensorflow:Reraising captured error. W0524 21:18:34.251592 140032543119168 error_handling.py:149] Reraising captured error. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1375, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.8",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:18910,performance,rendezv,rendezvous,18910,"rred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict. rendezvous.raise_errors(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors. six.reraise(typ, value, traceback). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/six_archive/six.py"", line 703, in reraise. raise value. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:21584,performance,error,error,21584,". File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 314, in prepare_session. sess, is_loaded_from_checkpoint = self._restore_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 233, in _restore_checkpoint. _restore_checkpoint_and_maybe_run_saved_model_initializers(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers. saver.restore(sess, path). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1339, in restore. raise _wrap_restore_error_with_msg(. tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/co",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:998,reliability,checkpoint,checkpoint,998,"ue with TPU Node v3-8 in call variants step; I am trying to use deepvariant to call variants using a TPU Node v3-8, but I am running into a persistent issue. Here is the command I am using:. ```bash. docker run \. -v `pwd`:`pwd` -w `pwd` \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --call_variants_extra_args use_tpu=true,tpu_name=""variantcaller-node1"",tpu_zone=""europe-west4-a"" \. --model_type=WGS \. --ref=""input/data/${REF}"" \. --reads=""input/data/${BAM}"" \. --output_vcf=""output/${OUTPUT_VCF}"" \. --output_gvcf=""output/${OUTPUT_GVCF}"" \. --regions chr20 \. --num_shards=$(nproc) \. --intermediate_results_dir /output/intermediate_results_dir. ```. However, I am seeing the following error in the call variants step. ```bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@96.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --openvino_model_dir ""/output/intermediate_results_dir"" --tpu_name ""variantcaller-node1"" --tpu_zone ""europe-west4-a"" --use_tpu. I0524 21:18:26.485428 140032543119168 transport.py:157] Attempting refresh to obtain initial access_token. I0524 21:18:26.576728 140032543119168 call_variants.py:336] Shape of input examples: [100, 221, 6]. I0524 21:18:26.579230 140032543119168 call_variants.py:361] /opt/models/wgs/model.ckpt.input_shape has the correct shape: [100, 221, 6]. 2022-05-24 21:18:26.581705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-05-24 21:18:26.586196: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tu",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:6348,reliability,Availab,Available,6348,"uerying Tensorflow master (grpc://10.73.74.226:8470) for TPU system metadata. I0524 21:18:26.625535 140032543119168 tpu_system_metadata.py:90] Querying Tensorflow master (grpc://10.73.74.226:8470) for TPU system metadata. 2022-05-24 21:18:26.626490: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:373] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created. INFO:tensorflow:Found TPU system:. I0524 21:18:26.631762 140032543119168 tpu_system_metadata.py:159] Found TPU system:. INFO:tensorflow:*** Num TPU Cores: 8. I0524 21:18:26.631872 140032543119168 tpu_system_metadata.py:160] *** Num TPU Cores: 8. INFO:tensorflow:*** Num TPU Workers: 1. I0524 21:18:26.631940 140032543119168 tpu_system_metadata.py:161] *** Num TPU Workers: 1. INFO:tensorflow:*** Num TPU Cores Per Worker: 8. I0524 21:18:26.631998 140032543119168 tpu_system_metadata.py:162] *** Num TPU Cores Per Worker: 8. INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available D",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:6532,reliability,Availab,Available,6532,"74.226:8470) for TPU system metadata. 2022-05-24 21:18:26.626490: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:373] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created. INFO:tensorflow:Found TPU system:. I0524 21:18:26.631762 140032543119168 tpu_system_metadata.py:159] Found TPU system:. INFO:tensorflow:*** Num TPU Cores: 8. I0524 21:18:26.631872 140032543119168 tpu_system_metadata.py:160] *** Num TPU Cores: 8. INFO:tensorflow:*** Num TPU Workers: 1. I0524 21:18:26.631940 140032543119168 tpu_system_metadata.py:161] *** Num TPU Workers: 1. INFO:tensorflow:*** Num TPU Cores Per Worker: 8. I0524 21:18:26.631998 140032543119168 tpu_system_metadata.py:162] *** Num TPU Cores Per Worker: 8. INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** A",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:6666,reliability,Availab,Available,6666,"ession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created. INFO:tensorflow:Found TPU system:. I0524 21:18:26.631762 140032543119168 tpu_system_metadata.py:159] Found TPU system:. INFO:tensorflow:*** Num TPU Cores: 8. I0524 21:18:26.631872 140032543119168 tpu_system_metadata.py:160] *** Num TPU Cores: 8. INFO:tensorflow:*** Num TPU Workers: 1. I0524 21:18:26.631940 140032543119168 tpu_system_metadata.py:161] *** Num TPU Workers: 1. INFO:tensorflow:*** Num TPU Cores Per Worker: 8. I0524 21:18:26.631998 140032543119168 tpu_system_metadata.py:162] *** Num TPU Cores Per Worker: 8. INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:6860,reliability,Availab,Available,6860,"32543119168 tpu_system_metadata.py:159] Found TPU system:. INFO:tensorflow:*** Num TPU Cores: 8. I0524 21:18:26.631872 140032543119168 tpu_system_metadata.py:160] *** Num TPU Cores: 8. INFO:tensorflow:*** Num TPU Workers: 1. I0524 21:18:26.631940 140032543119168 tpu_system_metadata.py:161] *** Num TPU Workers: 1. INFO:tensorflow:*** Num TPU Cores Per Worker: 8. I0524 21:18:26.631998 140032543119168 tpu_system_metadata.py:162] *** Num TPU Cores Per Worker: 8. INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:7004,reliability,Availab,Available,7004,"m_metadata.py:160] *** Num TPU Cores: 8. INFO:tensorflow:*** Num TPU Workers: 1. I0524 21:18:26.631940 140032543119168 tpu_system_metadata.py:161] *** Num TPU Workers: 1. INFO:tensorflow:*** Num TPU Cores Per Worker: 8. I0524 21:18:26.631998 140032543119168 tpu_system_metadata.py:162] *** Num TPU Cores Per Worker: 8. INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:7198,reliability,Availab,Available,7198," TPU Cores Per Worker: 8. I0524 21:18:26.631998 140032543119168 tpu_system_metadata.py:162] *** Num TPU Cores Per Worker: 8. INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:1",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:7342,reliability,Availab,Available,7342," Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:7536,reliability,Availab,Available,7536," Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:16",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:7680,reliability,Availab,Available,7680,"DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:7874,reliability,Availab,Available,7874,"DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:8018,reliability,Availab,Available,8018,"DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:8211,reliability,Availab,Available,8211,"_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:8354,reliability,Availab,Available,8354," _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:8548,reliability,Availab,Available,8548," _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_m",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:8692,reliability,Availab,Available,8692," _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 229118620",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:8885,reliability,Availab,Available,8885,": _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). I0524 21:18:26.632941 1400325",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:9028,reliability,Availab,Available,9028,"e: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). I0524 21:18:26.632941 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 171798",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:9222,reliability,Availab,Available,9222,": _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). I0524 21:18:26.632941 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). INFO:tensorflow:Calling model_fn. I0524 21:18:26.633588 140032543119168 estimator.py:1162] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:9366,reliability,Availab,Available,9366," _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). I0524 21:18:26.632941 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). INFO:tensorflow:Calling model_fn. I0524 21:18:26.633588 140032543119168 estimator.py:1162] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:9573,reliability,Availab,Available,9573,"butes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). I0524 21:18:26.632941 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). INFO:tensorflow:Calling model_fn. I0524 21:18:26.633588 140032543119168 estimator.py:1162] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn. INFO:tensorflow:TPU",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:9730,reliability,Availab,Available,9730,"u_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). I0524 21:18:26.632941 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). INFO:tensorflow:Calling model_fn. I0524 21:18:26.633588 140032543119168 estimator.py:1162] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn. INFO:tensorflow:TPU job name tpu_worker. I0524 21:18:33.019782 140032543119168 tpu_estimator.py:514] TPU job name tpu_worker. INFO:tensorflow:Graph was finalized. I0524 21:18:3",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:9931,reliability,Availab,Available,9931,"/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). I0524 21:18:26.632941 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). INFO:tensorflow:Calling model_fn. I0524 21:18:26.633588 140032543119168 estimator.py:1162] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn. INFO:tensorflow:TPU job name tpu_worker. I0524 21:18:33.019782 140032543119168 tpu_estimator.py:514] TPU job name tpu_worker. INFO:tensorflow:Graph was finalized. I0524 21:18:33.525068 140032543119168 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0524 21:18:33.525994 140032543119168 saver.py:1298] Restor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:10823,reliability,Restor,Restoring,10823,"179869184, 7884439564287565365). I0524 21:18:26.632941 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). INFO:tensorflow:Calling model_fn. I0524 21:18:26.633588 140032543119168 estimator.py:1162] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn. INFO:tensorflow:TPU job name tpu_worker. I0524 21:18:33.019782 140032543119168 tpu_estimator.py:514] TPU job name tpu_worker. INFO:tensorflow:Graph was finalized. I0524 21:18:33.525068 140032543119168 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0524 21:18:33.525994 140032543119168 saver.py:1298] Restoring parameters from /opt/models/wgs/model.ckpt. INFO:tensorflow:prediction_loop marked as finished. I0524 21:18:34.251420 140032543119168 error_handling.py:115] prediction_loop marked as finished. WARNING:tensorflow:Reraising captured error. W0524 21:18:34.251592 140032543119168 error_handling.py:149] Reraising captured error. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1375, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_im",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:10930,reliability,Restor,Restoring,10930,"lable Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). INFO:tensorflow:Calling model_fn. I0524 21:18:26.633588 140032543119168 estimator.py:1162] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn. INFO:tensorflow:TPU job name tpu_worker. I0524 21:18:33.019782 140032543119168 tpu_estimator.py:514] TPU job name tpu_worker. INFO:tensorflow:Graph was finalized. I0524 21:18:33.525068 140032543119168 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0524 21:18:33.525994 140032543119168 saver.py:1298] Restoring parameters from /opt/models/wgs/model.ckpt. INFO:tensorflow:prediction_loop marked as finished. I0524 21:18:34.251420 140032543119168 error_handling.py:115] prediction_loop marked as finished. WARNING:tensorflow:Reraising captured error. W0524 21:18:34.251592 140032543119168 error_handling.py:149] Reraising captured error. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1375, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:11937,reliability,Fail,Failed,11937,"parameters from /opt/models/wgs/model.ckpt. INFO:tensorflow:prediction_loop marked as finished. I0524 21:18:34.251420 140032543119168 error_handling.py:115] prediction_loop marked as finished. WARNING:tensorflow:Reraising captured error. W0524 21:18:34.251592 140032543119168 error_handling.py:149] Reraising captured error. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1375, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1303, in restore. sess.run(self.saver_def.restore_op_name,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 967, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1190, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1368, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/cl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:12328,reliability,restor,restore,12328,"dist-packages/tensorflow/python/client/session.py"", line 1375, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1303, in restore. sess.run(self.saver_def.restore_op_name,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 967, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1190, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1368, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1394, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node s",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:13166,reliability,Fail,Failed,13166,"ther exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1303, in restore. sess.run(self.saver_def.restore_op_name,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 967, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1190, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1368, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1394, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:14642,reliability,Monitor,MonitoredSession,14642,"le ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._sc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:14785,reliability,Monitor,MonitoredSession,14785,"lags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in finalize. self._s",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:17183,reliability,sli,slices,17183,".8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 876, in _build. self.saver_def = self._builder._build_internal( # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal. restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps. self._AddRestoreOps(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 335, in _AddRestoreOps. all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1490, in restore_v2. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 748, in _apply_op_helper. op = g._create_op_internal(op_type_name, inputs, dtypes=None,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3557, in _create_op_internal. ret = Operation(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 2045, in __init__. self._traceback = tf_stack.extract_stack_for_node(self._c_op). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", li",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:19530,reliability,Monitor,MonitoredSession,19530,"google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict. rendezvous.raise_errors(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors. six.reraise(typ, value, traceback). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/six_archive/six.py"", line 703, in reraise. raise value. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 662, in create_session. re",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:19674,reliability,Monitor,MonitoredSession,19674,"variant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict. rendezvous.raise_errors(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors. six.reraise(typ, value, traceback). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/six_archive/six.py"", line 703, in reraise. raise value. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 662, in create_session. return self._get_session_manager().prepare_session(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:21132,reliability,restor,restore,21132,"onitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 662, in create_session. return self._get_session_manager().prepare_session(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 314, in prepare_session. sess, is_loaded_from_checkpoint = self._restore_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 233, in _restore_checkpoint. _restore_checkpoint_and_maybe_run_saved_model_initializers(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers. saver.restore(sess, path). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1339, in restore. raise _wrap_restore_error_with_msg(. tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:21250,reliability,restor,restore,21250,"python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 662, in create_session. return self._get_session_manager().prepare_session(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 314, in prepare_session. sess, is_loaded_from_checkpoint = self._restore_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 233, in _restore_checkpoint. _restore_checkpoint_and_maybe_run_saved_model_initializers(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers. saver.restore(sess, path). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1339, in restore. raise _wrap_restore_error_with_msg(. tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:21358,reliability,Restor,Restoring,21358,"f_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 662, in create_session. return self._get_session_manager().prepare_session(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 314, in prepare_session. sess, is_loaded_from_checkpoint = self._restore_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 233, in _restore_checkpoint. _restore_checkpoint_and_maybe_run_saved_model_initializers(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers. saver.restore(sess, path). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1339, in restore. raise _wrap_restore_error_with_msg(. tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runf",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:21373,reliability,checkpoint,checkpoint,21373,"session_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 662, in create_session. return self._get_session_manager().prepare_session(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 314, in prepare_session. sess, is_loaded_from_checkpoint = self._restore_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 233, in _restore_checkpoint. _restore_checkpoint_and_maybe_run_saved_model_initializers(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers. saver.restore(sess, path). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1339, in restore. raise _wrap_restore_error_with_msg(. tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/ab",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:21384,reliability,fail,failed,21384,"reator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 662, in create_session. return self._get_session_manager().prepare_session(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 314, in prepare_session. sess, is_loaded_from_checkpoint = self._restore_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 233, in _restore_checkpoint. _restore_checkpoint_and_maybe_run_saved_model_initializers(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers. saver.restore(sess, path). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1339, in restore. raise _wrap_restore_error_with_msg(. tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:21479,reliability,checkpoint,checkpoint,21479,"monitored_session.py"", line 662, in create_session. return self._get_session_manager().prepare_session(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 314, in prepare_session. sess, is_loaded_from_checkpoint = self._restore_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 233, in _restore_checkpoint. _restore_checkpoint_and_maybe_run_saved_model_initializers(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers. saver.restore(sess, path). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1339, in restore. raise _wrap_restore_error_with_msg(. tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/ab",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:21563,reliability,checkpoint,checkpoint,21563,"().prepare_session(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 314, in prepare_session. sess, is_loaded_from_checkpoint = self._restore_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 233, in _restore_checkpoint. _restore_checkpoint_and_maybe_run_saved_model_initializers(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers. saver.restore(sess, path). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1339, in restore. raise _wrap_restore_error_with_msg(. tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:21676,reliability,Fail,Failed,21676,""", line 314, in prepare_session. sess, is_loaded_from_checkpoint = self._restore_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 233, in _restore_checkpoint. _restore_checkpoint_and_maybe_run_saved_model_initializers(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers. saver.restore(sess, path). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1339, in restore. raise _wrap_restore_error_with_msg(. tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:23152,reliability,Monitor,MonitoredSession,23152,"le ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._sc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:23295,reliability,Monitor,MonitoredSession,23295,"lags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in finalize. self._s",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:25693,reliability,sli,slices,25693,"tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 876, in _build. self.saver_def = self._builder._build_internal( # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal. restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps. self._AddRestoreOps(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 335, in _AddRestoreOps. all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1490, in restore_v2. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 748, in _apply_op_helper. op = g._create_op_internal(op_type_name, inputs, dtypes=None,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3557, in _create_op_internal. ret = Operation(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 2045, in __init__. self._traceback = tf_stack.extract_stack_for_node(self._c_op). real 0m10.757s. user 0m13.496s. sys 0m5.144s. ```. This same command works fine without using TPUs on this system, and it looks like the TPU node is being recognized by deepvariant. Is there something I'm missing for call_variants?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:450,safety,input,input,450,"Issue with TPU Node v3-8 in call variants step; I am trying to use deepvariant to call variants using a TPU Node v3-8, but I am running into a persistent issue. Here is the command I am using:. ```bash. docker run \. -v `pwd`:`pwd` -w `pwd` \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --call_variants_extra_args use_tpu=true,tpu_name=""variantcaller-node1"",tpu_zone=""europe-west4-a"" \. --model_type=WGS \. --ref=""input/data/${REF}"" \. --reads=""input/data/${BAM}"" \. --output_vcf=""output/${OUTPUT_VCF}"" \. --output_gvcf=""output/${OUTPUT_GVCF}"" \. --regions chr20 \. --num_shards=$(nproc) \. --intermediate_results_dir /output/intermediate_results_dir. ```. However, I am seeing the following error in the call variants step. ```bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@96.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --openvino_model_dir ""/output/intermediate_results_dir"" --tpu_name ""variantcaller-node1"" --tpu_zone ""europe-west4-a"" --use_tpu. I0524 21:18:26.485428 140032543119168 transport.py:157] Attempting refresh to obtain initial access_token. I0524 21:18:26.576728 140032543119168 call_variants.py:336] Shape of input examples: [100, 221, 6]. I0524 21:18:26.579230 140032543119168 call_variants.py:361] /opt/models/wgs/model.ckpt.input_shape has the correct shape: [100, 221, 6]. 2022-05-24 21:18:26.581705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-05-24 21:18:26.586196: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:481,safety,input,input,481,"Issue with TPU Node v3-8 in call variants step; I am trying to use deepvariant to call variants using a TPU Node v3-8, but I am running into a persistent issue. Here is the command I am using:. ```bash. docker run \. -v `pwd`:`pwd` -w `pwd` \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --call_variants_extra_args use_tpu=true,tpu_name=""variantcaller-node1"",tpu_zone=""europe-west4-a"" \. --model_type=WGS \. --ref=""input/data/${REF}"" \. --reads=""input/data/${BAM}"" \. --output_vcf=""output/${OUTPUT_VCF}"" \. --output_gvcf=""output/${OUTPUT_GVCF}"" \. --regions chr20 \. --num_shards=$(nproc) \. --intermediate_results_dir /output/intermediate_results_dir. ```. However, I am seeing the following error in the call variants step. ```bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@96.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --openvino_model_dir ""/output/intermediate_results_dir"" --tpu_name ""variantcaller-node1"" --tpu_zone ""europe-west4-a"" --use_tpu. I0524 21:18:26.485428 140032543119168 transport.py:157] Attempting refresh to obtain initial access_token. I0524 21:18:26.576728 140032543119168 call_variants.py:336] Shape of input examples: [100, 221, 6]. I0524 21:18:26.579230 140032543119168 call_variants.py:361] /opt/models/wgs/model.ckpt.input_shape has the correct shape: [100, 221, 6]. 2022-05-24 21:18:26.581705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-05-24 21:18:26.586196: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:728,safety,error,error,728,"Issue with TPU Node v3-8 in call variants step; I am trying to use deepvariant to call variants using a TPU Node v3-8, but I am running into a persistent issue. Here is the command I am using:. ```bash. docker run \. -v `pwd`:`pwd` -w `pwd` \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --call_variants_extra_args use_tpu=true,tpu_name=""variantcaller-node1"",tpu_zone=""europe-west4-a"" \. --model_type=WGS \. --ref=""input/data/${REF}"" \. --reads=""input/data/${BAM}"" \. --output_vcf=""output/${OUTPUT_VCF}"" \. --output_gvcf=""output/${OUTPUT_GVCF}"" \. --regions chr20 \. --num_shards=$(nproc) \. --intermediate_results_dir /output/intermediate_results_dir. ```. However, I am seeing the following error in the call variants step. ```bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@96.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --openvino_model_dir ""/output/intermediate_results_dir"" --tpu_name ""variantcaller-node1"" --tpu_zone ""europe-west4-a"" --use_tpu. I0524 21:18:26.485428 140032543119168 transport.py:157] Attempting refresh to obtain initial access_token. I0524 21:18:26.576728 140032543119168 call_variants.py:336] Shape of input examples: [100, 221, 6]. I0524 21:18:26.579230 140032543119168 call_variants.py:361] /opt/models/wgs/model.ckpt.input_shape has the correct shape: [100, 221, 6]. 2022-05-24 21:18:26.581705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-05-24 21:18:26.586196: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:1342,safety,input,input,1342,"args use_tpu=true,tpu_name=""variantcaller-node1"",tpu_zone=""europe-west4-a"" \. --model_type=WGS \. --ref=""input/data/${REF}"" \. --reads=""input/data/${BAM}"" \. --output_vcf=""output/${OUTPUT_VCF}"" \. --output_gvcf=""output/${OUTPUT_GVCF}"" \. --regions chr20 \. --num_shards=$(nproc) \. --intermediate_results_dir /output/intermediate_results_dir. ```. However, I am seeing the following error in the call variants step. ```bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@96.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --openvino_model_dir ""/output/intermediate_results_dir"" --tpu_name ""variantcaller-node1"" --tpu_zone ""europe-west4-a"" --use_tpu. I0524 21:18:26.485428 140032543119168 transport.py:157] Attempting refresh to obtain initial access_token. I0524 21:18:26.576728 140032543119168 call_variants.py:336] Shape of input examples: [100, 221, 6]. I0524 21:18:26.579230 140032543119168 call_variants.py:361] /opt/models/wgs/model.ckpt.input_shape has the correct shape: [100, 221, 6]. 2022-05-24 21:18:26.581705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-05-24 21:18:26.586196: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2022-05-24 21:18:26.587127: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2000160000 Hz. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp_f348kd0. W0524 21:18:26.619681 140032543119168 estimator.py:1846] Using temporary folder as mo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:6348,safety,Avail,Available,6348,"uerying Tensorflow master (grpc://10.73.74.226:8470) for TPU system metadata. I0524 21:18:26.625535 140032543119168 tpu_system_metadata.py:90] Querying Tensorflow master (grpc://10.73.74.226:8470) for TPU system metadata. 2022-05-24 21:18:26.626490: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:373] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created. INFO:tensorflow:Found TPU system:. I0524 21:18:26.631762 140032543119168 tpu_system_metadata.py:159] Found TPU system:. INFO:tensorflow:*** Num TPU Cores: 8. I0524 21:18:26.631872 140032543119168 tpu_system_metadata.py:160] *** Num TPU Cores: 8. INFO:tensorflow:*** Num TPU Workers: 1. I0524 21:18:26.631940 140032543119168 tpu_system_metadata.py:161] *** Num TPU Workers: 1. INFO:tensorflow:*** Num TPU Cores Per Worker: 8. I0524 21:18:26.631998 140032543119168 tpu_system_metadata.py:162] *** Num TPU Cores Per Worker: 8. INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available D",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:6532,safety,Avail,Available,6532,"74.226:8470) for TPU system metadata. 2022-05-24 21:18:26.626490: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:373] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created. INFO:tensorflow:Found TPU system:. I0524 21:18:26.631762 140032543119168 tpu_system_metadata.py:159] Found TPU system:. INFO:tensorflow:*** Num TPU Cores: 8. I0524 21:18:26.631872 140032543119168 tpu_system_metadata.py:160] *** Num TPU Cores: 8. INFO:tensorflow:*** Num TPU Workers: 1. I0524 21:18:26.631940 140032543119168 tpu_system_metadata.py:161] *** Num TPU Workers: 1. INFO:tensorflow:*** Num TPU Cores Per Worker: 8. I0524 21:18:26.631998 140032543119168 tpu_system_metadata.py:162] *** Num TPU Cores Per Worker: 8. INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** A",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:6666,safety,Avail,Available,6666,"ession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created. INFO:tensorflow:Found TPU system:. I0524 21:18:26.631762 140032543119168 tpu_system_metadata.py:159] Found TPU system:. INFO:tensorflow:*** Num TPU Cores: 8. I0524 21:18:26.631872 140032543119168 tpu_system_metadata.py:160] *** Num TPU Cores: 8. INFO:tensorflow:*** Num TPU Workers: 1. I0524 21:18:26.631940 140032543119168 tpu_system_metadata.py:161] *** Num TPU Workers: 1. INFO:tensorflow:*** Num TPU Cores Per Worker: 8. I0524 21:18:26.631998 140032543119168 tpu_system_metadata.py:162] *** Num TPU Cores Per Worker: 8. INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:6860,safety,Avail,Available,6860,"32543119168 tpu_system_metadata.py:159] Found TPU system:. INFO:tensorflow:*** Num TPU Cores: 8. I0524 21:18:26.631872 140032543119168 tpu_system_metadata.py:160] *** Num TPU Cores: 8. INFO:tensorflow:*** Num TPU Workers: 1. I0524 21:18:26.631940 140032543119168 tpu_system_metadata.py:161] *** Num TPU Workers: 1. INFO:tensorflow:*** Num TPU Cores Per Worker: 8. I0524 21:18:26.631998 140032543119168 tpu_system_metadata.py:162] *** Num TPU Cores Per Worker: 8. INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:7004,safety,Avail,Available,7004,"m_metadata.py:160] *** Num TPU Cores: 8. INFO:tensorflow:*** Num TPU Workers: 1. I0524 21:18:26.631940 140032543119168 tpu_system_metadata.py:161] *** Num TPU Workers: 1. INFO:tensorflow:*** Num TPU Cores Per Worker: 8. I0524 21:18:26.631998 140032543119168 tpu_system_metadata.py:162] *** Num TPU Cores Per Worker: 8. INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:7198,safety,Avail,Available,7198," TPU Cores Per Worker: 8. I0524 21:18:26.631998 140032543119168 tpu_system_metadata.py:162] *** Num TPU Cores Per Worker: 8. INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:1",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:7342,safety,Avail,Available,7342," Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:7536,safety,Avail,Available,7536," Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:16",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:7680,safety,Avail,Available,7680,"DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:7874,safety,Avail,Available,7874,"DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:8018,safety,Avail,Available,8018,"DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:8211,safety,Avail,Available,8211,"_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:8354,safety,Avail,Available,8354," _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:8548,safety,Avail,Available,8548," _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_m",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:8692,safety,Avail,Available,8692," _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 229118620",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:8885,safety,Avail,Available,8885,": _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). I0524 21:18:26.632941 1400325",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:9028,safety,Avail,Available,9028,"e: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). I0524 21:18:26.632941 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 171798",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:9222,safety,Avail,Available,9222,": _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). I0524 21:18:26.632941 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). INFO:tensorflow:Calling model_fn. I0524 21:18:26.633588 140032543119168 estimator.py:1162] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:9366,safety,Avail,Available,9366," _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). I0524 21:18:26.632941 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). INFO:tensorflow:Calling model_fn. I0524 21:18:26.633588 140032543119168 estimator.py:1162] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:9573,safety,Avail,Available,9573,"butes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). I0524 21:18:26.632941 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). INFO:tensorflow:Calling model_fn. I0524 21:18:26.633588 140032543119168 estimator.py:1162] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn. INFO:tensorflow:TPU",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:9730,safety,Avail,Available,9730,"u_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). I0524 21:18:26.632941 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). INFO:tensorflow:Calling model_fn. I0524 21:18:26.633588 140032543119168 estimator.py:1162] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn. INFO:tensorflow:TPU job name tpu_worker. I0524 21:18:33.019782 140032543119168 tpu_estimator.py:514] TPU job name tpu_worker. INFO:tensorflow:Graph was finalized. I0524 21:18:3",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:9931,safety,Avail,Available,9931,"/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). I0524 21:18:26.632941 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). INFO:tensorflow:Calling model_fn. I0524 21:18:26.633588 140032543119168 estimator.py:1162] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn. INFO:tensorflow:TPU job name tpu_worker. I0524 21:18:33.019782 140032543119168 tpu_estimator.py:514] TPU job name tpu_worker. INFO:tensorflow:Graph was finalized. I0524 21:18:33.525068 140032543119168 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0524 21:18:33.525994 140032543119168 saver.py:1298] Restor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:11171,safety,error,error,11171,"r/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn. INFO:tensorflow:TPU job name tpu_worker. I0524 21:18:33.019782 140032543119168 tpu_estimator.py:514] TPU job name tpu_worker. INFO:tensorflow:Graph was finalized. I0524 21:18:33.525068 140032543119168 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0524 21:18:33.525994 140032543119168 saver.py:1298] Restoring parameters from /opt/models/wgs/model.ckpt. INFO:tensorflow:prediction_loop marked as finished. I0524 21:18:34.251420 140032543119168 error_handling.py:115] prediction_loop marked as finished. WARNING:tensorflow:Reraising captured error. W0524 21:18:34.251592 140032543119168 error_handling.py:149] Reraising captured error. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1375, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:11258,safety,error,error,11258,"2: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn. INFO:tensorflow:TPU job name tpu_worker. I0524 21:18:33.019782 140032543119168 tpu_estimator.py:514] TPU job name tpu_worker. INFO:tensorflow:Graph was finalized. I0524 21:18:33.525068 140032543119168 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0524 21:18:33.525994 140032543119168 saver.py:1298] Restoring parameters from /opt/models/wgs/model.ckpt. INFO:tensorflow:prediction_loop marked as finished. I0524 21:18:34.251420 140032543119168 error_handling.py:115] prediction_loop marked as finished. WARNING:tensorflow:Reraising captured error. W0524 21:18:34.251592 140032543119168 error_handling.py:149] Reraising captured error. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1375, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.8",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:12155,safety,except,exception,12155,"g captured error. W0524 21:18:34.251592 140032543119168 error_handling.py:149] Reraising captured error. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1375, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1303, in restore. sess.run(self.saver_def.restore_op_name,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 967, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1190, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1368, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1394, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constru",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:12174,safety,except,exception,12174,"0524 21:18:34.251592 140032543119168 error_handling.py:149] Reraising captured error. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1375, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1303, in restore. sess.run(self.saver_def.restore_op_name,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 967, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1190, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1368, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1394, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:13615,safety,modul,module,13615,"y"", line 1190, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1368, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1394, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:14289,safety,predict,prediction,14289,"/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_sessio",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:14307,safety,predict,predictions,14307,"el.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._ses",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:14440,safety,predict,predict,14440,"r.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:14489,safety,predict,predict,14489,"storeV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:14609,safety,predict,predict,14609," <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:14642,safety,Monitor,MonitoredSession,14642,"le ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._sc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:14785,safety,Monitor,MonitoredSession,14785,"lags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in finalize. self._s",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:17529,safety,input,inputs,17529,"b/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal. restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps. self._AddRestoreOps(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 335, in _AddRestoreOps. all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1490, in restore_v2. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 748, in _apply_op_helper. op = g._create_op_internal(op_type_name, inputs, dtypes=None,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3557, in _create_op_internal. ret = Operation(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 2045, in __init__. self._traceback = tf_stack.extract_stack_for_node(self._c_op). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_o0nxhusg/runfile",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:17882,safety,except,exception,17882,"low/python/training/saver.py"", line 335, in _AddRestoreOps. all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1490, in restore_v2. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 748, in _apply_op_helper. op = g._create_op_internal(op_type_name, inputs, dtypes=None,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3557, in _create_op_internal. ret = Operation(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 2045, in __init__. self._traceback = tf_stack.extract_stack_for_node(self._c_op). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:17901,safety,except,exception,17901,"/saver.py"", line 335, in _AddRestoreOps. all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1490, in restore_v2. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 748, in _apply_op_helper. op = g._create_op_internal(op_type_name, inputs, dtypes=None,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3557, in _create_op_internal. ret = Operation(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 2045, in __init__. self._traceback = tf_stack.extract_stack_for_node(self._c_op). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predi",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:18070,safety,modul,module,18070,"orflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1490, in restore_v2. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 748, in _apply_op_helper. op = g._create_op_internal(op_type_name, inputs, dtypes=None,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3557, in _create_op_internal. ret = Operation(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 2045, in __init__. self._traceback = tf_stack.extract_stack_for_node(self._c_op). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict. rendezvous.raise_errors(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors. si",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:18749,safety,predict,prediction,18749,"rk/ops.py"", line 2045, in __init__. self._traceback = tf_stack.extract_stack_for_node(self._c_op). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict. rendezvous.raise_errors(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors. six.reraise(typ, value, traceback). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/six_archive/six.py"", line 703, in reraise. raise value. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""/usr/local/lib/python3.8/dist-packages/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:18767,safety,predict,predictions,18767,"45, in __init__. self._traceback = tf_stack.extract_stack_for_node(self._c_op). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict. rendezvous.raise_errors(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors. six.reraise(typ, value, traceback). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/six_archive/six.py"", line 703, in reraise. raise value. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:18901,safety,predict,predict,18901,"ption occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict. rendezvous.raise_errors(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors. six.reraise(typ, value, traceback). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/six_archive/six.py"", line 703, in reraise. raise value. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:19327,safety,predict,predict,19327,".py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict. rendezvous.raise_errors(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors. six.reraise(typ, value, traceback). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/six_archive/six.py"", line 703, in reraise. raise value. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", lin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:19376,safety,predict,predict,19376,"le ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict. rendezvous.raise_errors(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors. six.reraise(typ, value, traceback). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/six_archive/six.py"", line 703, in reraise. raise value. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._se",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:19497,safety,predict,predict,19497,"Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict. rendezvous.raise_errors(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors. six.reraise(typ, value, traceback). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/six_archive/six.py"", line 703, in reraise. raise value. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:19530,safety,Monitor,MonitoredSession,19530,"google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict. rendezvous.raise_errors(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors. six.reraise(typ, value, traceback). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/six_archive/six.py"", line 703, in reraise. raise value. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 662, in create_session. re",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:19674,safety,Monitor,MonitoredSession,19674,"variant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict. rendezvous.raise_errors(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors. six.reraise(typ, value, traceback). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/six_archive/six.py"", line 703, in reraise. raise value. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 662, in create_session. return self._get_session_manager().prepare_session(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:21584,safety,error,error,21584,". File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 314, in prepare_session. sess, is_loaded_from_checkpoint = self._restore_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 233, in _restore_checkpoint. _restore_checkpoint_and_maybe_run_saved_model_initializers(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers. saver.restore(sess, path). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1339, in restore. raise _wrap_restore_error_with_msg(. tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/co",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:22125,safety,modul,module,22125,"ver.restore(sess, path). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1339, in restore. raise _wrap_restore_error_with_msg(. tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:22799,safety,predict,prediction,22799,"/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_sessio",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:22817,safety,predict,predictions,22817,"el.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._ses",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:22950,safety,predict,predict,22950,"r.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:22999,safety,predict,predict,22999,"storeV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:23119,safety,predict,predict,23119," <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:23152,safety,Monitor,MonitoredSession,23152,"le ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._sc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:23295,safety,Monitor,MonitoredSession,23295,"lags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in finalize. self._s",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:26039,safety,input,inputs,26039,"tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 876, in _build. self.saver_def = self._builder._build_internal( # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal. restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps. self._AddRestoreOps(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 335, in _AddRestoreOps. all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1490, in restore_v2. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 748, in _apply_op_helper. op = g._create_op_internal(op_type_name, inputs, dtypes=None,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3557, in _create_op_internal. ret = Operation(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 2045, in __init__. self._traceback = tf_stack.extract_stack_for_node(self._c_op). real 0m10.757s. user 0m13.496s. sys 0m5.144s. ```. This same command works fine without using TPUs on this system, and it looks like the TPU node is being recognized by deepvariant. Is there something I'm missing for call_variants?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:1015,security,model,models,1015,"e v3-8 in call variants step; I am trying to use deepvariant to call variants using a TPU Node v3-8, but I am running into a persistent issue. Here is the command I am using:. ```bash. docker run \. -v `pwd`:`pwd` -w `pwd` \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --call_variants_extra_args use_tpu=true,tpu_name=""variantcaller-node1"",tpu_zone=""europe-west4-a"" \. --model_type=WGS \. --ref=""input/data/${REF}"" \. --reads=""input/data/${BAM}"" \. --output_vcf=""output/${OUTPUT_VCF}"" \. --output_gvcf=""output/${OUTPUT_GVCF}"" \. --regions chr20 \. --num_shards=$(nproc) \. --intermediate_results_dir /output/intermediate_results_dir. ```. However, I am seeing the following error in the call variants step. ```bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@96.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --openvino_model_dir ""/output/intermediate_results_dir"" --tpu_name ""variantcaller-node1"" --tpu_zone ""europe-west4-a"" --use_tpu. I0524 21:18:26.485428 140032543119168 transport.py:157] Attempting refresh to obtain initial access_token. I0524 21:18:26.576728 140032543119168 call_variants.py:336] Shape of input examples: [100, 221, 6]. I0524 21:18:26.579230 140032543119168 call_variants.py:361] /opt/models/wgs/model.ckpt.input_shape has the correct shape: [100, 221, 6]. 2022-05-24 21:18:26.581705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-05-24 21:18:26.586196: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:1026,security,model,model,1026,"all variants step; I am trying to use deepvariant to call variants using a TPU Node v3-8, but I am running into a persistent issue. Here is the command I am using:. ```bash. docker run \. -v `pwd`:`pwd` -w `pwd` \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --call_variants_extra_args use_tpu=true,tpu_name=""variantcaller-node1"",tpu_zone=""europe-west4-a"" \. --model_type=WGS \. --ref=""input/data/${REF}"" \. --reads=""input/data/${BAM}"" \. --output_vcf=""output/${OUTPUT_VCF}"" \. --output_gvcf=""output/${OUTPUT_GVCF}"" \. --regions chr20 \. --num_shards=$(nproc) \. --intermediate_results_dir /output/intermediate_results_dir. ```. However, I am seeing the following error in the call variants step. ```bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@96.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --openvino_model_dir ""/output/intermediate_results_dir"" --tpu_name ""variantcaller-node1"" --tpu_zone ""europe-west4-a"" --use_tpu. I0524 21:18:26.485428 140032543119168 transport.py:157] Attempting refresh to obtain initial access_token. I0524 21:18:26.576728 140032543119168 call_variants.py:336] Shape of input examples: [100, 221, 6]. I0524 21:18:26.579230 140032543119168 call_variants.py:361] /opt/models/wgs/model.ckpt.input_shape has the correct shape: [100, 221, 6]. 2022-05-24 21:18:26.581705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-05-24 21:18:26.586196: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:1438,security,model,models,1438,". --ref=""input/data/${REF}"" \. --reads=""input/data/${BAM}"" \. --output_vcf=""output/${OUTPUT_VCF}"" \. --output_gvcf=""output/${OUTPUT_GVCF}"" \. --regions chr20 \. --num_shards=$(nproc) \. --intermediate_results_dir /output/intermediate_results_dir. ```. However, I am seeing the following error in the call variants step. ```bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@96.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --openvino_model_dir ""/output/intermediate_results_dir"" --tpu_name ""variantcaller-node1"" --tpu_zone ""europe-west4-a"" --use_tpu. I0524 21:18:26.485428 140032543119168 transport.py:157] Attempting refresh to obtain initial access_token. I0524 21:18:26.576728 140032543119168 call_variants.py:336] Shape of input examples: [100, 221, 6]. I0524 21:18:26.579230 140032543119168 call_variants.py:361] /opt/models/wgs/model.ckpt.input_shape has the correct shape: [100, 221, 6]. 2022-05-24 21:18:26.581705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-05-24 21:18:26.586196: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2022-05-24 21:18:26.587127: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2000160000 Hz. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp_f348kd0. W0524 21:18:26.619681 140032543119168 estimator.py:1846] Using temporary folder as model directory: /tmp/tmp_f348kd0. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp_f348kd0'",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:1449,security,model,model,1449,"put/data/${REF}"" \. --reads=""input/data/${BAM}"" \. --output_vcf=""output/${OUTPUT_VCF}"" \. --output_gvcf=""output/${OUTPUT_GVCF}"" \. --regions chr20 \. --num_shards=$(nproc) \. --intermediate_results_dir /output/intermediate_results_dir. ```. However, I am seeing the following error in the call variants step. ```bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@96.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --openvino_model_dir ""/output/intermediate_results_dir"" --tpu_name ""variantcaller-node1"" --tpu_zone ""europe-west4-a"" --use_tpu. I0524 21:18:26.485428 140032543119168 transport.py:157] Attempting refresh to obtain initial access_token. I0524 21:18:26.576728 140032543119168 call_variants.py:336] Shape of input examples: [100, 221, 6]. I0524 21:18:26.579230 140032543119168 call_variants.py:361] /opt/models/wgs/model.ckpt.input_shape has the correct shape: [100, 221, 6]. 2022-05-24 21:18:26.581705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-05-24 21:18:26.586196: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2022-05-24 21:18:26.587127: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2000160000 Hz. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp_f348kd0. W0524 21:18:26.619681 140032543119168 estimator.py:1846] Using temporary folder as model directory: /tmp/tmp_f348kd0. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp_f348kd0', '_tf_rand",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:1651,security,Network,Network,1651,"output/intermediate_results_dir. ```. However, I am seeing the following error in the call variants step. ```bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@96.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --openvino_model_dir ""/output/intermediate_results_dir"" --tpu_name ""variantcaller-node1"" --tpu_zone ""europe-west4-a"" --use_tpu. I0524 21:18:26.485428 140032543119168 transport.py:157] Attempting refresh to obtain initial access_token. I0524 21:18:26.576728 140032543119168 call_variants.py:336] Shape of input examples: [100, 221, 6]. I0524 21:18:26.579230 140032543119168 call_variants.py:361] /opt/models/wgs/model.ckpt.input_shape has the correct shape: [100, 221, 6]. 2022-05-24 21:18:26.581705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-05-24 21:18:26.586196: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2022-05-24 21:18:26.587127: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2000160000 Hz. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp_f348kd0. W0524 21:18:26.619681 140032543119168 estimator.py:1846] Using temporary folder as model directory: /tmp/tmp_f348kd0. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp_f348kd0', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimiz",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:2225,security,model,model,2225,"ting refresh to obtain initial access_token. I0524 21:18:26.576728 140032543119168 call_variants.py:336] Shape of input examples: [100, 221, 6]. I0524 21:18:26.579230 140032543119168 call_variants.py:361] /opt/models/wgs/model.ckpt.input_shape has the correct shape: [100, 221, 6]. 2022-05-24 21:18:26.581705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-05-24 21:18:26.586196: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2022-05-24 21:18:26.587127: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2000160000 Hz. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp_f348kd0. W0524 21:18:26.619681 140032543119168 estimator.py:1846] Using temporary folder as model directory: /tmp/tmp_f348kd0. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp_f348kd0', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.73.74.226:8470', '_evaluation_master': 'grpc://10.73.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:2343,security,model,model,2343,"t examples: [100, 221, 6]. I0524 21:18:26.579230 140032543119168 call_variants.py:361] /opt/models/wgs/model.ckpt.input_shape has the correct shape: [100, 221, 6]. 2022-05-24 21:18:26.581705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-05-24 21:18:26.586196: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2022-05-24 21:18:26.587127: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2000160000 Hz. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp_f348kd0. W0524 21:18:26.619681 140032543119168 estimator.py:1846] Using temporary folder as model directory: /tmp/tmp_f348kd0. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp_f348kd0', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.73.74.226:8470', '_evaluation_master': 'grpc://10.73.74.226:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:5711,security,session,session,5711,"100, num_shards=None, num_cores_per_replica=None, per_host_input_for_training=2, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1, experimental_allow_per_host_v2_parallel_get_next=False, experimental_feed_hook=None), '_cluster': None}. INFO:tensorflow:_TPUContext: eval_on_tpu True. I0524 21:18:26.620373 140032543119168 tpu_context.py:271] _TPUContext: eval_on_tpu True. I0524 21:18:26.620768 140032543119168 call_variants.py:426] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz. INFO:tensorflow:Querying Tensorflow master (grpc://10.73.74.226:8470) for TPU system metadata. I0524 21:18:26.625535 140032543119168 tpu_system_metadata.py:90] Querying Tensorflow master (grpc://10.73.74.226:8470) for TPU system metadata. 2022-05-24 21:18:26.626490: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:373] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created. INFO:tensorflow:Found TPU system:. I0524 21:18:26.631762 140032543119168 tpu_system_metadata.py:159] Found TPU system:. INFO:tensorflow:*** Num TPU Cores: 8. I0524 21:18:26.631872 140032543119168 tpu_system_metadata.py:160] *** Num TPU Cores: 8. INFO:tensorflow:*** Num TPU Workers: 1. I0524 21:18:26.631940 140032543119168 tpu_system_metadata.py:161] *** Num TPU Workers: 1. INFO:tensorflow:*** Num TPU Cores Per Worker: 8. I0524 21:18:26.631998 140032543119168 tpu_system_metadata.py:162] *** Num TPU Cores Per Worker: 8. INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_work",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:5770,security,session,session,5770,"input_for_training=2, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1, experimental_allow_per_host_v2_parallel_get_next=False, experimental_feed_hook=None), '_cluster': None}. INFO:tensorflow:_TPUContext: eval_on_tpu True. I0524 21:18:26.620373 140032543119168 tpu_context.py:271] _TPUContext: eval_on_tpu True. I0524 21:18:26.620768 140032543119168 call_variants.py:426] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz. INFO:tensorflow:Querying Tensorflow master (grpc://10.73.74.226:8470) for TPU system metadata. I0524 21:18:26.625535 140032543119168 tpu_system_metadata.py:90] Querying Tensorflow master (grpc://10.73.74.226:8470) for TPU system metadata. 2022-05-24 21:18:26.626490: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:373] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created. INFO:tensorflow:Found TPU system:. I0524 21:18:26.631762 140032543119168 tpu_system_metadata.py:159] Found TPU system:. INFO:tensorflow:*** Num TPU Cores: 8. I0524 21:18:26.631872 140032543119168 tpu_system_metadata.py:160] *** Num TPU Cores: 8. INFO:tensorflow:*** Num TPU Workers: 1. I0524 21:18:26.631940 140032543119168 tpu_system_metadata.py:161] *** Num TPU Workers: 1. INFO:tensorflow:*** Num TPU Cores Per Worker: 8. I0524 21:18:26.631998 140032543119168 tpu_system_metadata.py:162] *** Num TPU Cores Per Worker: 8. INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -187377",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:6348,security,Availab,Available,6348,"uerying Tensorflow master (grpc://10.73.74.226:8470) for TPU system metadata. I0524 21:18:26.625535 140032543119168 tpu_system_metadata.py:90] Querying Tensorflow master (grpc://10.73.74.226:8470) for TPU system metadata. 2022-05-24 21:18:26.626490: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:373] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created. INFO:tensorflow:Found TPU system:. I0524 21:18:26.631762 140032543119168 tpu_system_metadata.py:159] Found TPU system:. INFO:tensorflow:*** Num TPU Cores: 8. I0524 21:18:26.631872 140032543119168 tpu_system_metadata.py:160] *** Num TPU Cores: 8. INFO:tensorflow:*** Num TPU Workers: 1. I0524 21:18:26.631940 140032543119168 tpu_system_metadata.py:161] *** Num TPU Workers: 1. INFO:tensorflow:*** Num TPU Cores Per Worker: 8. I0524 21:18:26.631998 140032543119168 tpu_system_metadata.py:162] *** Num TPU Cores Per Worker: 8. INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available D",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:6532,security,Availab,Available,6532,"74.226:8470) for TPU system metadata. 2022-05-24 21:18:26.626490: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:373] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created. INFO:tensorflow:Found TPU system:. I0524 21:18:26.631762 140032543119168 tpu_system_metadata.py:159] Found TPU system:. INFO:tensorflow:*** Num TPU Cores: 8. I0524 21:18:26.631872 140032543119168 tpu_system_metadata.py:160] *** Num TPU Cores: 8. INFO:tensorflow:*** Num TPU Workers: 1. I0524 21:18:26.631940 140032543119168 tpu_system_metadata.py:161] *** Num TPU Workers: 1. INFO:tensorflow:*** Num TPU Cores Per Worker: 8. I0524 21:18:26.631998 140032543119168 tpu_system_metadata.py:162] *** Num TPU Cores Per Worker: 8. INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** A",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:6666,security,Availab,Available,6666,"ession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created. INFO:tensorflow:Found TPU system:. I0524 21:18:26.631762 140032543119168 tpu_system_metadata.py:159] Found TPU system:. INFO:tensorflow:*** Num TPU Cores: 8. I0524 21:18:26.631872 140032543119168 tpu_system_metadata.py:160] *** Num TPU Cores: 8. INFO:tensorflow:*** Num TPU Workers: 1. I0524 21:18:26.631940 140032543119168 tpu_system_metadata.py:161] *** Num TPU Workers: 1. INFO:tensorflow:*** Num TPU Cores Per Worker: 8. I0524 21:18:26.631998 140032543119168 tpu_system_metadata.py:162] *** Num TPU Cores Per Worker: 8. INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:6860,security,Availab,Available,6860,"32543119168 tpu_system_metadata.py:159] Found TPU system:. INFO:tensorflow:*** Num TPU Cores: 8. I0524 21:18:26.631872 140032543119168 tpu_system_metadata.py:160] *** Num TPU Cores: 8. INFO:tensorflow:*** Num TPU Workers: 1. I0524 21:18:26.631940 140032543119168 tpu_system_metadata.py:161] *** Num TPU Workers: 1. INFO:tensorflow:*** Num TPU Cores Per Worker: 8. I0524 21:18:26.631998 140032543119168 tpu_system_metadata.py:162] *** Num TPU Cores Per Worker: 8. INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:7004,security,Availab,Available,7004,"m_metadata.py:160] *** Num TPU Cores: 8. INFO:tensorflow:*** Num TPU Workers: 1. I0524 21:18:26.631940 140032543119168 tpu_system_metadata.py:161] *** Num TPU Workers: 1. INFO:tensorflow:*** Num TPU Cores Per Worker: 8. I0524 21:18:26.631998 140032543119168 tpu_system_metadata.py:162] *** Num TPU Cores Per Worker: 8. INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:7198,security,Availab,Available,7198," TPU Cores Per Worker: 8. I0524 21:18:26.631998 140032543119168 tpu_system_metadata.py:162] *** Num TPU Cores Per Worker: 8. INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:1",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:7342,security,Availab,Available,7342," Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:7536,security,Availab,Available,7536," Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:16",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:7680,security,Availab,Available,7680,"DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:7874,security,Availab,Available,7874,"DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:8018,security,Availab,Available,8018,"DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:8211,security,Availab,Available,8211,"_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:8354,security,Availab,Available,8354," _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:8548,security,Availab,Available,8548," _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_m",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:8692,security,Availab,Available,8692," _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 229118620",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:8885,security,Availab,Available,8885,": _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). I0524 21:18:26.632941 1400325",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:9028,security,Availab,Available,9028,"e: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). I0524 21:18:26.632941 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 171798",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:9222,security,Availab,Available,9222,": _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). I0524 21:18:26.632941 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). INFO:tensorflow:Calling model_fn. I0524 21:18:26.633588 140032543119168 estimator.py:1162] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:9366,security,Availab,Available,9366," _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). I0524 21:18:26.632941 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). INFO:tensorflow:Calling model_fn. I0524 21:18:26.633588 140032543119168 estimator.py:1162] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:9573,security,Availab,Available,9573,"butes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). I0524 21:18:26.632941 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). INFO:tensorflow:Calling model_fn. I0524 21:18:26.633588 140032543119168 estimator.py:1162] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn. INFO:tensorflow:TPU",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:9730,security,Availab,Available,9730,"u_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). I0524 21:18:26.632941 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). INFO:tensorflow:Calling model_fn. I0524 21:18:26.633588 140032543119168 estimator.py:1162] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn. INFO:tensorflow:TPU job name tpu_worker. I0524 21:18:33.019782 140032543119168 tpu_estimator.py:514] TPU job name tpu_worker. INFO:tensorflow:Graph was finalized. I0524 21:18:3",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:9931,security,Availab,Available,9931,"/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). I0524 21:18:26.632941 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). INFO:tensorflow:Calling model_fn. I0524 21:18:26.633588 140032543119168 estimator.py:1162] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn. INFO:tensorflow:TPU job name tpu_worker. I0524 21:18:33.019782 140032543119168 tpu_estimator.py:514] TPU job name tpu_worker. INFO:tensorflow:Graph was finalized. I0524 21:18:33.525068 140032543119168 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0524 21:18:33.525994 140032543119168 saver.py:1298] Restor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:10854,security,model,models,10854,"5). I0524 21:18:26.632941 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). INFO:tensorflow:Calling model_fn. I0524 21:18:26.633588 140032543119168 estimator.py:1162] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn. INFO:tensorflow:TPU job name tpu_worker. I0524 21:18:33.019782 140032543119168 tpu_estimator.py:514] TPU job name tpu_worker. INFO:tensorflow:Graph was finalized. I0524 21:18:33.525068 140032543119168 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0524 21:18:33.525994 140032543119168 saver.py:1298] Restoring parameters from /opt/models/wgs/model.ckpt. INFO:tensorflow:prediction_loop marked as finished. I0524 21:18:34.251420 140032543119168 error_handling.py:115] prediction_loop marked as finished. WARNING:tensorflow:Reraising captured error. W0524 21:18:34.251592 140032543119168 error_handling.py:149] Reraising captured error. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1375, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.InvalidArgumentError: From",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:10865,security,model,model,10865,"1:18:26.632941 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). INFO:tensorflow:Calling model_fn. I0524 21:18:26.633588 140032543119168 estimator.py:1162] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn. INFO:tensorflow:TPU job name tpu_worker. I0524 21:18:33.019782 140032543119168 tpu_estimator.py:514] TPU job name tpu_worker. INFO:tensorflow:Graph was finalized. I0524 21:18:33.525068 140032543119168 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0524 21:18:33.525994 140032543119168 saver.py:1298] Restoring parameters from /opt/models/wgs/model.ckpt. INFO:tensorflow:prediction_loop marked as finished. I0524 21:18:34.251420 140032543119168 error_handling.py:115] prediction_loop marked as finished. WARNING:tensorflow:Reraising captured error. W0524 21:18:34.251592 140032543119168 error_handling.py:149] Reraising captured error. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1375, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_w",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:10961,security,model,models,10961,"es(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). INFO:tensorflow:Calling model_fn. I0524 21:18:26.633588 140032543119168 estimator.py:1162] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn. INFO:tensorflow:TPU job name tpu_worker. I0524 21:18:33.019782 140032543119168 tpu_estimator.py:514] TPU job name tpu_worker. INFO:tensorflow:Graph was finalized. I0524 21:18:33.525068 140032543119168 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0524 21:18:33.525994 140032543119168 saver.py:1298] Restoring parameters from /opt/models/wgs/model.ckpt. INFO:tensorflow:prediction_loop marked as finished. I0524 21:18:34.251420 140032543119168 error_handling.py:115] prediction_loop marked as finished. WARNING:tensorflow:Reraising captured error. W0524 21:18:34.251592 140032543119168 error_handling.py:149] Reraising captured error. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1375, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching file",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:10972,security,model,model,10972,"_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). INFO:tensorflow:Calling model_fn. I0524 21:18:26.633588 140032543119168 estimator.py:1162] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn. INFO:tensorflow:TPU job name tpu_worker. I0524 21:18:33.019782 140032543119168 tpu_estimator.py:514] TPU job name tpu_worker. INFO:tensorflow:Graph was finalized. I0524 21:18:33.525068 140032543119168 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0524 21:18:33.525994 140032543119168 saver.py:1298] Restoring parameters from /opt/models/wgs/model.ckpt. INFO:tensorflow:prediction_loop marked as finished. I0524 21:18:34.251420 140032543119168 error_handling.py:115] prediction_loop marked as finished. WARNING:tensorflow:Reraising captured error. W0524 21:18:34.251592 140032543119168 error_handling.py:149] Reraising captured error. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1375, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/m",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:11371,security,session,session,11371,"ethod instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn. INFO:tensorflow:TPU job name tpu_worker. I0524 21:18:33.019782 140032543119168 tpu_estimator.py:514] TPU job name tpu_worker. INFO:tensorflow:Graph was finalized. I0524 21:18:33.525068 140032543119168 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0524 21:18:33.525994 140032543119168 saver.py:1298] Restoring parameters from /opt/models/wgs/model.ckpt. INFO:tensorflow:prediction_loop marked as finished. I0524 21:18:34.251420 140032543119168 error_handling.py:115] prediction_loop marked as finished. WARNING:tensorflow:Reraising captured error. W0524 21:18:34.251592 140032543119168 error_handling.py:149] Reraising captured error. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1375, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1303, in restore. sess.run(self.saver_def.restore_op_nam",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:11496,security,session,session,11496," 140032543119168 estimator.py:1164] Done calling model_fn. INFO:tensorflow:TPU job name tpu_worker. I0524 21:18:33.019782 140032543119168 tpu_estimator.py:514] TPU job name tpu_worker. INFO:tensorflow:Graph was finalized. I0524 21:18:33.525068 140032543119168 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0524 21:18:33.525994 140032543119168 saver.py:1298] Restoring parameters from /opt/models/wgs/model.ckpt. INFO:tensorflow:prediction_loop marked as finished. I0524 21:18:34.251420 140032543119168 error_handling.py:115] prediction_loop marked as finished. WARNING:tensorflow:Reraising captured error. W0524 21:18:34.251592 140032543119168 error_handling.py:149] Reraising captured error. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1375, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1303, in restore. sess.run(self.saver_def.restore_op_name,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 967, in run. result = self._run(N",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:11667,security,session,session,11667,"e tpu_worker. INFO:tensorflow:Graph was finalized. I0524 21:18:33.525068 140032543119168 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0524 21:18:33.525994 140032543119168 saver.py:1298] Restoring parameters from /opt/models/wgs/model.ckpt. INFO:tensorflow:prediction_loop marked as finished. I0524 21:18:34.251420 140032543119168 error_handling.py:115] prediction_loop marked as finished. WARNING:tensorflow:Reraising captured error. W0524 21:18:34.251592 140032543119168 error_handling.py:149] Reraising captured error. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1375, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1303, in restore. sess.run(self.saver_def.restore_op_name,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 967, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1190, in _run. results = self._do_run(handle",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:11974,security,model,models,11974,".ckpt. INFO:tensorflow:prediction_loop marked as finished. I0524 21:18:34.251420 140032543119168 error_handling.py:115] prediction_loop marked as finished. WARNING:tensorflow:Reraising captured error. W0524 21:18:34.251592 140032543119168 error_handling.py:149] Reraising captured error. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1375, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1303, in restore. sess.run(self.saver_def.restore_op_name,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 967, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1190, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1368, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1394, in _do_c",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:11985,security,model,model,11985,":tensorflow:prediction_loop marked as finished. I0524 21:18:34.251420 140032543119168 error_handling.py:115] prediction_loop marked as finished. WARNING:tensorflow:Reraising captured error. W0524 21:18:34.251592 140032543119168 error_handling.py:149] Reraising captured error. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1375, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1303, in restore. sess.run(self.saver_def.restore_op_name,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 967, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1190, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1368, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1394, in _do_call. raise ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:12070,security,model,models,12070," error_handling.py:115] prediction_loop marked as finished. WARNING:tensorflow:Reraising captured error. W0524 21:18:34.251592 140032543119168 error_handling.py:149] Reraising captured error. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1375, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1303, in restore. sess.run(self.saver_def.restore_op_name,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 967, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1190, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1368, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1394, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.InvalidArgume",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:12081,security,model,model,12081,"ling.py:115] prediction_loop marked as finished. WARNING:tensorflow:Reraising captured error. W0524 21:18:34.251592 140032543119168 error_handling.py:149] Reraising captured error. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1375, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1303, in restore. sess.run(self.saver_def.restore_op_name,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 967, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1190, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1368, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1394, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.InvalidArgumentError: Fr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:12449,security,session,session,12449,"3.8/dist-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1303, in restore. sess.run(self.saver_def.restore_op_name,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 967, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1190, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1368, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1394, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:12609,security,session,session,12609,"/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1303, in restore. sess.run(self.saver_def.restore_op_name,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 967, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1190, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1368, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1394, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:12774,security,session,session,12774," feed_dict,. tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1303, in restore. sess.run(self.saver_def.restore_op_name,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 967, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1190, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1368, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1394, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:12945,security,session,session,12945,"t matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1303, in restore. sess.run(self.saver_def.restore_op_name,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 967, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1190, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1368, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1394, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0n",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:13203,security,model,models,13203,"most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1303, in restore. sess.run(self.saver_def.restore_op_name,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 967, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1190, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1368, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1394, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:13214,security,model,model,13214," call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1303, in restore. sess.run(self.saver_def.restore_op_name,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 967, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1190, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1368, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1394, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:13299,security,model,models,13299,"/saver.py"", line 1303, in restore. sess.run(self.saver_def.restore_op_name,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 967, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1190, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1368, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1394, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:13310,security,model,model,13310," line 1303, in restore. sess.run(self.saver_def.restore_op_name,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 967, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1190, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1368, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1394, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predic",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:15867,security,access,access,15867,"flow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 876, in _build. self.saver_def = self._builder._build_internal( # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal. restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps. self._AddRestoreOps(. File ""usr/local/lib/python3.8/dist",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:16506,security,access,access,16506,"ession(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 876, in _build. self.saver_def = self._builder._build_internal( # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal. restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps. self._AddRestoreOps(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 335, in _AddRestoreOps. all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1490, in restore_v2. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 748, in _apply_op_helper. op = g._create_op_int",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:21713,security,model,models,21713,", is_loaded_from_checkpoint = self._restore_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 233, in _restore_checkpoint. _restore_checkpoint_and_maybe_run_saved_model_initializers(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers. saver.restore(sess, path). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1339, in restore. raise _wrap_restore_error_with_msg(. tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:21724,security,model,model,21724,"_from_checkpoint = self._restore_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 233, in _restore_checkpoint. _restore_checkpoint_and_maybe_run_saved_model_initializers(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers. saver.restore(sess, path). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1339, in restore. raise _wrap_restore_error_with_msg(. tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:21809,security,model,models,21809,"kages/tensorflow/python/training/session_manager.py"", line 233, in _restore_checkpoint. _restore_checkpoint_and_maybe_run_saved_model_initializers(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers. saver.restore(sess, path). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1339, in restore. raise _wrap_restore_error_with_msg(. tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:21820,security,model,model,21820,"rflow/python/training/session_manager.py"", line 233, in _restore_checkpoint. _restore_checkpoint_and_maybe_run_saved_model_initializers(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers. saver.restore(sess, path). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1339, in restore. raise _wrap_restore_error_with_msg(. tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predic",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:24377,security,access,access,24377,"flow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 876, in _build. self.saver_def = self._builder._build_internal( # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal. restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps. self._AddRestoreOps(. File ""usr/local/lib/python3.8/dist",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:25016,security,access,access,25016,"ession(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 876, in _build. self.saver_def = self._builder._build_internal( # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal. restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps. self._AddRestoreOps(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 335, in _AddRestoreOps. all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1490, in restore_v2. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 748, in _apply_op_helper. op = g._create_op_int",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:11265,testability,Trace,Traceback,11265,"rning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn. INFO:tensorflow:TPU job name tpu_worker. I0524 21:18:33.019782 140032543119168 tpu_estimator.py:514] TPU job name tpu_worker. INFO:tensorflow:Graph was finalized. I0524 21:18:33.525068 140032543119168 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0524 21:18:33.525994 140032543119168 saver.py:1298] Restoring parameters from /opt/models/wgs/model.ckpt. INFO:tensorflow:prediction_loop marked as finished. I0524 21:18:34.251420 140032543119168 error_handling.py:115] prediction_loop marked as finished. WARNING:tensorflow:Reraising captured error. W0524 21:18:34.251592 140032543119168 error_handling.py:149] Reraising captured error. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1375, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-pac",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:12195,testability,Trace,Traceback,12195,"140032543119168 error_handling.py:149] Reraising captured error. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1375, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1303, in restore. sess.run(self.saver_def.restore_op_name,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 967, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1190, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1368, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1394, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /o",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:13473,testability,trace,trace,13473,"un. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1190, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1368, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1394, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEsti",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:14642,testability,Monitor,MonitoredSession,14642,"le ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._sc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:14785,testability,Monitor,MonitoredSession,14785,"lags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in finalize. self._s",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:17922,testability,Trace,Traceback,17922," in _AddRestoreOps. all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1490, in restore_v2. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 748, in _apply_op_helper. op = g._create_op_internal(op_type_name, inputs, dtypes=None,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3557, in _create_op_internal. ret = Operation(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 2045, in __init__. self._traceback = tf_stack.extract_stack_for_node(self._c_op). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict. rendezvous.raise_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:19095,testability,trace,traceback,19095,"). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict. rendezvous.raise_errors(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors. six.reraise(typ, value, traceback). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/six_archive/six.py"", line 703, in reraise. raise value. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.8/dist-p",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:19530,testability,Monitor,MonitoredSession,19530,"google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict. rendezvous.raise_errors(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors. six.reraise(typ, value, traceback). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/six_archive/six.py"", line 703, in reraise. raise value. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 662, in create_session. re",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:19674,testability,Monitor,MonitoredSession,19674,"variant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict. rendezvous.raise_errors(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors. six.reraise(typ, value, traceback). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/six_archive/six.py"", line 703, in reraise. raise value. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 662, in create_session. return self._get_session_manager().prepare_session(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:21983,testability,trace,trace,21983,"on3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers. saver.restore(sess, path). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1339, in restore. raise _wrap_restore_error_with_msg(. tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEsti",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:23152,testability,Monitor,MonitoredSession,23152,"le ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._sc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:23295,testability,Monitor,MonitoredSession,23295,"lags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main. call_variants(. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict. for result in super(TPUEstimator, self).predict(. File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict. with tf.compat.v1.train.MonitoredSession(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__. super(MonitoredSession, self).__init__(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in finalize. self._s",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:173,usability,command,command,173,"Issue with TPU Node v3-8 in call variants step; I am trying to use deepvariant to call variants using a TPU Node v3-8, but I am running into a persistent issue. Here is the command I am using:. ```bash. docker run \. -v `pwd`:`pwd` -w `pwd` \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --call_variants_extra_args use_tpu=true,tpu_name=""variantcaller-node1"",tpu_zone=""europe-west4-a"" \. --model_type=WGS \. --ref=""input/data/${REF}"" \. --reads=""input/data/${BAM}"" \. --output_vcf=""output/${OUTPUT_VCF}"" \. --output_gvcf=""output/${OUTPUT_GVCF}"" \. --regions chr20 \. --num_shards=$(nproc) \. --intermediate_results_dir /output/intermediate_results_dir. ```. However, I am seeing the following error in the call variants step. ```bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@96.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --openvino_model_dir ""/output/intermediate_results_dir"" --tpu_name ""variantcaller-node1"" --tpu_zone ""europe-west4-a"" --use_tpu. I0524 21:18:26.485428 140032543119168 transport.py:157] Attempting refresh to obtain initial access_token. I0524 21:18:26.576728 140032543119168 call_variants.py:336] Shape of input examples: [100, 221, 6]. I0524 21:18:26.579230 140032543119168 call_variants.py:361] /opt/models/wgs/model.ckpt.input_shape has the correct shape: [100, 221, 6]. 2022-05-24 21:18:26.581705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-05-24 21:18:26.586196: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:450,usability,input,input,450,"Issue with TPU Node v3-8 in call variants step; I am trying to use deepvariant to call variants using a TPU Node v3-8, but I am running into a persistent issue. Here is the command I am using:. ```bash. docker run \. -v `pwd`:`pwd` -w `pwd` \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --call_variants_extra_args use_tpu=true,tpu_name=""variantcaller-node1"",tpu_zone=""europe-west4-a"" \. --model_type=WGS \. --ref=""input/data/${REF}"" \. --reads=""input/data/${BAM}"" \. --output_vcf=""output/${OUTPUT_VCF}"" \. --output_gvcf=""output/${OUTPUT_GVCF}"" \. --regions chr20 \. --num_shards=$(nproc) \. --intermediate_results_dir /output/intermediate_results_dir. ```. However, I am seeing the following error in the call variants step. ```bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@96.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --openvino_model_dir ""/output/intermediate_results_dir"" --tpu_name ""variantcaller-node1"" --tpu_zone ""europe-west4-a"" --use_tpu. I0524 21:18:26.485428 140032543119168 transport.py:157] Attempting refresh to obtain initial access_token. I0524 21:18:26.576728 140032543119168 call_variants.py:336] Shape of input examples: [100, 221, 6]. I0524 21:18:26.579230 140032543119168 call_variants.py:361] /opt/models/wgs/model.ckpt.input_shape has the correct shape: [100, 221, 6]. 2022-05-24 21:18:26.581705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-05-24 21:18:26.586196: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:481,usability,input,input,481,"Issue with TPU Node v3-8 in call variants step; I am trying to use deepvariant to call variants using a TPU Node v3-8, but I am running into a persistent issue. Here is the command I am using:. ```bash. docker run \. -v `pwd`:`pwd` -w `pwd` \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --call_variants_extra_args use_tpu=true,tpu_name=""variantcaller-node1"",tpu_zone=""europe-west4-a"" \. --model_type=WGS \. --ref=""input/data/${REF}"" \. --reads=""input/data/${BAM}"" \. --output_vcf=""output/${OUTPUT_VCF}"" \. --output_gvcf=""output/${OUTPUT_GVCF}"" \. --regions chr20 \. --num_shards=$(nproc) \. --intermediate_results_dir /output/intermediate_results_dir. ```. However, I am seeing the following error in the call variants step. ```bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@96.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --openvino_model_dir ""/output/intermediate_results_dir"" --tpu_name ""variantcaller-node1"" --tpu_zone ""europe-west4-a"" --use_tpu. I0524 21:18:26.485428 140032543119168 transport.py:157] Attempting refresh to obtain initial access_token. I0524 21:18:26.576728 140032543119168 call_variants.py:336] Shape of input examples: [100, 221, 6]. I0524 21:18:26.579230 140032543119168 call_variants.py:361] /opt/models/wgs/model.ckpt.input_shape has the correct shape: [100, 221, 6]. 2022-05-24 21:18:26.581705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-05-24 21:18:26.586196: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:728,usability,error,error,728,"Issue with TPU Node v3-8 in call variants step; I am trying to use deepvariant to call variants using a TPU Node v3-8, but I am running into a persistent issue. Here is the command I am using:. ```bash. docker run \. -v `pwd`:`pwd` -w `pwd` \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --call_variants_extra_args use_tpu=true,tpu_name=""variantcaller-node1"",tpu_zone=""europe-west4-a"" \. --model_type=WGS \. --ref=""input/data/${REF}"" \. --reads=""input/data/${BAM}"" \. --output_vcf=""output/${OUTPUT_VCF}"" \. --output_gvcf=""output/${OUTPUT_GVCF}"" \. --regions chr20 \. --num_shards=$(nproc) \. --intermediate_results_dir /output/intermediate_results_dir. ```. However, I am seeing the following error in the call variants step. ```bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@96.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --openvino_model_dir ""/output/intermediate_results_dir"" --tpu_name ""variantcaller-node1"" --tpu_zone ""europe-west4-a"" --use_tpu. I0524 21:18:26.485428 140032543119168 transport.py:157] Attempting refresh to obtain initial access_token. I0524 21:18:26.576728 140032543119168 call_variants.py:336] Shape of input examples: [100, 221, 6]. I0524 21:18:26.579230 140032543119168 call_variants.py:361] /opt/models/wgs/model.ckpt.input_shape has the correct shape: [100, 221, 6]. 2022-05-24 21:18:26.581705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-05-24 21:18:26.586196: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:788,usability,command,command,788,"Issue with TPU Node v3-8 in call variants step; I am trying to use deepvariant to call variants using a TPU Node v3-8, but I am running into a persistent issue. Here is the command I am using:. ```bash. docker run \. -v `pwd`:`pwd` -w `pwd` \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --call_variants_extra_args use_tpu=true,tpu_name=""variantcaller-node1"",tpu_zone=""europe-west4-a"" \. --model_type=WGS \. --ref=""input/data/${REF}"" \. --reads=""input/data/${BAM}"" \. --output_vcf=""output/${OUTPUT_VCF}"" \. --output_gvcf=""output/${OUTPUT_GVCF}"" \. --regions chr20 \. --num_shards=$(nproc) \. --intermediate_results_dir /output/intermediate_results_dir. ```. However, I am seeing the following error in the call variants step. ```bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@96.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --openvino_model_dir ""/output/intermediate_results_dir"" --tpu_name ""variantcaller-node1"" --tpu_zone ""europe-west4-a"" --use_tpu. I0524 21:18:26.485428 140032543119168 transport.py:157] Attempting refresh to obtain initial access_token. I0524 21:18:26.576728 140032543119168 call_variants.py:336] Shape of input examples: [100, 221, 6]. I0524 21:18:26.579230 140032543119168 call_variants.py:361] /opt/models/wgs/model.ckpt.input_shape has the correct shape: [100, 221, 6]. 2022-05-24 21:18:26.581705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-05-24 21:18:26.586196: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:1342,usability,input,input,1342,"args use_tpu=true,tpu_name=""variantcaller-node1"",tpu_zone=""europe-west4-a"" \. --model_type=WGS \. --ref=""input/data/${REF}"" \. --reads=""input/data/${BAM}"" \. --output_vcf=""output/${OUTPUT_VCF}"" \. --output_gvcf=""output/${OUTPUT_GVCF}"" \. --regions chr20 \. --num_shards=$(nproc) \. --intermediate_results_dir /output/intermediate_results_dir. ```. However, I am seeing the following error in the call variants step. ```bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@96.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --openvino_model_dir ""/output/intermediate_results_dir"" --tpu_name ""variantcaller-node1"" --tpu_zone ""europe-west4-a"" --use_tpu. I0524 21:18:26.485428 140032543119168 transport.py:157] Attempting refresh to obtain initial access_token. I0524 21:18:26.576728 140032543119168 call_variants.py:336] Shape of input examples: [100, 221, 6]. I0524 21:18:26.579230 140032543119168 call_variants.py:361] /opt/models/wgs/model.ckpt.input_shape has the correct shape: [100, 221, 6]. 2022-05-24 21:18:26.581705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-05-24 21:18:26.586196: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2022-05-24 21:18:26.587127: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2000160000 Hz. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp_f348kd0. W0524 21:18:26.619681 140032543119168 estimator.py:1846] Using temporary folder as mo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:1717,usability,perform,performance-critical,1717," error in the call variants step. ```bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@96.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --openvino_model_dir ""/output/intermediate_results_dir"" --tpu_name ""variantcaller-node1"" --tpu_zone ""europe-west4-a"" --use_tpu. I0524 21:18:26.485428 140032543119168 transport.py:157] Attempting refresh to obtain initial access_token. I0524 21:18:26.576728 140032543119168 call_variants.py:336] Shape of input examples: [100, 221, 6]. I0524 21:18:26.579230 140032543119168 call_variants.py:361] /opt/models/wgs/model.ckpt.input_shape has the correct shape: [100, 221, 6]. 2022-05-24 21:18:26.581705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-05-24 21:18:26.586196: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2022-05-24 21:18:26.587127: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2000160000 Hz. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp_f348kd0. W0524 21:18:26.619681 140032543119168 estimator.py:1846] Using temporary folder as model directory: /tmp/tmp_f348kd0. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp_f348kd0', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_keep_checkpoint_max': 100000, '_keep_check",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:2050,usability,perform,performance,2050,"ir ""/output/intermediate_results_dir"" --tpu_name ""variantcaller-node1"" --tpu_zone ""europe-west4-a"" --use_tpu. I0524 21:18:26.485428 140032543119168 transport.py:157] Attempting refresh to obtain initial access_token. I0524 21:18:26.576728 140032543119168 call_variants.py:336] Shape of input examples: [100, 221, 6]. I0524 21:18:26.579230 140032543119168 call_variants.py:361] /opt/models/wgs/model.ckpt.input_shape has the correct shape: [100, 221, 6]. 2022-05-24 21:18:26.581705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-05-24 21:18:26.586196: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2022-05-24 21:18:26.587127: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2000160000 Hz. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp_f348kd0. W0524 21:18:26.619681 140032543119168 estimator.py:1846] Using temporary folder as model directory: /tmp/tmp_f348kd0. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp_f348kd0', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:10264,usability,User,UserWarning,10264,"ker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287). INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). I0524 21:18:26.632941 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365). INFO:tensorflow:Calling model_fn. I0524 21:18:26.633588 140032543119168 estimator.py:1162] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn. INFO:tensorflow:TPU job name tpu_worker. I0524 21:18:33.019782 140032543119168 tpu_estimator.py:514] TPU job name tpu_worker. INFO:tensorflow:Graph was finalized. I0524 21:18:33.525068 140032543119168 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0524 21:18:33.525994 140032543119168 saver.py:1298] Restoring parameters from /opt/models/wgs/model.ckpt. INFO:tensorflow:prediction_loop marked as finished. I0524 21:18:34.251420 140032543119168 error_handling.py:115] prediction_loop marked as finished. WARNING:tensorflow:Reraising captured error. W0524 21:18:34.251592 140032543119168 error_handling.py:149] Reraising captured error. Trace",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:11171,usability,error,error,11171,"r/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn. INFO:tensorflow:TPU job name tpu_worker. I0524 21:18:33.019782 140032543119168 tpu_estimator.py:514] TPU job name tpu_worker. INFO:tensorflow:Graph was finalized. I0524 21:18:33.525068 140032543119168 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0524 21:18:33.525994 140032543119168 saver.py:1298] Restoring parameters from /opt/models/wgs/model.ckpt. INFO:tensorflow:prediction_loop marked as finished. I0524 21:18:34.251420 140032543119168 error_handling.py:115] prediction_loop marked as finished. WARNING:tensorflow:Reraising captured error. W0524 21:18:34.251592 140032543119168 error_handling.py:149] Reraising captured error. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1375, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:11258,usability,error,error,11258,"2: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn. INFO:tensorflow:TPU job name tpu_worker. I0524 21:18:33.019782 140032543119168 tpu_estimator.py:514] TPU job name tpu_worker. INFO:tensorflow:Graph was finalized. I0524 21:18:33.525068 140032543119168 monitored_session.py:247] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0524 21:18:33.525994 140032543119168 saver.py:1298] Restoring parameters from /opt/models/wgs/model.ckpt. INFO:tensorflow:prediction_loop marked as finished. I0524 21:18:34.251420 140032543119168 error_handling.py:115] prediction_loop marked as finished. WARNING:tensorflow:Reraising captured error. W0524 21:18:34.251592 140032543119168 error_handling.py:149] Reraising captured error. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1375, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.8",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:17529,usability,input,inputs,17529,"b/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal. restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps. self._AddRestoreOps(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 335, in _AddRestoreOps. all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1490, in restore_v2. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 748, in _apply_op_helper. op = g._create_op_internal(op_type_name, inputs, dtypes=None,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3557, in _create_op_internal. ret = Operation(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 2045, in __init__. self._traceback = tf_stack.extract_stack_for_node(self._c_op). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_o0nxhusg/runfile",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:21584,usability,error,error,21584,". File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 314, in prepare_session. sess, is_loaded_from_checkpoint = self._restore_checkpoint(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 233, in _restore_checkpoint. _restore_checkpoint_and_maybe_run_saved_model_initializers(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers. saver.restore(sess, path). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1339, in restore. raise _wrap_restore_error_with_msg(. tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:. Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'). [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/co",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:26039,usability,input,inputs,26039,"tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 876, in _build. self.saver_def = self._builder._build_internal( # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal. restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps. self._AddRestoreOps(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 335, in _AddRestoreOps. all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1490, in restore_v2. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 748, in _apply_op_helper. op = g._create_op_internal(op_type_name, inputs, dtypes=None,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3557, in _create_op_internal. ret = Operation(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 2045, in __init__. self._traceback = tf_stack.extract_stack_for_node(self._c_op). real 0m10.757s. user 0m13.496s. sys 0m5.144s. ```. This same command works fine without using TPUs on this system, and it looks like the TPU node is being recognized by deepvariant. Is there something I'm missing for call_variants?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:26379,usability,user,user,26379,"tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 876, in _build. self.saver_def = self._builder._build_internal( # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal. restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps. self._AddRestoreOps(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 335, in _AddRestoreOps. all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1490, in restore_v2. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 748, in _apply_op_helper. op = g._create_op_internal(op_type_name, inputs, dtypes=None,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3557, in _create_op_internal. ret = Operation(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 2045, in __init__. self._traceback = tf_stack.extract_stack_for_node(self._c_op). real 0m10.757s. user 0m13.496s. sys 0m5.144s. ```. This same command works fine without using TPUs on this system, and it looks like the TPU node is being recognized by deepvariant. Is there something I'm missing for call_variants?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/537:26424,usability,command,command,26424,"tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 876, in _build. self.saver_def = self._builder._build_internal( # pylint: disable=protected-access. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal. restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps. self._AddRestoreOps(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 335, in _AddRestoreOps. all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1490, in restore_v2. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 748, in _apply_op_helper. op = g._create_op_internal(op_type_name, inputs, dtypes=None,. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3557, in _create_op_internal. ret = Operation(. File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 2045, in __init__. self._traceback = tf_stack.extract_stack_for_node(self._c_op). real 0m10.757s. user 0m13.496s. sys 0m5.144s. ```. This same command works fine without using TPUs on this system, and it looks like the TPU node is being recognized by deepvariant. Is there something I'm missing for call_variants?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537
https://github.com/google/deepvariant/issues/539:899,availability,error,error,899,"Could not read base quality scores; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: yes . **Describe the issue:**. I am running deep-variant trough a docker installation of the pepper-margin-deepvariant pipeline `kishwars/pepper_deepvariant:r0.8-gpu` on data aligned with minimap2 and data aligned with lra. It is working fine with the minimap2 aligned data, but deepvariant does not produce a final VCF with lra aligned data. . It seems that deep-variant cannot read the base quality score during SNP calling:. ```. 2022-05-26 00:08:16.416812: W third_party/nucleus/io/sam_reader.cc:599] Could not read base quality scores 2e95d959-f3f1-403f-acff-a2bf4f2c12fe: Not found: Could not read base quality scores. 2022-05-26 00:08:16.450548: F deepvariant/allelecounter.cc:198] Check failed: offset + len <= read.aligned_quality_size() (81 vs. 0). Fatal Python error: Aborted. ```. and the job eventually fails:. ```. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /media/euphrasie/DATA/reference_genome/hg38/hg38_GenDev.fa --reads /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup*",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/issues/539:2005,availability,Operat,Operating,2005,"examples --mode calling --ref /media/euphrasie/DATA/reference_genome/hg38/hg38_GenDev.fa --reads /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup**. - Operating system: Ubuntu 20.04.4. - DeepVariant version: pepper_deepvariant:r0.8-gpu. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command: . ```. 	docker run --ipc=host \. 	--gpus all \. 	-v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \. 	-v ""${BASE}"":""${BASE}"" \. 	-v ""${REF}"":""${REF}"" \. 	-v ""${BAMPATH}"":""${BAMPATH}"" \. 	kishwars/pepper_deepvariant:r0.8-gpu \. 	run_pepper_margin_deepvariant call_variant \. 	-o ""${OUTPUT_DIR}"" \. 	-b ""${BAM}"" \. 	-f ""${REF}"" \. 	-p ""${OUTPUT_PREFIX}"" \. 	-t ${THREADS} \. 	-g \. 	--ont_r9_guppy5_sup. ```. - Error trace: (if applicable). . [5.1_DeepVariant_SNP.log](https://github.com/google/deepvariant/files/8785347/5.1_DeepVariant_SNP.log). **Does the quick start test work on your system?** yes . Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/issues/539:2698,availability,Error,Error,2698,"s /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup**. - Operating system: Ubuntu 20.04.4. - DeepVariant version: pepper_deepvariant:r0.8-gpu. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command: . ```. 	docker run --ipc=host \. 	--gpus all \. 	-v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \. 	-v ""${BASE}"":""${BASE}"" \. 	-v ""${REF}"":""${REF}"" \. 	-v ""${BAMPATH}"":""${BAMPATH}"" \. 	kishwars/pepper_deepvariant:r0.8-gpu \. 	run_pepper_margin_deepvariant call_variant \. 	-o ""${OUTPUT_DIR}"" \. 	-b ""${BAM}"" \. 	-f ""${REF}"" \. 	-p ""${OUTPUT_PREFIX}"" \. 	-t ${THREADS} \. 	-g \. 	--ont_r9_guppy5_sup. ```. - Error trace: (if applicable). . [5.1_DeepVariant_SNP.log](https://github.com/google/deepvariant/files/8785347/5.1_DeepVariant_SNP.log). **Does the quick start test work on your system?** yes . Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? no. **Any additional context:** Ultra-long reads.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/issues/539:200,deployability,instal,installation,200,"Could not read base quality scores; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: yes . **Describe the issue:**. I am running deep-variant trough a docker installation of the pepper-margin-deepvariant pipeline `kishwars/pepper_deepvariant:r0.8-gpu` on data aligned with minimap2 and data aligned with lra. It is working fine with the minimap2 aligned data, but deepvariant does not produce a final VCF with lra aligned data. . It seems that deep-variant cannot read the base quality score during SNP calling:. ```. 2022-05-26 00:08:16.416812: W third_party/nucleus/io/sam_reader.cc:599] Could not read base quality scores 2e95d959-f3f1-403f-acff-a2bf4f2c12fe: Not found: Could not read base quality scores. 2022-05-26 00:08:16.450548: F deepvariant/allelecounter.cc:198] Check failed: offset + len <= read.aligned_quality_size() (81 vs. 0). Fatal Python error: Aborted. ```. and the job eventually fails:. ```. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /media/euphrasie/DATA/reference_genome/hg38/hg38_GenDev.fa --reads /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup*",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/issues/539:246,deployability,pipelin,pipeline,246,"Could not read base quality scores; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: yes . **Describe the issue:**. I am running deep-variant trough a docker installation of the pepper-margin-deepvariant pipeline `kishwars/pepper_deepvariant:r0.8-gpu` on data aligned with minimap2 and data aligned with lra. It is working fine with the minimap2 aligned data, but deepvariant does not produce a final VCF with lra aligned data. . It seems that deep-variant cannot read the base quality score during SNP calling:. ```. 2022-05-26 00:08:16.416812: W third_party/nucleus/io/sam_reader.cc:599] Could not read base quality scores 2e95d959-f3f1-403f-acff-a2bf4f2c12fe: Not found: Could not read base quality scores. 2022-05-26 00:08:16.450548: F deepvariant/allelecounter.cc:198] Check failed: offset + len <= read.aligned_quality_size() (81 vs. 0). Fatal Python error: Aborted. ```. and the job eventually fails:. ```. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /media/euphrasie/DATA/reference_genome/hg38/hg38_GenDev.fa --reads /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup*",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/issues/539:822,deployability,fail,failed,822,"Could not read base quality scores; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: yes . **Describe the issue:**. I am running deep-variant trough a docker installation of the pepper-margin-deepvariant pipeline `kishwars/pepper_deepvariant:r0.8-gpu` on data aligned with minimap2 and data aligned with lra. It is working fine with the minimap2 aligned data, but deepvariant does not produce a final VCF with lra aligned data. . It seems that deep-variant cannot read the base quality score during SNP calling:. ```. 2022-05-26 00:08:16.416812: W third_party/nucleus/io/sam_reader.cc:599] Could not read base quality scores 2e95d959-f3f1-403f-acff-a2bf4f2c12fe: Not found: Could not read base quality scores. 2022-05-26 00:08:16.450548: F deepvariant/allelecounter.cc:198] Check failed: offset + len <= read.aligned_quality_size() (81 vs. 0). Fatal Python error: Aborted. ```. and the job eventually fails:. ```. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /media/euphrasie/DATA/reference_genome/hg38/hg38_GenDev.fa --reads /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup*",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/issues/539:943,deployability,fail,fails,943,"Could not read base quality scores; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: yes . **Describe the issue:**. I am running deep-variant trough a docker installation of the pepper-margin-deepvariant pipeline `kishwars/pepper_deepvariant:r0.8-gpu` on data aligned with minimap2 and data aligned with lra. It is working fine with the minimap2 aligned data, but deepvariant does not produce a final VCF with lra aligned data. . It seems that deep-variant cannot read the base quality score during SNP calling:. ```. 2022-05-26 00:08:16.416812: W third_party/nucleus/io/sam_reader.cc:599] Could not read base quality scores 2e95d959-f3f1-403f-acff-a2bf4f2c12fe: Not found: Could not read base quality scores. 2022-05-26 00:08:16.450548: F deepvariant/allelecounter.cc:198] Check failed: offset + len <= read.aligned_quality_size() (81 vs. 0). Fatal Python error: Aborted. ```. and the job eventually fails:. ```. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /media/euphrasie/DATA/reference_genome/hg38/hg38_GenDev.fa --reads /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup*",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/issues/539:975,deployability,fail,failed,975,"Could not read base quality scores; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: yes . **Describe the issue:**. I am running deep-variant trough a docker installation of the pepper-margin-deepvariant pipeline `kishwars/pepper_deepvariant:r0.8-gpu` on data aligned with minimap2 and data aligned with lra. It is working fine with the minimap2 aligned data, but deepvariant does not produce a final VCF with lra aligned data. . It seems that deep-variant cannot read the base quality score during SNP calling:. ```. 2022-05-26 00:08:16.416812: W third_party/nucleus/io/sam_reader.cc:599] Could not read base quality scores 2e95d959-f3f1-403f-acff-a2bf4f2c12fe: Not found: Could not read base quality scores. 2022-05-26 00:08:16.450548: F deepvariant/allelecounter.cc:198] Check failed: offset + len <= read.aligned_quality_size() (81 vs. 0). Fatal Python error: Aborted. ```. and the job eventually fails:. ```. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /media/euphrasie/DATA/reference_genome/hg38/hg38_GenDev.fa --reads /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup*",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/issues/539:1974,deployability,log,log,1974,"ailed:. /opt/deepvariant/bin/make_examples --mode calling --ref /media/euphrasie/DATA/reference_genome/hg38/hg38_GenDev.fa --reads /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup**. - Operating system: Ubuntu 20.04.4. - DeepVariant version: pepper_deepvariant:r0.8-gpu. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command: . ```. 	docker run --ipc=host \. 	--gpus all \. 	-v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \. 	-v ""${BASE}"":""${BASE}"" \. 	-v ""${REF}"":""${REF}"" \. 	-v ""${BAMPATH}"":""${BAMPATH}"" \. 	kishwars/pepper_deepvariant:r0.8-gpu \. 	run_pepper_margin_deepvariant call_variant \. 	-o ""${OUTPUT_DIR}"" \. 	-b ""${BAM}"" \. 	-f ""${REF}"" \. 	-p ""${OUTPUT_PREFIX}"" \. 	-t ${THREADS} \. 	-g \. 	--ont_r9_guppy5_sup. ```. - Error trace: (if applicable). . [5.1_DeepVariant_SNP.log](https://github.com/google/deepvariant/files/8785347/5.1_DeepVariant_SNP.log). **Does the quick start test work on your system?** yes . Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-qu",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/issues/539:2053,deployability,version,version,2053,"DATA/reference_genome/hg38/hg38_GenDev.fa --reads /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup**. - Operating system: Ubuntu 20.04.4. - DeepVariant version: pepper_deepvariant:r0.8-gpu. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command: . ```. 	docker run --ipc=host \. 	--gpus all \. 	-v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \. 	-v ""${BASE}"":""${BASE}"" \. 	-v ""${REF}"":""${REF}"" \. 	-v ""${BAMPATH}"":""${BAMPATH}"" \. 	kishwars/pepper_deepvariant:r0.8-gpu \. 	run_pepper_margin_deepvariant call_variant \. 	-o ""${OUTPUT_DIR}"" \. 	-b ""${BAM}"" \. 	-f ""${REF}"" \. 	-p ""${OUTPUT_PREFIX}"" \. 	-t ${THREADS} \. 	-g \. 	--ont_r9_guppy5_sup. ```. - Error trace: (if applicable). . [5.1_DeepVariant_SNP.log](https://github.com/google/deepvariant/files/8785347/5.1_DeepVariant_SNP.log). **Does the quick start test work on your system?** yes . Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? n",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/issues/539:2093,deployability,Instal,Installation,2093,"--reads /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup**. - Operating system: Ubuntu 20.04.4. - DeepVariant version: pepper_deepvariant:r0.8-gpu. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command: . ```. 	docker run --ipc=host \. 	--gpus all \. 	-v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \. 	-v ""${BASE}"":""${BASE}"" \. 	-v ""${REF}"":""${REF}"" \. 	-v ""${BAMPATH}"":""${BAMPATH}"" \. 	kishwars/pepper_deepvariant:r0.8-gpu \. 	run_pepper_margin_deepvariant call_variant \. 	-o ""${OUTPUT_DIR}"" \. 	-b ""${BAM}"" \. 	-f ""${REF}"" \. 	-p ""${OUTPUT_PREFIX}"" \. 	-t ${THREADS} \. 	-g \. 	--ont_r9_guppy5_sup. ```. - Error trace: (if applicable). . [5.1_DeepVariant_SNP.log](https://github.com/google/deepvariant/files/8785347/5.1_DeepVariant_SNP.log). **Does the quick start test work on your system?** yes . Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? no. **Any additional context:** Ultra-long ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/issues/539:2751,deployability,log,log,2751,"s /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup**. - Operating system: Ubuntu 20.04.4. - DeepVariant version: pepper_deepvariant:r0.8-gpu. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command: . ```. 	docker run --ipc=host \. 	--gpus all \. 	-v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \. 	-v ""${BASE}"":""${BASE}"" \. 	-v ""${REF}"":""${REF}"" \. 	-v ""${BAMPATH}"":""${BAMPATH}"" \. 	kishwars/pepper_deepvariant:r0.8-gpu \. 	run_pepper_margin_deepvariant call_variant \. 	-o ""${OUTPUT_DIR}"" \. 	-b ""${BAM}"" \. 	-f ""${REF}"" \. 	-p ""${OUTPUT_PREFIX}"" \. 	-t ${THREADS} \. 	-g \. 	--ont_r9_guppy5_sup. ```. - Error trace: (if applicable). . [5.1_DeepVariant_SNP.log](https://github.com/google/deepvariant/files/8785347/5.1_DeepVariant_SNP.log). **Does the quick start test work on your system?** yes . Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? no. **Any additional context:** Ultra-long reads.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/issues/539:2828,deployability,log,log,2828,"s /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup**. - Operating system: Ubuntu 20.04.4. - DeepVariant version: pepper_deepvariant:r0.8-gpu. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command: . ```. 	docker run --ipc=host \. 	--gpus all \. 	-v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \. 	-v ""${BASE}"":""${BASE}"" \. 	-v ""${REF}"":""${REF}"" \. 	-v ""${BAMPATH}"":""${BAMPATH}"" \. 	kishwars/pepper_deepvariant:r0.8-gpu \. 	run_pepper_margin_deepvariant call_variant \. 	-o ""${OUTPUT_DIR}"" \. 	-b ""${BAM}"" \. 	-f ""${REF}"" \. 	-p ""${OUTPUT_PREFIX}"" \. 	-t ${THREADS} \. 	-g \. 	--ont_r9_guppy5_sup. ```. - Error trace: (if applicable). . [5.1_DeepVariant_SNP.log](https://github.com/google/deepvariant/files/8785347/5.1_DeepVariant_SNP.log). **Does the quick start test work on your system?** yes . Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? no. **Any additional context:** Ultra-long reads.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/issues/539:289,energy efficiency,gpu,gpu,289,"Could not read base quality scores; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: yes . **Describe the issue:**. I am running deep-variant trough a docker installation of the pepper-margin-deepvariant pipeline `kishwars/pepper_deepvariant:r0.8-gpu` on data aligned with minimap2 and data aligned with lra. It is working fine with the minimap2 aligned data, but deepvariant does not produce a final VCF with lra aligned data. . It seems that deep-variant cannot read the base quality score during SNP calling:. ```. 2022-05-26 00:08:16.416812: W third_party/nucleus/io/sam_reader.cc:599] Could not read base quality scores 2e95d959-f3f1-403f-acff-a2bf4f2c12fe: Not found: Could not read base quality scores. 2022-05-26 00:08:16.450548: F deepvariant/allelecounter.cc:198] Check failed: offset + len <= read.aligned_quality_size() (81 vs. 0). Fatal Python error: Aborted. ```. and the job eventually fails:. ```. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /media/euphrasie/DATA/reference_genome/hg38/hg38_GenDev.fa --reads /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup*",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/issues/539:2086,energy efficiency,gpu,gpu,2086,"_GenDev.fa --reads /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup**. - Operating system: Ubuntu 20.04.4. - DeepVariant version: pepper_deepvariant:r0.8-gpu. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command: . ```. 	docker run --ipc=host \. 	--gpus all \. 	-v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \. 	-v ""${BASE}"":""${BASE}"" \. 	-v ""${REF}"":""${REF}"" \. 	-v ""${BAMPATH}"":""${BAMPATH}"" \. 	kishwars/pepper_deepvariant:r0.8-gpu \. 	run_pepper_margin_deepvariant call_variant \. 	-o ""${OUTPUT_DIR}"" \. 	-b ""${BAM}"" \. 	-f ""${REF}"" \. 	-p ""${OUTPUT_PREFIX}"" \. 	-t ${THREADS} \. 	-g \. 	--ont_r9_guppy5_sup. ```. - Error trace: (if applicable). . [5.1_DeepVariant_SNP.log](https://github.com/google/deepvariant/files/8785347/5.1_DeepVariant_SNP.log). **Does the quick start test work on your system?** yes . Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? no. **Any additional context:** ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/issues/539:2339,energy efficiency,gpu,gpus,2339,"s /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup**. - Operating system: Ubuntu 20.04.4. - DeepVariant version: pepper_deepvariant:r0.8-gpu. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command: . ```. 	docker run --ipc=host \. 	--gpus all \. 	-v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \. 	-v ""${BASE}"":""${BASE}"" \. 	-v ""${REF}"":""${REF}"" \. 	-v ""${BAMPATH}"":""${BAMPATH}"" \. 	kishwars/pepper_deepvariant:r0.8-gpu \. 	run_pepper_margin_deepvariant call_variant \. 	-o ""${OUTPUT_DIR}"" \. 	-b ""${BAM}"" \. 	-f ""${REF}"" \. 	-p ""${OUTPUT_PREFIX}"" \. 	-t ${THREADS} \. 	-g \. 	--ont_r9_guppy5_sup. ```. - Error trace: (if applicable). . [5.1_DeepVariant_SNP.log](https://github.com/google/deepvariant/files/8785347/5.1_DeepVariant_SNP.log). **Does the quick start test work on your system?** yes . Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? no. **Any additional context:** Ultra-long reads.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/issues/539:2509,energy efficiency,gpu,gpu,2509,"s /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup**. - Operating system: Ubuntu 20.04.4. - DeepVariant version: pepper_deepvariant:r0.8-gpu. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command: . ```. 	docker run --ipc=host \. 	--gpus all \. 	-v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \. 	-v ""${BASE}"":""${BASE}"" \. 	-v ""${REF}"":""${REF}"" \. 	-v ""${BAMPATH}"":""${BAMPATH}"" \. 	kishwars/pepper_deepvariant:r0.8-gpu \. 	run_pepper_margin_deepvariant call_variant \. 	-o ""${OUTPUT_DIR}"" \. 	-b ""${BAM}"" \. 	-f ""${REF}"" \. 	-p ""${OUTPUT_PREFIX}"" \. 	-t ${THREADS} \. 	-g \. 	--ont_r9_guppy5_sup. ```. - Error trace: (if applicable). . [5.1_DeepVariant_SNP.log](https://github.com/google/deepvariant/files/8785347/5.1_DeepVariant_SNP.log). **Does the quick start test work on your system?** yes . Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? no. **Any additional context:** Ultra-long reads.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/issues/539:246,integrability,pipelin,pipeline,246,"Could not read base quality scores; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: yes . **Describe the issue:**. I am running deep-variant trough a docker installation of the pepper-margin-deepvariant pipeline `kishwars/pepper_deepvariant:r0.8-gpu` on data aligned with minimap2 and data aligned with lra. It is working fine with the minimap2 aligned data, but deepvariant does not produce a final VCF with lra aligned data. . It seems that deep-variant cannot read the base quality score during SNP calling:. ```. 2022-05-26 00:08:16.416812: W third_party/nucleus/io/sam_reader.cc:599] Could not read base quality scores 2e95d959-f3f1-403f-acff-a2bf4f2c12fe: Not found: Could not read base quality scores. 2022-05-26 00:08:16.450548: F deepvariant/allelecounter.cc:198] Check failed: offset + len <= read.aligned_quality_size() (81 vs. 0). Fatal Python error: Aborted. ```. and the job eventually fails:. ```. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /media/euphrasie/DATA/reference_genome/hg38/hg38_GenDev.fa --reads /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup*",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/issues/539:932,integrability,event,eventually,932,"Could not read base quality scores; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: yes . **Describe the issue:**. I am running deep-variant trough a docker installation of the pepper-margin-deepvariant pipeline `kishwars/pepper_deepvariant:r0.8-gpu` on data aligned with minimap2 and data aligned with lra. It is working fine with the minimap2 aligned data, but deepvariant does not produce a final VCF with lra aligned data. . It seems that deep-variant cannot read the base quality score during SNP calling:. ```. 2022-05-26 00:08:16.416812: W third_party/nucleus/io/sam_reader.cc:599] Could not read base quality scores 2e95d959-f3f1-403f-acff-a2bf4f2c12fe: Not found: Could not read base quality scores. 2022-05-26 00:08:16.450548: F deepvariant/allelecounter.cc:198] Check failed: offset + len <= read.aligned_quality_size() (81 vs. 0). Fatal Python error: Aborted. ```. and the job eventually fails:. ```. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /media/euphrasie/DATA/reference_genome/hg38/hg38_GenDev.fa --reads /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup*",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/issues/539:2053,integrability,version,version,2053,"DATA/reference_genome/hg38/hg38_GenDev.fa --reads /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup**. - Operating system: Ubuntu 20.04.4. - DeepVariant version: pepper_deepvariant:r0.8-gpu. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command: . ```. 	docker run --ipc=host \. 	--gpus all \. 	-v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \. 	-v ""${BASE}"":""${BASE}"" \. 	-v ""${REF}"":""${REF}"" \. 	-v ""${BAMPATH}"":""${BAMPATH}"" \. 	kishwars/pepper_deepvariant:r0.8-gpu \. 	run_pepper_margin_deepvariant call_variant \. 	-o ""${OUTPUT_DIR}"" \. 	-b ""${BAM}"" \. 	-f ""${REF}"" \. 	-p ""${OUTPUT_PREFIX}"" \. 	-t ${THREADS} \. 	-g \. 	--ont_r9_guppy5_sup. ```. - Error trace: (if applicable). . [5.1_DeepVariant_SNP.log](https://github.com/google/deepvariant/files/8785347/5.1_DeepVariant_SNP.log). **Does the quick start test work on your system?** yes . Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? n",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/issues/539:2053,modifiability,version,version,2053,"DATA/reference_genome/hg38/hg38_GenDev.fa --reads /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup**. - Operating system: Ubuntu 20.04.4. - DeepVariant version: pepper_deepvariant:r0.8-gpu. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command: . ```. 	docker run --ipc=host \. 	--gpus all \. 	-v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \. 	-v ""${BASE}"":""${BASE}"" \. 	-v ""${REF}"":""${REF}"" \. 	-v ""${BAMPATH}"":""${BAMPATH}"" \. 	kishwars/pepper_deepvariant:r0.8-gpu \. 	run_pepper_margin_deepvariant call_variant \. 	-o ""${OUTPUT_DIR}"" \. 	-b ""${BAM}"" \. 	-f ""${REF}"" \. 	-p ""${OUTPUT_PREFIX}"" \. 	-t ${THREADS} \. 	-g \. 	--ont_r9_guppy5_sup. ```. - Error trace: (if applicable). . [5.1_DeepVariant_SNP.log](https://github.com/google/deepvariant/files/8785347/5.1_DeepVariant_SNP.log). **Does the quick start test work on your system?** yes . Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? n",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/issues/539:289,performance,gpu,gpu,289,"Could not read base quality scores; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: yes . **Describe the issue:**. I am running deep-variant trough a docker installation of the pepper-margin-deepvariant pipeline `kishwars/pepper_deepvariant:r0.8-gpu` on data aligned with minimap2 and data aligned with lra. It is working fine with the minimap2 aligned data, but deepvariant does not produce a final VCF with lra aligned data. . It seems that deep-variant cannot read the base quality score during SNP calling:. ```. 2022-05-26 00:08:16.416812: W third_party/nucleus/io/sam_reader.cc:599] Could not read base quality scores 2e95d959-f3f1-403f-acff-a2bf4f2c12fe: Not found: Could not read base quality scores. 2022-05-26 00:08:16.450548: F deepvariant/allelecounter.cc:198] Check failed: offset + len <= read.aligned_quality_size() (81 vs. 0). Fatal Python error: Aborted. ```. and the job eventually fails:. ```. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /media/euphrasie/DATA/reference_genome/hg38/hg38_GenDev.fa --reads /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup*",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/issues/539:899,performance,error,error,899,"Could not read base quality scores; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: yes . **Describe the issue:**. I am running deep-variant trough a docker installation of the pepper-margin-deepvariant pipeline `kishwars/pepper_deepvariant:r0.8-gpu` on data aligned with minimap2 and data aligned with lra. It is working fine with the minimap2 aligned data, but deepvariant does not produce a final VCF with lra aligned data. . It seems that deep-variant cannot read the base quality score during SNP calling:. ```. 2022-05-26 00:08:16.416812: W third_party/nucleus/io/sam_reader.cc:599] Could not read base quality scores 2e95d959-f3f1-403f-acff-a2bf4f2c12fe: Not found: Could not read base quality scores. 2022-05-26 00:08:16.450548: F deepvariant/allelecounter.cc:198] Check failed: offset + len <= read.aligned_quality_size() (81 vs. 0). Fatal Python error: Aborted. ```. and the job eventually fails:. ```. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /media/euphrasie/DATA/reference_genome/hg38/hg38_GenDev.fa --reads /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup*",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/issues/539:956,performance,parallel,parallel,956,"Could not read base quality scores; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: yes . **Describe the issue:**. I am running deep-variant trough a docker installation of the pepper-margin-deepvariant pipeline `kishwars/pepper_deepvariant:r0.8-gpu` on data aligned with minimap2 and data aligned with lra. It is working fine with the minimap2 aligned data, but deepvariant does not produce a final VCF with lra aligned data. . It seems that deep-variant cannot read the base quality score during SNP calling:. ```. 2022-05-26 00:08:16.416812: W third_party/nucleus/io/sam_reader.cc:599] Could not read base quality scores 2e95d959-f3f1-403f-acff-a2bf4f2c12fe: Not found: Could not read base quality scores. 2022-05-26 00:08:16.450548: F deepvariant/allelecounter.cc:198] Check failed: offset + len <= read.aligned_quality_size() (81 vs. 0). Fatal Python error: Aborted. ```. and the job eventually fails:. ```. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /media/euphrasie/DATA/reference_genome/hg38/hg38_GenDev.fa --reads /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup*",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/issues/539:2086,performance,gpu,gpu,2086,"_GenDev.fa --reads /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup**. - Operating system: Ubuntu 20.04.4. - DeepVariant version: pepper_deepvariant:r0.8-gpu. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command: . ```. 	docker run --ipc=host \. 	--gpus all \. 	-v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \. 	-v ""${BASE}"":""${BASE}"" \. 	-v ""${REF}"":""${REF}"" \. 	-v ""${BAMPATH}"":""${BAMPATH}"" \. 	kishwars/pepper_deepvariant:r0.8-gpu \. 	run_pepper_margin_deepvariant call_variant \. 	-o ""${OUTPUT_DIR}"" \. 	-b ""${BAM}"" \. 	-f ""${REF}"" \. 	-p ""${OUTPUT_PREFIX}"" \. 	-t ${THREADS} \. 	-g \. 	--ont_r9_guppy5_sup. ```. - Error trace: (if applicable). . [5.1_DeepVariant_SNP.log](https://github.com/google/deepvariant/files/8785347/5.1_DeepVariant_SNP.log). **Does the quick start test work on your system?** yes . Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? no. **Any additional context:** ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/issues/539:2339,performance,gpu,gpus,2339,"s /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup**. - Operating system: Ubuntu 20.04.4. - DeepVariant version: pepper_deepvariant:r0.8-gpu. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command: . ```. 	docker run --ipc=host \. 	--gpus all \. 	-v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \. 	-v ""${BASE}"":""${BASE}"" \. 	-v ""${REF}"":""${REF}"" \. 	-v ""${BAMPATH}"":""${BAMPATH}"" \. 	kishwars/pepper_deepvariant:r0.8-gpu \. 	run_pepper_margin_deepvariant call_variant \. 	-o ""${OUTPUT_DIR}"" \. 	-b ""${BAM}"" \. 	-f ""${REF}"" \. 	-p ""${OUTPUT_PREFIX}"" \. 	-t ${THREADS} \. 	-g \. 	--ont_r9_guppy5_sup. ```. - Error trace: (if applicable). . [5.1_DeepVariant_SNP.log](https://github.com/google/deepvariant/files/8785347/5.1_DeepVariant_SNP.log). **Does the quick start test work on your system?** yes . Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? no. **Any additional context:** Ultra-long reads.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/issues/539:2509,performance,gpu,gpu,2509,"s /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup**. - Operating system: Ubuntu 20.04.4. - DeepVariant version: pepper_deepvariant:r0.8-gpu. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command: . ```. 	docker run --ipc=host \. 	--gpus all \. 	-v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \. 	-v ""${BASE}"":""${BASE}"" \. 	-v ""${REF}"":""${REF}"" \. 	-v ""${BAMPATH}"":""${BAMPATH}"" \. 	kishwars/pepper_deepvariant:r0.8-gpu \. 	run_pepper_margin_deepvariant call_variant \. 	-o ""${OUTPUT_DIR}"" \. 	-b ""${BAM}"" \. 	-f ""${REF}"" \. 	-p ""${OUTPUT_PREFIX}"" \. 	-t ${THREADS} \. 	-g \. 	--ont_r9_guppy5_sup. ```. - Error trace: (if applicable). . [5.1_DeepVariant_SNP.log](https://github.com/google/deepvariant/files/8785347/5.1_DeepVariant_SNP.log). **Does the quick start test work on your system?** yes . Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? no. **Any additional context:** Ultra-long reads.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/issues/539:2698,performance,Error,Error,2698,"s /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup**. - Operating system: Ubuntu 20.04.4. - DeepVariant version: pepper_deepvariant:r0.8-gpu. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command: . ```. 	docker run --ipc=host \. 	--gpus all \. 	-v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \. 	-v ""${BASE}"":""${BASE}"" \. 	-v ""${REF}"":""${REF}"" \. 	-v ""${BAMPATH}"":""${BAMPATH}"" \. 	kishwars/pepper_deepvariant:r0.8-gpu \. 	run_pepper_margin_deepvariant call_variant \. 	-o ""${OUTPUT_DIR}"" \. 	-b ""${BAM}"" \. 	-f ""${REF}"" \. 	-p ""${OUTPUT_PREFIX}"" \. 	-t ${THREADS} \. 	-g \. 	--ont_r9_guppy5_sup. ```. - Error trace: (if applicable). . [5.1_DeepVariant_SNP.log](https://github.com/google/deepvariant/files/8785347/5.1_DeepVariant_SNP.log). **Does the quick start test work on your system?** yes . Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? no. **Any additional context:** Ultra-long reads.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/issues/539:418,reliability,doe,does,418,"Could not read base quality scores; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: yes . **Describe the issue:**. I am running deep-variant trough a docker installation of the pepper-margin-deepvariant pipeline `kishwars/pepper_deepvariant:r0.8-gpu` on data aligned with minimap2 and data aligned with lra. It is working fine with the minimap2 aligned data, but deepvariant does not produce a final VCF with lra aligned data. . It seems that deep-variant cannot read the base quality score during SNP calling:. ```. 2022-05-26 00:08:16.416812: W third_party/nucleus/io/sam_reader.cc:599] Could not read base quality scores 2e95d959-f3f1-403f-acff-a2bf4f2c12fe: Not found: Could not read base quality scores. 2022-05-26 00:08:16.450548: F deepvariant/allelecounter.cc:198] Check failed: offset + len <= read.aligned_quality_size() (81 vs. 0). Fatal Python error: Aborted. ```. and the job eventually fails:. ```. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /media/euphrasie/DATA/reference_genome/hg38/hg38_GenDev.fa --reads /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup*",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/issues/539:822,reliability,fail,failed,822,"Could not read base quality scores; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: yes . **Describe the issue:**. I am running deep-variant trough a docker installation of the pepper-margin-deepvariant pipeline `kishwars/pepper_deepvariant:r0.8-gpu` on data aligned with minimap2 and data aligned with lra. It is working fine with the minimap2 aligned data, but deepvariant does not produce a final VCF with lra aligned data. . It seems that deep-variant cannot read the base quality score during SNP calling:. ```. 2022-05-26 00:08:16.416812: W third_party/nucleus/io/sam_reader.cc:599] Could not read base quality scores 2e95d959-f3f1-403f-acff-a2bf4f2c12fe: Not found: Could not read base quality scores. 2022-05-26 00:08:16.450548: F deepvariant/allelecounter.cc:198] Check failed: offset + len <= read.aligned_quality_size() (81 vs. 0). Fatal Python error: Aborted. ```. and the job eventually fails:. ```. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /media/euphrasie/DATA/reference_genome/hg38/hg38_GenDev.fa --reads /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup*",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/issues/539:943,reliability,fail,fails,943,"Could not read base quality scores; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: yes . **Describe the issue:**. I am running deep-variant trough a docker installation of the pepper-margin-deepvariant pipeline `kishwars/pepper_deepvariant:r0.8-gpu` on data aligned with minimap2 and data aligned with lra. It is working fine with the minimap2 aligned data, but deepvariant does not produce a final VCF with lra aligned data. . It seems that deep-variant cannot read the base quality score during SNP calling:. ```. 2022-05-26 00:08:16.416812: W third_party/nucleus/io/sam_reader.cc:599] Could not read base quality scores 2e95d959-f3f1-403f-acff-a2bf4f2c12fe: Not found: Could not read base quality scores. 2022-05-26 00:08:16.450548: F deepvariant/allelecounter.cc:198] Check failed: offset + len <= read.aligned_quality_size() (81 vs. 0). Fatal Python error: Aborted. ```. and the job eventually fails:. ```. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /media/euphrasie/DATA/reference_genome/hg38/hg38_GenDev.fa --reads /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup*",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/issues/539:975,reliability,fail,failed,975,"Could not read base quality scores; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: yes . **Describe the issue:**. I am running deep-variant trough a docker installation of the pepper-margin-deepvariant pipeline `kishwars/pepper_deepvariant:r0.8-gpu` on data aligned with minimap2 and data aligned with lra. It is working fine with the minimap2 aligned data, but deepvariant does not produce a final VCF with lra aligned data. . It seems that deep-variant cannot read the base quality score during SNP calling:. ```. 2022-05-26 00:08:16.416812: W third_party/nucleus/io/sam_reader.cc:599] Could not read base quality scores 2e95d959-f3f1-403f-acff-a2bf4f2c12fe: Not found: Could not read base quality scores. 2022-05-26 00:08:16.450548: F deepvariant/allelecounter.cc:198] Check failed: offset + len <= read.aligned_quality_size() (81 vs. 0). Fatal Python error: Aborted. ```. and the job eventually fails:. ```. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /media/euphrasie/DATA/reference_genome/hg38/hg38_GenDev.fa --reads /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup*",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/issues/539:2836,reliability,Doe,Does,2836,"s /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup**. - Operating system: Ubuntu 20.04.4. - DeepVariant version: pepper_deepvariant:r0.8-gpu. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command: . ```. 	docker run --ipc=host \. 	--gpus all \. 	-v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \. 	-v ""${BASE}"":""${BASE}"" \. 	-v ""${REF}"":""${REF}"" \. 	-v ""${BAMPATH}"":""${BAMPATH}"" \. 	kishwars/pepper_deepvariant:r0.8-gpu \. 	run_pepper_margin_deepvariant call_variant \. 	-o ""${OUTPUT_DIR}"" \. 	-b ""${BAM}"" \. 	-f ""${REF}"" \. 	-p ""${OUTPUT_PREFIX}"" \. 	-t ${THREADS} \. 	-g \. 	--ont_r9_guppy5_sup. ```. - Error trace: (if applicable). . [5.1_DeepVariant_SNP.log](https://github.com/google/deepvariant/files/8785347/5.1_DeepVariant_SNP.log). **Does the quick start test work on your system?** yes . Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? no. **Any additional context:** Ultra-long reads.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/issues/539:899,safety,error,error,899,"Could not read base quality scores; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: yes . **Describe the issue:**. I am running deep-variant trough a docker installation of the pepper-margin-deepvariant pipeline `kishwars/pepper_deepvariant:r0.8-gpu` on data aligned with minimap2 and data aligned with lra. It is working fine with the minimap2 aligned data, but deepvariant does not produce a final VCF with lra aligned data. . It seems that deep-variant cannot read the base quality score during SNP calling:. ```. 2022-05-26 00:08:16.416812: W third_party/nucleus/io/sam_reader.cc:599] Could not read base quality scores 2e95d959-f3f1-403f-acff-a2bf4f2c12fe: Not found: Could not read base quality scores. 2022-05-26 00:08:16.450548: F deepvariant/allelecounter.cc:198] Check failed: offset + len <= read.aligned_quality_size() (81 vs. 0). Fatal Python error: Aborted. ```. and the job eventually fails:. ```. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /media/euphrasie/DATA/reference_genome/hg38/hg38_GenDev.fa --reads /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup*",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/issues/539:1974,safety,log,log,1974,"ailed:. /opt/deepvariant/bin/make_examples --mode calling --ref /media/euphrasie/DATA/reference_genome/hg38/hg38_GenDev.fa --reads /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup**. - Operating system: Ubuntu 20.04.4. - DeepVariant version: pepper_deepvariant:r0.8-gpu. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command: . ```. 	docker run --ipc=host \. 	--gpus all \. 	-v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \. 	-v ""${BASE}"":""${BASE}"" \. 	-v ""${REF}"":""${REF}"" \. 	-v ""${BAMPATH}"":""${BAMPATH}"" \. 	kishwars/pepper_deepvariant:r0.8-gpu \. 	run_pepper_margin_deepvariant call_variant \. 	-o ""${OUTPUT_DIR}"" \. 	-b ""${BAM}"" \. 	-f ""${REF}"" \. 	-p ""${OUTPUT_PREFIX}"" \. 	-t ${THREADS} \. 	-g \. 	--ont_r9_guppy5_sup. ```. - Error trace: (if applicable). . [5.1_DeepVariant_SNP.log](https://github.com/google/deepvariant/files/8785347/5.1_DeepVariant_SNP.log). **Does the quick start test work on your system?** yes . Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-qu",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/issues/539:2698,safety,Error,Error,2698,"s /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup**. - Operating system: Ubuntu 20.04.4. - DeepVariant version: pepper_deepvariant:r0.8-gpu. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command: . ```. 	docker run --ipc=host \. 	--gpus all \. 	-v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \. 	-v ""${BASE}"":""${BASE}"" \. 	-v ""${REF}"":""${REF}"" \. 	-v ""${BAMPATH}"":""${BAMPATH}"" \. 	kishwars/pepper_deepvariant:r0.8-gpu \. 	run_pepper_margin_deepvariant call_variant \. 	-o ""${OUTPUT_DIR}"" \. 	-b ""${BAM}"" \. 	-f ""${REF}"" \. 	-p ""${OUTPUT_PREFIX}"" \. 	-t ${THREADS} \. 	-g \. 	--ont_r9_guppy5_sup. ```. - Error trace: (if applicable). . [5.1_DeepVariant_SNP.log](https://github.com/google/deepvariant/files/8785347/5.1_DeepVariant_SNP.log). **Does the quick start test work on your system?** yes . Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? no. **Any additional context:** Ultra-long reads.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/issues/539:2751,safety,log,log,2751,"s /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup**. - Operating system: Ubuntu 20.04.4. - DeepVariant version: pepper_deepvariant:r0.8-gpu. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command: . ```. 	docker run --ipc=host \. 	--gpus all \. 	-v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \. 	-v ""${BASE}"":""${BASE}"" \. 	-v ""${REF}"":""${REF}"" \. 	-v ""${BAMPATH}"":""${BAMPATH}"" \. 	kishwars/pepper_deepvariant:r0.8-gpu \. 	run_pepper_margin_deepvariant call_variant \. 	-o ""${OUTPUT_DIR}"" \. 	-b ""${BAM}"" \. 	-f ""${REF}"" \. 	-p ""${OUTPUT_PREFIX}"" \. 	-t ${THREADS} \. 	-g \. 	--ont_r9_guppy5_sup. ```. - Error trace: (if applicable). . [5.1_DeepVariant_SNP.log](https://github.com/google/deepvariant/files/8785347/5.1_DeepVariant_SNP.log). **Does the quick start test work on your system?** yes . Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? no. **Any additional context:** Ultra-long reads.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/issues/539:2828,safety,log,log,2828,"s /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup**. - Operating system: Ubuntu 20.04.4. - DeepVariant version: pepper_deepvariant:r0.8-gpu. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command: . ```. 	docker run --ipc=host \. 	--gpus all \. 	-v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \. 	-v ""${BASE}"":""${BASE}"" \. 	-v ""${REF}"":""${REF}"" \. 	-v ""${BAMPATH}"":""${BAMPATH}"" \. 	kishwars/pepper_deepvariant:r0.8-gpu \. 	run_pepper_margin_deepvariant call_variant \. 	-o ""${OUTPUT_DIR}"" \. 	-b ""${BAM}"" \. 	-f ""${REF}"" \. 	-p ""${OUTPUT_PREFIX}"" \. 	-t ${THREADS} \. 	-g \. 	--ont_r9_guppy5_sup. ```. - Error trace: (if applicable). . [5.1_DeepVariant_SNP.log](https://github.com/google/deepvariant/files/8785347/5.1_DeepVariant_SNP.log). **Does the quick start test work on your system?** yes . Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? no. **Any additional context:** Ultra-long reads.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/issues/539:2857,safety,test,test,2857,"s /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup**. - Operating system: Ubuntu 20.04.4. - DeepVariant version: pepper_deepvariant:r0.8-gpu. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command: . ```. 	docker run --ipc=host \. 	--gpus all \. 	-v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \. 	-v ""${BASE}"":""${BASE}"" \. 	-v ""${REF}"":""${REF}"" \. 	-v ""${BAMPATH}"":""${BAMPATH}"" \. 	kishwars/pepper_deepvariant:r0.8-gpu \. 	run_pepper_margin_deepvariant call_variant \. 	-o ""${OUTPUT_DIR}"" \. 	-b ""${BAM}"" \. 	-f ""${REF}"" \. 	-p ""${OUTPUT_PREFIX}"" \. 	-t ${THREADS} \. 	-g \. 	--ont_r9_guppy5_sup. ```. - Error trace: (if applicable). . [5.1_DeepVariant_SNP.log](https://github.com/google/deepvariant/files/8785347/5.1_DeepVariant_SNP.log). **Does the quick start test work on your system?** yes . Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? no. **Any additional context:** Ultra-long reads.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/issues/539:2898,safety,test,test,2898,"s /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup**. - Operating system: Ubuntu 20.04.4. - DeepVariant version: pepper_deepvariant:r0.8-gpu. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command: . ```. 	docker run --ipc=host \. 	--gpus all \. 	-v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \. 	-v ""${BASE}"":""${BASE}"" \. 	-v ""${REF}"":""${REF}"" \. 	-v ""${BAMPATH}"":""${BAMPATH}"" \. 	kishwars/pepper_deepvariant:r0.8-gpu \. 	run_pepper_margin_deepvariant call_variant \. 	-o ""${OUTPUT_DIR}"" \. 	-b ""${BAM}"" \. 	-f ""${REF}"" \. 	-p ""${OUTPUT_PREFIX}"" \. 	-t ${THREADS} \. 	-g \. 	--ont_r9_guppy5_sup. ```. - Error trace: (if applicable). . [5.1_DeepVariant_SNP.log](https://github.com/google/deepvariant/files/8785347/5.1_DeepVariant_SNP.log). **Does the quick start test work on your system?** yes . Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? no. **Any additional context:** Ultra-long reads.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/issues/539:1974,security,log,log,1974,"ailed:. /opt/deepvariant/bin/make_examples --mode calling --ref /media/euphrasie/DATA/reference_genome/hg38/hg38_GenDev.fa --reads /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup**. - Operating system: Ubuntu 20.04.4. - DeepVariant version: pepper_deepvariant:r0.8-gpu. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command: . ```. 	docker run --ipc=host \. 	--gpus all \. 	-v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \. 	-v ""${BASE}"":""${BASE}"" \. 	-v ""${REF}"":""${REF}"" \. 	-v ""${BAMPATH}"":""${BAMPATH}"" \. 	kishwars/pepper_deepvariant:r0.8-gpu \. 	run_pepper_margin_deepvariant call_variant \. 	-o ""${OUTPUT_DIR}"" \. 	-b ""${BAM}"" \. 	-f ""${REF}"" \. 	-p ""${OUTPUT_PREFIX}"" \. 	-t ${THREADS} \. 	-g \. 	--ont_r9_guppy5_sup. ```. - Error trace: (if applicable). . [5.1_DeepVariant_SNP.log](https://github.com/google/deepvariant/files/8785347/5.1_DeepVariant_SNP.log). **Does the quick start test work on your system?** yes . Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-qu",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/issues/539:2751,security,log,log,2751,"s /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup**. - Operating system: Ubuntu 20.04.4. - DeepVariant version: pepper_deepvariant:r0.8-gpu. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command: . ```. 	docker run --ipc=host \. 	--gpus all \. 	-v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \. 	-v ""${BASE}"":""${BASE}"" \. 	-v ""${REF}"":""${REF}"" \. 	-v ""${BAMPATH}"":""${BAMPATH}"" \. 	kishwars/pepper_deepvariant:r0.8-gpu \. 	run_pepper_margin_deepvariant call_variant \. 	-o ""${OUTPUT_DIR}"" \. 	-b ""${BAM}"" \. 	-f ""${REF}"" \. 	-p ""${OUTPUT_PREFIX}"" \. 	-t ${THREADS} \. 	-g \. 	--ont_r9_guppy5_sup. ```. - Error trace: (if applicable). . [5.1_DeepVariant_SNP.log](https://github.com/google/deepvariant/files/8785347/5.1_DeepVariant_SNP.log). **Does the quick start test work on your system?** yes . Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? no. **Any additional context:** Ultra-long reads.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/issues/539:2828,security,log,log,2828,"s /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup**. - Operating system: Ubuntu 20.04.4. - DeepVariant version: pepper_deepvariant:r0.8-gpu. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command: . ```. 	docker run --ipc=host \. 	--gpus all \. 	-v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \. 	-v ""${BASE}"":""${BASE}"" \. 	-v ""${REF}"":""${REF}"" \. 	-v ""${BAMPATH}"":""${BAMPATH}"" \. 	kishwars/pepper_deepvariant:r0.8-gpu \. 	run_pepper_margin_deepvariant call_variant \. 	-o ""${OUTPUT_DIR}"" \. 	-b ""${BAM}"" \. 	-f ""${REF}"" \. 	-p ""${OUTPUT_PREFIX}"" \. 	-t ${THREADS} \. 	-g \. 	--ont_r9_guppy5_sup. ```. - Error trace: (if applicable). . [5.1_DeepVariant_SNP.log](https://github.com/google/deepvariant/files/8785347/5.1_DeepVariant_SNP.log). **Does the quick start test work on your system?** yes . Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? no. **Any additional context:** Ultra-long reads.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/issues/539:1974,testability,log,log,1974,"ailed:. /opt/deepvariant/bin/make_examples --mode calling --ref /media/euphrasie/DATA/reference_genome/hg38/hg38_GenDev.fa --reads /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup**. - Operating system: Ubuntu 20.04.4. - DeepVariant version: pepper_deepvariant:r0.8-gpu. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command: . ```. 	docker run --ipc=host \. 	--gpus all \. 	-v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \. 	-v ""${BASE}"":""${BASE}"" \. 	-v ""${REF}"":""${REF}"" \. 	-v ""${BAMPATH}"":""${BAMPATH}"" \. 	kishwars/pepper_deepvariant:r0.8-gpu \. 	run_pepper_margin_deepvariant call_variant \. 	-o ""${OUTPUT_DIR}"" \. 	-b ""${BAM}"" \. 	-f ""${REF}"" \. 	-p ""${OUTPUT_PREFIX}"" \. 	-t ${THREADS} \. 	-g \. 	--ont_r9_guppy5_sup. ```. - Error trace: (if applicable). . [5.1_DeepVariant_SNP.log](https://github.com/google/deepvariant/files/8785347/5.1_DeepVariant_SNP.log). **Does the quick start test work on your system?** yes . Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-qu",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/issues/539:2184,testability,instrument,instrument,2184,"s /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup**. - Operating system: Ubuntu 20.04.4. - DeepVariant version: pepper_deepvariant:r0.8-gpu. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command: . ```. 	docker run --ipc=host \. 	--gpus all \. 	-v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \. 	-v ""${BASE}"":""${BASE}"" \. 	-v ""${REF}"":""${REF}"" \. 	-v ""${BAMPATH}"":""${BAMPATH}"" \. 	kishwars/pepper_deepvariant:r0.8-gpu \. 	run_pepper_margin_deepvariant call_variant \. 	-o ""${OUTPUT_DIR}"" \. 	-b ""${BAM}"" \. 	-f ""${REF}"" \. 	-p ""${OUTPUT_PREFIX}"" \. 	-t ${THREADS} \. 	-g \. 	--ont_r9_guppy5_sup. ```. - Error trace: (if applicable). . [5.1_DeepVariant_SNP.log](https://github.com/google/deepvariant/files/8785347/5.1_DeepVariant_SNP.log). **Does the quick start test work on your system?** yes . Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? no. **Any additional context:** Ultra-long reads.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/issues/539:2704,testability,trace,trace,2704,"s /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup**. - Operating system: Ubuntu 20.04.4. - DeepVariant version: pepper_deepvariant:r0.8-gpu. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command: . ```. 	docker run --ipc=host \. 	--gpus all \. 	-v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \. 	-v ""${BASE}"":""${BASE}"" \. 	-v ""${REF}"":""${REF}"" \. 	-v ""${BAMPATH}"":""${BAMPATH}"" \. 	kishwars/pepper_deepvariant:r0.8-gpu \. 	run_pepper_margin_deepvariant call_variant \. 	-o ""${OUTPUT_DIR}"" \. 	-b ""${BAM}"" \. 	-f ""${REF}"" \. 	-p ""${OUTPUT_PREFIX}"" \. 	-t ${THREADS} \. 	-g \. 	--ont_r9_guppy5_sup. ```. - Error trace: (if applicable). . [5.1_DeepVariant_SNP.log](https://github.com/google/deepvariant/files/8785347/5.1_DeepVariant_SNP.log). **Does the quick start test work on your system?** yes . Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? no. **Any additional context:** Ultra-long reads.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/issues/539:2751,testability,log,log,2751,"s /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup**. - Operating system: Ubuntu 20.04.4. - DeepVariant version: pepper_deepvariant:r0.8-gpu. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command: . ```. 	docker run --ipc=host \. 	--gpus all \. 	-v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \. 	-v ""${BASE}"":""${BASE}"" \. 	-v ""${REF}"":""${REF}"" \. 	-v ""${BAMPATH}"":""${BAMPATH}"" \. 	kishwars/pepper_deepvariant:r0.8-gpu \. 	run_pepper_margin_deepvariant call_variant \. 	-o ""${OUTPUT_DIR}"" \. 	-b ""${BAM}"" \. 	-f ""${REF}"" \. 	-p ""${OUTPUT_PREFIX}"" \. 	-t ${THREADS} \. 	-g \. 	--ont_r9_guppy5_sup. ```. - Error trace: (if applicable). . [5.1_DeepVariant_SNP.log](https://github.com/google/deepvariant/files/8785347/5.1_DeepVariant_SNP.log). **Does the quick start test work on your system?** yes . Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? no. **Any additional context:** Ultra-long reads.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/issues/539:2828,testability,log,log,2828,"s /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup**. - Operating system: Ubuntu 20.04.4. - DeepVariant version: pepper_deepvariant:r0.8-gpu. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command: . ```. 	docker run --ipc=host \. 	--gpus all \. 	-v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \. 	-v ""${BASE}"":""${BASE}"" \. 	-v ""${REF}"":""${REF}"" \. 	-v ""${BAMPATH}"":""${BAMPATH}"" \. 	kishwars/pepper_deepvariant:r0.8-gpu \. 	run_pepper_margin_deepvariant call_variant \. 	-o ""${OUTPUT_DIR}"" \. 	-b ""${BAM}"" \. 	-f ""${REF}"" \. 	-p ""${OUTPUT_PREFIX}"" \. 	-t ${THREADS} \. 	-g \. 	--ont_r9_guppy5_sup. ```. - Error trace: (if applicable). . [5.1_DeepVariant_SNP.log](https://github.com/google/deepvariant/files/8785347/5.1_DeepVariant_SNP.log). **Does the quick start test work on your system?** yes . Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? no. **Any additional context:** Ultra-long reads.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/issues/539:2857,testability,test,test,2857,"s /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup**. - Operating system: Ubuntu 20.04.4. - DeepVariant version: pepper_deepvariant:r0.8-gpu. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command: . ```. 	docker run --ipc=host \. 	--gpus all \. 	-v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \. 	-v ""${BASE}"":""${BASE}"" \. 	-v ""${REF}"":""${REF}"" \. 	-v ""${BAMPATH}"":""${BAMPATH}"" \. 	kishwars/pepper_deepvariant:r0.8-gpu \. 	run_pepper_margin_deepvariant call_variant \. 	-o ""${OUTPUT_DIR}"" \. 	-b ""${BAM}"" \. 	-f ""${REF}"" \. 	-p ""${OUTPUT_PREFIX}"" \. 	-t ${THREADS} \. 	-g \. 	--ont_r9_guppy5_sup. ```. - Error trace: (if applicable). . [5.1_DeepVariant_SNP.log](https://github.com/google/deepvariant/files/8785347/5.1_DeepVariant_SNP.log). **Does the quick start test work on your system?** yes . Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? no. **Any additional context:** Ultra-long reads.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/issues/539:2898,testability,test,test,2898,"s /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup**. - Operating system: Ubuntu 20.04.4. - DeepVariant version: pepper_deepvariant:r0.8-gpu. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command: . ```. 	docker run --ipc=host \. 	--gpus all \. 	-v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \. 	-v ""${BASE}"":""${BASE}"" \. 	-v ""${REF}"":""${REF}"" \. 	-v ""${BAMPATH}"":""${BAMPATH}"" \. 	kishwars/pepper_deepvariant:r0.8-gpu \. 	run_pepper_margin_deepvariant call_variant \. 	-o ""${OUTPUT_DIR}"" \. 	-b ""${BAM}"" \. 	-f ""${REF}"" \. 	-p ""${OUTPUT_PREFIX}"" \. 	-t ${THREADS} \. 	-g \. 	--ont_r9_guppy5_sup. ```. - Error trace: (if applicable). . [5.1_DeepVariant_SNP.log](https://github.com/google/deepvariant/files/8785347/5.1_DeepVariant_SNP.log). **Does the quick start test work on your system?** yes . Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? no. **Any additional context:** Ultra-long reads.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/issues/539:3077,testability,context,context,3077,"s /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup**. - Operating system: Ubuntu 20.04.4. - DeepVariant version: pepper_deepvariant:r0.8-gpu. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command: . ```. 	docker run --ipc=host \. 	--gpus all \. 	-v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \. 	-v ""${BASE}"":""${BASE}"" \. 	-v ""${REF}"":""${REF}"" \. 	-v ""${BAMPATH}"":""${BAMPATH}"" \. 	kishwars/pepper_deepvariant:r0.8-gpu \. 	run_pepper_margin_deepvariant call_variant \. 	-o ""${OUTPUT_DIR}"" \. 	-b ""${BAM}"" \. 	-f ""${REF}"" \. 	-p ""${OUTPUT_PREFIX}"" \. 	-t ${THREADS} \. 	-g \. 	--ont_r9_guppy5_sup. ```. - Error trace: (if applicable). . [5.1_DeepVariant_SNP.log](https://github.com/google/deepvariant/files/8785347/5.1_DeepVariant_SNP.log). **Does the quick start test work on your system?** yes . Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? no. **Any additional context:** Ultra-long reads.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/issues/539:899,usability,error,error,899,"Could not read base quality scores; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: yes . **Describe the issue:**. I am running deep-variant trough a docker installation of the pepper-margin-deepvariant pipeline `kishwars/pepper_deepvariant:r0.8-gpu` on data aligned with minimap2 and data aligned with lra. It is working fine with the minimap2 aligned data, but deepvariant does not produce a final VCF with lra aligned data. . It seems that deep-variant cannot read the base quality score during SNP calling:. ```. 2022-05-26 00:08:16.416812: W third_party/nucleus/io/sam_reader.cc:599] Could not read base quality scores 2e95d959-f3f1-403f-acff-a2bf4f2c12fe: Not found: Could not read base quality scores. 2022-05-26 00:08:16.450548: F deepvariant/allelecounter.cc:198] Check failed: offset + len <= read.aligned_quality_size() (81 vs. 0). Fatal Python error: Aborted. ```. and the job eventually fails:. ```. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /media/euphrasie/DATA/reference_genome/hg38/hg38_GenDev.fa --reads /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup*",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/issues/539:2294,usability,Command,Command,2294,"s /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup**. - Operating system: Ubuntu 20.04.4. - DeepVariant version: pepper_deepvariant:r0.8-gpu. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command: . ```. 	docker run --ipc=host \. 	--gpus all \. 	-v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \. 	-v ""${BASE}"":""${BASE}"" \. 	-v ""${REF}"":""${REF}"" \. 	-v ""${BAMPATH}"":""${BAMPATH}"" \. 	kishwars/pepper_deepvariant:r0.8-gpu \. 	run_pepper_margin_deepvariant call_variant \. 	-o ""${OUTPUT_DIR}"" \. 	-b ""${BAM}"" \. 	-f ""${REF}"" \. 	-p ""${OUTPUT_PREFIX}"" \. 	-t ${THREADS} \. 	-g \. 	--ont_r9_guppy5_sup. ```. - Error trace: (if applicable). . [5.1_DeepVariant_SNP.log](https://github.com/google/deepvariant/files/8785347/5.1_DeepVariant_SNP.log). **Does the quick start test work on your system?** yes . Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? no. **Any additional context:** Ultra-long reads.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/issues/539:2698,usability,Error,Error,2698,"s /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7. ```. I checked the lra bam with samtools view and the base quality scores are there. I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup**. - Operating system: Ubuntu 20.04.4. - DeepVariant version: pepper_deepvariant:r0.8-gpu. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**. - Command: . ```. 	docker run --ipc=host \. 	--gpus all \. 	-v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \. 	-v ""${BASE}"":""${BASE}"" \. 	-v ""${REF}"":""${REF}"" \. 	-v ""${BAMPATH}"":""${BAMPATH}"" \. 	kishwars/pepper_deepvariant:r0.8-gpu \. 	run_pepper_margin_deepvariant call_variant \. 	-o ""${OUTPUT_DIR}"" \. 	-b ""${BAM}"" \. 	-f ""${REF}"" \. 	-p ""${OUTPUT_PREFIX}"" \. 	-t ${THREADS} \. 	-g \. 	--ont_r9_guppy5_sup. ```. - Error trace: (if applicable). . [5.1_DeepVariant_SNP.log](https://github.com/google/deepvariant/files/8785347/5.1_DeepVariant_SNP.log). **Does the quick start test work on your system?** yes . Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? no. **Any additional context:** Ultra-long reads.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539
https://github.com/google/deepvariant/pull/540:7,usability,Custom,Custom,7,Adding Custom Channels to DeepVariant (blog); Adds the Custom Channel Blog Post,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/540
https://github.com/google/deepvariant/pull/540:55,usability,Custom,Custom,55,Adding Custom Channels to DeepVariant (blog); Adds the Custom Channel Blog Post,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/540
https://github.com/google/deepvariant/issues/541:235,availability,error,error,235,"v1.4 deepvariant-cpu docker image does not contain openvino; I've been trying out the new v1.4 docker image through singularity, but have been running into openvino issues again (#404 and #416, maybe I should just move on :wink:). The error during call_variants is below (and similar to #432). ```. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 416, in call_variants. ie_estimator = OpenVINOEstimator(. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 90, in __init__. freeze_graph(model, checkpoint_path, tensor_shape, openvino_model_pb). File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, in freeze_graph. graph_def = optimize_for_inference_lib.optimize_for_inference(. NameError: name 'optimize_for_inference_lib' is not defined. ```. Which comes from [here](https://github.com/google/deepvariant/blob/d2a3aca8691318221e794594ea08e7c88e21359b/deepvariant/openvino_estimator.py#L42). However, after playing around inside the image, the line `from tensorflow.python.tools import optimize_for_inference_lib` works fine as I can successfully run. ```. python -c 'from tensorflow.python.tools import optimize_for_inference_lib'. ```. The real issue is openvino is not installed . ```. python -c 'from openvino.runtime import Core, AsyncInferQueue, Type'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. ModuleNotFoundError: No module named 'openvino'. ```. which triggers the ImportError and pass statement skipping the import of optimize_for_inference_lib. Not to expose my limited understanding of dockerfiles, but [here](https://hub.docker.com/layers/deepvariant/google/deepvariant/latest/images/sha256-83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18?context=explore) it appears that the current latest build has ` ENV DV_OPENVINO_BUILD=0`. I've seen a lot of back and forth with",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/541
https://github.com/google/deepvariant/issues/541:1598,availability,state,statement,1598,"nd #416, maybe I should just move on :wink:). The error during call_variants is below (and similar to #432). ```. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 416, in call_variants. ie_estimator = OpenVINOEstimator(. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 90, in __init__. freeze_graph(model, checkpoint_path, tensor_shape, openvino_model_pb). File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, in freeze_graph. graph_def = optimize_for_inference_lib.optimize_for_inference(. NameError: name 'optimize_for_inference_lib' is not defined. ```. Which comes from [here](https://github.com/google/deepvariant/blob/d2a3aca8691318221e794594ea08e7c88e21359b/deepvariant/openvino_estimator.py#L42). However, after playing around inside the image, the line `from tensorflow.python.tools import optimize_for_inference_lib` works fine as I can successfully run. ```. python -c 'from tensorflow.python.tools import optimize_for_inference_lib'. ```. The real issue is openvino is not installed . ```. python -c 'from openvino.runtime import Core, AsyncInferQueue, Type'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. ModuleNotFoundError: No module named 'openvino'. ```. which triggers the ImportError and pass statement skipping the import of optimize_for_inference_lib. Not to expose my limited understanding of dockerfiles, but [here](https://hub.docker.com/layers/deepvariant/google/deepvariant/latest/images/sha256-83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18?context=explore) it appears that the current latest build has ` ENV DV_OPENVINO_BUILD=0`. I've seen a lot of back and forth with openvino no longer being as helpful, but then there has been some recent updates, so not sure if it is still recommended or deprecated as it has disappeared from some docs. Best,. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/541
https://github.com/google/deepvariant/issues/541:43,deployability,contain,contain,43,"v1.4 deepvariant-cpu docker image does not contain openvino; I've been trying out the new v1.4 docker image through singularity, but have been running into openvino issues again (#404 and #416, maybe I should just move on :wink:). The error during call_variants is below (and similar to #432). ```. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 416, in call_variants. ie_estimator = OpenVINOEstimator(. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 90, in __init__. freeze_graph(model, checkpoint_path, tensor_shape, openvino_model_pb). File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, in freeze_graph. graph_def = optimize_for_inference_lib.optimize_for_inference(. NameError: name 'optimize_for_inference_lib' is not defined. ```. Which comes from [here](https://github.com/google/deepvariant/blob/d2a3aca8691318221e794594ea08e7c88e21359b/deepvariant/openvino_estimator.py#L42). However, after playing around inside the image, the line `from tensorflow.python.tools import optimize_for_inference_lib` works fine as I can successfully run. ```. python -c 'from tensorflow.python.tools import optimize_for_inference_lib'. ```. The real issue is openvino is not installed . ```. python -c 'from openvino.runtime import Core, AsyncInferQueue, Type'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. ModuleNotFoundError: No module named 'openvino'. ```. which triggers the ImportError and pass statement skipping the import of optimize_for_inference_lib. Not to expose my limited understanding of dockerfiles, but [here](https://hub.docker.com/layers/deepvariant/google/deepvariant/latest/images/sha256-83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18?context=explore) it appears that the current latest build has ` ENV DV_OPENVINO_BUILD=0`. I've seen a lot of back and forth with",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/541
https://github.com/google/deepvariant/issues/541:1343,deployability,instal,installed,1343,"nd #416, maybe I should just move on :wink:). The error during call_variants is below (and similar to #432). ```. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 416, in call_variants. ie_estimator = OpenVINOEstimator(. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 90, in __init__. freeze_graph(model, checkpoint_path, tensor_shape, openvino_model_pb). File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, in freeze_graph. graph_def = optimize_for_inference_lib.optimize_for_inference(. NameError: name 'optimize_for_inference_lib' is not defined. ```. Which comes from [here](https://github.com/google/deepvariant/blob/d2a3aca8691318221e794594ea08e7c88e21359b/deepvariant/openvino_estimator.py#L42). However, after playing around inside the image, the line `from tensorflow.python.tools import optimize_for_inference_lib` works fine as I can successfully run. ```. python -c 'from tensorflow.python.tools import optimize_for_inference_lib'. ```. The real issue is openvino is not installed . ```. python -c 'from openvino.runtime import Core, AsyncInferQueue, Type'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. ModuleNotFoundError: No module named 'openvino'. ```. which triggers the ImportError and pass statement skipping the import of optimize_for_inference_lib. Not to expose my limited understanding of dockerfiles, but [here](https://hub.docker.com/layers/deepvariant/google/deepvariant/latest/images/sha256-83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18?context=explore) it appears that the current latest build has ` ENV DV_OPENVINO_BUILD=0`. I've seen a lot of back and forth with openvino no longer being as helpful, but then there has been some recent updates, so not sure if it is still recommended or deprecated as it has disappeared from some docs. Best,. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/541
https://github.com/google/deepvariant/issues/541:1495,deployability,modul,module,1495,"nd #416, maybe I should just move on :wink:). The error during call_variants is below (and similar to #432). ```. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 416, in call_variants. ie_estimator = OpenVINOEstimator(. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 90, in __init__. freeze_graph(model, checkpoint_path, tensor_shape, openvino_model_pb). File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, in freeze_graph. graph_def = optimize_for_inference_lib.optimize_for_inference(. NameError: name 'optimize_for_inference_lib' is not defined. ```. Which comes from [here](https://github.com/google/deepvariant/blob/d2a3aca8691318221e794594ea08e7c88e21359b/deepvariant/openvino_estimator.py#L42). However, after playing around inside the image, the line `from tensorflow.python.tools import optimize_for_inference_lib` works fine as I can successfully run. ```. python -c 'from tensorflow.python.tools import optimize_for_inference_lib'. ```. The real issue is openvino is not installed . ```. python -c 'from openvino.runtime import Core, AsyncInferQueue, Type'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. ModuleNotFoundError: No module named 'openvino'. ```. which triggers the ImportError and pass statement skipping the import of optimize_for_inference_lib. Not to expose my limited understanding of dockerfiles, but [here](https://hub.docker.com/layers/deepvariant/google/deepvariant/latest/images/sha256-83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18?context=explore) it appears that the current latest build has ` ENV DV_OPENVINO_BUILD=0`. I've seen a lot of back and forth with openvino no longer being as helpful, but then there has been some recent updates, so not sure if it is still recommended or deprecated as it has disappeared from some docs. Best,. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/541
https://github.com/google/deepvariant/issues/541:1504,deployability,Modul,ModuleNotFoundError,1504,"nd #416, maybe I should just move on :wink:). The error during call_variants is below (and similar to #432). ```. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 416, in call_variants. ie_estimator = OpenVINOEstimator(. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 90, in __init__. freeze_graph(model, checkpoint_path, tensor_shape, openvino_model_pb). File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, in freeze_graph. graph_def = optimize_for_inference_lib.optimize_for_inference(. NameError: name 'optimize_for_inference_lib' is not defined. ```. Which comes from [here](https://github.com/google/deepvariant/blob/d2a3aca8691318221e794594ea08e7c88e21359b/deepvariant/openvino_estimator.py#L42). However, after playing around inside the image, the line `from tensorflow.python.tools import optimize_for_inference_lib` works fine as I can successfully run. ```. python -c 'from tensorflow.python.tools import optimize_for_inference_lib'. ```. The real issue is openvino is not installed . ```. python -c 'from openvino.runtime import Core, AsyncInferQueue, Type'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. ModuleNotFoundError: No module named 'openvino'. ```. which triggers the ImportError and pass statement skipping the import of optimize_for_inference_lib. Not to expose my limited understanding of dockerfiles, but [here](https://hub.docker.com/layers/deepvariant/google/deepvariant/latest/images/sha256-83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18?context=explore) it appears that the current latest build has ` ENV DV_OPENVINO_BUILD=0`. I've seen a lot of back and forth with openvino no longer being as helpful, but then there has been some recent updates, so not sure if it is still recommended or deprecated as it has disappeared from some docs. Best,. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/541
https://github.com/google/deepvariant/issues/541:1528,deployability,modul,module,1528,"nd #416, maybe I should just move on :wink:). The error during call_variants is below (and similar to #432). ```. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 416, in call_variants. ie_estimator = OpenVINOEstimator(. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 90, in __init__. freeze_graph(model, checkpoint_path, tensor_shape, openvino_model_pb). File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, in freeze_graph. graph_def = optimize_for_inference_lib.optimize_for_inference(. NameError: name 'optimize_for_inference_lib' is not defined. ```. Which comes from [here](https://github.com/google/deepvariant/blob/d2a3aca8691318221e794594ea08e7c88e21359b/deepvariant/openvino_estimator.py#L42). However, after playing around inside the image, the line `from tensorflow.python.tools import optimize_for_inference_lib` works fine as I can successfully run. ```. python -c 'from tensorflow.python.tools import optimize_for_inference_lib'. ```. The real issue is openvino is not installed . ```. python -c 'from openvino.runtime import Core, AsyncInferQueue, Type'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. ModuleNotFoundError: No module named 'openvino'. ```. which triggers the ImportError and pass statement skipping the import of optimize_for_inference_lib. Not to expose my limited understanding of dockerfiles, but [here](https://hub.docker.com/layers/deepvariant/google/deepvariant/latest/images/sha256-83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18?context=explore) it appears that the current latest build has ` ENV DV_OPENVINO_BUILD=0`. I've seen a lot of back and forth with openvino no longer being as helpful, but then there has been some recent updates, so not sure if it is still recommended or deprecated as it has disappeared from some docs. Best,. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/541
https://github.com/google/deepvariant/issues/541:1924,deployability,build,build,1924,"nd #416, maybe I should just move on :wink:). The error during call_variants is below (and similar to #432). ```. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 416, in call_variants. ie_estimator = OpenVINOEstimator(. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 90, in __init__. freeze_graph(model, checkpoint_path, tensor_shape, openvino_model_pb). File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, in freeze_graph. graph_def = optimize_for_inference_lib.optimize_for_inference(. NameError: name 'optimize_for_inference_lib' is not defined. ```. Which comes from [here](https://github.com/google/deepvariant/blob/d2a3aca8691318221e794594ea08e7c88e21359b/deepvariant/openvino_estimator.py#L42). However, after playing around inside the image, the line `from tensorflow.python.tools import optimize_for_inference_lib` works fine as I can successfully run. ```. python -c 'from tensorflow.python.tools import optimize_for_inference_lib'. ```. The real issue is openvino is not installed . ```. python -c 'from openvino.runtime import Core, AsyncInferQueue, Type'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. ModuleNotFoundError: No module named 'openvino'. ```. which triggers the ImportError and pass statement skipping the import of optimize_for_inference_lib. Not to expose my limited understanding of dockerfiles, but [here](https://hub.docker.com/layers/deepvariant/google/deepvariant/latest/images/sha256-83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18?context=explore) it appears that the current latest build has ` ENV DV_OPENVINO_BUILD=0`. I've seen a lot of back and forth with openvino no longer being as helpful, but then there has been some recent updates, so not sure if it is still recommended or deprecated as it has disappeared from some docs. Best,. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/541
https://github.com/google/deepvariant/issues/541:2074,deployability,updat,updates,2074,"nd #416, maybe I should just move on :wink:). The error during call_variants is below (and similar to #432). ```. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 416, in call_variants. ie_estimator = OpenVINOEstimator(. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 90, in __init__. freeze_graph(model, checkpoint_path, tensor_shape, openvino_model_pb). File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, in freeze_graph. graph_def = optimize_for_inference_lib.optimize_for_inference(. NameError: name 'optimize_for_inference_lib' is not defined. ```. Which comes from [here](https://github.com/google/deepvariant/blob/d2a3aca8691318221e794594ea08e7c88e21359b/deepvariant/openvino_estimator.py#L42). However, after playing around inside the image, the line `from tensorflow.python.tools import optimize_for_inference_lib` works fine as I can successfully run. ```. python -c 'from tensorflow.python.tools import optimize_for_inference_lib'. ```. The real issue is openvino is not installed . ```. python -c 'from openvino.runtime import Core, AsyncInferQueue, Type'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. ModuleNotFoundError: No module named 'openvino'. ```. which triggers the ImportError and pass statement skipping the import of optimize_for_inference_lib. Not to expose my limited understanding of dockerfiles, but [here](https://hub.docker.com/layers/deepvariant/google/deepvariant/latest/images/sha256-83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18?context=explore) it appears that the current latest build has ` ENV DV_OPENVINO_BUILD=0`. I've seen a lot of back and forth with openvino no longer being as helpful, but then there has been some recent updates, so not sure if it is still recommended or deprecated as it has disappeared from some docs. Best,. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/541
https://github.com/google/deepvariant/issues/541:17,energy efficiency,cpu,cpu,17,"v1.4 deepvariant-cpu docker image does not contain openvino; I've been trying out the new v1.4 docker image through singularity, but have been running into openvino issues again (#404 and #416, maybe I should just move on :wink:). The error during call_variants is below (and similar to #432). ```. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 416, in call_variants. ie_estimator = OpenVINOEstimator(. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 90, in __init__. freeze_graph(model, checkpoint_path, tensor_shape, openvino_model_pb). File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, in freeze_graph. graph_def = optimize_for_inference_lib.optimize_for_inference(. NameError: name 'optimize_for_inference_lib' is not defined. ```. Which comes from [here](https://github.com/google/deepvariant/blob/d2a3aca8691318221e794594ea08e7c88e21359b/deepvariant/openvino_estimator.py#L42). However, after playing around inside the image, the line `from tensorflow.python.tools import optimize_for_inference_lib` works fine as I can successfully run. ```. python -c 'from tensorflow.python.tools import optimize_for_inference_lib'. ```. The real issue is openvino is not installed . ```. python -c 'from openvino.runtime import Core, AsyncInferQueue, Type'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. ModuleNotFoundError: No module named 'openvino'. ```. which triggers the ImportError and pass statement skipping the import of optimize_for_inference_lib. Not to expose my limited understanding of dockerfiles, but [here](https://hub.docker.com/layers/deepvariant/google/deepvariant/latest/images/sha256-83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18?context=explore) it appears that the current latest build has ` ENV DV_OPENVINO_BUILD=0`. I've seen a lot of back and forth with",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/541
https://github.com/google/deepvariant/issues/541:598,energy efficiency,model,model,598,"v1.4 deepvariant-cpu docker image does not contain openvino; I've been trying out the new v1.4 docker image through singularity, but have been running into openvino issues again (#404 and #416, maybe I should just move on :wink:). The error during call_variants is below (and similar to #432). ```. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 416, in call_variants. ie_estimator = OpenVINOEstimator(. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 90, in __init__. freeze_graph(model, checkpoint_path, tensor_shape, openvino_model_pb). File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, in freeze_graph. graph_def = optimize_for_inference_lib.optimize_for_inference(. NameError: name 'optimize_for_inference_lib' is not defined. ```. Which comes from [here](https://github.com/google/deepvariant/blob/d2a3aca8691318221e794594ea08e7c88e21359b/deepvariant/openvino_estimator.py#L42). However, after playing around inside the image, the line `from tensorflow.python.tools import optimize_for_inference_lib` works fine as I can successfully run. ```. python -c 'from tensorflow.python.tools import optimize_for_inference_lib'. ```. The real issue is openvino is not installed . ```. python -c 'from openvino.runtime import Core, AsyncInferQueue, Type'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. ModuleNotFoundError: No module named 'openvino'. ```. which triggers the ImportError and pass statement skipping the import of optimize_for_inference_lib. Not to expose my limited understanding of dockerfiles, but [here](https://hub.docker.com/layers/deepvariant/google/deepvariant/latest/images/sha256-83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18?context=explore) it appears that the current latest build has ` ENV DV_OPENVINO_BUILD=0`. I've seen a lot of back and forth with",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/541
https://github.com/google/deepvariant/issues/541:1400,energy efficiency,Core,Core,1400,"nd #416, maybe I should just move on :wink:). The error during call_variants is below (and similar to #432). ```. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 416, in call_variants. ie_estimator = OpenVINOEstimator(. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 90, in __init__. freeze_graph(model, checkpoint_path, tensor_shape, openvino_model_pb). File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, in freeze_graph. graph_def = optimize_for_inference_lib.optimize_for_inference(. NameError: name 'optimize_for_inference_lib' is not defined. ```. Which comes from [here](https://github.com/google/deepvariant/blob/d2a3aca8691318221e794594ea08e7c88e21359b/deepvariant/openvino_estimator.py#L42). However, after playing around inside the image, the line `from tensorflow.python.tools import optimize_for_inference_lib` works fine as I can successfully run. ```. python -c 'from tensorflow.python.tools import optimize_for_inference_lib'. ```. The real issue is openvino is not installed . ```. python -c 'from openvino.runtime import Core, AsyncInferQueue, Type'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. ModuleNotFoundError: No module named 'openvino'. ```. which triggers the ImportError and pass statement skipping the import of optimize_for_inference_lib. Not to expose my limited understanding of dockerfiles, but [here](https://hub.docker.com/layers/deepvariant/google/deepvariant/latest/images/sha256-83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18?context=explore) it appears that the current latest build has ` ENV DV_OPENVINO_BUILD=0`. I've seen a lot of back and forth with openvino no longer being as helpful, but then there has been some recent updates, so not sure if it is still recommended or deprecated as it has disappeared from some docs. Best,. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/541
https://github.com/google/deepvariant/issues/541:1909,energy efficiency,current,current,1909,"nd #416, maybe I should just move on :wink:). The error during call_variants is below (and similar to #432). ```. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 416, in call_variants. ie_estimator = OpenVINOEstimator(. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 90, in __init__. freeze_graph(model, checkpoint_path, tensor_shape, openvino_model_pb). File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, in freeze_graph. graph_def = optimize_for_inference_lib.optimize_for_inference(. NameError: name 'optimize_for_inference_lib' is not defined. ```. Which comes from [here](https://github.com/google/deepvariant/blob/d2a3aca8691318221e794594ea08e7c88e21359b/deepvariant/openvino_estimator.py#L42). However, after playing around inside the image, the line `from tensorflow.python.tools import optimize_for_inference_lib` works fine as I can successfully run. ```. python -c 'from tensorflow.python.tools import optimize_for_inference_lib'. ```. The real issue is openvino is not installed . ```. python -c 'from openvino.runtime import Core, AsyncInferQueue, Type'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. ModuleNotFoundError: No module named 'openvino'. ```. which triggers the ImportError and pass statement skipping the import of optimize_for_inference_lib. Not to expose my limited understanding of dockerfiles, but [here](https://hub.docker.com/layers/deepvariant/google/deepvariant/latest/images/sha256-83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18?context=explore) it appears that the current latest build has ` ENV DV_OPENVINO_BUILD=0`. I've seen a lot of back and forth with openvino no longer being as helpful, but then there has been some recent updates, so not sure if it is still recommended or deprecated as it has disappeared from some docs. Best,. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/541
https://github.com/google/deepvariant/issues/541:1598,integrability,state,statement,1598,"nd #416, maybe I should just move on :wink:). The error during call_variants is below (and similar to #432). ```. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 416, in call_variants. ie_estimator = OpenVINOEstimator(. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 90, in __init__. freeze_graph(model, checkpoint_path, tensor_shape, openvino_model_pb). File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, in freeze_graph. graph_def = optimize_for_inference_lib.optimize_for_inference(. NameError: name 'optimize_for_inference_lib' is not defined. ```. Which comes from [here](https://github.com/google/deepvariant/blob/d2a3aca8691318221e794594ea08e7c88e21359b/deepvariant/openvino_estimator.py#L42). However, after playing around inside the image, the line `from tensorflow.python.tools import optimize_for_inference_lib` works fine as I can successfully run. ```. python -c 'from tensorflow.python.tools import optimize_for_inference_lib'. ```. The real issue is openvino is not installed . ```. python -c 'from openvino.runtime import Core, AsyncInferQueue, Type'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. ModuleNotFoundError: No module named 'openvino'. ```. which triggers the ImportError and pass statement skipping the import of optimize_for_inference_lib. Not to expose my limited understanding of dockerfiles, but [here](https://hub.docker.com/layers/deepvariant/google/deepvariant/latest/images/sha256-83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18?context=explore) it appears that the current latest build has ` ENV DV_OPENVINO_BUILD=0`. I've seen a lot of back and forth with openvino no longer being as helpful, but then there has been some recent updates, so not sure if it is still recommended or deprecated as it has disappeared from some docs. Best,. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/541
https://github.com/google/deepvariant/issues/541:1495,modifiability,modul,module,1495,"nd #416, maybe I should just move on :wink:). The error during call_variants is below (and similar to #432). ```. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 416, in call_variants. ie_estimator = OpenVINOEstimator(. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 90, in __init__. freeze_graph(model, checkpoint_path, tensor_shape, openvino_model_pb). File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, in freeze_graph. graph_def = optimize_for_inference_lib.optimize_for_inference(. NameError: name 'optimize_for_inference_lib' is not defined. ```. Which comes from [here](https://github.com/google/deepvariant/blob/d2a3aca8691318221e794594ea08e7c88e21359b/deepvariant/openvino_estimator.py#L42). However, after playing around inside the image, the line `from tensorflow.python.tools import optimize_for_inference_lib` works fine as I can successfully run. ```. python -c 'from tensorflow.python.tools import optimize_for_inference_lib'. ```. The real issue is openvino is not installed . ```. python -c 'from openvino.runtime import Core, AsyncInferQueue, Type'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. ModuleNotFoundError: No module named 'openvino'. ```. which triggers the ImportError and pass statement skipping the import of optimize_for_inference_lib. Not to expose my limited understanding of dockerfiles, but [here](https://hub.docker.com/layers/deepvariant/google/deepvariant/latest/images/sha256-83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18?context=explore) it appears that the current latest build has ` ENV DV_OPENVINO_BUILD=0`. I've seen a lot of back and forth with openvino no longer being as helpful, but then there has been some recent updates, so not sure if it is still recommended or deprecated as it has disappeared from some docs. Best,. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/541
https://github.com/google/deepvariant/issues/541:1504,modifiability,Modul,ModuleNotFoundError,1504,"nd #416, maybe I should just move on :wink:). The error during call_variants is below (and similar to #432). ```. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 416, in call_variants. ie_estimator = OpenVINOEstimator(. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 90, in __init__. freeze_graph(model, checkpoint_path, tensor_shape, openvino_model_pb). File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, in freeze_graph. graph_def = optimize_for_inference_lib.optimize_for_inference(. NameError: name 'optimize_for_inference_lib' is not defined. ```. Which comes from [here](https://github.com/google/deepvariant/blob/d2a3aca8691318221e794594ea08e7c88e21359b/deepvariant/openvino_estimator.py#L42). However, after playing around inside the image, the line `from tensorflow.python.tools import optimize_for_inference_lib` works fine as I can successfully run. ```. python -c 'from tensorflow.python.tools import optimize_for_inference_lib'. ```. The real issue is openvino is not installed . ```. python -c 'from openvino.runtime import Core, AsyncInferQueue, Type'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. ModuleNotFoundError: No module named 'openvino'. ```. which triggers the ImportError and pass statement skipping the import of optimize_for_inference_lib. Not to expose my limited understanding of dockerfiles, but [here](https://hub.docker.com/layers/deepvariant/google/deepvariant/latest/images/sha256-83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18?context=explore) it appears that the current latest build has ` ENV DV_OPENVINO_BUILD=0`. I've seen a lot of back and forth with openvino no longer being as helpful, but then there has been some recent updates, so not sure if it is still recommended or deprecated as it has disappeared from some docs. Best,. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/541
https://github.com/google/deepvariant/issues/541:1528,modifiability,modul,module,1528,"nd #416, maybe I should just move on :wink:). The error during call_variants is below (and similar to #432). ```. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 416, in call_variants. ie_estimator = OpenVINOEstimator(. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 90, in __init__. freeze_graph(model, checkpoint_path, tensor_shape, openvino_model_pb). File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, in freeze_graph. graph_def = optimize_for_inference_lib.optimize_for_inference(. NameError: name 'optimize_for_inference_lib' is not defined. ```. Which comes from [here](https://github.com/google/deepvariant/blob/d2a3aca8691318221e794594ea08e7c88e21359b/deepvariant/openvino_estimator.py#L42). However, after playing around inside the image, the line `from tensorflow.python.tools import optimize_for_inference_lib` works fine as I can successfully run. ```. python -c 'from tensorflow.python.tools import optimize_for_inference_lib'. ```. The real issue is openvino is not installed . ```. python -c 'from openvino.runtime import Core, AsyncInferQueue, Type'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. ModuleNotFoundError: No module named 'openvino'. ```. which triggers the ImportError and pass statement skipping the import of optimize_for_inference_lib. Not to expose my limited understanding of dockerfiles, but [here](https://hub.docker.com/layers/deepvariant/google/deepvariant/latest/images/sha256-83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18?context=explore) it appears that the current latest build has ` ENV DV_OPENVINO_BUILD=0`. I've seen a lot of back and forth with openvino no longer being as helpful, but then there has been some recent updates, so not sure if it is still recommended or deprecated as it has disappeared from some docs. Best,. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/541
https://github.com/google/deepvariant/issues/541:1748,modifiability,layer,layers,1748,"nd #416, maybe I should just move on :wink:). The error during call_variants is below (and similar to #432). ```. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 416, in call_variants. ie_estimator = OpenVINOEstimator(. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 90, in __init__. freeze_graph(model, checkpoint_path, tensor_shape, openvino_model_pb). File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, in freeze_graph. graph_def = optimize_for_inference_lib.optimize_for_inference(. NameError: name 'optimize_for_inference_lib' is not defined. ```. Which comes from [here](https://github.com/google/deepvariant/blob/d2a3aca8691318221e794594ea08e7c88e21359b/deepvariant/openvino_estimator.py#L42). However, after playing around inside the image, the line `from tensorflow.python.tools import optimize_for_inference_lib` works fine as I can successfully run. ```. python -c 'from tensorflow.python.tools import optimize_for_inference_lib'. ```. The real issue is openvino is not installed . ```. python -c 'from openvino.runtime import Core, AsyncInferQueue, Type'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. ModuleNotFoundError: No module named 'openvino'. ```. which triggers the ImportError and pass statement skipping the import of optimize_for_inference_lib. Not to expose my limited understanding of dockerfiles, but [here](https://hub.docker.com/layers/deepvariant/google/deepvariant/latest/images/sha256-83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18?context=explore) it appears that the current latest build has ` ENV DV_OPENVINO_BUILD=0`. I've seen a lot of back and forth with openvino no longer being as helpful, but then there has been some recent updates, so not sure if it is still recommended or deprecated as it has disappeared from some docs. Best,. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/541
https://github.com/google/deepvariant/issues/541:17,performance,cpu,cpu,17,"v1.4 deepvariant-cpu docker image does not contain openvino; I've been trying out the new v1.4 docker image through singularity, but have been running into openvino issues again (#404 and #416, maybe I should just move on :wink:). The error during call_variants is below (and similar to #432). ```. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 416, in call_variants. ie_estimator = OpenVINOEstimator(. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 90, in __init__. freeze_graph(model, checkpoint_path, tensor_shape, openvino_model_pb). File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, in freeze_graph. graph_def = optimize_for_inference_lib.optimize_for_inference(. NameError: name 'optimize_for_inference_lib' is not defined. ```. Which comes from [here](https://github.com/google/deepvariant/blob/d2a3aca8691318221e794594ea08e7c88e21359b/deepvariant/openvino_estimator.py#L42). However, after playing around inside the image, the line `from tensorflow.python.tools import optimize_for_inference_lib` works fine as I can successfully run. ```. python -c 'from tensorflow.python.tools import optimize_for_inference_lib'. ```. The real issue is openvino is not installed . ```. python -c 'from openvino.runtime import Core, AsyncInferQueue, Type'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. ModuleNotFoundError: No module named 'openvino'. ```. which triggers the ImportError and pass statement skipping the import of optimize_for_inference_lib. Not to expose my limited understanding of dockerfiles, but [here](https://hub.docker.com/layers/deepvariant/google/deepvariant/latest/images/sha256-83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18?context=explore) it appears that the current latest build has ` ENV DV_OPENVINO_BUILD=0`. I've seen a lot of back and forth with",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/541
https://github.com/google/deepvariant/issues/541:235,performance,error,error,235,"v1.4 deepvariant-cpu docker image does not contain openvino; I've been trying out the new v1.4 docker image through singularity, but have been running into openvino issues again (#404 and #416, maybe I should just move on :wink:). The error during call_variants is below (and similar to #432). ```. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 416, in call_variants. ie_estimator = OpenVINOEstimator(. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 90, in __init__. freeze_graph(model, checkpoint_path, tensor_shape, openvino_model_pb). File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, in freeze_graph. graph_def = optimize_for_inference_lib.optimize_for_inference(. NameError: name 'optimize_for_inference_lib' is not defined. ```. Which comes from [here](https://github.com/google/deepvariant/blob/d2a3aca8691318221e794594ea08e7c88e21359b/deepvariant/openvino_estimator.py#L42). However, after playing around inside the image, the line `from tensorflow.python.tools import optimize_for_inference_lib` works fine as I can successfully run. ```. python -c 'from tensorflow.python.tools import optimize_for_inference_lib'. ```. The real issue is openvino is not installed . ```. python -c 'from openvino.runtime import Core, AsyncInferQueue, Type'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. ModuleNotFoundError: No module named 'openvino'. ```. which triggers the ImportError and pass statement skipping the import of optimize_for_inference_lib. Not to expose my limited understanding of dockerfiles, but [here](https://hub.docker.com/layers/deepvariant/google/deepvariant/latest/images/sha256-83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18?context=explore) it appears that the current latest build has ` ENV DV_OPENVINO_BUILD=0`. I've seen a lot of back and forth with",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/541
https://github.com/google/deepvariant/issues/541:34,reliability,doe,does,34,"v1.4 deepvariant-cpu docker image does not contain openvino; I've been trying out the new v1.4 docker image through singularity, but have been running into openvino issues again (#404 and #416, maybe I should just move on :wink:). The error during call_variants is below (and similar to #432). ```. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 416, in call_variants. ie_estimator = OpenVINOEstimator(. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 90, in __init__. freeze_graph(model, checkpoint_path, tensor_shape, openvino_model_pb). File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, in freeze_graph. graph_def = optimize_for_inference_lib.optimize_for_inference(. NameError: name 'optimize_for_inference_lib' is not defined. ```. Which comes from [here](https://github.com/google/deepvariant/blob/d2a3aca8691318221e794594ea08e7c88e21359b/deepvariant/openvino_estimator.py#L42). However, after playing around inside the image, the line `from tensorflow.python.tools import optimize_for_inference_lib` works fine as I can successfully run. ```. python -c 'from tensorflow.python.tools import optimize_for_inference_lib'. ```. The real issue is openvino is not installed . ```. python -c 'from openvino.runtime import Core, AsyncInferQueue, Type'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. ModuleNotFoundError: No module named 'openvino'. ```. which triggers the ImportError and pass statement skipping the import of optimize_for_inference_lib. Not to expose my limited understanding of dockerfiles, but [here](https://hub.docker.com/layers/deepvariant/google/deepvariant/latest/images/sha256-83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18?context=explore) it appears that the current latest build has ` ENV DV_OPENVINO_BUILD=0`. I've seen a lot of back and forth with",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/541
https://github.com/google/deepvariant/issues/541:235,safety,error,error,235,"v1.4 deepvariant-cpu docker image does not contain openvino; I've been trying out the new v1.4 docker image through singularity, but have been running into openvino issues again (#404 and #416, maybe I should just move on :wink:). The error during call_variants is below (and similar to #432). ```. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 416, in call_variants. ie_estimator = OpenVINOEstimator(. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 90, in __init__. freeze_graph(model, checkpoint_path, tensor_shape, openvino_model_pb). File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, in freeze_graph. graph_def = optimize_for_inference_lib.optimize_for_inference(. NameError: name 'optimize_for_inference_lib' is not defined. ```. Which comes from [here](https://github.com/google/deepvariant/blob/d2a3aca8691318221e794594ea08e7c88e21359b/deepvariant/openvino_estimator.py#L42). However, after playing around inside the image, the line `from tensorflow.python.tools import optimize_for_inference_lib` works fine as I can successfully run. ```. python -c 'from tensorflow.python.tools import optimize_for_inference_lib'. ```. The real issue is openvino is not installed . ```. python -c 'from openvino.runtime import Core, AsyncInferQueue, Type'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. ModuleNotFoundError: No module named 'openvino'. ```. which triggers the ImportError and pass statement skipping the import of optimize_for_inference_lib. Not to expose my limited understanding of dockerfiles, but [here](https://hub.docker.com/layers/deepvariant/google/deepvariant/latest/images/sha256-83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18?context=explore) it appears that the current latest build has ` ENV DV_OPENVINO_BUILD=0`. I've seen a lot of back and forth with",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/541
https://github.com/google/deepvariant/issues/541:1495,safety,modul,module,1495,"nd #416, maybe I should just move on :wink:). The error during call_variants is below (and similar to #432). ```. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 416, in call_variants. ie_estimator = OpenVINOEstimator(. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 90, in __init__. freeze_graph(model, checkpoint_path, tensor_shape, openvino_model_pb). File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, in freeze_graph. graph_def = optimize_for_inference_lib.optimize_for_inference(. NameError: name 'optimize_for_inference_lib' is not defined. ```. Which comes from [here](https://github.com/google/deepvariant/blob/d2a3aca8691318221e794594ea08e7c88e21359b/deepvariant/openvino_estimator.py#L42). However, after playing around inside the image, the line `from tensorflow.python.tools import optimize_for_inference_lib` works fine as I can successfully run. ```. python -c 'from tensorflow.python.tools import optimize_for_inference_lib'. ```. The real issue is openvino is not installed . ```. python -c 'from openvino.runtime import Core, AsyncInferQueue, Type'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. ModuleNotFoundError: No module named 'openvino'. ```. which triggers the ImportError and pass statement skipping the import of optimize_for_inference_lib. Not to expose my limited understanding of dockerfiles, but [here](https://hub.docker.com/layers/deepvariant/google/deepvariant/latest/images/sha256-83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18?context=explore) it appears that the current latest build has ` ENV DV_OPENVINO_BUILD=0`. I've seen a lot of back and forth with openvino no longer being as helpful, but then there has been some recent updates, so not sure if it is still recommended or deprecated as it has disappeared from some docs. Best,. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/541
https://github.com/google/deepvariant/issues/541:1504,safety,Modul,ModuleNotFoundError,1504,"nd #416, maybe I should just move on :wink:). The error during call_variants is below (and similar to #432). ```. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 416, in call_variants. ie_estimator = OpenVINOEstimator(. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 90, in __init__. freeze_graph(model, checkpoint_path, tensor_shape, openvino_model_pb). File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, in freeze_graph. graph_def = optimize_for_inference_lib.optimize_for_inference(. NameError: name 'optimize_for_inference_lib' is not defined. ```. Which comes from [here](https://github.com/google/deepvariant/blob/d2a3aca8691318221e794594ea08e7c88e21359b/deepvariant/openvino_estimator.py#L42). However, after playing around inside the image, the line `from tensorflow.python.tools import optimize_for_inference_lib` works fine as I can successfully run. ```. python -c 'from tensorflow.python.tools import optimize_for_inference_lib'. ```. The real issue is openvino is not installed . ```. python -c 'from openvino.runtime import Core, AsyncInferQueue, Type'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. ModuleNotFoundError: No module named 'openvino'. ```. which triggers the ImportError and pass statement skipping the import of optimize_for_inference_lib. Not to expose my limited understanding of dockerfiles, but [here](https://hub.docker.com/layers/deepvariant/google/deepvariant/latest/images/sha256-83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18?context=explore) it appears that the current latest build has ` ENV DV_OPENVINO_BUILD=0`. I've seen a lot of back and forth with openvino no longer being as helpful, but then there has been some recent updates, so not sure if it is still recommended or deprecated as it has disappeared from some docs. Best,. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/541
https://github.com/google/deepvariant/issues/541:1528,safety,modul,module,1528,"nd #416, maybe I should just move on :wink:). The error during call_variants is below (and similar to #432). ```. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 416, in call_variants. ie_estimator = OpenVINOEstimator(. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 90, in __init__. freeze_graph(model, checkpoint_path, tensor_shape, openvino_model_pb). File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, in freeze_graph. graph_def = optimize_for_inference_lib.optimize_for_inference(. NameError: name 'optimize_for_inference_lib' is not defined. ```. Which comes from [here](https://github.com/google/deepvariant/blob/d2a3aca8691318221e794594ea08e7c88e21359b/deepvariant/openvino_estimator.py#L42). However, after playing around inside the image, the line `from tensorflow.python.tools import optimize_for_inference_lib` works fine as I can successfully run. ```. python -c 'from tensorflow.python.tools import optimize_for_inference_lib'. ```. The real issue is openvino is not installed . ```. python -c 'from openvino.runtime import Core, AsyncInferQueue, Type'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. ModuleNotFoundError: No module named 'openvino'. ```. which triggers the ImportError and pass statement skipping the import of optimize_for_inference_lib. Not to expose my limited understanding of dockerfiles, but [here](https://hub.docker.com/layers/deepvariant/google/deepvariant/latest/images/sha256-83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18?context=explore) it appears that the current latest build has ` ENV DV_OPENVINO_BUILD=0`. I've seen a lot of back and forth with openvino no longer being as helpful, but then there has been some recent updates, so not sure if it is still recommended or deprecated as it has disappeared from some docs. Best,. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/541
https://github.com/google/deepvariant/issues/541:2074,safety,updat,updates,2074,"nd #416, maybe I should just move on :wink:). The error during call_variants is below (and similar to #432). ```. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 416, in call_variants. ie_estimator = OpenVINOEstimator(. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 90, in __init__. freeze_graph(model, checkpoint_path, tensor_shape, openvino_model_pb). File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, in freeze_graph. graph_def = optimize_for_inference_lib.optimize_for_inference(. NameError: name 'optimize_for_inference_lib' is not defined. ```. Which comes from [here](https://github.com/google/deepvariant/blob/d2a3aca8691318221e794594ea08e7c88e21359b/deepvariant/openvino_estimator.py#L42). However, after playing around inside the image, the line `from tensorflow.python.tools import optimize_for_inference_lib` works fine as I can successfully run. ```. python -c 'from tensorflow.python.tools import optimize_for_inference_lib'. ```. The real issue is openvino is not installed . ```. python -c 'from openvino.runtime import Core, AsyncInferQueue, Type'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. ModuleNotFoundError: No module named 'openvino'. ```. which triggers the ImportError and pass statement skipping the import of optimize_for_inference_lib. Not to expose my limited understanding of dockerfiles, but [here](https://hub.docker.com/layers/deepvariant/google/deepvariant/latest/images/sha256-83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18?context=explore) it appears that the current latest build has ` ENV DV_OPENVINO_BUILD=0`. I've seen a lot of back and forth with openvino no longer being as helpful, but then there has been some recent updates, so not sure if it is still recommended or deprecated as it has disappeared from some docs. Best,. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/541
https://github.com/google/deepvariant/issues/541:598,security,model,model,598,"v1.4 deepvariant-cpu docker image does not contain openvino; I've been trying out the new v1.4 docker image through singularity, but have been running into openvino issues again (#404 and #416, maybe I should just move on :wink:). The error during call_variants is below (and similar to #432). ```. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 416, in call_variants. ie_estimator = OpenVINOEstimator(. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 90, in __init__. freeze_graph(model, checkpoint_path, tensor_shape, openvino_model_pb). File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, in freeze_graph. graph_def = optimize_for_inference_lib.optimize_for_inference(. NameError: name 'optimize_for_inference_lib' is not defined. ```. Which comes from [here](https://github.com/google/deepvariant/blob/d2a3aca8691318221e794594ea08e7c88e21359b/deepvariant/openvino_estimator.py#L42). However, after playing around inside the image, the line `from tensorflow.python.tools import optimize_for_inference_lib` works fine as I can successfully run. ```. python -c 'from tensorflow.python.tools import optimize_for_inference_lib'. ```. The real issue is openvino is not installed . ```. python -c 'from openvino.runtime import Core, AsyncInferQueue, Type'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. ModuleNotFoundError: No module named 'openvino'. ```. which triggers the ImportError and pass statement skipping the import of optimize_for_inference_lib. Not to expose my limited understanding of dockerfiles, but [here](https://hub.docker.com/layers/deepvariant/google/deepvariant/latest/images/sha256-83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18?context=explore) it appears that the current latest build has ` ENV DV_OPENVINO_BUILD=0`. I've seen a lot of back and forth with",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/541
https://github.com/google/deepvariant/issues/541:1666,security,expos,expose,1666,"nd #416, maybe I should just move on :wink:). The error during call_variants is below (and similar to #432). ```. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 416, in call_variants. ie_estimator = OpenVINOEstimator(. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 90, in __init__. freeze_graph(model, checkpoint_path, tensor_shape, openvino_model_pb). File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, in freeze_graph. graph_def = optimize_for_inference_lib.optimize_for_inference(. NameError: name 'optimize_for_inference_lib' is not defined. ```. Which comes from [here](https://github.com/google/deepvariant/blob/d2a3aca8691318221e794594ea08e7c88e21359b/deepvariant/openvino_estimator.py#L42). However, after playing around inside the image, the line `from tensorflow.python.tools import optimize_for_inference_lib` works fine as I can successfully run. ```. python -c 'from tensorflow.python.tools import optimize_for_inference_lib'. ```. The real issue is openvino is not installed . ```. python -c 'from openvino.runtime import Core, AsyncInferQueue, Type'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. ModuleNotFoundError: No module named 'openvino'. ```. which triggers the ImportError and pass statement skipping the import of optimize_for_inference_lib. Not to expose my limited understanding of dockerfiles, but [here](https://hub.docker.com/layers/deepvariant/google/deepvariant/latest/images/sha256-83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18?context=explore) it appears that the current latest build has ` ENV DV_OPENVINO_BUILD=0`. I've seen a lot of back and forth with openvino no longer being as helpful, but then there has been some recent updates, so not sure if it is still recommended or deprecated as it has disappeared from some docs. Best,. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/541
https://github.com/google/deepvariant/issues/541:2074,security,updat,updates,2074,"nd #416, maybe I should just move on :wink:). The error during call_variants is below (and similar to #432). ```. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 416, in call_variants. ie_estimator = OpenVINOEstimator(. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 90, in __init__. freeze_graph(model, checkpoint_path, tensor_shape, openvino_model_pb). File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, in freeze_graph. graph_def = optimize_for_inference_lib.optimize_for_inference(. NameError: name 'optimize_for_inference_lib' is not defined. ```. Which comes from [here](https://github.com/google/deepvariant/blob/d2a3aca8691318221e794594ea08e7c88e21359b/deepvariant/openvino_estimator.py#L42). However, after playing around inside the image, the line `from tensorflow.python.tools import optimize_for_inference_lib` works fine as I can successfully run. ```. python -c 'from tensorflow.python.tools import optimize_for_inference_lib'. ```. The real issue is openvino is not installed . ```. python -c 'from openvino.runtime import Core, AsyncInferQueue, Type'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. ModuleNotFoundError: No module named 'openvino'. ```. which triggers the ImportError and pass statement skipping the import of optimize_for_inference_lib. Not to expose my limited understanding of dockerfiles, but [here](https://hub.docker.com/layers/deepvariant/google/deepvariant/latest/images/sha256-83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18?context=explore) it appears that the current latest build has ` ENV DV_OPENVINO_BUILD=0`. I've seen a lot of back and forth with openvino no longer being as helpful, but then there has been some recent updates, so not sure if it is still recommended or deprecated as it has disappeared from some docs. Best,. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/541
https://github.com/google/deepvariant/issues/541:1430,testability,Trace,Traceback,1430,"nd #416, maybe I should just move on :wink:). The error during call_variants is below (and similar to #432). ```. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 416, in call_variants. ie_estimator = OpenVINOEstimator(. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 90, in __init__. freeze_graph(model, checkpoint_path, tensor_shape, openvino_model_pb). File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, in freeze_graph. graph_def = optimize_for_inference_lib.optimize_for_inference(. NameError: name 'optimize_for_inference_lib' is not defined. ```. Which comes from [here](https://github.com/google/deepvariant/blob/d2a3aca8691318221e794594ea08e7c88e21359b/deepvariant/openvino_estimator.py#L42). However, after playing around inside the image, the line `from tensorflow.python.tools import optimize_for_inference_lib` works fine as I can successfully run. ```. python -c 'from tensorflow.python.tools import optimize_for_inference_lib'. ```. The real issue is openvino is not installed . ```. python -c 'from openvino.runtime import Core, AsyncInferQueue, Type'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. ModuleNotFoundError: No module named 'openvino'. ```. which triggers the ImportError and pass statement skipping the import of optimize_for_inference_lib. Not to expose my limited understanding of dockerfiles, but [here](https://hub.docker.com/layers/deepvariant/google/deepvariant/latest/images/sha256-83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18?context=explore) it appears that the current latest build has ` ENV DV_OPENVINO_BUILD=0`. I've seen a lot of back and forth with openvino no longer being as helpful, but then there has been some recent updates, so not sure if it is still recommended or deprecated as it has disappeared from some docs. Best,. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/541
https://github.com/google/deepvariant/issues/541:1684,testability,understand,understanding,1684,"nd #416, maybe I should just move on :wink:). The error during call_variants is below (and similar to #432). ```. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 416, in call_variants. ie_estimator = OpenVINOEstimator(. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 90, in __init__. freeze_graph(model, checkpoint_path, tensor_shape, openvino_model_pb). File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, in freeze_graph. graph_def = optimize_for_inference_lib.optimize_for_inference(. NameError: name 'optimize_for_inference_lib' is not defined. ```. Which comes from [here](https://github.com/google/deepvariant/blob/d2a3aca8691318221e794594ea08e7c88e21359b/deepvariant/openvino_estimator.py#L42). However, after playing around inside the image, the line `from tensorflow.python.tools import optimize_for_inference_lib` works fine as I can successfully run. ```. python -c 'from tensorflow.python.tools import optimize_for_inference_lib'. ```. The real issue is openvino is not installed . ```. python -c 'from openvino.runtime import Core, AsyncInferQueue, Type'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. ModuleNotFoundError: No module named 'openvino'. ```. which triggers the ImportError and pass statement skipping the import of optimize_for_inference_lib. Not to expose my limited understanding of dockerfiles, but [here](https://hub.docker.com/layers/deepvariant/google/deepvariant/latest/images/sha256-83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18?context=explore) it appears that the current latest build has ` ENV DV_OPENVINO_BUILD=0`. I've seen a lot of back and forth with openvino no longer being as helpful, but then there has been some recent updates, so not sure if it is still recommended or deprecated as it has disappeared from some docs. Best,. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/541
https://github.com/google/deepvariant/issues/541:1872,testability,context,context,1872,"nd #416, maybe I should just move on :wink:). The error during call_variants is below (and similar to #432). ```. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 416, in call_variants. ie_estimator = OpenVINOEstimator(. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 90, in __init__. freeze_graph(model, checkpoint_path, tensor_shape, openvino_model_pb). File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, in freeze_graph. graph_def = optimize_for_inference_lib.optimize_for_inference(. NameError: name 'optimize_for_inference_lib' is not defined. ```. Which comes from [here](https://github.com/google/deepvariant/blob/d2a3aca8691318221e794594ea08e7c88e21359b/deepvariant/openvino_estimator.py#L42). However, after playing around inside the image, the line `from tensorflow.python.tools import optimize_for_inference_lib` works fine as I can successfully run. ```. python -c 'from tensorflow.python.tools import optimize_for_inference_lib'. ```. The real issue is openvino is not installed . ```. python -c 'from openvino.runtime import Core, AsyncInferQueue, Type'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. ModuleNotFoundError: No module named 'openvino'. ```. which triggers the ImportError and pass statement skipping the import of optimize_for_inference_lib. Not to expose my limited understanding of dockerfiles, but [here](https://hub.docker.com/layers/deepvariant/google/deepvariant/latest/images/sha256-83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18?context=explore) it appears that the current latest build has ` ENV DV_OPENVINO_BUILD=0`. I've seen a lot of back and forth with openvino no longer being as helpful, but then there has been some recent updates, so not sure if it is still recommended or deprecated as it has disappeared from some docs. Best,. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/541
https://github.com/google/deepvariant/issues/541:235,usability,error,error,235,"v1.4 deepvariant-cpu docker image does not contain openvino; I've been trying out the new v1.4 docker image through singularity, but have been running into openvino issues again (#404 and #416, maybe I should just move on :wink:). The error during call_variants is below (and similar to #432). ```. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 416, in call_variants. ie_estimator = OpenVINOEstimator(. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 90, in __init__. freeze_graph(model, checkpoint_path, tensor_shape, openvino_model_pb). File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, in freeze_graph. graph_def = optimize_for_inference_lib.optimize_for_inference(. NameError: name 'optimize_for_inference_lib' is not defined. ```. Which comes from [here](https://github.com/google/deepvariant/blob/d2a3aca8691318221e794594ea08e7c88e21359b/deepvariant/openvino_estimator.py#L42). However, after playing around inside the image, the line `from tensorflow.python.tools import optimize_for_inference_lib` works fine as I can successfully run. ```. python -c 'from tensorflow.python.tools import optimize_for_inference_lib'. ```. The real issue is openvino is not installed . ```. python -c 'from openvino.runtime import Core, AsyncInferQueue, Type'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. ModuleNotFoundError: No module named 'openvino'. ```. which triggers the ImportError and pass statement skipping the import of optimize_for_inference_lib. Not to expose my limited understanding of dockerfiles, but [here](https://hub.docker.com/layers/deepvariant/google/deepvariant/latest/images/sha256-83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18?context=explore) it appears that the current latest build has ` ENV DV_OPENVINO_BUILD=0`. I've seen a lot of back and forth with",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/541
https://github.com/google/deepvariant/issues/541:1144,usability,tool,tools,1144,"ing into openvino issues again (#404 and #416, maybe I should just move on :wink:). The error during call_variants is below (and similar to #432). ```. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 416, in call_variants. ie_estimator = OpenVINOEstimator(. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 90, in __init__. freeze_graph(model, checkpoint_path, tensor_shape, openvino_model_pb). File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, in freeze_graph. graph_def = optimize_for_inference_lib.optimize_for_inference(. NameError: name 'optimize_for_inference_lib' is not defined. ```. Which comes from [here](https://github.com/google/deepvariant/blob/d2a3aca8691318221e794594ea08e7c88e21359b/deepvariant/openvino_estimator.py#L42). However, after playing around inside the image, the line `from tensorflow.python.tools import optimize_for_inference_lib` works fine as I can successfully run. ```. python -c 'from tensorflow.python.tools import optimize_for_inference_lib'. ```. The real issue is openvino is not installed . ```. python -c 'from openvino.runtime import Core, AsyncInferQueue, Type'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. ModuleNotFoundError: No module named 'openvino'. ```. which triggers the ImportError and pass statement skipping the import of optimize_for_inference_lib. Not to expose my limited understanding of dockerfiles, but [here](https://hub.docker.com/layers/deepvariant/google/deepvariant/latest/images/sha256-83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18?context=explore) it appears that the current latest build has ` ENV DV_OPENVINO_BUILD=0`. I've seen a lot of back and forth with openvino no longer being as helpful, but then there has been some recent updates, so not sure if it is still recommended or deprecated as it has d",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/541
https://github.com/google/deepvariant/issues/541:1262,usability,tool,tools,1262,"nd #416, maybe I should just move on :wink:). The error during call_variants is below (and similar to #432). ```. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 416, in call_variants. ie_estimator = OpenVINOEstimator(. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 90, in __init__. freeze_graph(model, checkpoint_path, tensor_shape, openvino_model_pb). File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, in freeze_graph. graph_def = optimize_for_inference_lib.optimize_for_inference(. NameError: name 'optimize_for_inference_lib' is not defined. ```. Which comes from [here](https://github.com/google/deepvariant/blob/d2a3aca8691318221e794594ea08e7c88e21359b/deepvariant/openvino_estimator.py#L42). However, after playing around inside the image, the line `from tensorflow.python.tools import optimize_for_inference_lib` works fine as I can successfully run. ```. python -c 'from tensorflow.python.tools import optimize_for_inference_lib'. ```. The real issue is openvino is not installed . ```. python -c 'from openvino.runtime import Core, AsyncInferQueue, Type'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. ModuleNotFoundError: No module named 'openvino'. ```. which triggers the ImportError and pass statement skipping the import of optimize_for_inference_lib. Not to expose my limited understanding of dockerfiles, but [here](https://hub.docker.com/layers/deepvariant/google/deepvariant/latest/images/sha256-83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18?context=explore) it appears that the current latest build has ` ENV DV_OPENVINO_BUILD=0`. I've seen a lot of back and forth with openvino no longer being as helpful, but then there has been some recent updates, so not sure if it is still recommended or deprecated as it has disappeared from some docs. Best,. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/541
https://github.com/google/deepvariant/issues/541:2029,usability,help,helpful,2029,"nd #416, maybe I should just move on :wink:). The error during call_variants is below (and similar to #432). ```. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 416, in call_variants. ie_estimator = OpenVINOEstimator(. File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 90, in __init__. freeze_graph(model, checkpoint_path, tensor_shape, openvino_model_pb). File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, in freeze_graph. graph_def = optimize_for_inference_lib.optimize_for_inference(. NameError: name 'optimize_for_inference_lib' is not defined. ```. Which comes from [here](https://github.com/google/deepvariant/blob/d2a3aca8691318221e794594ea08e7c88e21359b/deepvariant/openvino_estimator.py#L42). However, after playing around inside the image, the line `from tensorflow.python.tools import optimize_for_inference_lib` works fine as I can successfully run. ```. python -c 'from tensorflow.python.tools import optimize_for_inference_lib'. ```. The real issue is openvino is not installed . ```. python -c 'from openvino.runtime import Core, AsyncInferQueue, Type'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. ModuleNotFoundError: No module named 'openvino'. ```. which triggers the ImportError and pass statement skipping the import of optimize_for_inference_lib. Not to expose my limited understanding of dockerfiles, but [here](https://hub.docker.com/layers/deepvariant/google/deepvariant/latest/images/sha256-83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18?context=explore) it appears that the current latest build has ` ENV DV_OPENVINO_BUILD=0`. I've seen a lot of back and forth with openvino no longer being as helpful, but then there has been some recent updates, so not sure if it is still recommended or deprecated as it has disappeared from some docs. Best,. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/541
https://github.com/google/deepvariant/issues/542:70,availability,Operat,Operatin,70,"alueError: Unknown: BED record has invalid number of fields; Hello, . Operatin system: Linux HPC . Version: 1.3.0 . Installation: Singularity . Data: WES - with Agilent SureSelect DNA Human All ExonV5_hg38 bed file. **Steps to reproduce:**. **Command**. ```. `#!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p compute. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-23. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=Polyposis_Exome_Analysis_JOB27/fastp/All_fastp_input/IDswithoutR1R2_JOB27. HG38_REFERENCE=Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna. PICARDMARKDUPLICATES_SORTEDBAM=Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam. BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed. OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz. OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz. INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. *",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:1490,availability,error,error,1490,"# Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-23. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=Polyposis_Exome_Analysis_JOB27/fastp/All_fastp_input/IDswithoutR1R2_JOB27. HG38_REFERENCE=Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna. PICARDMARKDUPLICATES_SORTEDBAM=Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam. BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed. OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz. OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz. INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. ***** Intermediate results will be written to Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate in docker. ****. ***** Running the command:*****. ```. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna"" --reads ""Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:1522,availability,error,error,1522," FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-23. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=Polyposis_Exome_Analysis_JOB27/fastp/All_fastp_input/IDswithoutR1R2_JOB27. HG38_REFERENCE=Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna. PICARDMARKDUPLICATES_SORTEDBAM=Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam. BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed. OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz. OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz. INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. ***** Intermediate results will be written to Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate in docker. ****. ***** Running the command:*****. ```. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna"" --reads ""Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:1983,availability,Error,Error,1983,"lyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam. BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed. OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz. OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz. INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. ***** Intermediate results will be written to Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate in docker. ****. ***** Running the command:*****. ```. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna"" --reads ""Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam"" --examples ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/make_examples.tfrecord@1.gz"" --gvcf ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/gvcf.tfrecord@1.gz"" --regions ""Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed"" --task {}. perl: warning: Setting locale failed. perl: warning: ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:18577,availability,error,error,18577,"cleus/util/ranges.py"", line 459, in bed_parser. for r in fin.iterate():. File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 102, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Unknown: BED record has invalid number of fields. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna --reads Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam --examples Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/make_examples.tfrecord@1.gz --gvcf Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/gvcf.tfrecord@1.gz --regions Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed --task 0. real	0m3.367s. user	0m2.683s. sys	0m0.545s. ```. **First lines:**. **First 10 lines of sorted marked duplicate bam is: ** . BAM?P@HD	VN:1.6	SO:coordinate. @SQ	SN:NC_000001.11	LN:248956422. @SQ	SN:NT_187361.1	LN:175055. @SQ	SN:NT_187362.1	LN:32032. @SQ	SN:NT_187363.1	LN:127682. @SQ	SN:NT_187364.1	LN:66860. @SQ	SN:NT_187365.1	LN:40176. @SQ	SN:NT_187366.1	LN:42210. @SQ	SN:NT_187367.1	LN:176043. @SQ	SN:NT_187368.1	LN:40745. **First line of reference hg38 is:**. >NC_000001.11 Homo sapiens chromosome 1, GRCh38.p13 Primary Assembly. **First line of bed file is:**. NC_000001.11 65509 65625. I have got deepvariant and the above code to work for another dataset with a different bed file used - but I'm not sure why the ValueError: Unknown: BED record has invalid number of fields error is occurring. . Thanks! Amy.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:99,deployability,Version,Version,99,"alueError: Unknown: BED record has invalid number of fields; Hello, . Operatin system: Linux HPC . Version: 1.3.0 . Installation: Singularity . Data: WES - with Agilent SureSelect DNA Human All ExonV5_hg38 bed file. **Steps to reproduce:**. **Command**. ```. `#!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p compute. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-23. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=Polyposis_Exome_Analysis_JOB27/fastp/All_fastp_input/IDswithoutR1R2_JOB27. HG38_REFERENCE=Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna. PICARDMARKDUPLICATES_SORTEDBAM=Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam. BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed. OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz. OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz. INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. *",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:116,deployability,Instal,Installation,116,"alueError: Unknown: BED record has invalid number of fields; Hello, . Operatin system: Linux HPC . Version: 1.3.0 . Installation: Singularity . Data: WES - with Agilent SureSelect DNA Human All ExonV5_hg38 bed file. **Steps to reproduce:**. **Command**. ```. `#!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p compute. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-23. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=Polyposis_Exome_Analysis_JOB27/fastp/All_fastp_input/IDswithoutR1R2_JOB27. HG38_REFERENCE=Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna. PICARDMARKDUPLICATES_SORTEDBAM=Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam. BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed. OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz. OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz. INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. *",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:274,deployability,log,login,274,"alueError: Unknown: BED record has invalid number of fields; Hello, . Operatin system: Linux HPC . Version: 1.3.0 . Installation: Singularity . Data: WES - with Agilent SureSelect DNA Human All ExonV5_hg38 bed file. **Steps to reproduce:**. **Command**. ```. `#!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p compute. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-23. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=Polyposis_Exome_Analysis_JOB27/fastp/All_fastp_input/IDswithoutR1R2_JOB27. HG38_REFERENCE=Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna. PICARDMARKDUPLICATES_SORTEDBAM=Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam. BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed. OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz. OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz. INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. *",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:526,deployability,FAIL,FAIL,526,"alueError: Unknown: BED record has invalid number of fields; Hello, . Operatin system: Linux HPC . Version: 1.3.0 . Installation: Singularity . Data: WES - with Agilent SureSelect DNA Human All ExonV5_hg38 bed file. **Steps to reproduce:**. **Command**. ```. `#!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p compute. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-23. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=Polyposis_Exome_Analysis_JOB27/fastp/All_fastp_input/IDswithoutR1R2_JOB27. HG38_REFERENCE=Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna. PICARDMARKDUPLICATES_SORTEDBAM=Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam. BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed. OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz. OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz. INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. *",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:680,deployability,modul,module,680,"alueError: Unknown: BED record has invalid number of fields; Hello, . Operatin system: Linux HPC . Version: 1.3.0 . Installation: Singularity . Data: WES - with Agilent SureSelect DNA Human All ExonV5_hg38 bed file. **Steps to reproduce:**. **Command**. ```. `#!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p compute. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-23. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=Polyposis_Exome_Analysis_JOB27/fastp/All_fastp_input/IDswithoutR1R2_JOB27. HG38_REFERENCE=Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna. PICARDMARKDUPLICATES_SORTEDBAM=Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam. BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed. OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz. OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz. INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. *",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:694,deployability,modul,module,694,"alueError: Unknown: BED record has invalid number of fields; Hello, . Operatin system: Linux HPC . Version: 1.3.0 . Installation: Singularity . Data: WES - with Agilent SureSelect DNA Human All ExonV5_hg38 bed file. **Steps to reproduce:**. **Command**. ```. `#!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p compute. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-23. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=Polyposis_Exome_Analysis_JOB27/fastp/All_fastp_input/IDswithoutR1R2_JOB27. HG38_REFERENCE=Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna. PICARDMARKDUPLICATES_SORTEDBAM=Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam. BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed. OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz. OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz. INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. *",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:716,deployability,modul,module,716,"alueError: Unknown: BED record has invalid number of fields; Hello, . Operatin system: Linux HPC . Version: 1.3.0 . Installation: Singularity . Data: WES - with Agilent SureSelect DNA Human All ExonV5_hg38 bed file. **Steps to reproduce:**. **Command**. ```. `#!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p compute. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-23. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=Polyposis_Exome_Analysis_JOB27/fastp/All_fastp_input/IDswithoutR1R2_JOB27. HG38_REFERENCE=Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna. PICARDMARKDUPLICATES_SORTEDBAM=Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam. BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed. OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz. OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz. INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. *",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:1683,deployability,contain,containers,1683,"urge. module load parallel. module load singularity. EXOME_IDs_FILE=Polyposis_Exome_Analysis_JOB27/fastp/All_fastp_input/IDswithoutR1R2_JOB27. HG38_REFERENCE=Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna. PICARDMARKDUPLICATES_SORTEDBAM=Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam. BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed. OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz. OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz. INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. ***** Intermediate results will be written to Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate in docker. ****. ***** Running the command:*****. ```. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna"" --reads ""Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam"" --examples ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/make_examples.tfrecord@1.gz"" --gvcf ""Polyposis_Exom",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:2963,deployability,fail,failed,2963,"RESULTS"". ```. **Error trace:**. ***** Intermediate results will be written to Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate in docker. ****. ***** Running the command:*****. ```. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna"" --reads ""Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam"" --examples ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/make_examples.tfrecord@1.gz"" --gvcf ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/gvcf.tfrecord@1.gz"" --regions ""Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). I0614 20:20:45.812468 47288495204160 genomics_reader.py:222] Reading Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam with NativeSamReader. W0614 20:20:45.812704 47288495204160 make_examples_core.py:276] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. I0614 20:20:45.822298 47288495204160 make_examples_core.py:239] Preparing inputs. I",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:3110,deployability,instal,installed,3110,"3_L7_PE_output_intermediate in docker. ****. ***** Running the command:*****. ```. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna"" --reads ""Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam"" --examples ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/make_examples.tfrecord@1.gz"" --gvcf ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/gvcf.tfrecord@1.gz"" --regions ""Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). I0614 20:20:45.812468 47288495204160 genomics_reader.py:222] Reading Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam with NativeSamReader. W0614 20:20:45.812704 47288495204160 make_examples_core.py:276] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. I0614 20:20:45.822298 47288495204160 make_examples_core.py:239] Preparing inputs. I0614 20:20:45.836667 47288495204160 genomics_reader.py:222] Reading Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:3224,deployability,fail,failed,3224,"halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna"" --reads ""Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam"" --examples ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/make_examples.tfrecord@1.gz"" --gvcf ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/gvcf.tfrecord@1.gz"" --regions ""Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). I0614 20:20:45.812468 47288495204160 genomics_reader.py:222] Reading Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam with NativeSamReader. W0614 20:20:45.812704 47288495204160 make_examples_core.py:276] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. I0614 20:20:45.822298 47288495204160 make_examples_core.py:239] Preparing inputs. I0614 20:20:45.836667 47288495204160 genomics_reader.py:222] Reading Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam with NativeSamReader. I0614 20:20:46.057183 47288495204160 make_examples_core.py:239] Co",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:3371,deployability,instal,installed,3371,"ls_faidx/GRCh38_latest_genomic.fna"" --reads ""Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam"" --examples ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/make_examples.tfrecord@1.gz"" --gvcf ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/gvcf.tfrecord@1.gz"" --regions ""Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). I0614 20:20:45.812468 47288495204160 genomics_reader.py:222] Reading Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam with NativeSamReader. W0614 20:20:45.812704 47288495204160 make_examples_core.py:276] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. I0614 20:20:45.822298 47288495204160 make_examples_core.py:239] Preparing inputs. I0614 20:20:45.836667 47288495204160 genomics_reader.py:222] Reading Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam with NativeSamReader. I0614 20:20:46.057183 47288495204160 make_examples_core.py:239] Common contigs are ['NC_000001.11', 'NT_187361.1', 'NT_187362.1', 'NT_187363.1', 'NT_187364.1', 'NT_187365.1', 'NT_187366.1', 'NT_187367.1', 'NT_187368",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:14978,deployability,modul,module,14978,"7688.1', 'NT_167246.2', 'NW_003571057.2', 'NT_187689.1', 'NT_167247.2', 'NW_003571058.2', 'NT_187690.1', 'NT_167248.2', 'NW_003571059.2', 'NT_187691.1', 'NT_167249.2', 'NW_003571060.1', 'NT_187692.1', 'NW_003571061.2', 'NT_187693.1', 'NT_187636.1', 'NT_187637.1', 'NT_187638.1', 'NT_187639.1', 'NT_187640.1', 'NT_187641.1', 'NT_187642.1', 'NT_187643.1', 'NT_187644.1', 'NT_187645.1', 'NT_187668.1', 'NT_187669.1', 'NT_187670.1', 'NT_187671.1', 'NT_187672.1', 'NT_187673.1', 'NT_187674.1', 'NT_187675.1', 'NT_187676.1', 'NT_187677.1', 'NT_187683.1', 'NT_187684.1', 'NT_187685.1', 'NT_187686.1', 'NT_187687.1', 'NT_113949.2', 'NC_012920.1']. I0614 20:20:46.158432 47288495204160 genomics_reader.py:222] Reading Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed with NativeBedReader. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 170, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1626, in make_examples_runner. regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1466, in processing_regions_from_options. calling_regions = build_calling_regions(ref_contigs, options.calling_regions,. File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 468, in build_calling_regions. ranges.RangeSet.from_regions(regio",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:17115,deployability,fail,failed,17115,"e 161, in from_regions. return cls(ranges=from_regions(regions, contig_map=contig_map)). File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 113, in __init__. for i, range_ in enumerate(ranges):. File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 493, in from_regions. for elt in reader(region):. File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 459, in bed_parser. for r in fin.iterate():. File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 102, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Unknown: BED record has invalid number of fields. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna --reads Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam --examples Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/make_examples.tfrecord@1.gz --gvcf Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/gvcf.tfrecord@1.gz --regions Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed --task 0. real	0m3.367s. user	0m2.683s. sys	0m0.545s. ```. **First lines:**. **First 10 lines of sorted marked duplicate bam is: ** . BAM?P@HD	VN:1.6	SO:coordinate. @SQ	SN:NC_000001.11	LN:248956422. @SQ	SN:NT_187361.1	LN:175055. @SQ	SN:NT_187362.1	LN:32032. @SQ	SN:NT_187363.1	LN:127682. @SQ	SN:NT_187364.1	LN:66860. @SQ	SN:NT_187",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:669,energy efficiency,cpu,cpu,669,"alueError: Unknown: BED record has invalid number of fields; Hello, . Operatin system: Linux HPC . Version: 1.3.0 . Installation: Singularity . Data: WES - with Agilent SureSelect DNA Human All ExonV5_hg38 bed file. **Steps to reproduce:**. **Command**. ```. `#!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p compute. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-23. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=Polyposis_Exome_Analysis_JOB27/fastp/All_fastp_input/IDswithoutR1R2_JOB27. HG38_REFERENCE=Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna. PICARDMARKDUPLICATES_SORTEDBAM=Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam. BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed. OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz. OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz. INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. *",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:701,energy efficiency,load,load,701,"alueError: Unknown: BED record has invalid number of fields; Hello, . Operatin system: Linux HPC . Version: 1.3.0 . Installation: Singularity . Data: WES - with Agilent SureSelect DNA Human All ExonV5_hg38 bed file. **Steps to reproduce:**. **Command**. ```. `#!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p compute. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-23. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=Polyposis_Exome_Analysis_JOB27/fastp/All_fastp_input/IDswithoutR1R2_JOB27. HG38_REFERENCE=Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna. PICARDMARKDUPLICATES_SORTEDBAM=Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam. BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed. OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz. OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz. INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. *",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:723,energy efficiency,load,load,723,"alueError: Unknown: BED record has invalid number of fields; Hello, . Operatin system: Linux HPC . Version: 1.3.0 . Installation: Singularity . Data: WES - with Agilent SureSelect DNA Human All ExonV5_hg38 bed file. **Steps to reproduce:**. **Command**. ```. `#!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p compute. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-23. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=Polyposis_Exome_Analysis_JOB27/fastp/All_fastp_input/IDswithoutR1R2_JOB27. HG38_REFERENCE=Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna. PICARDMARKDUPLICATES_SORTEDBAM=Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam. BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed. OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz. OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz. INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. *",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:99,integrability,Version,Version,99,"alueError: Unknown: BED record has invalid number of fields; Hello, . Operatin system: Linux HPC . Version: 1.3.0 . Installation: Singularity . Data: WES - with Agilent SureSelect DNA Human All ExonV5_hg38 bed file. **Steps to reproduce:**. **Command**. ```. `#!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p compute. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-23. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=Polyposis_Exome_Analysis_JOB27/fastp/All_fastp_input/IDswithoutR1R2_JOB27. HG38_REFERENCE=Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna. PICARDMARKDUPLICATES_SORTEDBAM=Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam. BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed. OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz. OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz. INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. *",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:500,integrability,event,events,500,"alueError: Unknown: BED record has invalid number of fields; Hello, . Operatin system: Linux HPC . Version: 1.3.0 . Installation: Singularity . Data: WES - with Agilent SureSelect DNA Human All ExonV5_hg38 bed file. **Steps to reproduce:**. **Command**. ```. `#!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p compute. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-23. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=Polyposis_Exome_Analysis_JOB27/fastp/All_fastp_input/IDswithoutR1R2_JOB27. HG38_REFERENCE=Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna. PICARDMARKDUPLICATES_SORTEDBAM=Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam. BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed. OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz. OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz. INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. *",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:2241,integrability,buffer,buffer,2241,"is_JOB27/deepvariant/vcf/{}PE_output.vcf.gz. OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz. INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. ***** Intermediate results will be written to Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate in docker. ****. ***** Running the command:*****. ```. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna"" --reads ""Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam"" --examples ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/make_examples.tfrecord@1.gz"" --gvcf ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/gvcf.tfrecord@1.gz"" --regions ""Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warnin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:3171,interoperability,standard,standard,3171,"he command:*****. ```. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna"" --reads ""Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam"" --examples ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/make_examples.tfrecord@1.gz"" --gvcf ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/gvcf.tfrecord@1.gz"" --regions ""Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). I0614 20:20:45.812468 47288495204160 genomics_reader.py:222] Reading Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam with NativeSamReader. W0614 20:20:45.812704 47288495204160 make_examples_core.py:276] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. I0614 20:20:45.822298 47288495204160 make_examples_core.py:239] Preparing inputs. I0614 20:20:45.836667 47288495204160 genomics_reader.py:222] Reading Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam with NativeSamReader. I0614 20:20:46",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:3432,interoperability,standard,standard,3432,"_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam"" --examples ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/make_examples.tfrecord@1.gz"" --gvcf ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/gvcf.tfrecord@1.gz"" --regions ""Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). I0614 20:20:45.812468 47288495204160 genomics_reader.py:222] Reading Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam with NativeSamReader. W0614 20:20:45.812704 47288495204160 make_examples_core.py:276] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. I0614 20:20:45.822298 47288495204160 make_examples_core.py:239] Preparing inputs. I0614 20:20:45.836667 47288495204160 genomics_reader.py:222] Reading Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam with NativeSamReader. I0614 20:20:46.057183 47288495204160 make_examples_core.py:239] Common contigs are ['NC_000001.11', 'NT_187361.1', 'NT_187362.1', 'NT_187363.1', 'NT_187364.1', 'NT_187365.1', 'NT_187366.1', 'NT_187367.1', 'NT_187368.1', 'NT_187369.1', 'NC_000002.12', 'NT_187370.1', 'NT_18737",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:17941,interoperability,coordinat,coordinate,17941,"cleus/util/ranges.py"", line 459, in bed_parser. for r in fin.iterate():. File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 102, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Unknown: BED record has invalid number of fields. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna --reads Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam --examples Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/make_examples.tfrecord@1.gz --gvcf Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/gvcf.tfrecord@1.gz --regions Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed --task 0. real	0m3.367s. user	0m2.683s. sys	0m0.545s. ```. **First lines:**. **First 10 lines of sorted marked duplicate bam is: ** . BAM?P@HD	VN:1.6	SO:coordinate. @SQ	SN:NC_000001.11	LN:248956422. @SQ	SN:NT_187361.1	LN:175055. @SQ	SN:NT_187362.1	LN:32032. @SQ	SN:NT_187363.1	LN:127682. @SQ	SN:NT_187364.1	LN:66860. @SQ	SN:NT_187365.1	LN:40176. @SQ	SN:NT_187366.1	LN:42210. @SQ	SN:NT_187367.1	LN:176043. @SQ	SN:NT_187368.1	LN:40745. **First line of reference hg38 is:**. >NC_000001.11 Homo sapiens chromosome 1, GRCh38.p13 Primary Assembly. **First line of bed file is:**. NC_000001.11 65509 65625. I have got deepvariant and the above code to work for another dataset with a different bed file used - but I'm not sure why the ValueError: Unknown: BED record has invalid number of fields error is occurring. . Thanks! Amy.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:99,modifiability,Version,Version,99,"alueError: Unknown: BED record has invalid number of fields; Hello, . Operatin system: Linux HPC . Version: 1.3.0 . Installation: Singularity . Data: WES - with Agilent SureSelect DNA Human All ExonV5_hg38 bed file. **Steps to reproduce:**. **Command**. ```. `#!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p compute. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-23. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=Polyposis_Exome_Analysis_JOB27/fastp/All_fastp_input/IDswithoutR1R2_JOB27. HG38_REFERENCE=Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna. PICARDMARKDUPLICATES_SORTEDBAM=Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam. BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed. OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz. OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz. INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. *",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:680,modifiability,modul,module,680,"alueError: Unknown: BED record has invalid number of fields; Hello, . Operatin system: Linux HPC . Version: 1.3.0 . Installation: Singularity . Data: WES - with Agilent SureSelect DNA Human All ExonV5_hg38 bed file. **Steps to reproduce:**. **Command**. ```. `#!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p compute. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-23. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=Polyposis_Exome_Analysis_JOB27/fastp/All_fastp_input/IDswithoutR1R2_JOB27. HG38_REFERENCE=Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna. PICARDMARKDUPLICATES_SORTEDBAM=Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam. BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed. OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz. OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz. INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. *",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:694,modifiability,modul,module,694,"alueError: Unknown: BED record has invalid number of fields; Hello, . Operatin system: Linux HPC . Version: 1.3.0 . Installation: Singularity . Data: WES - with Agilent SureSelect DNA Human All ExonV5_hg38 bed file. **Steps to reproduce:**. **Command**. ```. `#!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p compute. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-23. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=Polyposis_Exome_Analysis_JOB27/fastp/All_fastp_input/IDswithoutR1R2_JOB27. HG38_REFERENCE=Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna. PICARDMARKDUPLICATES_SORTEDBAM=Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam. BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed. OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz. OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz. INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. *",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:716,modifiability,modul,module,716,"alueError: Unknown: BED record has invalid number of fields; Hello, . Operatin system: Linux HPC . Version: 1.3.0 . Installation: Singularity . Data: WES - with Agilent SureSelect DNA Human All ExonV5_hg38 bed file. **Steps to reproduce:**. **Command**. ```. `#!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p compute. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-23. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=Polyposis_Exome_Analysis_JOB27/fastp/All_fastp_input/IDswithoutR1R2_JOB27. HG38_REFERENCE=Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna. PICARDMARKDUPLICATES_SORTEDBAM=Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam. BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed. OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz. OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz. INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. *",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:1433,modifiability,interm,intermediateresults,1433,"SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-23. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=Polyposis_Exome_Analysis_JOB27/fastp/All_fastp_input/IDswithoutR1R2_JOB27. HG38_REFERENCE=Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna. PICARDMARKDUPLICATES_SORTEDBAM=Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam. BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed. OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz. OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz. INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. ***** Intermediate results will be written to Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate in docker. ****. ***** Running the command:*****. ```. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna"" --reads ""Polyposis_Exome_Analys",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:2005,modifiability,Interm,Intermediate,2005,"B27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam. BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed. OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz. OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz. INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. ***** Intermediate results will be written to Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate in docker. ****. ***** Running the command:*****. ```. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna"" --reads ""Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam"" --examples ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/make_examples.tfrecord@1.gz"" --gvcf ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/gvcf.tfrecord@1.gz"" --regions ""Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your lo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:2088,modifiability,interm,intermediateresults,2088,"sis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed. OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz. OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz. INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. ***** Intermediate results will be written to Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate in docker. ****. ***** Running the command:*****. ```. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna"" --reads ""Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam"" --examples ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/make_examples.tfrecord@1.gz"" --gvcf ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/gvcf.tfrecord@1.gz"" --regions ""Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are su",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:2582,modifiability,interm,intermediateresults,2582,"D}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. ***** Intermediate results will be written to Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate in docker. ****. ***** Running the command:*****. ```. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna"" --reads ""Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam"" --examples ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/make_examples.tfrecord@1.gz"" --gvcf ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/gvcf.tfrecord@1.gz"" --regions ""Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). I0614 20:20:45.812468 47288495204160 genomics_reader.py:222] Reading Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:2717,modifiability,interm,intermediateresults,2717,"riant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. ***** Intermediate results will be written to Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate in docker. ****. ***** Running the command:*****. ```. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna"" --reads ""Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam"" --examples ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/make_examples.tfrecord@1.gz"" --gvcf ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/gvcf.tfrecord@1.gz"" --regions ""Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). I0614 20:20:45.812468 47288495204160 genomics_reader.py:222] Reading Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam with NativeSamReader. W0614 20:20:45.812704 47288495204160 make_examples_core.py:276] No non-empty",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:14978,modifiability,modul,module,14978,"7688.1', 'NT_167246.2', 'NW_003571057.2', 'NT_187689.1', 'NT_167247.2', 'NW_003571058.2', 'NT_187690.1', 'NT_167248.2', 'NW_003571059.2', 'NT_187691.1', 'NT_167249.2', 'NW_003571060.1', 'NT_187692.1', 'NW_003571061.2', 'NT_187693.1', 'NT_187636.1', 'NT_187637.1', 'NT_187638.1', 'NT_187639.1', 'NT_187640.1', 'NT_187641.1', 'NT_187642.1', 'NT_187643.1', 'NT_187644.1', 'NT_187645.1', 'NT_187668.1', 'NT_187669.1', 'NT_187670.1', 'NT_187671.1', 'NT_187672.1', 'NT_187673.1', 'NT_187674.1', 'NT_187675.1', 'NT_187676.1', 'NT_187677.1', 'NT_187683.1', 'NT_187684.1', 'NT_187685.1', 'NT_187686.1', 'NT_187687.1', 'NT_113949.2', 'NC_012920.1']. I0614 20:20:46.158432 47288495204160 genomics_reader.py:222] Reading Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed with NativeBedReader. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 170, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1626, in make_examples_runner. regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1466, in processing_regions_from_options. calling_regions = build_calling_regions(ref_contigs, options.calling_regions,. File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 468, in build_calling_regions. ranges.RangeSet.from_regions(regio",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:17453,modifiability,interm,intermediateresults,17453,"ranges.py"", line 493, in from_regions. for elt in reader(region):. File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 459, in bed_parser. for r in fin.iterate():. File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 102, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Unknown: BED record has invalid number of fields. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna --reads Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam --examples Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/make_examples.tfrecord@1.gz --gvcf Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/gvcf.tfrecord@1.gz --regions Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed --task 0. real	0m3.367s. user	0m2.683s. sys	0m0.545s. ```. **First lines:**. **First 10 lines of sorted marked duplicate bam is: ** . BAM?P@HD	VN:1.6	SO:coordinate. @SQ	SN:NC_000001.11	LN:248956422. @SQ	SN:NT_187361.1	LN:175055. @SQ	SN:NT_187362.1	LN:32032. @SQ	SN:NT_187363.1	LN:127682. @SQ	SN:NT_187364.1	LN:66860. @SQ	SN:NT_187365.1	LN:40176. @SQ	SN:NT_187366.1	LN:42210. @SQ	SN:NT_187367.1	LN:176043. @SQ	SN:NT_187368.1	LN:40745. **First line of reference hg38 is:**. >NC_000001.11 Homo sapiens chromosome 1, GRCh38.p13 Primary Assembly. **First line of bed file is:**. NC_000001.11 65509 65625. I have got deepvariant and the above code to work for another dataset with ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:17586,modifiability,interm,intermediateresults,17586,"/third_party/nucleus/util/ranges.py"", line 459, in bed_parser. for r in fin.iterate():. File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 102, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Unknown: BED record has invalid number of fields. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna --reads Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam --examples Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/make_examples.tfrecord@1.gz --gvcf Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/gvcf.tfrecord@1.gz --regions Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed --task 0. real	0m3.367s. user	0m2.683s. sys	0m0.545s. ```. **First lines:**. **First 10 lines of sorted marked duplicate bam is: ** . BAM?P@HD	VN:1.6	SO:coordinate. @SQ	SN:NC_000001.11	LN:248956422. @SQ	SN:NT_187361.1	LN:175055. @SQ	SN:NT_187362.1	LN:32032. @SQ	SN:NT_187363.1	LN:127682. @SQ	SN:NT_187364.1	LN:66860. @SQ	SN:NT_187365.1	LN:40176. @SQ	SN:NT_187366.1	LN:42210. @SQ	SN:NT_187367.1	LN:176043. @SQ	SN:NT_187368.1	LN:40745. **First line of reference hg38 is:**. >NC_000001.11 Homo sapiens chromosome 1, GRCh38.p13 Primary Assembly. **First line of bed file is:**. NC_000001.11 65509 65625. I have got deepvariant and the above code to work for another dataset with a different bed file used - but I'm not sure why the ValueError: Unknown: BED record has invalid number of fields error is occurring.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:636,performance,time,time,636,"alueError: Unknown: BED record has invalid number of fields; Hello, . Operatin system: Linux HPC . Version: 1.3.0 . Installation: Singularity . Data: WES - with Agilent SureSelect DNA Human All ExonV5_hg38 bed file. **Steps to reproduce:**. **Command**. ```. `#!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p compute. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-23. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=Polyposis_Exome_Analysis_JOB27/fastp/All_fastp_input/IDswithoutR1R2_JOB27. HG38_REFERENCE=Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna. PICARDMARKDUPLICATES_SORTEDBAM=Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam. BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed. OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz. OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz. INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. *",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:669,performance,cpu,cpu,669,"alueError: Unknown: BED record has invalid number of fields; Hello, . Operatin system: Linux HPC . Version: 1.3.0 . Installation: Singularity . Data: WES - with Agilent SureSelect DNA Human All ExonV5_hg38 bed file. **Steps to reproduce:**. **Command**. ```. `#!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p compute. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-23. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=Polyposis_Exome_Analysis_JOB27/fastp/All_fastp_input/IDswithoutR1R2_JOB27. HG38_REFERENCE=Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna. PICARDMARKDUPLICATES_SORTEDBAM=Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam. BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed. OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz. OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz. INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. *",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:701,performance,load,load,701,"alueError: Unknown: BED record has invalid number of fields; Hello, . Operatin system: Linux HPC . Version: 1.3.0 . Installation: Singularity . Data: WES - with Agilent SureSelect DNA Human All ExonV5_hg38 bed file. **Steps to reproduce:**. **Command**. ```. `#!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p compute. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-23. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=Polyposis_Exome_Analysis_JOB27/fastp/All_fastp_input/IDswithoutR1R2_JOB27. HG38_REFERENCE=Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna. PICARDMARKDUPLICATES_SORTEDBAM=Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam. BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed. OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz. OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz. INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. *",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:706,performance,parallel,parallel,706,"alueError: Unknown: BED record has invalid number of fields; Hello, . Operatin system: Linux HPC . Version: 1.3.0 . Installation: Singularity . Data: WES - with Agilent SureSelect DNA Human All ExonV5_hg38 bed file. **Steps to reproduce:**. **Command**. ```. `#!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p compute. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-23. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=Polyposis_Exome_Analysis_JOB27/fastp/All_fastp_input/IDswithoutR1R2_JOB27. HG38_REFERENCE=Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna. PICARDMARKDUPLICATES_SORTEDBAM=Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam. BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed. OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz. OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz. INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. *",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:723,performance,load,load,723,"alueError: Unknown: BED record has invalid number of fields; Hello, . Operatin system: Linux HPC . Version: 1.3.0 . Installation: Singularity . Data: WES - with Agilent SureSelect DNA Human All ExonV5_hg38 bed file. **Steps to reproduce:**. **Command**. ```. `#!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p compute. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-23. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=Polyposis_Exome_Analysis_JOB27/fastp/All_fastp_input/IDswithoutR1R2_JOB27. HG38_REFERENCE=Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna. PICARDMARKDUPLICATES_SORTEDBAM=Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam. BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed. OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz. OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz. INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. *",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:1490,performance,error,error,1490,"# Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-23. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=Polyposis_Exome_Analysis_JOB27/fastp/All_fastp_input/IDswithoutR1R2_JOB27. HG38_REFERENCE=Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna. PICARDMARKDUPLICATES_SORTEDBAM=Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam. BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed. OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz. OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz. INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. ***** Intermediate results will be written to Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate in docker. ****. ***** Running the command:*****. ```. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna"" --reads ""Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:1522,performance,error,error,1522," FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-23. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=Polyposis_Exome_Analysis_JOB27/fastp/All_fastp_input/IDswithoutR1R2_JOB27. HG38_REFERENCE=Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna. PICARDMARKDUPLICATES_SORTEDBAM=Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam. BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed. OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz. OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz. INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. ***** Intermediate results will be written to Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate in docker. ****. ***** Running the command:*****. ```. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna"" --reads ""Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:1615,performance,parallel,parallel,1615,"=1-23. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=Polyposis_Exome_Analysis_JOB27/fastp/All_fastp_input/IDswithoutR1R2_JOB27. HG38_REFERENCE=Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna. PICARDMARKDUPLICATES_SORTEDBAM=Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam. BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed. OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz. OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz. INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. ***** Intermediate results will be written to Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate in docker. ****. ***** Running the command:*****. ```. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna"" --reads ""Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam"" --examples ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_ou",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:1983,performance,Error,Error,1983,"lyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam. BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed. OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz. OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz. INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. ***** Intermediate results will be written to Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate in docker. ****. ***** Running the command:*****. ```. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna"" --reads ""Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam"" --examples ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/make_examples.tfrecord@1.gz"" --gvcf ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/gvcf.tfrecord@1.gz"" --regions ""Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed"" --task {}. perl: warning: Setting locale failed. perl: warning: ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:2198,performance,time,time,2198,"ocol4.bed. OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz. OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz. INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. ***** Intermediate results will be written to Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate in docker. ****. ***** Running the command:*****. ```. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna"" --reads ""Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam"" --examples ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/make_examples.tfrecord@1.gz"" --gvcf ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/gvcf.tfrecord@1.gz"" --regions ""Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:2213,performance,parallel,parallel,2213,"_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz. OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz. INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. ***** Intermediate results will be written to Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate in docker. ****. ***** Running the command:*****. ```. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna"" --reads ""Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam"" --examples ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/make_examples.tfrecord@1.gz"" --gvcf ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/gvcf.tfrecord@1.gz"" --regions ""Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:17096,performance,parallel,parallel,17096,"il/ranges.py"", line 161, in from_regions. return cls(ranges=from_regions(regions, contig_map=contig_map)). File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 113, in __init__. for i, range_ in enumerate(ranges):. File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 493, in from_regions. for elt in reader(region):. File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 459, in bed_parser. for r in fin.iterate():. File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 102, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Unknown: BED record has invalid number of fields. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna --reads Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam --examples Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/make_examples.tfrecord@1.gz --gvcf Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/gvcf.tfrecord@1.gz --regions Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed --task 0. real	0m3.367s. user	0m2.683s. sys	0m0.545s. ```. **First lines:**. **First 10 lines of sorted marked duplicate bam is: ** . BAM?P@HD	VN:1.6	SO:coordinate. @SQ	SN:NC_000001.11	LN:248956422. @SQ	SN:NT_187361.1	LN:175055. @SQ	SN:NT_187362.1	LN:32032. @SQ	SN:NT_187363.1	LN:127682. @SQ	SN:NT_187364.1	LN:66",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:18577,performance,error,error,18577,"cleus/util/ranges.py"", line 459, in bed_parser. for r in fin.iterate():. File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 102, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Unknown: BED record has invalid number of fields. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna --reads Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam --examples Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/make_examples.tfrecord@1.gz --gvcf Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/gvcf.tfrecord@1.gz --regions Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed --task 0. real	0m3.367s. user	0m2.683s. sys	0m0.545s. ```. **First lines:**. **First 10 lines of sorted marked duplicate bam is: ** . BAM?P@HD	VN:1.6	SO:coordinate. @SQ	SN:NC_000001.11	LN:248956422. @SQ	SN:NT_187361.1	LN:175055. @SQ	SN:NT_187362.1	LN:32032. @SQ	SN:NT_187363.1	LN:127682. @SQ	SN:NT_187364.1	LN:66860. @SQ	SN:NT_187365.1	LN:40176. @SQ	SN:NT_187366.1	LN:42210. @SQ	SN:NT_187367.1	LN:176043. @SQ	SN:NT_187368.1	LN:40745. **First line of reference hg38 is:**. >NC_000001.11 Homo sapiens chromosome 1, GRCh38.p13 Primary Assembly. **First line of bed file is:**. NC_000001.11 65509 65625. I have got deepvariant and the above code to work for another dataset with a different bed file used - but I'm not sure why the ValueError: Unknown: BED record has invalid number of fields error is occurring. . Thanks! Amy.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:526,reliability,FAIL,FAIL,526,"alueError: Unknown: BED record has invalid number of fields; Hello, . Operatin system: Linux HPC . Version: 1.3.0 . Installation: Singularity . Data: WES - with Agilent SureSelect DNA Human All ExonV5_hg38 bed file. **Steps to reproduce:**. **Command**. ```. `#!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p compute. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-23. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=Polyposis_Exome_Analysis_JOB27/fastp/All_fastp_input/IDswithoutR1R2_JOB27. HG38_REFERENCE=Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna. PICARDMARKDUPLICATES_SORTEDBAM=Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam. BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed. OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz. OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz. INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. *",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:2963,reliability,fail,failed,2963,"RESULTS"". ```. **Error trace:**. ***** Intermediate results will be written to Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate in docker. ****. ***** Running the command:*****. ```. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna"" --reads ""Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam"" --examples ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/make_examples.tfrecord@1.gz"" --gvcf ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/gvcf.tfrecord@1.gz"" --regions ""Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). I0614 20:20:45.812468 47288495204160 genomics_reader.py:222] Reading Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam with NativeSamReader. W0614 20:20:45.812704 47288495204160 make_examples_core.py:276] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. I0614 20:20:45.822298 47288495204160 make_examples_core.py:239] Preparing inputs. I",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:3224,reliability,fail,failed,3224,"halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna"" --reads ""Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam"" --examples ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/make_examples.tfrecord@1.gz"" --gvcf ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/gvcf.tfrecord@1.gz"" --regions ""Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). I0614 20:20:45.812468 47288495204160 genomics_reader.py:222] Reading Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam with NativeSamReader. W0614 20:20:45.812704 47288495204160 make_examples_core.py:276] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. I0614 20:20:45.822298 47288495204160 make_examples_core.py:239] Preparing inputs. I0614 20:20:45.836667 47288495204160 genomics_reader.py:222] Reading Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam with NativeSamReader. I0614 20:20:46.057183 47288495204160 make_examples_core.py:239] Co",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:17115,reliability,fail,failed,17115,"e 161, in from_regions. return cls(ranges=from_regions(regions, contig_map=contig_map)). File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 113, in __init__. for i, range_ in enumerate(ranges):. File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 493, in from_regions. for elt in reader(region):. File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 459, in bed_parser. for r in fin.iterate():. File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 102, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Unknown: BED record has invalid number of fields. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna --reads Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam --examples Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/make_examples.tfrecord@1.gz --gvcf Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/gvcf.tfrecord@1.gz --regions Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed --task 0. real	0m3.367s. user	0m2.683s. sys	0m0.545s. ```. **First lines:**. **First 10 lines of sorted marked duplicate bam is: ** . BAM?P@HD	VN:1.6	SO:coordinate. @SQ	SN:NC_000001.11	LN:248956422. @SQ	SN:NT_187361.1	LN:175055. @SQ	SN:NT_187362.1	LN:32032. @SQ	SN:NT_187363.1	LN:127682. @SQ	SN:NT_187364.1	LN:66860. @SQ	SN:NT_187",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:274,safety,log,login,274,"alueError: Unknown: BED record has invalid number of fields; Hello, . Operatin system: Linux HPC . Version: 1.3.0 . Installation: Singularity . Data: WES - with Agilent SureSelect DNA Human All ExonV5_hg38 bed file. **Steps to reproduce:**. **Command**. ```. `#!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p compute. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-23. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=Polyposis_Exome_Analysis_JOB27/fastp/All_fastp_input/IDswithoutR1R2_JOB27. HG38_REFERENCE=Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna. PICARDMARKDUPLICATES_SORTEDBAM=Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam. BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed. OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz. OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz. INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. *",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:680,safety,modul,module,680,"alueError: Unknown: BED record has invalid number of fields; Hello, . Operatin system: Linux HPC . Version: 1.3.0 . Installation: Singularity . Data: WES - with Agilent SureSelect DNA Human All ExonV5_hg38 bed file. **Steps to reproduce:**. **Command**. ```. `#!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p compute. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-23. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=Polyposis_Exome_Analysis_JOB27/fastp/All_fastp_input/IDswithoutR1R2_JOB27. HG38_REFERENCE=Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna. PICARDMARKDUPLICATES_SORTEDBAM=Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam. BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed. OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz. OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz. INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. *",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:694,safety,modul,module,694,"alueError: Unknown: BED record has invalid number of fields; Hello, . Operatin system: Linux HPC . Version: 1.3.0 . Installation: Singularity . Data: WES - with Agilent SureSelect DNA Human All ExonV5_hg38 bed file. **Steps to reproduce:**. **Command**. ```. `#!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p compute. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-23. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=Polyposis_Exome_Analysis_JOB27/fastp/All_fastp_input/IDswithoutR1R2_JOB27. HG38_REFERENCE=Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna. PICARDMARKDUPLICATES_SORTEDBAM=Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam. BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed. OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz. OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz. INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. *",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:716,safety,modul,module,716,"alueError: Unknown: BED record has invalid number of fields; Hello, . Operatin system: Linux HPC . Version: 1.3.0 . Installation: Singularity . Data: WES - with Agilent SureSelect DNA Human All ExonV5_hg38 bed file. **Steps to reproduce:**. **Command**. ```. `#!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p compute. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-23. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=Polyposis_Exome_Analysis_JOB27/fastp/All_fastp_input/IDswithoutR1R2_JOB27. HG38_REFERENCE=Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna. PICARDMARKDUPLICATES_SORTEDBAM=Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam. BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed. OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz. OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz. INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. *",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:1490,safety,error,error,1490,"# Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-23. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=Polyposis_Exome_Analysis_JOB27/fastp/All_fastp_input/IDswithoutR1R2_JOB27. HG38_REFERENCE=Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna. PICARDMARKDUPLICATES_SORTEDBAM=Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam. BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed. OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz. OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz. INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. ***** Intermediate results will be written to Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate in docker. ****. ***** Running the command:*****. ```. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna"" --reads ""Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:1522,safety,error,error,1522," FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-23. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=Polyposis_Exome_Analysis_JOB27/fastp/All_fastp_input/IDswithoutR1R2_JOB27. HG38_REFERENCE=Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna. PICARDMARKDUPLICATES_SORTEDBAM=Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam. BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed. OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz. OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz. INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. ***** Intermediate results will be written to Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate in docker. ****. ***** Running the command:*****. ```. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna"" --reads ""Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:1983,safety,Error,Error,1983,"lyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam. BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed. OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz. OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz. INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. ***** Intermediate results will be written to Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate in docker. ****. ***** Running the command:*****. ```. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna"" --reads ""Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam"" --examples ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/make_examples.tfrecord@1.gz"" --gvcf ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/gvcf.tfrecord@1.gz"" --regions ""Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed"" --task {}. perl: warning: Setting locale failed. perl: warning: ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:3753,safety,input,input,3753,"ut_intermediate/gvcf.tfrecord@1.gz"" --regions ""Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). I0614 20:20:45.812468 47288495204160 genomics_reader.py:222] Reading Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam with NativeSamReader. W0614 20:20:45.812704 47288495204160 make_examples_core.py:276] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. I0614 20:20:45.822298 47288495204160 make_examples_core.py:239] Preparing inputs. I0614 20:20:45.836667 47288495204160 genomics_reader.py:222] Reading Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam with NativeSamReader. I0614 20:20:46.057183 47288495204160 make_examples_core.py:239] Common contigs are ['NC_000001.11', 'NT_187361.1', 'NT_187362.1', 'NT_187363.1', 'NT_187364.1', 'NT_187365.1', 'NT_187366.1', 'NT_187367.1', 'NT_187368.1', 'NT_187369.1', 'NC_000002.12', 'NT_187370.1', 'NT_187371.1', 'NC_000003.12', 'NT_167215.1', 'NC_000004.12', 'NT_113793.3', 'NC_000005.10', 'NT_113948.1', 'NC_000006.12', 'NC_000007.14', 'NC_000008.11', 'NC_000009.12', 'NT_187372.1', 'NT_187373.1', 'NT_187374.1', 'NT_187375.1', 'NC_000010.11', 'NC_000011.10', 'NT_187376.1', 'NC_000012.12', 'NC_000013.11', 'NC_000014.9', 'NT",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:3957,safety,input,inputs,3957,"le failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). I0614 20:20:45.812468 47288495204160 genomics_reader.py:222] Reading Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam with NativeSamReader. W0614 20:20:45.812704 47288495204160 make_examples_core.py:276] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. I0614 20:20:45.822298 47288495204160 make_examples_core.py:239] Preparing inputs. I0614 20:20:45.836667 47288495204160 genomics_reader.py:222] Reading Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam with NativeSamReader. I0614 20:20:46.057183 47288495204160 make_examples_core.py:239] Common contigs are ['NC_000001.11', 'NT_187361.1', 'NT_187362.1', 'NT_187363.1', 'NT_187364.1', 'NT_187365.1', 'NT_187366.1', 'NT_187367.1', 'NT_187368.1', 'NT_187369.1', 'NC_000002.12', 'NT_187370.1', 'NT_187371.1', 'NC_000003.12', 'NT_167215.1', 'NC_000004.12', 'NT_113793.3', 'NC_000005.10', 'NT_113948.1', 'NC_000006.12', 'NC_000007.14', 'NC_000008.11', 'NC_000009.12', 'NT_187372.1', 'NT_187373.1', 'NT_187374.1', 'NT_187375.1', 'NC_000010.11', 'NC_000011.10', 'NT_187376.1', 'NC_000012.12', 'NC_000013.11', 'NC_000014.9', 'NT_113796.3', 'NT_167219.1', 'NT_187377.1', 'NT_113888.1', 'NT_187378.1', 'NT_187379.1', 'NT_187380.1', 'NT_187381.1', 'NC_000015.10', 'NT_187382.1', 'NC_000016.10', 'NT_187383.1', 'NC_000017.11', 'NT_11393",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:14978,safety,modul,module,14978,"7688.1', 'NT_167246.2', 'NW_003571057.2', 'NT_187689.1', 'NT_167247.2', 'NW_003571058.2', 'NT_187690.1', 'NT_167248.2', 'NW_003571059.2', 'NT_187691.1', 'NT_167249.2', 'NW_003571060.1', 'NT_187692.1', 'NW_003571061.2', 'NT_187693.1', 'NT_187636.1', 'NT_187637.1', 'NT_187638.1', 'NT_187639.1', 'NT_187640.1', 'NT_187641.1', 'NT_187642.1', 'NT_187643.1', 'NT_187644.1', 'NT_187645.1', 'NT_187668.1', 'NT_187669.1', 'NT_187670.1', 'NT_187671.1', 'NT_187672.1', 'NT_187673.1', 'NT_187674.1', 'NT_187675.1', 'NT_187676.1', 'NT_187677.1', 'NT_187683.1', 'NT_187684.1', 'NT_187685.1', 'NT_187686.1', 'NT_187687.1', 'NT_113949.2', 'NC_012920.1']. I0614 20:20:46.158432 47288495204160 genomics_reader.py:222] Reading Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed with NativeBedReader. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 170, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1626, in make_examples_runner. regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1466, in processing_regions_from_options. calling_regions = build_calling_regions(ref_contigs, options.calling_regions,. File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 468, in build_calling_regions. ranges.RangeSet.from_regions(regio",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:18577,safety,error,error,18577,"cleus/util/ranges.py"", line 459, in bed_parser. for r in fin.iterate():. File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 102, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Unknown: BED record has invalid number of fields. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna --reads Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam --examples Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/make_examples.tfrecord@1.gz --gvcf Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/gvcf.tfrecord@1.gz --regions Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed --task 0. real	0m3.367s. user	0m2.683s. sys	0m0.545s. ```. **First lines:**. **First 10 lines of sorted marked duplicate bam is: ** . BAM?P@HD	VN:1.6	SO:coordinate. @SQ	SN:NC_000001.11	LN:248956422. @SQ	SN:NT_187361.1	LN:175055. @SQ	SN:NT_187362.1	LN:32032. @SQ	SN:NT_187363.1	LN:127682. @SQ	SN:NT_187364.1	LN:66860. @SQ	SN:NT_187365.1	LN:40176. @SQ	SN:NT_187366.1	LN:42210. @SQ	SN:NT_187367.1	LN:176043. @SQ	SN:NT_187368.1	LN:40745. **First line of reference hg38 is:**. >NC_000001.11 Homo sapiens chromosome 1, GRCh38.p13 Primary Assembly. **First line of bed file is:**. NC_000001.11 65509 65625. I have got deepvariant and the above code to work for another dataset with a different bed file used - but I'm not sure why the ValueError: Unknown: BED record has invalid number of fields error is occurring. . Thanks! Amy.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:274,security,log,login,274,"alueError: Unknown: BED record has invalid number of fields; Hello, . Operatin system: Linux HPC . Version: 1.3.0 . Installation: Singularity . Data: WES - with Agilent SureSelect DNA Human All ExonV5_hg38 bed file. **Steps to reproduce:**. **Command**. ```. `#!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p compute. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-23. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=Polyposis_Exome_Analysis_JOB27/fastp/All_fastp_input/IDswithoutR1R2_JOB27. HG38_REFERENCE=Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna. PICARDMARKDUPLICATES_SORTEDBAM=Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam. BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed. OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz. OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz. INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. *",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:274,testability,log,login,274,"alueError: Unknown: BED record has invalid number of fields; Hello, . Operatin system: Linux HPC . Version: 1.3.0 . Installation: Singularity . Data: WES - with Agilent SureSelect DNA Human All ExonV5_hg38 bed file. **Steps to reproduce:**. **Command**. ```. `#!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p compute. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-23. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=Polyposis_Exome_Analysis_JOB27/fastp/All_fastp_input/IDswithoutR1R2_JOB27. HG38_REFERENCE=Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna. PICARDMARKDUPLICATES_SORTEDBAM=Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam. BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed. OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz. OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz. INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. *",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:1989,testability,trace,trace,1989,"s_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam. BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed. OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz. OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz. INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. ***** Intermediate results will be written to Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate in docker. ****. ***** Running the command:*****. ```. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna"" --reads ""Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam"" --examples ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/make_examples.tfrecord@1.gz"" --gvcf ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/gvcf.tfrecord@1.gz"" --regions ""Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed"" --task {}. perl: warning: Setting locale failed. perl: warning: Please",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:14830,testability,Trace,Traceback,14830,"NW_004504305.1', 'NT_187667.1', 'NT_187678.1', 'NT_187679.1', 'NT_167245.2', 'NT_187680.1', 'NT_187681.1', 'NW_003571056.2', 'NT_187682.1', 'NT_187688.1', 'NT_167246.2', 'NW_003571057.2', 'NT_187689.1', 'NT_167247.2', 'NW_003571058.2', 'NT_187690.1', 'NT_167248.2', 'NW_003571059.2', 'NT_187691.1', 'NT_167249.2', 'NW_003571060.1', 'NT_187692.1', 'NW_003571061.2', 'NT_187693.1', 'NT_187636.1', 'NT_187637.1', 'NT_187638.1', 'NT_187639.1', 'NT_187640.1', 'NT_187641.1', 'NT_187642.1', 'NT_187643.1', 'NT_187644.1', 'NT_187645.1', 'NT_187668.1', 'NT_187669.1', 'NT_187670.1', 'NT_187671.1', 'NT_187672.1', 'NT_187673.1', 'NT_187674.1', 'NT_187675.1', 'NT_187676.1', 'NT_187677.1', 'NT_187683.1', 'NT_187684.1', 'NT_187685.1', 'NT_187686.1', 'NT_187687.1', 'NT_113949.2', 'NC_012920.1']. I0614 20:20:46.158432 47288495204160 genomics_reader.py:222] Reading Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed with NativeBedReader. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 170, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1626, in make_examples_runner. regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1466, in processing_regions_from_options. calling_regions = build_calling_regions(ref_contigs, options.calling_regions,. File ""/tmp/Bazel.runfiles_p",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:243,usability,Command,Command,243,"alueError: Unknown: BED record has invalid number of fields; Hello, . Operatin system: Linux HPC . Version: 1.3.0 . Installation: Singularity . Data: WES - with Agilent SureSelect DNA Human All ExonV5_hg38 bed file. **Steps to reproduce:**. **Command**. ```. `#!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p compute. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-23. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=Polyposis_Exome_Analysis_JOB27/fastp/All_fastp_input/IDswithoutR1R2_JOB27. HG38_REFERENCE=Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna. PICARDMARKDUPLICATES_SORTEDBAM=Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam. BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed. OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz. OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz. INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. *",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:553,usability,user,user,553,"alueError: Unknown: BED record has invalid number of fields; Hello, . Operatin system: Linux HPC . Version: 1.3.0 . Installation: Singularity . Data: WES - with Agilent SureSelect DNA Human All ExonV5_hg38 bed file. **Steps to reproduce:**. **Command**. ```. `#!/bin/bash --login. #SBATCH -J AmyHouseman_deepvariant. #SBATCH -o %x.stdout.%J.%N. #SBATCH -e %x.stderr.%J.%N. #SBATCH --ntasks=1. #SBATCH --ntasks-per-node=1. #SBATCH -p compute. #SBATCH --account=scw1581. #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-23. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=Polyposis_Exome_Analysis_JOB27/fastp/All_fastp_input/IDswithoutR1R2_JOB27. HG38_REFERENCE=Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna. PICARDMARKDUPLICATES_SORTEDBAM=Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam. BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed. OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz. OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz. INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. *",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:1490,usability,error,error,1490,"# Mail events (NONE, BEGIN, END, FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-23. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=Polyposis_Exome_Analysis_JOB27/fastp/All_fastp_input/IDswithoutR1R2_JOB27. HG38_REFERENCE=Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna. PICARDMARKDUPLICATES_SORTEDBAM=Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam. BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed. OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz. OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz. INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. ***** Intermediate results will be written to Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate in docker. ****. ***** Running the command:*****. ```. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna"" --reads ""Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:1522,usability,error,error,1522," FAIL, ALL). #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail. #SBATCH --array=1-23. #SBATCH --time=12:00:00. #SBATCH --mem-per-cpu=128GB. module purge. module load parallel. module load singularity. EXOME_IDs_FILE=Polyposis_Exome_Analysis_JOB27/fastp/All_fastp_input/IDswithoutR1R2_JOB27. HG38_REFERENCE=Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna. PICARDMARKDUPLICATES_SORTEDBAM=Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam. BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed. OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz. OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz. INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. ***** Intermediate results will be written to Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate in docker. ****. ***** Running the command:*****. ```. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna"" --reads ""Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:1983,usability,Error,Error,1983,"lyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam. BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed. OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz. OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz. INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. ***** Intermediate results will be written to Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate in docker. ****. ***** Running the command:*****. ```. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna"" --reads ""Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam"" --examples ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/make_examples.tfrecord@1.gz"" --gvcf ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/gvcf.tfrecord@1.gz"" --regions ""Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed"" --task {}. perl: warning: Setting locale failed. perl: warning: ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:2178,usability,command,command,2178,"nV5_hg38_recoded_nocol4.bed. OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz. OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz. INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error. set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. --ref=$HG38_REFERENCE \. --reads=$PICARDMARKDUPLICATES_SORTEDBAM \. --regions=$BED_REGIONS \. --output_vcf=$OUTPUT_VCF \. --output_gvcf=$OUTPUT_GVCF \. --intermediate_results_dir=$INTERMEDIATE_RESULTS"". ```. **Error trace:**. ***** Intermediate results will be written to Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate in docker. ****. ***** Running the command:*****. ```. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna"" --reads ""Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam"" --examples ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/make_examples.tfrecord@1.gz"" --gvcf ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/gvcf.tfrecord@1.gz"" --regions ""Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard lo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:3096,usability,support,supported,3096,"esults/15M11163_L7_PE_output_intermediate in docker. ****. ***** Running the command:*****. ```. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna"" --reads ""Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam"" --examples ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/make_examples.tfrecord@1.gz"" --gvcf ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/gvcf.tfrecord@1.gz"" --regions ""Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). I0614 20:20:45.812468 47288495204160 genomics_reader.py:222] Reading Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam with NativeSamReader. W0614 20:20:45.812704 47288495204160 make_examples_core.py:276] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. I0614 20:20:45.822298 47288495204160 make_examples_core.py:239] Preparing inputs. I0614 20:20:45.836667 47288495204160 genomics_reader.py:222] Reading Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicate",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:3357,usability,support,supported,3357,"_output/samtools_faidx/GRCh38_latest_genomic.fna"" --reads ""Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam"" --examples ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/make_examples.tfrecord@1.gz"" --gvcf ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/gvcf.tfrecord@1.gz"" --regions ""Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). I0614 20:20:45.812468 47288495204160 genomics_reader.py:222] Reading Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam with NativeSamReader. W0614 20:20:45.812704 47288495204160 make_examples_core.py:276] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. I0614 20:20:45.822298 47288495204160 make_examples_core.py:239] Preparing inputs. I0614 20:20:45.836667 47288495204160 genomics_reader.py:222] Reading Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam with NativeSamReader. I0614 20:20:46.057183 47288495204160 make_examples_core.py:239] Common contigs are ['NC_000001.11', 'NT_187361.1', 'NT_187362.1', 'NT_187363.1', 'NT_187364.1', 'NT_187365.1', 'NT_187366.1', 'NT_187367.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:3753,usability,input,input,3753,"ut_intermediate/gvcf.tfrecord@1.gz"" --regions ""Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed"" --task {}. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). I0614 20:20:45.812468 47288495204160 genomics_reader.py:222] Reading Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam with NativeSamReader. W0614 20:20:45.812704 47288495204160 make_examples_core.py:276] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. I0614 20:20:45.822298 47288495204160 make_examples_core.py:239] Preparing inputs. I0614 20:20:45.836667 47288495204160 genomics_reader.py:222] Reading Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam with NativeSamReader. I0614 20:20:46.057183 47288495204160 make_examples_core.py:239] Common contigs are ['NC_000001.11', 'NT_187361.1', 'NT_187362.1', 'NT_187363.1', 'NT_187364.1', 'NT_187365.1', 'NT_187366.1', 'NT_187367.1', 'NT_187368.1', 'NT_187369.1', 'NC_000002.12', 'NT_187370.1', 'NT_187371.1', 'NC_000003.12', 'NT_167215.1', 'NC_000004.12', 'NT_113793.3', 'NC_000005.10', 'NT_113948.1', 'NC_000006.12', 'NC_000007.14', 'NC_000008.11', 'NC_000009.12', 'NT_187372.1', 'NT_187373.1', 'NT_187374.1', 'NT_187375.1', 'NC_000010.11', 'NC_000011.10', 'NT_187376.1', 'NC_000012.12', 'NC_000013.11', 'NC_000014.9', 'NT",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:3957,usability,input,inputs,3957,"le failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LANG = ""en_GB.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). I0614 20:20:45.812468 47288495204160 genomics_reader.py:222] Reading Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam with NativeSamReader. W0614 20:20:45.812704 47288495204160 make_examples_core.py:276] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. I0614 20:20:45.822298 47288495204160 make_examples_core.py:239] Preparing inputs. I0614 20:20:45.836667 47288495204160 genomics_reader.py:222] Reading Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam with NativeSamReader. I0614 20:20:46.057183 47288495204160 make_examples_core.py:239] Common contigs are ['NC_000001.11', 'NT_187361.1', 'NT_187362.1', 'NT_187363.1', 'NT_187364.1', 'NT_187365.1', 'NT_187366.1', 'NT_187367.1', 'NT_187368.1', 'NT_187369.1', 'NC_000002.12', 'NT_187370.1', 'NT_187371.1', 'NC_000003.12', 'NT_167215.1', 'NC_000004.12', 'NT_113793.3', 'NC_000005.10', 'NT_113948.1', 'NC_000006.12', 'NC_000007.14', 'NC_000008.11', 'NC_000009.12', 'NT_187372.1', 'NT_187373.1', 'NT_187374.1', 'NT_187375.1', 'NC_000010.11', 'NC_000011.10', 'NT_187376.1', 'NC_000012.12', 'NC_000013.11', 'NC_000014.9', 'NT_113796.3', 'NT_167219.1', 'NT_187377.1', 'NT_113888.1', 'NT_187378.1', 'NT_187379.1', 'NT_187380.1', 'NT_187381.1', 'NC_000015.10', 'NT_187382.1', 'NC_000016.10', 'NT_187383.1', 'NC_000017.11', 'NT_11393",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:17813,usability,user,user,17813,"cleus/util/ranges.py"", line 459, in bed_parser. for r in fin.iterate():. File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 102, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Unknown: BED record has invalid number of fields. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna --reads Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam --examples Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/make_examples.tfrecord@1.gz --gvcf Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/gvcf.tfrecord@1.gz --regions Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed --task 0. real	0m3.367s. user	0m2.683s. sys	0m0.545s. ```. **First lines:**. **First 10 lines of sorted marked duplicate bam is: ** . BAM?P@HD	VN:1.6	SO:coordinate. @SQ	SN:NC_000001.11	LN:248956422. @SQ	SN:NT_187361.1	LN:175055. @SQ	SN:NT_187362.1	LN:32032. @SQ	SN:NT_187363.1	LN:127682. @SQ	SN:NT_187364.1	LN:66860. @SQ	SN:NT_187365.1	LN:40176. @SQ	SN:NT_187366.1	LN:42210. @SQ	SN:NT_187367.1	LN:176043. @SQ	SN:NT_187368.1	LN:40745. **First line of reference hg38 is:**. >NC_000001.11 Homo sapiens chromosome 1, GRCh38.p13 Primary Assembly. **First line of bed file is:**. NC_000001.11 65509 65625. I have got deepvariant and the above code to work for another dataset with a different bed file used - but I'm not sure why the ValueError: Unknown: BED record has invalid number of fields error is occurring. . Thanks! Amy.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/542:18577,usability,error,error,18577,"cleus/util/ranges.py"", line 459, in bed_parser. for r in fin.iterate():. File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 102, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: Unknown: BED record has invalid number of fields. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna --reads Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam --examples Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/make_examples.tfrecord@1.gz --gvcf Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/gvcf.tfrecord@1.gz --regions Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed --task 0. real	0m3.367s. user	0m2.683s. sys	0m0.545s. ```. **First lines:**. **First 10 lines of sorted marked duplicate bam is: ** . BAM?P@HD	VN:1.6	SO:coordinate. @SQ	SN:NC_000001.11	LN:248956422. @SQ	SN:NT_187361.1	LN:175055. @SQ	SN:NT_187362.1	LN:32032. @SQ	SN:NT_187363.1	LN:127682. @SQ	SN:NT_187364.1	LN:66860. @SQ	SN:NT_187365.1	LN:40176. @SQ	SN:NT_187366.1	LN:42210. @SQ	SN:NT_187367.1	LN:176043. @SQ	SN:NT_187368.1	LN:40745. **First line of reference hg38 is:**. >NC_000001.11 Homo sapiens chromosome 1, GRCh38.p13 Primary Assembly. **First line of bed file is:**. NC_000001.11 65509 65625. I have got deepvariant and the above code to work for another dataset with a different bed file used - but I'm not sure why the ValueError: Unknown: BED record has invalid number of fields error is occurring. . Thanks! Amy.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/542
https://github.com/google/deepvariant/issues/543:0,availability,Error,Error,0,"Error - reference genome files don't exist; Hello, . When running DeepVariant I have a persistent error that the .fa and .fai reference genome files don't exist. I have checked that the given path is correct by displaying the files via copying the path given in the error sheet - the paths are correct and I don't have this problem with the input bam files, . I'm running the program via a script on a Linux Ubuntu server. I'm using singularity v3.5.3, which is pre-installed and loaded as a module. The data is Illumina short read which has been mapped with BWA-Kit. The following is the script I'm using is: . # Load modules needed. . /etc/profile.d/modules.sh. module load xxxxx/singularity/3.5.3. # inputs. reference=$2. bam=$1.final.bam. sampleid=$1. outdir=deepvar. # Create output directories. if [ ! -e deepvar ]; then mkdir deepvar; fi. if [ ! -e deepvar/$sampleid ]; then mkdir deepvar/$sampleid; fi. # Set singularity caches. if [ ! -e ${PWD}/.singularity ]; then mkdir ${PWD}/.singularity; fi. export SINGULARITY_TMPDIR=$PWD/.singularity. export SINGULARITY_CACHEDIR=$PWD/.singularity. # Download the image. if [ ! -e deepvariant.sif ]; then singularity build deepvariant.sif docker://google/deepvariant:latest; fi. # Run Deepvariant. singularity exec -p -B ${TMPDIR} -B ${PWD} deepvariant.sif /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${reference} \. --reads=${bam} \. --output_vcf=deepvar/${sampleid}/${sampleid}.vcf.gz \. --output_gvcf=deepvar/${sampleid}/${sampleid}.g.vcf.gz \. --num_shards=${NSLOTS}. I can run the test data on the command line but have the same problem when I use the above script to run it. I've not been able to find a fix, and have tried fixes suggested for similar issues on this site. . Very appreciative of any suggestion for a solve. . [runDV.sh.o21362497.txt](https://github.com/google/deepvariant/files/8985669/runDV.sh.o21362497.txt).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/543
https://github.com/google/deepvariant/issues/543:98,availability,error,error,98,"Error - reference genome files don't exist; Hello, . When running DeepVariant I have a persistent error that the .fa and .fai reference genome files don't exist. I have checked that the given path is correct by displaying the files via copying the path given in the error sheet - the paths are correct and I don't have this problem with the input bam files, . I'm running the program via a script on a Linux Ubuntu server. I'm using singularity v3.5.3, which is pre-installed and loaded as a module. The data is Illumina short read which has been mapped with BWA-Kit. The following is the script I'm using is: . # Load modules needed. . /etc/profile.d/modules.sh. module load xxxxx/singularity/3.5.3. # inputs. reference=$2. bam=$1.final.bam. sampleid=$1. outdir=deepvar. # Create output directories. if [ ! -e deepvar ]; then mkdir deepvar; fi. if [ ! -e deepvar/$sampleid ]; then mkdir deepvar/$sampleid; fi. # Set singularity caches. if [ ! -e ${PWD}/.singularity ]; then mkdir ${PWD}/.singularity; fi. export SINGULARITY_TMPDIR=$PWD/.singularity. export SINGULARITY_CACHEDIR=$PWD/.singularity. # Download the image. if [ ! -e deepvariant.sif ]; then singularity build deepvariant.sif docker://google/deepvariant:latest; fi. # Run Deepvariant. singularity exec -p -B ${TMPDIR} -B ${PWD} deepvariant.sif /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${reference} \. --reads=${bam} \. --output_vcf=deepvar/${sampleid}/${sampleid}.vcf.gz \. --output_gvcf=deepvar/${sampleid}/${sampleid}.g.vcf.gz \. --num_shards=${NSLOTS}. I can run the test data on the command line but have the same problem when I use the above script to run it. I've not been able to find a fix, and have tried fixes suggested for similar issues on this site. . Very appreciative of any suggestion for a solve. . [runDV.sh.o21362497.txt](https://github.com/google/deepvariant/files/8985669/runDV.sh.o21362497.txt).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/543
https://github.com/google/deepvariant/issues/543:266,availability,error,error,266,"Error - reference genome files don't exist; Hello, . When running DeepVariant I have a persistent error that the .fa and .fai reference genome files don't exist. I have checked that the given path is correct by displaying the files via copying the path given in the error sheet - the paths are correct and I don't have this problem with the input bam files, . I'm running the program via a script on a Linux Ubuntu server. I'm using singularity v3.5.3, which is pre-installed and loaded as a module. The data is Illumina short read which has been mapped with BWA-Kit. The following is the script I'm using is: . # Load modules needed. . /etc/profile.d/modules.sh. module load xxxxx/singularity/3.5.3. # inputs. reference=$2. bam=$1.final.bam. sampleid=$1. outdir=deepvar. # Create output directories. if [ ! -e deepvar ]; then mkdir deepvar; fi. if [ ! -e deepvar/$sampleid ]; then mkdir deepvar/$sampleid; fi. # Set singularity caches. if [ ! -e ${PWD}/.singularity ]; then mkdir ${PWD}/.singularity; fi. export SINGULARITY_TMPDIR=$PWD/.singularity. export SINGULARITY_CACHEDIR=$PWD/.singularity. # Download the image. if [ ! -e deepvariant.sif ]; then singularity build deepvariant.sif docker://google/deepvariant:latest; fi. # Run Deepvariant. singularity exec -p -B ${TMPDIR} -B ${PWD} deepvariant.sif /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${reference} \. --reads=${bam} \. --output_vcf=deepvar/${sampleid}/${sampleid}.vcf.gz \. --output_gvcf=deepvar/${sampleid}/${sampleid}.g.vcf.gz \. --num_shards=${NSLOTS}. I can run the test data on the command line but have the same problem when I use the above script to run it. I've not been able to find a fix, and have tried fixes suggested for similar issues on this site. . Very appreciative of any suggestion for a solve. . [runDV.sh.o21362497.txt](https://github.com/google/deepvariant/files/8985669/runDV.sh.o21362497.txt).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/543
https://github.com/google/deepvariant/issues/543:1100,availability,Down,Download,1100,"Error - reference genome files don't exist; Hello, . When running DeepVariant I have a persistent error that the .fa and .fai reference genome files don't exist. I have checked that the given path is correct by displaying the files via copying the path given in the error sheet - the paths are correct and I don't have this problem with the input bam files, . I'm running the program via a script on a Linux Ubuntu server. I'm using singularity v3.5.3, which is pre-installed and loaded as a module. The data is Illumina short read which has been mapped with BWA-Kit. The following is the script I'm using is: . # Load modules needed. . /etc/profile.d/modules.sh. module load xxxxx/singularity/3.5.3. # inputs. reference=$2. bam=$1.final.bam. sampleid=$1. outdir=deepvar. # Create output directories. if [ ! -e deepvar ]; then mkdir deepvar; fi. if [ ! -e deepvar/$sampleid ]; then mkdir deepvar/$sampleid; fi. # Set singularity caches. if [ ! -e ${PWD}/.singularity ]; then mkdir ${PWD}/.singularity; fi. export SINGULARITY_TMPDIR=$PWD/.singularity. export SINGULARITY_CACHEDIR=$PWD/.singularity. # Download the image. if [ ! -e deepvariant.sif ]; then singularity build deepvariant.sif docker://google/deepvariant:latest; fi. # Run Deepvariant. singularity exec -p -B ${TMPDIR} -B ${PWD} deepvariant.sif /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${reference} \. --reads=${bam} \. --output_vcf=deepvar/${sampleid}/${sampleid}.vcf.gz \. --output_gvcf=deepvar/${sampleid}/${sampleid}.g.vcf.gz \. --num_shards=${NSLOTS}. I can run the test data on the command line but have the same problem when I use the above script to run it. I've not been able to find a fix, and have tried fixes suggested for similar issues on this site. . Very appreciative of any suggestion for a solve. . [runDV.sh.o21362497.txt](https://github.com/google/deepvariant/files/8985669/runDV.sh.o21362497.txt).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/543
https://github.com/google/deepvariant/issues/543:466,deployability,instal,installed,466,"Error - reference genome files don't exist; Hello, . When running DeepVariant I have a persistent error that the .fa and .fai reference genome files don't exist. I have checked that the given path is correct by displaying the files via copying the path given in the error sheet - the paths are correct and I don't have this problem with the input bam files, . I'm running the program via a script on a Linux Ubuntu server. I'm using singularity v3.5.3, which is pre-installed and loaded as a module. The data is Illumina short read which has been mapped with BWA-Kit. The following is the script I'm using is: . # Load modules needed. . /etc/profile.d/modules.sh. module load xxxxx/singularity/3.5.3. # inputs. reference=$2. bam=$1.final.bam. sampleid=$1. outdir=deepvar. # Create output directories. if [ ! -e deepvar ]; then mkdir deepvar; fi. if [ ! -e deepvar/$sampleid ]; then mkdir deepvar/$sampleid; fi. # Set singularity caches. if [ ! -e ${PWD}/.singularity ]; then mkdir ${PWD}/.singularity; fi. export SINGULARITY_TMPDIR=$PWD/.singularity. export SINGULARITY_CACHEDIR=$PWD/.singularity. # Download the image. if [ ! -e deepvariant.sif ]; then singularity build deepvariant.sif docker://google/deepvariant:latest; fi. # Run Deepvariant. singularity exec -p -B ${TMPDIR} -B ${PWD} deepvariant.sif /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${reference} \. --reads=${bam} \. --output_vcf=deepvar/${sampleid}/${sampleid}.vcf.gz \. --output_gvcf=deepvar/${sampleid}/${sampleid}.g.vcf.gz \. --num_shards=${NSLOTS}. I can run the test data on the command line but have the same problem when I use the above script to run it. I've not been able to find a fix, and have tried fixes suggested for similar issues on this site. . Very appreciative of any suggestion for a solve. . [runDV.sh.o21362497.txt](https://github.com/google/deepvariant/files/8985669/runDV.sh.o21362497.txt).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/543
https://github.com/google/deepvariant/issues/543:492,deployability,modul,module,492,"Error - reference genome files don't exist; Hello, . When running DeepVariant I have a persistent error that the .fa and .fai reference genome files don't exist. I have checked that the given path is correct by displaying the files via copying the path given in the error sheet - the paths are correct and I don't have this problem with the input bam files, . I'm running the program via a script on a Linux Ubuntu server. I'm using singularity v3.5.3, which is pre-installed and loaded as a module. The data is Illumina short read which has been mapped with BWA-Kit. The following is the script I'm using is: . # Load modules needed. . /etc/profile.d/modules.sh. module load xxxxx/singularity/3.5.3. # inputs. reference=$2. bam=$1.final.bam. sampleid=$1. outdir=deepvar. # Create output directories. if [ ! -e deepvar ]; then mkdir deepvar; fi. if [ ! -e deepvar/$sampleid ]; then mkdir deepvar/$sampleid; fi. # Set singularity caches. if [ ! -e ${PWD}/.singularity ]; then mkdir ${PWD}/.singularity; fi. export SINGULARITY_TMPDIR=$PWD/.singularity. export SINGULARITY_CACHEDIR=$PWD/.singularity. # Download the image. if [ ! -e deepvariant.sif ]; then singularity build deepvariant.sif docker://google/deepvariant:latest; fi. # Run Deepvariant. singularity exec -p -B ${TMPDIR} -B ${PWD} deepvariant.sif /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${reference} \. --reads=${bam} \. --output_vcf=deepvar/${sampleid}/${sampleid}.vcf.gz \. --output_gvcf=deepvar/${sampleid}/${sampleid}.g.vcf.gz \. --num_shards=${NSLOTS}. I can run the test data on the command line but have the same problem when I use the above script to run it. I've not been able to find a fix, and have tried fixes suggested for similar issues on this site. . Very appreciative of any suggestion for a solve. . [runDV.sh.o21362497.txt](https://github.com/google/deepvariant/files/8985669/runDV.sh.o21362497.txt).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/543
https://github.com/google/deepvariant/issues/543:619,deployability,modul,modules,619,"Error - reference genome files don't exist; Hello, . When running DeepVariant I have a persistent error that the .fa and .fai reference genome files don't exist. I have checked that the given path is correct by displaying the files via copying the path given in the error sheet - the paths are correct and I don't have this problem with the input bam files, . I'm running the program via a script on a Linux Ubuntu server. I'm using singularity v3.5.3, which is pre-installed and loaded as a module. The data is Illumina short read which has been mapped with BWA-Kit. The following is the script I'm using is: . # Load modules needed. . /etc/profile.d/modules.sh. module load xxxxx/singularity/3.5.3. # inputs. reference=$2. bam=$1.final.bam. sampleid=$1. outdir=deepvar. # Create output directories. if [ ! -e deepvar ]; then mkdir deepvar; fi. if [ ! -e deepvar/$sampleid ]; then mkdir deepvar/$sampleid; fi. # Set singularity caches. if [ ! -e ${PWD}/.singularity ]; then mkdir ${PWD}/.singularity; fi. export SINGULARITY_TMPDIR=$PWD/.singularity. export SINGULARITY_CACHEDIR=$PWD/.singularity. # Download the image. if [ ! -e deepvariant.sif ]; then singularity build deepvariant.sif docker://google/deepvariant:latest; fi. # Run Deepvariant. singularity exec -p -B ${TMPDIR} -B ${PWD} deepvariant.sif /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${reference} \. --reads=${bam} \. --output_vcf=deepvar/${sampleid}/${sampleid}.vcf.gz \. --output_gvcf=deepvar/${sampleid}/${sampleid}.g.vcf.gz \. --num_shards=${NSLOTS}. I can run the test data on the command line but have the same problem when I use the above script to run it. I've not been able to find a fix, and have tried fixes suggested for similar issues on this site. . Very appreciative of any suggestion for a solve. . [runDV.sh.o21362497.txt](https://github.com/google/deepvariant/files/8985669/runDV.sh.o21362497.txt).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/543
https://github.com/google/deepvariant/issues/543:652,deployability,modul,modules,652,"Error - reference genome files don't exist; Hello, . When running DeepVariant I have a persistent error that the .fa and .fai reference genome files don't exist. I have checked that the given path is correct by displaying the files via copying the path given in the error sheet - the paths are correct and I don't have this problem with the input bam files, . I'm running the program via a script on a Linux Ubuntu server. I'm using singularity v3.5.3, which is pre-installed and loaded as a module. The data is Illumina short read which has been mapped with BWA-Kit. The following is the script I'm using is: . # Load modules needed. . /etc/profile.d/modules.sh. module load xxxxx/singularity/3.5.3. # inputs. reference=$2. bam=$1.final.bam. sampleid=$1. outdir=deepvar. # Create output directories. if [ ! -e deepvar ]; then mkdir deepvar; fi. if [ ! -e deepvar/$sampleid ]; then mkdir deepvar/$sampleid; fi. # Set singularity caches. if [ ! -e ${PWD}/.singularity ]; then mkdir ${PWD}/.singularity; fi. export SINGULARITY_TMPDIR=$PWD/.singularity. export SINGULARITY_CACHEDIR=$PWD/.singularity. # Download the image. if [ ! -e deepvariant.sif ]; then singularity build deepvariant.sif docker://google/deepvariant:latest; fi. # Run Deepvariant. singularity exec -p -B ${TMPDIR} -B ${PWD} deepvariant.sif /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${reference} \. --reads=${bam} \. --output_vcf=deepvar/${sampleid}/${sampleid}.vcf.gz \. --output_gvcf=deepvar/${sampleid}/${sampleid}.g.vcf.gz \. --num_shards=${NSLOTS}. I can run the test data on the command line but have the same problem when I use the above script to run it. I've not been able to find a fix, and have tried fixes suggested for similar issues on this site. . Very appreciative of any suggestion for a solve. . [runDV.sh.o21362497.txt](https://github.com/google/deepvariant/files/8985669/runDV.sh.o21362497.txt).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/543
https://github.com/google/deepvariant/issues/543:664,deployability,modul,module,664,"Error - reference genome files don't exist; Hello, . When running DeepVariant I have a persistent error that the .fa and .fai reference genome files don't exist. I have checked that the given path is correct by displaying the files via copying the path given in the error sheet - the paths are correct and I don't have this problem with the input bam files, . I'm running the program via a script on a Linux Ubuntu server. I'm using singularity v3.5.3, which is pre-installed and loaded as a module. The data is Illumina short read which has been mapped with BWA-Kit. The following is the script I'm using is: . # Load modules needed. . /etc/profile.d/modules.sh. module load xxxxx/singularity/3.5.3. # inputs. reference=$2. bam=$1.final.bam. sampleid=$1. outdir=deepvar. # Create output directories. if [ ! -e deepvar ]; then mkdir deepvar; fi. if [ ! -e deepvar/$sampleid ]; then mkdir deepvar/$sampleid; fi. # Set singularity caches. if [ ! -e ${PWD}/.singularity ]; then mkdir ${PWD}/.singularity; fi. export SINGULARITY_TMPDIR=$PWD/.singularity. export SINGULARITY_CACHEDIR=$PWD/.singularity. # Download the image. if [ ! -e deepvariant.sif ]; then singularity build deepvariant.sif docker://google/deepvariant:latest; fi. # Run Deepvariant. singularity exec -p -B ${TMPDIR} -B ${PWD} deepvariant.sif /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${reference} \. --reads=${bam} \. --output_vcf=deepvar/${sampleid}/${sampleid}.vcf.gz \. --output_gvcf=deepvar/${sampleid}/${sampleid}.g.vcf.gz \. --num_shards=${NSLOTS}. I can run the test data on the command line but have the same problem when I use the above script to run it. I've not been able to find a fix, and have tried fixes suggested for similar issues on this site. . Very appreciative of any suggestion for a solve. . [runDV.sh.o21362497.txt](https://github.com/google/deepvariant/files/8985669/runDV.sh.o21362497.txt).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/543
https://github.com/google/deepvariant/issues/543:1166,deployability,build,build,1166,"Error - reference genome files don't exist; Hello, . When running DeepVariant I have a persistent error that the .fa and .fai reference genome files don't exist. I have checked that the given path is correct by displaying the files via copying the path given in the error sheet - the paths are correct and I don't have this problem with the input bam files, . I'm running the program via a script on a Linux Ubuntu server. I'm using singularity v3.5.3, which is pre-installed and loaded as a module. The data is Illumina short read which has been mapped with BWA-Kit. The following is the script I'm using is: . # Load modules needed. . /etc/profile.d/modules.sh. module load xxxxx/singularity/3.5.3. # inputs. reference=$2. bam=$1.final.bam. sampleid=$1. outdir=deepvar. # Create output directories. if [ ! -e deepvar ]; then mkdir deepvar; fi. if [ ! -e deepvar/$sampleid ]; then mkdir deepvar/$sampleid; fi. # Set singularity caches. if [ ! -e ${PWD}/.singularity ]; then mkdir ${PWD}/.singularity; fi. export SINGULARITY_TMPDIR=$PWD/.singularity. export SINGULARITY_CACHEDIR=$PWD/.singularity. # Download the image. if [ ! -e deepvariant.sif ]; then singularity build deepvariant.sif docker://google/deepvariant:latest; fi. # Run Deepvariant. singularity exec -p -B ${TMPDIR} -B ${PWD} deepvariant.sif /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${reference} \. --reads=${bam} \. --output_vcf=deepvar/${sampleid}/${sampleid}.vcf.gz \. --output_gvcf=deepvar/${sampleid}/${sampleid}.g.vcf.gz \. --num_shards=${NSLOTS}. I can run the test data on the command line but have the same problem when I use the above script to run it. I've not been able to find a fix, and have tried fixes suggested for similar issues on this site. . Very appreciative of any suggestion for a solve. . [runDV.sh.o21362497.txt](https://github.com/google/deepvariant/files/8985669/runDV.sh.o21362497.txt).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/543
https://github.com/google/deepvariant/issues/543:480,energy efficiency,load,loaded,480,"Error - reference genome files don't exist; Hello, . When running DeepVariant I have a persistent error that the .fa and .fai reference genome files don't exist. I have checked that the given path is correct by displaying the files via copying the path given in the error sheet - the paths are correct and I don't have this problem with the input bam files, . I'm running the program via a script on a Linux Ubuntu server. I'm using singularity v3.5.3, which is pre-installed and loaded as a module. The data is Illumina short read which has been mapped with BWA-Kit. The following is the script I'm using is: . # Load modules needed. . /etc/profile.d/modules.sh. module load xxxxx/singularity/3.5.3. # inputs. reference=$2. bam=$1.final.bam. sampleid=$1. outdir=deepvar. # Create output directories. if [ ! -e deepvar ]; then mkdir deepvar; fi. if [ ! -e deepvar/$sampleid ]; then mkdir deepvar/$sampleid; fi. # Set singularity caches. if [ ! -e ${PWD}/.singularity ]; then mkdir ${PWD}/.singularity; fi. export SINGULARITY_TMPDIR=$PWD/.singularity. export SINGULARITY_CACHEDIR=$PWD/.singularity. # Download the image. if [ ! -e deepvariant.sif ]; then singularity build deepvariant.sif docker://google/deepvariant:latest; fi. # Run Deepvariant. singularity exec -p -B ${TMPDIR} -B ${PWD} deepvariant.sif /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${reference} \. --reads=${bam} \. --output_vcf=deepvar/${sampleid}/${sampleid}.vcf.gz \. --output_gvcf=deepvar/${sampleid}/${sampleid}.g.vcf.gz \. --num_shards=${NSLOTS}. I can run the test data on the command line but have the same problem when I use the above script to run it. I've not been able to find a fix, and have tried fixes suggested for similar issues on this site. . Very appreciative of any suggestion for a solve. . [runDV.sh.o21362497.txt](https://github.com/google/deepvariant/files/8985669/runDV.sh.o21362497.txt).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/543
https://github.com/google/deepvariant/issues/543:614,energy efficiency,Load,Load,614,"Error - reference genome files don't exist; Hello, . When running DeepVariant I have a persistent error that the .fa and .fai reference genome files don't exist. I have checked that the given path is correct by displaying the files via copying the path given in the error sheet - the paths are correct and I don't have this problem with the input bam files, . I'm running the program via a script on a Linux Ubuntu server. I'm using singularity v3.5.3, which is pre-installed and loaded as a module. The data is Illumina short read which has been mapped with BWA-Kit. The following is the script I'm using is: . # Load modules needed. . /etc/profile.d/modules.sh. module load xxxxx/singularity/3.5.3. # inputs. reference=$2. bam=$1.final.bam. sampleid=$1. outdir=deepvar. # Create output directories. if [ ! -e deepvar ]; then mkdir deepvar; fi. if [ ! -e deepvar/$sampleid ]; then mkdir deepvar/$sampleid; fi. # Set singularity caches. if [ ! -e ${PWD}/.singularity ]; then mkdir ${PWD}/.singularity; fi. export SINGULARITY_TMPDIR=$PWD/.singularity. export SINGULARITY_CACHEDIR=$PWD/.singularity. # Download the image. if [ ! -e deepvariant.sif ]; then singularity build deepvariant.sif docker://google/deepvariant:latest; fi. # Run Deepvariant. singularity exec -p -B ${TMPDIR} -B ${PWD} deepvariant.sif /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${reference} \. --reads=${bam} \. --output_vcf=deepvar/${sampleid}/${sampleid}.vcf.gz \. --output_gvcf=deepvar/${sampleid}/${sampleid}.g.vcf.gz \. --num_shards=${NSLOTS}. I can run the test data on the command line but have the same problem when I use the above script to run it. I've not been able to find a fix, and have tried fixes suggested for similar issues on this site. . Very appreciative of any suggestion for a solve. . [runDV.sh.o21362497.txt](https://github.com/google/deepvariant/files/8985669/runDV.sh.o21362497.txt).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/543
https://github.com/google/deepvariant/issues/543:642,energy efficiency,profil,profile,642,"Error - reference genome files don't exist; Hello, . When running DeepVariant I have a persistent error that the .fa and .fai reference genome files don't exist. I have checked that the given path is correct by displaying the files via copying the path given in the error sheet - the paths are correct and I don't have this problem with the input bam files, . I'm running the program via a script on a Linux Ubuntu server. I'm using singularity v3.5.3, which is pre-installed and loaded as a module. The data is Illumina short read which has been mapped with BWA-Kit. The following is the script I'm using is: . # Load modules needed. . /etc/profile.d/modules.sh. module load xxxxx/singularity/3.5.3. # inputs. reference=$2. bam=$1.final.bam. sampleid=$1. outdir=deepvar. # Create output directories. if [ ! -e deepvar ]; then mkdir deepvar; fi. if [ ! -e deepvar/$sampleid ]; then mkdir deepvar/$sampleid; fi. # Set singularity caches. if [ ! -e ${PWD}/.singularity ]; then mkdir ${PWD}/.singularity; fi. export SINGULARITY_TMPDIR=$PWD/.singularity. export SINGULARITY_CACHEDIR=$PWD/.singularity. # Download the image. if [ ! -e deepvariant.sif ]; then singularity build deepvariant.sif docker://google/deepvariant:latest; fi. # Run Deepvariant. singularity exec -p -B ${TMPDIR} -B ${PWD} deepvariant.sif /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${reference} \. --reads=${bam} \. --output_vcf=deepvar/${sampleid}/${sampleid}.vcf.gz \. --output_gvcf=deepvar/${sampleid}/${sampleid}.g.vcf.gz \. --num_shards=${NSLOTS}. I can run the test data on the command line but have the same problem when I use the above script to run it. I've not been able to find a fix, and have tried fixes suggested for similar issues on this site. . Very appreciative of any suggestion for a solve. . [runDV.sh.o21362497.txt](https://github.com/google/deepvariant/files/8985669/runDV.sh.o21362497.txt).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/543
https://github.com/google/deepvariant/issues/543:671,energy efficiency,load,load,671,"Error - reference genome files don't exist; Hello, . When running DeepVariant I have a persistent error that the .fa and .fai reference genome files don't exist. I have checked that the given path is correct by displaying the files via copying the path given in the error sheet - the paths are correct and I don't have this problem with the input bam files, . I'm running the program via a script on a Linux Ubuntu server. I'm using singularity v3.5.3, which is pre-installed and loaded as a module. The data is Illumina short read which has been mapped with BWA-Kit. The following is the script I'm using is: . # Load modules needed. . /etc/profile.d/modules.sh. module load xxxxx/singularity/3.5.3. # inputs. reference=$2. bam=$1.final.bam. sampleid=$1. outdir=deepvar. # Create output directories. if [ ! -e deepvar ]; then mkdir deepvar; fi. if [ ! -e deepvar/$sampleid ]; then mkdir deepvar/$sampleid; fi. # Set singularity caches. if [ ! -e ${PWD}/.singularity ]; then mkdir ${PWD}/.singularity; fi. export SINGULARITY_TMPDIR=$PWD/.singularity. export SINGULARITY_CACHEDIR=$PWD/.singularity. # Download the image. if [ ! -e deepvariant.sif ]; then singularity build deepvariant.sif docker://google/deepvariant:latest; fi. # Run Deepvariant. singularity exec -p -B ${TMPDIR} -B ${PWD} deepvariant.sif /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${reference} \. --reads=${bam} \. --output_vcf=deepvar/${sampleid}/${sampleid}.vcf.gz \. --output_gvcf=deepvar/${sampleid}/${sampleid}.g.vcf.gz \. --num_shards=${NSLOTS}. I can run the test data on the command line but have the same problem when I use the above script to run it. I've not been able to find a fix, and have tried fixes suggested for similar issues on this site. . Very appreciative of any suggestion for a solve. . [runDV.sh.o21362497.txt](https://github.com/google/deepvariant/files/8985669/runDV.sh.o21362497.txt).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/543
https://github.com/google/deepvariant/issues/543:492,modifiability,modul,module,492,"Error - reference genome files don't exist; Hello, . When running DeepVariant I have a persistent error that the .fa and .fai reference genome files don't exist. I have checked that the given path is correct by displaying the files via copying the path given in the error sheet - the paths are correct and I don't have this problem with the input bam files, . I'm running the program via a script on a Linux Ubuntu server. I'm using singularity v3.5.3, which is pre-installed and loaded as a module. The data is Illumina short read which has been mapped with BWA-Kit. The following is the script I'm using is: . # Load modules needed. . /etc/profile.d/modules.sh. module load xxxxx/singularity/3.5.3. # inputs. reference=$2. bam=$1.final.bam. sampleid=$1. outdir=deepvar. # Create output directories. if [ ! -e deepvar ]; then mkdir deepvar; fi. if [ ! -e deepvar/$sampleid ]; then mkdir deepvar/$sampleid; fi. # Set singularity caches. if [ ! -e ${PWD}/.singularity ]; then mkdir ${PWD}/.singularity; fi. export SINGULARITY_TMPDIR=$PWD/.singularity. export SINGULARITY_CACHEDIR=$PWD/.singularity. # Download the image. if [ ! -e deepvariant.sif ]; then singularity build deepvariant.sif docker://google/deepvariant:latest; fi. # Run Deepvariant. singularity exec -p -B ${TMPDIR} -B ${PWD} deepvariant.sif /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${reference} \. --reads=${bam} \. --output_vcf=deepvar/${sampleid}/${sampleid}.vcf.gz \. --output_gvcf=deepvar/${sampleid}/${sampleid}.g.vcf.gz \. --num_shards=${NSLOTS}. I can run the test data on the command line but have the same problem when I use the above script to run it. I've not been able to find a fix, and have tried fixes suggested for similar issues on this site. . Very appreciative of any suggestion for a solve. . [runDV.sh.o21362497.txt](https://github.com/google/deepvariant/files/8985669/runDV.sh.o21362497.txt).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/543
https://github.com/google/deepvariant/issues/543:619,modifiability,modul,modules,619,"Error - reference genome files don't exist; Hello, . When running DeepVariant I have a persistent error that the .fa and .fai reference genome files don't exist. I have checked that the given path is correct by displaying the files via copying the path given in the error sheet - the paths are correct and I don't have this problem with the input bam files, . I'm running the program via a script on a Linux Ubuntu server. I'm using singularity v3.5.3, which is pre-installed and loaded as a module. The data is Illumina short read which has been mapped with BWA-Kit. The following is the script I'm using is: . # Load modules needed. . /etc/profile.d/modules.sh. module load xxxxx/singularity/3.5.3. # inputs. reference=$2. bam=$1.final.bam. sampleid=$1. outdir=deepvar. # Create output directories. if [ ! -e deepvar ]; then mkdir deepvar; fi. if [ ! -e deepvar/$sampleid ]; then mkdir deepvar/$sampleid; fi. # Set singularity caches. if [ ! -e ${PWD}/.singularity ]; then mkdir ${PWD}/.singularity; fi. export SINGULARITY_TMPDIR=$PWD/.singularity. export SINGULARITY_CACHEDIR=$PWD/.singularity. # Download the image. if [ ! -e deepvariant.sif ]; then singularity build deepvariant.sif docker://google/deepvariant:latest; fi. # Run Deepvariant. singularity exec -p -B ${TMPDIR} -B ${PWD} deepvariant.sif /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${reference} \. --reads=${bam} \. --output_vcf=deepvar/${sampleid}/${sampleid}.vcf.gz \. --output_gvcf=deepvar/${sampleid}/${sampleid}.g.vcf.gz \. --num_shards=${NSLOTS}. I can run the test data on the command line but have the same problem when I use the above script to run it. I've not been able to find a fix, and have tried fixes suggested for similar issues on this site. . Very appreciative of any suggestion for a solve. . [runDV.sh.o21362497.txt](https://github.com/google/deepvariant/files/8985669/runDV.sh.o21362497.txt).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/543
https://github.com/google/deepvariant/issues/543:652,modifiability,modul,modules,652,"Error - reference genome files don't exist; Hello, . When running DeepVariant I have a persistent error that the .fa and .fai reference genome files don't exist. I have checked that the given path is correct by displaying the files via copying the path given in the error sheet - the paths are correct and I don't have this problem with the input bam files, . I'm running the program via a script on a Linux Ubuntu server. I'm using singularity v3.5.3, which is pre-installed and loaded as a module. The data is Illumina short read which has been mapped with BWA-Kit. The following is the script I'm using is: . # Load modules needed. . /etc/profile.d/modules.sh. module load xxxxx/singularity/3.5.3. # inputs. reference=$2. bam=$1.final.bam. sampleid=$1. outdir=deepvar. # Create output directories. if [ ! -e deepvar ]; then mkdir deepvar; fi. if [ ! -e deepvar/$sampleid ]; then mkdir deepvar/$sampleid; fi. # Set singularity caches. if [ ! -e ${PWD}/.singularity ]; then mkdir ${PWD}/.singularity; fi. export SINGULARITY_TMPDIR=$PWD/.singularity. export SINGULARITY_CACHEDIR=$PWD/.singularity. # Download the image. if [ ! -e deepvariant.sif ]; then singularity build deepvariant.sif docker://google/deepvariant:latest; fi. # Run Deepvariant. singularity exec -p -B ${TMPDIR} -B ${PWD} deepvariant.sif /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${reference} \. --reads=${bam} \. --output_vcf=deepvar/${sampleid}/${sampleid}.vcf.gz \. --output_gvcf=deepvar/${sampleid}/${sampleid}.g.vcf.gz \. --num_shards=${NSLOTS}. I can run the test data on the command line but have the same problem when I use the above script to run it. I've not been able to find a fix, and have tried fixes suggested for similar issues on this site. . Very appreciative of any suggestion for a solve. . [runDV.sh.o21362497.txt](https://github.com/google/deepvariant/files/8985669/runDV.sh.o21362497.txt).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/543
https://github.com/google/deepvariant/issues/543:664,modifiability,modul,module,664,"Error - reference genome files don't exist; Hello, . When running DeepVariant I have a persistent error that the .fa and .fai reference genome files don't exist. I have checked that the given path is correct by displaying the files via copying the path given in the error sheet - the paths are correct and I don't have this problem with the input bam files, . I'm running the program via a script on a Linux Ubuntu server. I'm using singularity v3.5.3, which is pre-installed and loaded as a module. The data is Illumina short read which has been mapped with BWA-Kit. The following is the script I'm using is: . # Load modules needed. . /etc/profile.d/modules.sh. module load xxxxx/singularity/3.5.3. # inputs. reference=$2. bam=$1.final.bam. sampleid=$1. outdir=deepvar. # Create output directories. if [ ! -e deepvar ]; then mkdir deepvar; fi. if [ ! -e deepvar/$sampleid ]; then mkdir deepvar/$sampleid; fi. # Set singularity caches. if [ ! -e ${PWD}/.singularity ]; then mkdir ${PWD}/.singularity; fi. export SINGULARITY_TMPDIR=$PWD/.singularity. export SINGULARITY_CACHEDIR=$PWD/.singularity. # Download the image. if [ ! -e deepvariant.sif ]; then singularity build deepvariant.sif docker://google/deepvariant:latest; fi. # Run Deepvariant. singularity exec -p -B ${TMPDIR} -B ${PWD} deepvariant.sif /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${reference} \. --reads=${bam} \. --output_vcf=deepvar/${sampleid}/${sampleid}.vcf.gz \. --output_gvcf=deepvar/${sampleid}/${sampleid}.g.vcf.gz \. --num_shards=${NSLOTS}. I can run the test data on the command line but have the same problem when I use the above script to run it. I've not been able to find a fix, and have tried fixes suggested for similar issues on this site. . Very appreciative of any suggestion for a solve. . [runDV.sh.o21362497.txt](https://github.com/google/deepvariant/files/8985669/runDV.sh.o21362497.txt).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/543
https://github.com/google/deepvariant/issues/543:0,performance,Error,Error,0,"Error - reference genome files don't exist; Hello, . When running DeepVariant I have a persistent error that the .fa and .fai reference genome files don't exist. I have checked that the given path is correct by displaying the files via copying the path given in the error sheet - the paths are correct and I don't have this problem with the input bam files, . I'm running the program via a script on a Linux Ubuntu server. I'm using singularity v3.5.3, which is pre-installed and loaded as a module. The data is Illumina short read which has been mapped with BWA-Kit. The following is the script I'm using is: . # Load modules needed. . /etc/profile.d/modules.sh. module load xxxxx/singularity/3.5.3. # inputs. reference=$2. bam=$1.final.bam. sampleid=$1. outdir=deepvar. # Create output directories. if [ ! -e deepvar ]; then mkdir deepvar; fi. if [ ! -e deepvar/$sampleid ]; then mkdir deepvar/$sampleid; fi. # Set singularity caches. if [ ! -e ${PWD}/.singularity ]; then mkdir ${PWD}/.singularity; fi. export SINGULARITY_TMPDIR=$PWD/.singularity. export SINGULARITY_CACHEDIR=$PWD/.singularity. # Download the image. if [ ! -e deepvariant.sif ]; then singularity build deepvariant.sif docker://google/deepvariant:latest; fi. # Run Deepvariant. singularity exec -p -B ${TMPDIR} -B ${PWD} deepvariant.sif /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${reference} \. --reads=${bam} \. --output_vcf=deepvar/${sampleid}/${sampleid}.vcf.gz \. --output_gvcf=deepvar/${sampleid}/${sampleid}.g.vcf.gz \. --num_shards=${NSLOTS}. I can run the test data on the command line but have the same problem when I use the above script to run it. I've not been able to find a fix, and have tried fixes suggested for similar issues on this site. . Very appreciative of any suggestion for a solve. . [runDV.sh.o21362497.txt](https://github.com/google/deepvariant/files/8985669/runDV.sh.o21362497.txt).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/543
https://github.com/google/deepvariant/issues/543:98,performance,error,error,98,"Error - reference genome files don't exist; Hello, . When running DeepVariant I have a persistent error that the .fa and .fai reference genome files don't exist. I have checked that the given path is correct by displaying the files via copying the path given in the error sheet - the paths are correct and I don't have this problem with the input bam files, . I'm running the program via a script on a Linux Ubuntu server. I'm using singularity v3.5.3, which is pre-installed and loaded as a module. The data is Illumina short read which has been mapped with BWA-Kit. The following is the script I'm using is: . # Load modules needed. . /etc/profile.d/modules.sh. module load xxxxx/singularity/3.5.3. # inputs. reference=$2. bam=$1.final.bam. sampleid=$1. outdir=deepvar. # Create output directories. if [ ! -e deepvar ]; then mkdir deepvar; fi. if [ ! -e deepvar/$sampleid ]; then mkdir deepvar/$sampleid; fi. # Set singularity caches. if [ ! -e ${PWD}/.singularity ]; then mkdir ${PWD}/.singularity; fi. export SINGULARITY_TMPDIR=$PWD/.singularity. export SINGULARITY_CACHEDIR=$PWD/.singularity. # Download the image. if [ ! -e deepvariant.sif ]; then singularity build deepvariant.sif docker://google/deepvariant:latest; fi. # Run Deepvariant. singularity exec -p -B ${TMPDIR} -B ${PWD} deepvariant.sif /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${reference} \. --reads=${bam} \. --output_vcf=deepvar/${sampleid}/${sampleid}.vcf.gz \. --output_gvcf=deepvar/${sampleid}/${sampleid}.g.vcf.gz \. --num_shards=${NSLOTS}. I can run the test data on the command line but have the same problem when I use the above script to run it. I've not been able to find a fix, and have tried fixes suggested for similar issues on this site. . Very appreciative of any suggestion for a solve. . [runDV.sh.o21362497.txt](https://github.com/google/deepvariant/files/8985669/runDV.sh.o21362497.txt).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/543
https://github.com/google/deepvariant/issues/543:266,performance,error,error,266,"Error - reference genome files don't exist; Hello, . When running DeepVariant I have a persistent error that the .fa and .fai reference genome files don't exist. I have checked that the given path is correct by displaying the files via copying the path given in the error sheet - the paths are correct and I don't have this problem with the input bam files, . I'm running the program via a script on a Linux Ubuntu server. I'm using singularity v3.5.3, which is pre-installed and loaded as a module. The data is Illumina short read which has been mapped with BWA-Kit. The following is the script I'm using is: . # Load modules needed. . /etc/profile.d/modules.sh. module load xxxxx/singularity/3.5.3. # inputs. reference=$2. bam=$1.final.bam. sampleid=$1. outdir=deepvar. # Create output directories. if [ ! -e deepvar ]; then mkdir deepvar; fi. if [ ! -e deepvar/$sampleid ]; then mkdir deepvar/$sampleid; fi. # Set singularity caches. if [ ! -e ${PWD}/.singularity ]; then mkdir ${PWD}/.singularity; fi. export SINGULARITY_TMPDIR=$PWD/.singularity. export SINGULARITY_CACHEDIR=$PWD/.singularity. # Download the image. if [ ! -e deepvariant.sif ]; then singularity build deepvariant.sif docker://google/deepvariant:latest; fi. # Run Deepvariant. singularity exec -p -B ${TMPDIR} -B ${PWD} deepvariant.sif /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${reference} \. --reads=${bam} \. --output_vcf=deepvar/${sampleid}/${sampleid}.vcf.gz \. --output_gvcf=deepvar/${sampleid}/${sampleid}.g.vcf.gz \. --num_shards=${NSLOTS}. I can run the test data on the command line but have the same problem when I use the above script to run it. I've not been able to find a fix, and have tried fixes suggested for similar issues on this site. . Very appreciative of any suggestion for a solve. . [runDV.sh.o21362497.txt](https://github.com/google/deepvariant/files/8985669/runDV.sh.o21362497.txt).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/543
https://github.com/google/deepvariant/issues/543:480,performance,load,loaded,480,"Error - reference genome files don't exist; Hello, . When running DeepVariant I have a persistent error that the .fa and .fai reference genome files don't exist. I have checked that the given path is correct by displaying the files via copying the path given in the error sheet - the paths are correct and I don't have this problem with the input bam files, . I'm running the program via a script on a Linux Ubuntu server. I'm using singularity v3.5.3, which is pre-installed and loaded as a module. The data is Illumina short read which has been mapped with BWA-Kit. The following is the script I'm using is: . # Load modules needed. . /etc/profile.d/modules.sh. module load xxxxx/singularity/3.5.3. # inputs. reference=$2. bam=$1.final.bam. sampleid=$1. outdir=deepvar. # Create output directories. if [ ! -e deepvar ]; then mkdir deepvar; fi. if [ ! -e deepvar/$sampleid ]; then mkdir deepvar/$sampleid; fi. # Set singularity caches. if [ ! -e ${PWD}/.singularity ]; then mkdir ${PWD}/.singularity; fi. export SINGULARITY_TMPDIR=$PWD/.singularity. export SINGULARITY_CACHEDIR=$PWD/.singularity. # Download the image. if [ ! -e deepvariant.sif ]; then singularity build deepvariant.sif docker://google/deepvariant:latest; fi. # Run Deepvariant. singularity exec -p -B ${TMPDIR} -B ${PWD} deepvariant.sif /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${reference} \. --reads=${bam} \. --output_vcf=deepvar/${sampleid}/${sampleid}.vcf.gz \. --output_gvcf=deepvar/${sampleid}/${sampleid}.g.vcf.gz \. --num_shards=${NSLOTS}. I can run the test data on the command line but have the same problem when I use the above script to run it. I've not been able to find a fix, and have tried fixes suggested for similar issues on this site. . Very appreciative of any suggestion for a solve. . [runDV.sh.o21362497.txt](https://github.com/google/deepvariant/files/8985669/runDV.sh.o21362497.txt).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/543
https://github.com/google/deepvariant/issues/543:614,performance,Load,Load,614,"Error - reference genome files don't exist; Hello, . When running DeepVariant I have a persistent error that the .fa and .fai reference genome files don't exist. I have checked that the given path is correct by displaying the files via copying the path given in the error sheet - the paths are correct and I don't have this problem with the input bam files, . I'm running the program via a script on a Linux Ubuntu server. I'm using singularity v3.5.3, which is pre-installed and loaded as a module. The data is Illumina short read which has been mapped with BWA-Kit. The following is the script I'm using is: . # Load modules needed. . /etc/profile.d/modules.sh. module load xxxxx/singularity/3.5.3. # inputs. reference=$2. bam=$1.final.bam. sampleid=$1. outdir=deepvar. # Create output directories. if [ ! -e deepvar ]; then mkdir deepvar; fi. if [ ! -e deepvar/$sampleid ]; then mkdir deepvar/$sampleid; fi. # Set singularity caches. if [ ! -e ${PWD}/.singularity ]; then mkdir ${PWD}/.singularity; fi. export SINGULARITY_TMPDIR=$PWD/.singularity. export SINGULARITY_CACHEDIR=$PWD/.singularity. # Download the image. if [ ! -e deepvariant.sif ]; then singularity build deepvariant.sif docker://google/deepvariant:latest; fi. # Run Deepvariant. singularity exec -p -B ${TMPDIR} -B ${PWD} deepvariant.sif /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${reference} \. --reads=${bam} \. --output_vcf=deepvar/${sampleid}/${sampleid}.vcf.gz \. --output_gvcf=deepvar/${sampleid}/${sampleid}.g.vcf.gz \. --num_shards=${NSLOTS}. I can run the test data on the command line but have the same problem when I use the above script to run it. I've not been able to find a fix, and have tried fixes suggested for similar issues on this site. . Very appreciative of any suggestion for a solve. . [runDV.sh.o21362497.txt](https://github.com/google/deepvariant/files/8985669/runDV.sh.o21362497.txt).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/543
https://github.com/google/deepvariant/issues/543:642,performance,profil,profile,642,"Error - reference genome files don't exist; Hello, . When running DeepVariant I have a persistent error that the .fa and .fai reference genome files don't exist. I have checked that the given path is correct by displaying the files via copying the path given in the error sheet - the paths are correct and I don't have this problem with the input bam files, . I'm running the program via a script on a Linux Ubuntu server. I'm using singularity v3.5.3, which is pre-installed and loaded as a module. The data is Illumina short read which has been mapped with BWA-Kit. The following is the script I'm using is: . # Load modules needed. . /etc/profile.d/modules.sh. module load xxxxx/singularity/3.5.3. # inputs. reference=$2. bam=$1.final.bam. sampleid=$1. outdir=deepvar. # Create output directories. if [ ! -e deepvar ]; then mkdir deepvar; fi. if [ ! -e deepvar/$sampleid ]; then mkdir deepvar/$sampleid; fi. # Set singularity caches. if [ ! -e ${PWD}/.singularity ]; then mkdir ${PWD}/.singularity; fi. export SINGULARITY_TMPDIR=$PWD/.singularity. export SINGULARITY_CACHEDIR=$PWD/.singularity. # Download the image. if [ ! -e deepvariant.sif ]; then singularity build deepvariant.sif docker://google/deepvariant:latest; fi. # Run Deepvariant. singularity exec -p -B ${TMPDIR} -B ${PWD} deepvariant.sif /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${reference} \. --reads=${bam} \. --output_vcf=deepvar/${sampleid}/${sampleid}.vcf.gz \. --output_gvcf=deepvar/${sampleid}/${sampleid}.g.vcf.gz \. --num_shards=${NSLOTS}. I can run the test data on the command line but have the same problem when I use the above script to run it. I've not been able to find a fix, and have tried fixes suggested for similar issues on this site. . Very appreciative of any suggestion for a solve. . [runDV.sh.o21362497.txt](https://github.com/google/deepvariant/files/8985669/runDV.sh.o21362497.txt).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/543
https://github.com/google/deepvariant/issues/543:671,performance,load,load,671,"Error - reference genome files don't exist; Hello, . When running DeepVariant I have a persistent error that the .fa and .fai reference genome files don't exist. I have checked that the given path is correct by displaying the files via copying the path given in the error sheet - the paths are correct and I don't have this problem with the input bam files, . I'm running the program via a script on a Linux Ubuntu server. I'm using singularity v3.5.3, which is pre-installed and loaded as a module. The data is Illumina short read which has been mapped with BWA-Kit. The following is the script I'm using is: . # Load modules needed. . /etc/profile.d/modules.sh. module load xxxxx/singularity/3.5.3. # inputs. reference=$2. bam=$1.final.bam. sampleid=$1. outdir=deepvar. # Create output directories. if [ ! -e deepvar ]; then mkdir deepvar; fi. if [ ! -e deepvar/$sampleid ]; then mkdir deepvar/$sampleid; fi. # Set singularity caches. if [ ! -e ${PWD}/.singularity ]; then mkdir ${PWD}/.singularity; fi. export SINGULARITY_TMPDIR=$PWD/.singularity. export SINGULARITY_CACHEDIR=$PWD/.singularity. # Download the image. if [ ! -e deepvariant.sif ]; then singularity build deepvariant.sif docker://google/deepvariant:latest; fi. # Run Deepvariant. singularity exec -p -B ${TMPDIR} -B ${PWD} deepvariant.sif /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${reference} \. --reads=${bam} \. --output_vcf=deepvar/${sampleid}/${sampleid}.vcf.gz \. --output_gvcf=deepvar/${sampleid}/${sampleid}.g.vcf.gz \. --num_shards=${NSLOTS}. I can run the test data on the command line but have the same problem when I use the above script to run it. I've not been able to find a fix, and have tried fixes suggested for similar issues on this site. . Very appreciative of any suggestion for a solve. . [runDV.sh.o21362497.txt](https://github.com/google/deepvariant/files/8985669/runDV.sh.o21362497.txt).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/543
https://github.com/google/deepvariant/issues/543:929,performance,cach,caches,929,"Error - reference genome files don't exist; Hello, . When running DeepVariant I have a persistent error that the .fa and .fai reference genome files don't exist. I have checked that the given path is correct by displaying the files via copying the path given in the error sheet - the paths are correct and I don't have this problem with the input bam files, . I'm running the program via a script on a Linux Ubuntu server. I'm using singularity v3.5.3, which is pre-installed and loaded as a module. The data is Illumina short read which has been mapped with BWA-Kit. The following is the script I'm using is: . # Load modules needed. . /etc/profile.d/modules.sh. module load xxxxx/singularity/3.5.3. # inputs. reference=$2. bam=$1.final.bam. sampleid=$1. outdir=deepvar. # Create output directories. if [ ! -e deepvar ]; then mkdir deepvar; fi. if [ ! -e deepvar/$sampleid ]; then mkdir deepvar/$sampleid; fi. # Set singularity caches. if [ ! -e ${PWD}/.singularity ]; then mkdir ${PWD}/.singularity; fi. export SINGULARITY_TMPDIR=$PWD/.singularity. export SINGULARITY_CACHEDIR=$PWD/.singularity. # Download the image. if [ ! -e deepvariant.sif ]; then singularity build deepvariant.sif docker://google/deepvariant:latest; fi. # Run Deepvariant. singularity exec -p -B ${TMPDIR} -B ${PWD} deepvariant.sif /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${reference} \. --reads=${bam} \. --output_vcf=deepvar/${sampleid}/${sampleid}.vcf.gz \. --output_gvcf=deepvar/${sampleid}/${sampleid}.g.vcf.gz \. --num_shards=${NSLOTS}. I can run the test data on the command line but have the same problem when I use the above script to run it. I've not been able to find a fix, and have tried fixes suggested for similar issues on this site. . Very appreciative of any suggestion for a solve. . [runDV.sh.o21362497.txt](https://github.com/google/deepvariant/files/8985669/runDV.sh.o21362497.txt).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/543
https://github.com/google/deepvariant/issues/543:0,safety,Error,Error,0,"Error - reference genome files don't exist; Hello, . When running DeepVariant I have a persistent error that the .fa and .fai reference genome files don't exist. I have checked that the given path is correct by displaying the files via copying the path given in the error sheet - the paths are correct and I don't have this problem with the input bam files, . I'm running the program via a script on a Linux Ubuntu server. I'm using singularity v3.5.3, which is pre-installed and loaded as a module. The data is Illumina short read which has been mapped with BWA-Kit. The following is the script I'm using is: . # Load modules needed. . /etc/profile.d/modules.sh. module load xxxxx/singularity/3.5.3. # inputs. reference=$2. bam=$1.final.bam. sampleid=$1. outdir=deepvar. # Create output directories. if [ ! -e deepvar ]; then mkdir deepvar; fi. if [ ! -e deepvar/$sampleid ]; then mkdir deepvar/$sampleid; fi. # Set singularity caches. if [ ! -e ${PWD}/.singularity ]; then mkdir ${PWD}/.singularity; fi. export SINGULARITY_TMPDIR=$PWD/.singularity. export SINGULARITY_CACHEDIR=$PWD/.singularity. # Download the image. if [ ! -e deepvariant.sif ]; then singularity build deepvariant.sif docker://google/deepvariant:latest; fi. # Run Deepvariant. singularity exec -p -B ${TMPDIR} -B ${PWD} deepvariant.sif /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${reference} \. --reads=${bam} \. --output_vcf=deepvar/${sampleid}/${sampleid}.vcf.gz \. --output_gvcf=deepvar/${sampleid}/${sampleid}.g.vcf.gz \. --num_shards=${NSLOTS}. I can run the test data on the command line but have the same problem when I use the above script to run it. I've not been able to find a fix, and have tried fixes suggested for similar issues on this site. . Very appreciative of any suggestion for a solve. . [runDV.sh.o21362497.txt](https://github.com/google/deepvariant/files/8985669/runDV.sh.o21362497.txt).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/543
https://github.com/google/deepvariant/issues/543:98,safety,error,error,98,"Error - reference genome files don't exist; Hello, . When running DeepVariant I have a persistent error that the .fa and .fai reference genome files don't exist. I have checked that the given path is correct by displaying the files via copying the path given in the error sheet - the paths are correct and I don't have this problem with the input bam files, . I'm running the program via a script on a Linux Ubuntu server. I'm using singularity v3.5.3, which is pre-installed and loaded as a module. The data is Illumina short read which has been mapped with BWA-Kit. The following is the script I'm using is: . # Load modules needed. . /etc/profile.d/modules.sh. module load xxxxx/singularity/3.5.3. # inputs. reference=$2. bam=$1.final.bam. sampleid=$1. outdir=deepvar. # Create output directories. if [ ! -e deepvar ]; then mkdir deepvar; fi. if [ ! -e deepvar/$sampleid ]; then mkdir deepvar/$sampleid; fi. # Set singularity caches. if [ ! -e ${PWD}/.singularity ]; then mkdir ${PWD}/.singularity; fi. export SINGULARITY_TMPDIR=$PWD/.singularity. export SINGULARITY_CACHEDIR=$PWD/.singularity. # Download the image. if [ ! -e deepvariant.sif ]; then singularity build deepvariant.sif docker://google/deepvariant:latest; fi. # Run Deepvariant. singularity exec -p -B ${TMPDIR} -B ${PWD} deepvariant.sif /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${reference} \. --reads=${bam} \. --output_vcf=deepvar/${sampleid}/${sampleid}.vcf.gz \. --output_gvcf=deepvar/${sampleid}/${sampleid}.g.vcf.gz \. --num_shards=${NSLOTS}. I can run the test data on the command line but have the same problem when I use the above script to run it. I've not been able to find a fix, and have tried fixes suggested for similar issues on this site. . Very appreciative of any suggestion for a solve. . [runDV.sh.o21362497.txt](https://github.com/google/deepvariant/files/8985669/runDV.sh.o21362497.txt).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/543
https://github.com/google/deepvariant/issues/543:266,safety,error,error,266,"Error - reference genome files don't exist; Hello, . When running DeepVariant I have a persistent error that the .fa and .fai reference genome files don't exist. I have checked that the given path is correct by displaying the files via copying the path given in the error sheet - the paths are correct and I don't have this problem with the input bam files, . I'm running the program via a script on a Linux Ubuntu server. I'm using singularity v3.5.3, which is pre-installed and loaded as a module. The data is Illumina short read which has been mapped with BWA-Kit. The following is the script I'm using is: . # Load modules needed. . /etc/profile.d/modules.sh. module load xxxxx/singularity/3.5.3. # inputs. reference=$2. bam=$1.final.bam. sampleid=$1. outdir=deepvar. # Create output directories. if [ ! -e deepvar ]; then mkdir deepvar; fi. if [ ! -e deepvar/$sampleid ]; then mkdir deepvar/$sampleid; fi. # Set singularity caches. if [ ! -e ${PWD}/.singularity ]; then mkdir ${PWD}/.singularity; fi. export SINGULARITY_TMPDIR=$PWD/.singularity. export SINGULARITY_CACHEDIR=$PWD/.singularity. # Download the image. if [ ! -e deepvariant.sif ]; then singularity build deepvariant.sif docker://google/deepvariant:latest; fi. # Run Deepvariant. singularity exec -p -B ${TMPDIR} -B ${PWD} deepvariant.sif /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${reference} \. --reads=${bam} \. --output_vcf=deepvar/${sampleid}/${sampleid}.vcf.gz \. --output_gvcf=deepvar/${sampleid}/${sampleid}.g.vcf.gz \. --num_shards=${NSLOTS}. I can run the test data on the command line but have the same problem when I use the above script to run it. I've not been able to find a fix, and have tried fixes suggested for similar issues on this site. . Very appreciative of any suggestion for a solve. . [runDV.sh.o21362497.txt](https://github.com/google/deepvariant/files/8985669/runDV.sh.o21362497.txt).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/543
https://github.com/google/deepvariant/issues/543:341,safety,input,input,341,"Error - reference genome files don't exist; Hello, . When running DeepVariant I have a persistent error that the .fa and .fai reference genome files don't exist. I have checked that the given path is correct by displaying the files via copying the path given in the error sheet - the paths are correct and I don't have this problem with the input bam files, . I'm running the program via a script on a Linux Ubuntu server. I'm using singularity v3.5.3, which is pre-installed and loaded as a module. The data is Illumina short read which has been mapped with BWA-Kit. The following is the script I'm using is: . # Load modules needed. . /etc/profile.d/modules.sh. module load xxxxx/singularity/3.5.3. # inputs. reference=$2. bam=$1.final.bam. sampleid=$1. outdir=deepvar. # Create output directories. if [ ! -e deepvar ]; then mkdir deepvar; fi. if [ ! -e deepvar/$sampleid ]; then mkdir deepvar/$sampleid; fi. # Set singularity caches. if [ ! -e ${PWD}/.singularity ]; then mkdir ${PWD}/.singularity; fi. export SINGULARITY_TMPDIR=$PWD/.singularity. export SINGULARITY_CACHEDIR=$PWD/.singularity. # Download the image. if [ ! -e deepvariant.sif ]; then singularity build deepvariant.sif docker://google/deepvariant:latest; fi. # Run Deepvariant. singularity exec -p -B ${TMPDIR} -B ${PWD} deepvariant.sif /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${reference} \. --reads=${bam} \. --output_vcf=deepvar/${sampleid}/${sampleid}.vcf.gz \. --output_gvcf=deepvar/${sampleid}/${sampleid}.g.vcf.gz \. --num_shards=${NSLOTS}. I can run the test data on the command line but have the same problem when I use the above script to run it. I've not been able to find a fix, and have tried fixes suggested for similar issues on this site. . Very appreciative of any suggestion for a solve. . [runDV.sh.o21362497.txt](https://github.com/google/deepvariant/files/8985669/runDV.sh.o21362497.txt).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/543
https://github.com/google/deepvariant/issues/543:492,safety,modul,module,492,"Error - reference genome files don't exist; Hello, . When running DeepVariant I have a persistent error that the .fa and .fai reference genome files don't exist. I have checked that the given path is correct by displaying the files via copying the path given in the error sheet - the paths are correct and I don't have this problem with the input bam files, . I'm running the program via a script on a Linux Ubuntu server. I'm using singularity v3.5.3, which is pre-installed and loaded as a module. The data is Illumina short read which has been mapped with BWA-Kit. The following is the script I'm using is: . # Load modules needed. . /etc/profile.d/modules.sh. module load xxxxx/singularity/3.5.3. # inputs. reference=$2. bam=$1.final.bam. sampleid=$1. outdir=deepvar. # Create output directories. if [ ! -e deepvar ]; then mkdir deepvar; fi. if [ ! -e deepvar/$sampleid ]; then mkdir deepvar/$sampleid; fi. # Set singularity caches. if [ ! -e ${PWD}/.singularity ]; then mkdir ${PWD}/.singularity; fi. export SINGULARITY_TMPDIR=$PWD/.singularity. export SINGULARITY_CACHEDIR=$PWD/.singularity. # Download the image. if [ ! -e deepvariant.sif ]; then singularity build deepvariant.sif docker://google/deepvariant:latest; fi. # Run Deepvariant. singularity exec -p -B ${TMPDIR} -B ${PWD} deepvariant.sif /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${reference} \. --reads=${bam} \. --output_vcf=deepvar/${sampleid}/${sampleid}.vcf.gz \. --output_gvcf=deepvar/${sampleid}/${sampleid}.g.vcf.gz \. --num_shards=${NSLOTS}. I can run the test data on the command line but have the same problem when I use the above script to run it. I've not been able to find a fix, and have tried fixes suggested for similar issues on this site. . Very appreciative of any suggestion for a solve. . [runDV.sh.o21362497.txt](https://github.com/google/deepvariant/files/8985669/runDV.sh.o21362497.txt).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/543
https://github.com/google/deepvariant/issues/543:619,safety,modul,modules,619,"Error - reference genome files don't exist; Hello, . When running DeepVariant I have a persistent error that the .fa and .fai reference genome files don't exist. I have checked that the given path is correct by displaying the files via copying the path given in the error sheet - the paths are correct and I don't have this problem with the input bam files, . I'm running the program via a script on a Linux Ubuntu server. I'm using singularity v3.5.3, which is pre-installed and loaded as a module. The data is Illumina short read which has been mapped with BWA-Kit. The following is the script I'm using is: . # Load modules needed. . /etc/profile.d/modules.sh. module load xxxxx/singularity/3.5.3. # inputs. reference=$2. bam=$1.final.bam. sampleid=$1. outdir=deepvar. # Create output directories. if [ ! -e deepvar ]; then mkdir deepvar; fi. if [ ! -e deepvar/$sampleid ]; then mkdir deepvar/$sampleid; fi. # Set singularity caches. if [ ! -e ${PWD}/.singularity ]; then mkdir ${PWD}/.singularity; fi. export SINGULARITY_TMPDIR=$PWD/.singularity. export SINGULARITY_CACHEDIR=$PWD/.singularity. # Download the image. if [ ! -e deepvariant.sif ]; then singularity build deepvariant.sif docker://google/deepvariant:latest; fi. # Run Deepvariant. singularity exec -p -B ${TMPDIR} -B ${PWD} deepvariant.sif /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${reference} \. --reads=${bam} \. --output_vcf=deepvar/${sampleid}/${sampleid}.vcf.gz \. --output_gvcf=deepvar/${sampleid}/${sampleid}.g.vcf.gz \. --num_shards=${NSLOTS}. I can run the test data on the command line but have the same problem when I use the above script to run it. I've not been able to find a fix, and have tried fixes suggested for similar issues on this site. . Very appreciative of any suggestion for a solve. . [runDV.sh.o21362497.txt](https://github.com/google/deepvariant/files/8985669/runDV.sh.o21362497.txt).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/543
https://github.com/google/deepvariant/issues/543:652,safety,modul,modules,652,"Error - reference genome files don't exist; Hello, . When running DeepVariant I have a persistent error that the .fa and .fai reference genome files don't exist. I have checked that the given path is correct by displaying the files via copying the path given in the error sheet - the paths are correct and I don't have this problem with the input bam files, . I'm running the program via a script on a Linux Ubuntu server. I'm using singularity v3.5.3, which is pre-installed and loaded as a module. The data is Illumina short read which has been mapped with BWA-Kit. The following is the script I'm using is: . # Load modules needed. . /etc/profile.d/modules.sh. module load xxxxx/singularity/3.5.3. # inputs. reference=$2. bam=$1.final.bam. sampleid=$1. outdir=deepvar. # Create output directories. if [ ! -e deepvar ]; then mkdir deepvar; fi. if [ ! -e deepvar/$sampleid ]; then mkdir deepvar/$sampleid; fi. # Set singularity caches. if [ ! -e ${PWD}/.singularity ]; then mkdir ${PWD}/.singularity; fi. export SINGULARITY_TMPDIR=$PWD/.singularity. export SINGULARITY_CACHEDIR=$PWD/.singularity. # Download the image. if [ ! -e deepvariant.sif ]; then singularity build deepvariant.sif docker://google/deepvariant:latest; fi. # Run Deepvariant. singularity exec -p -B ${TMPDIR} -B ${PWD} deepvariant.sif /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${reference} \. --reads=${bam} \. --output_vcf=deepvar/${sampleid}/${sampleid}.vcf.gz \. --output_gvcf=deepvar/${sampleid}/${sampleid}.g.vcf.gz \. --num_shards=${NSLOTS}. I can run the test data on the command line but have the same problem when I use the above script to run it. I've not been able to find a fix, and have tried fixes suggested for similar issues on this site. . Very appreciative of any suggestion for a solve. . [runDV.sh.o21362497.txt](https://github.com/google/deepvariant/files/8985669/runDV.sh.o21362497.txt).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/543
https://github.com/google/deepvariant/issues/543:664,safety,modul,module,664,"Error - reference genome files don't exist; Hello, . When running DeepVariant I have a persistent error that the .fa and .fai reference genome files don't exist. I have checked that the given path is correct by displaying the files via copying the path given in the error sheet - the paths are correct and I don't have this problem with the input bam files, . I'm running the program via a script on a Linux Ubuntu server. I'm using singularity v3.5.3, which is pre-installed and loaded as a module. The data is Illumina short read which has been mapped with BWA-Kit. The following is the script I'm using is: . # Load modules needed. . /etc/profile.d/modules.sh. module load xxxxx/singularity/3.5.3. # inputs. reference=$2. bam=$1.final.bam. sampleid=$1. outdir=deepvar. # Create output directories. if [ ! -e deepvar ]; then mkdir deepvar; fi. if [ ! -e deepvar/$sampleid ]; then mkdir deepvar/$sampleid; fi. # Set singularity caches. if [ ! -e ${PWD}/.singularity ]; then mkdir ${PWD}/.singularity; fi. export SINGULARITY_TMPDIR=$PWD/.singularity. export SINGULARITY_CACHEDIR=$PWD/.singularity. # Download the image. if [ ! -e deepvariant.sif ]; then singularity build deepvariant.sif docker://google/deepvariant:latest; fi. # Run Deepvariant. singularity exec -p -B ${TMPDIR} -B ${PWD} deepvariant.sif /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${reference} \. --reads=${bam} \. --output_vcf=deepvar/${sampleid}/${sampleid}.vcf.gz \. --output_gvcf=deepvar/${sampleid}/${sampleid}.g.vcf.gz \. --num_shards=${NSLOTS}. I can run the test data on the command line but have the same problem when I use the above script to run it. I've not been able to find a fix, and have tried fixes suggested for similar issues on this site. . Very appreciative of any suggestion for a solve. . [runDV.sh.o21362497.txt](https://github.com/google/deepvariant/files/8985669/runDV.sh.o21362497.txt).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/543
https://github.com/google/deepvariant/issues/543:703,safety,input,inputs,703,"Error - reference genome files don't exist; Hello, . When running DeepVariant I have a persistent error that the .fa and .fai reference genome files don't exist. I have checked that the given path is correct by displaying the files via copying the path given in the error sheet - the paths are correct and I don't have this problem with the input bam files, . I'm running the program via a script on a Linux Ubuntu server. I'm using singularity v3.5.3, which is pre-installed and loaded as a module. The data is Illumina short read which has been mapped with BWA-Kit. The following is the script I'm using is: . # Load modules needed. . /etc/profile.d/modules.sh. module load xxxxx/singularity/3.5.3. # inputs. reference=$2. bam=$1.final.bam. sampleid=$1. outdir=deepvar. # Create output directories. if [ ! -e deepvar ]; then mkdir deepvar; fi. if [ ! -e deepvar/$sampleid ]; then mkdir deepvar/$sampleid; fi. # Set singularity caches. if [ ! -e ${PWD}/.singularity ]; then mkdir ${PWD}/.singularity; fi. export SINGULARITY_TMPDIR=$PWD/.singularity. export SINGULARITY_CACHEDIR=$PWD/.singularity. # Download the image. if [ ! -e deepvariant.sif ]; then singularity build deepvariant.sif docker://google/deepvariant:latest; fi. # Run Deepvariant. singularity exec -p -B ${TMPDIR} -B ${PWD} deepvariant.sif /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${reference} \. --reads=${bam} \. --output_vcf=deepvar/${sampleid}/${sampleid}.vcf.gz \. --output_gvcf=deepvar/${sampleid}/${sampleid}.g.vcf.gz \. --num_shards=${NSLOTS}. I can run the test data on the command line but have the same problem when I use the above script to run it. I've not been able to find a fix, and have tried fixes suggested for similar issues on this site. . Very appreciative of any suggestion for a solve. . [runDV.sh.o21362497.txt](https://github.com/google/deepvariant/files/8985669/runDV.sh.o21362497.txt).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/543
https://github.com/google/deepvariant/issues/543:1557,safety,test,test,1557,"Error - reference genome files don't exist; Hello, . When running DeepVariant I have a persistent error that the .fa and .fai reference genome files don't exist. I have checked that the given path is correct by displaying the files via copying the path given in the error sheet - the paths are correct and I don't have this problem with the input bam files, . I'm running the program via a script on a Linux Ubuntu server. I'm using singularity v3.5.3, which is pre-installed and loaded as a module. The data is Illumina short read which has been mapped with BWA-Kit. The following is the script I'm using is: . # Load modules needed. . /etc/profile.d/modules.sh. module load xxxxx/singularity/3.5.3. # inputs. reference=$2. bam=$1.final.bam. sampleid=$1. outdir=deepvar. # Create output directories. if [ ! -e deepvar ]; then mkdir deepvar; fi. if [ ! -e deepvar/$sampleid ]; then mkdir deepvar/$sampleid; fi. # Set singularity caches. if [ ! -e ${PWD}/.singularity ]; then mkdir ${PWD}/.singularity; fi. export SINGULARITY_TMPDIR=$PWD/.singularity. export SINGULARITY_CACHEDIR=$PWD/.singularity. # Download the image. if [ ! -e deepvariant.sif ]; then singularity build deepvariant.sif docker://google/deepvariant:latest; fi. # Run Deepvariant. singularity exec -p -B ${TMPDIR} -B ${PWD} deepvariant.sif /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${reference} \. --reads=${bam} \. --output_vcf=deepvar/${sampleid}/${sampleid}.vcf.gz \. --output_gvcf=deepvar/${sampleid}/${sampleid}.g.vcf.gz \. --num_shards=${NSLOTS}. I can run the test data on the command line but have the same problem when I use the above script to run it. I've not been able to find a fix, and have tried fixes suggested for similar issues on this site. . Very appreciative of any suggestion for a solve. . [runDV.sh.o21362497.txt](https://github.com/google/deepvariant/files/8985669/runDV.sh.o21362497.txt).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/543
https://github.com/google/deepvariant/issues/543:1557,testability,test,test,1557,"Error - reference genome files don't exist; Hello, . When running DeepVariant I have a persistent error that the .fa and .fai reference genome files don't exist. I have checked that the given path is correct by displaying the files via copying the path given in the error sheet - the paths are correct and I don't have this problem with the input bam files, . I'm running the program via a script on a Linux Ubuntu server. I'm using singularity v3.5.3, which is pre-installed and loaded as a module. The data is Illumina short read which has been mapped with BWA-Kit. The following is the script I'm using is: . # Load modules needed. . /etc/profile.d/modules.sh. module load xxxxx/singularity/3.5.3. # inputs. reference=$2. bam=$1.final.bam. sampleid=$1. outdir=deepvar. # Create output directories. if [ ! -e deepvar ]; then mkdir deepvar; fi. if [ ! -e deepvar/$sampleid ]; then mkdir deepvar/$sampleid; fi. # Set singularity caches. if [ ! -e ${PWD}/.singularity ]; then mkdir ${PWD}/.singularity; fi. export SINGULARITY_TMPDIR=$PWD/.singularity. export SINGULARITY_CACHEDIR=$PWD/.singularity. # Download the image. if [ ! -e deepvariant.sif ]; then singularity build deepvariant.sif docker://google/deepvariant:latest; fi. # Run Deepvariant. singularity exec -p -B ${TMPDIR} -B ${PWD} deepvariant.sif /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${reference} \. --reads=${bam} \. --output_vcf=deepvar/${sampleid}/${sampleid}.vcf.gz \. --output_gvcf=deepvar/${sampleid}/${sampleid}.g.vcf.gz \. --num_shards=${NSLOTS}. I can run the test data on the command line but have the same problem when I use the above script to run it. I've not been able to find a fix, and have tried fixes suggested for similar issues on this site. . Very appreciative of any suggestion for a solve. . [runDV.sh.o21362497.txt](https://github.com/google/deepvariant/files/8985669/runDV.sh.o21362497.txt).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/543
https://github.com/google/deepvariant/issues/543:0,usability,Error,Error,0,"Error - reference genome files don't exist; Hello, . When running DeepVariant I have a persistent error that the .fa and .fai reference genome files don't exist. I have checked that the given path is correct by displaying the files via copying the path given in the error sheet - the paths are correct and I don't have this problem with the input bam files, . I'm running the program via a script on a Linux Ubuntu server. I'm using singularity v3.5.3, which is pre-installed and loaded as a module. The data is Illumina short read which has been mapped with BWA-Kit. The following is the script I'm using is: . # Load modules needed. . /etc/profile.d/modules.sh. module load xxxxx/singularity/3.5.3. # inputs. reference=$2. bam=$1.final.bam. sampleid=$1. outdir=deepvar. # Create output directories. if [ ! -e deepvar ]; then mkdir deepvar; fi. if [ ! -e deepvar/$sampleid ]; then mkdir deepvar/$sampleid; fi. # Set singularity caches. if [ ! -e ${PWD}/.singularity ]; then mkdir ${PWD}/.singularity; fi. export SINGULARITY_TMPDIR=$PWD/.singularity. export SINGULARITY_CACHEDIR=$PWD/.singularity. # Download the image. if [ ! -e deepvariant.sif ]; then singularity build deepvariant.sif docker://google/deepvariant:latest; fi. # Run Deepvariant. singularity exec -p -B ${TMPDIR} -B ${PWD} deepvariant.sif /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${reference} \. --reads=${bam} \. --output_vcf=deepvar/${sampleid}/${sampleid}.vcf.gz \. --output_gvcf=deepvar/${sampleid}/${sampleid}.g.vcf.gz \. --num_shards=${NSLOTS}. I can run the test data on the command line but have the same problem when I use the above script to run it. I've not been able to find a fix, and have tried fixes suggested for similar issues on this site. . Very appreciative of any suggestion for a solve. . [runDV.sh.o21362497.txt](https://github.com/google/deepvariant/files/8985669/runDV.sh.o21362497.txt).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/543
https://github.com/google/deepvariant/issues/543:98,usability,error,error,98,"Error - reference genome files don't exist; Hello, . When running DeepVariant I have a persistent error that the .fa and .fai reference genome files don't exist. I have checked that the given path is correct by displaying the files via copying the path given in the error sheet - the paths are correct and I don't have this problem with the input bam files, . I'm running the program via a script on a Linux Ubuntu server. I'm using singularity v3.5.3, which is pre-installed and loaded as a module. The data is Illumina short read which has been mapped with BWA-Kit. The following is the script I'm using is: . # Load modules needed. . /etc/profile.d/modules.sh. module load xxxxx/singularity/3.5.3. # inputs. reference=$2. bam=$1.final.bam. sampleid=$1. outdir=deepvar. # Create output directories. if [ ! -e deepvar ]; then mkdir deepvar; fi. if [ ! -e deepvar/$sampleid ]; then mkdir deepvar/$sampleid; fi. # Set singularity caches. if [ ! -e ${PWD}/.singularity ]; then mkdir ${PWD}/.singularity; fi. export SINGULARITY_TMPDIR=$PWD/.singularity. export SINGULARITY_CACHEDIR=$PWD/.singularity. # Download the image. if [ ! -e deepvariant.sif ]; then singularity build deepvariant.sif docker://google/deepvariant:latest; fi. # Run Deepvariant. singularity exec -p -B ${TMPDIR} -B ${PWD} deepvariant.sif /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${reference} \. --reads=${bam} \. --output_vcf=deepvar/${sampleid}/${sampleid}.vcf.gz \. --output_gvcf=deepvar/${sampleid}/${sampleid}.g.vcf.gz \. --num_shards=${NSLOTS}. I can run the test data on the command line but have the same problem when I use the above script to run it. I've not been able to find a fix, and have tried fixes suggested for similar issues on this site. . Very appreciative of any suggestion for a solve. . [runDV.sh.o21362497.txt](https://github.com/google/deepvariant/files/8985669/runDV.sh.o21362497.txt).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/543
https://github.com/google/deepvariant/issues/543:266,usability,error,error,266,"Error - reference genome files don't exist; Hello, . When running DeepVariant I have a persistent error that the .fa and .fai reference genome files don't exist. I have checked that the given path is correct by displaying the files via copying the path given in the error sheet - the paths are correct and I don't have this problem with the input bam files, . I'm running the program via a script on a Linux Ubuntu server. I'm using singularity v3.5.3, which is pre-installed and loaded as a module. The data is Illumina short read which has been mapped with BWA-Kit. The following is the script I'm using is: . # Load modules needed. . /etc/profile.d/modules.sh. module load xxxxx/singularity/3.5.3. # inputs. reference=$2. bam=$1.final.bam. sampleid=$1. outdir=deepvar. # Create output directories. if [ ! -e deepvar ]; then mkdir deepvar; fi. if [ ! -e deepvar/$sampleid ]; then mkdir deepvar/$sampleid; fi. # Set singularity caches. if [ ! -e ${PWD}/.singularity ]; then mkdir ${PWD}/.singularity; fi. export SINGULARITY_TMPDIR=$PWD/.singularity. export SINGULARITY_CACHEDIR=$PWD/.singularity. # Download the image. if [ ! -e deepvariant.sif ]; then singularity build deepvariant.sif docker://google/deepvariant:latest; fi. # Run Deepvariant. singularity exec -p -B ${TMPDIR} -B ${PWD} deepvariant.sif /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${reference} \. --reads=${bam} \. --output_vcf=deepvar/${sampleid}/${sampleid}.vcf.gz \. --output_gvcf=deepvar/${sampleid}/${sampleid}.g.vcf.gz \. --num_shards=${NSLOTS}. I can run the test data on the command line but have the same problem when I use the above script to run it. I've not been able to find a fix, and have tried fixes suggested for similar issues on this site. . Very appreciative of any suggestion for a solve. . [runDV.sh.o21362497.txt](https://github.com/google/deepvariant/files/8985669/runDV.sh.o21362497.txt).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/543
https://github.com/google/deepvariant/issues/543:341,usability,input,input,341,"Error - reference genome files don't exist; Hello, . When running DeepVariant I have a persistent error that the .fa and .fai reference genome files don't exist. I have checked that the given path is correct by displaying the files via copying the path given in the error sheet - the paths are correct and I don't have this problem with the input bam files, . I'm running the program via a script on a Linux Ubuntu server. I'm using singularity v3.5.3, which is pre-installed and loaded as a module. The data is Illumina short read which has been mapped with BWA-Kit. The following is the script I'm using is: . # Load modules needed. . /etc/profile.d/modules.sh. module load xxxxx/singularity/3.5.3. # inputs. reference=$2. bam=$1.final.bam. sampleid=$1. outdir=deepvar. # Create output directories. if [ ! -e deepvar ]; then mkdir deepvar; fi. if [ ! -e deepvar/$sampleid ]; then mkdir deepvar/$sampleid; fi. # Set singularity caches. if [ ! -e ${PWD}/.singularity ]; then mkdir ${PWD}/.singularity; fi. export SINGULARITY_TMPDIR=$PWD/.singularity. export SINGULARITY_CACHEDIR=$PWD/.singularity. # Download the image. if [ ! -e deepvariant.sif ]; then singularity build deepvariant.sif docker://google/deepvariant:latest; fi. # Run Deepvariant. singularity exec -p -B ${TMPDIR} -B ${PWD} deepvariant.sif /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${reference} \. --reads=${bam} \. --output_vcf=deepvar/${sampleid}/${sampleid}.vcf.gz \. --output_gvcf=deepvar/${sampleid}/${sampleid}.g.vcf.gz \. --num_shards=${NSLOTS}. I can run the test data on the command line but have the same problem when I use the above script to run it. I've not been able to find a fix, and have tried fixes suggested for similar issues on this site. . Very appreciative of any suggestion for a solve. . [runDV.sh.o21362497.txt](https://github.com/google/deepvariant/files/8985669/runDV.sh.o21362497.txt).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/543
https://github.com/google/deepvariant/issues/543:703,usability,input,inputs,703,"Error - reference genome files don't exist; Hello, . When running DeepVariant I have a persistent error that the .fa and .fai reference genome files don't exist. I have checked that the given path is correct by displaying the files via copying the path given in the error sheet - the paths are correct and I don't have this problem with the input bam files, . I'm running the program via a script on a Linux Ubuntu server. I'm using singularity v3.5.3, which is pre-installed and loaded as a module. The data is Illumina short read which has been mapped with BWA-Kit. The following is the script I'm using is: . # Load modules needed. . /etc/profile.d/modules.sh. module load xxxxx/singularity/3.5.3. # inputs. reference=$2. bam=$1.final.bam. sampleid=$1. outdir=deepvar. # Create output directories. if [ ! -e deepvar ]; then mkdir deepvar; fi. if [ ! -e deepvar/$sampleid ]; then mkdir deepvar/$sampleid; fi. # Set singularity caches. if [ ! -e ${PWD}/.singularity ]; then mkdir ${PWD}/.singularity; fi. export SINGULARITY_TMPDIR=$PWD/.singularity. export SINGULARITY_CACHEDIR=$PWD/.singularity. # Download the image. if [ ! -e deepvariant.sif ]; then singularity build deepvariant.sif docker://google/deepvariant:latest; fi. # Run Deepvariant. singularity exec -p -B ${TMPDIR} -B ${PWD} deepvariant.sif /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${reference} \. --reads=${bam} \. --output_vcf=deepvar/${sampleid}/${sampleid}.vcf.gz \. --output_gvcf=deepvar/${sampleid}/${sampleid}.g.vcf.gz \. --num_shards=${NSLOTS}. I can run the test data on the command line but have the same problem when I use the above script to run it. I've not been able to find a fix, and have tried fixes suggested for similar issues on this site. . Very appreciative of any suggestion for a solve. . [runDV.sh.o21362497.txt](https://github.com/google/deepvariant/files/8985669/runDV.sh.o21362497.txt).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/543
https://github.com/google/deepvariant/issues/543:1574,usability,command,command,1574,"Error - reference genome files don't exist; Hello, . When running DeepVariant I have a persistent error that the .fa and .fai reference genome files don't exist. I have checked that the given path is correct by displaying the files via copying the path given in the error sheet - the paths are correct and I don't have this problem with the input bam files, . I'm running the program via a script on a Linux Ubuntu server. I'm using singularity v3.5.3, which is pre-installed and loaded as a module. The data is Illumina short read which has been mapped with BWA-Kit. The following is the script I'm using is: . # Load modules needed. . /etc/profile.d/modules.sh. module load xxxxx/singularity/3.5.3. # inputs. reference=$2. bam=$1.final.bam. sampleid=$1. outdir=deepvar. # Create output directories. if [ ! -e deepvar ]; then mkdir deepvar; fi. if [ ! -e deepvar/$sampleid ]; then mkdir deepvar/$sampleid; fi. # Set singularity caches. if [ ! -e ${PWD}/.singularity ]; then mkdir ${PWD}/.singularity; fi. export SINGULARITY_TMPDIR=$PWD/.singularity. export SINGULARITY_CACHEDIR=$PWD/.singularity. # Download the image. if [ ! -e deepvariant.sif ]; then singularity build deepvariant.sif docker://google/deepvariant:latest; fi. # Run Deepvariant. singularity exec -p -B ${TMPDIR} -B ${PWD} deepvariant.sif /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=${reference} \. --reads=${bam} \. --output_vcf=deepvar/${sampleid}/${sampleid}.vcf.gz \. --output_gvcf=deepvar/${sampleid}/${sampleid}.g.vcf.gz \. --num_shards=${NSLOTS}. I can run the test data on the command line but have the same problem when I use the above script to run it. I've not been able to find a fix, and have tried fixes suggested for similar issues on this site. . Very appreciative of any suggestion for a solve. . [runDV.sh.o21362497.txt](https://github.com/google/deepvariant/files/8985669/runDV.sh.o21362497.txt).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/543
https://github.com/google/deepvariant/issues/544:353,availability,error,error,353,"DeepTrio : using the flag --output_gvcf_merged does not create a merged file; Hello,. Please find below the description of an issue with the flag --output_gvcf_merged in DeepTrio. Thanks for considering this request. Fred-07. **Describe the issue:**. Running the ""DeepTrio quick start"" commands with the additional flag --output_gvcf_merged produces no error but the""merged"" file is not created. https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-quick-start.md. All other expected files are created. **Setup**. - Operating system: Linux 3.10.0-1160.71.1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method: singularity pull from docker, LSF as batch system. - Type of data: ""DeepTrio quick start"" data. **Steps to reproduce:**. - Command: additional flag. `--output_gvcf_merged ""${OUTPUT_DIR}""/ALL.g.vcf.gz`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/544
https://github.com/google/deepvariant/issues/544:525,availability,Operat,Operating,525,"DeepTrio : using the flag --output_gvcf_merged does not create a merged file; Hello,. Please find below the description of an issue with the flag --output_gvcf_merged in DeepTrio. Thanks for considering this request. Fred-07. **Describe the issue:**. Running the ""DeepTrio quick start"" commands with the additional flag --output_gvcf_merged produces no error but the""merged"" file is not created. https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-quick-start.md. All other expected files are created. **Setup**. - Operating system: Linux 3.10.0-1160.71.1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method: singularity pull from docker, LSF as batch system. - Type of data: ""DeepTrio quick start"" data. **Steps to reproduce:**. - Command: additional flag. `--output_gvcf_merged ""${OUTPUT_DIR}""/ALL.g.vcf.gz`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/544
https://github.com/google/deepvariant/issues/544:592,deployability,version,version,592,"DeepTrio : using the flag --output_gvcf_merged does not create a merged file; Hello,. Please find below the description of an issue with the flag --output_gvcf_merged in DeepTrio. Thanks for considering this request. Fred-07. **Describe the issue:**. Running the ""DeepTrio quick start"" commands with the additional flag --output_gvcf_merged produces no error but the""merged"" file is not created. https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-quick-start.md. All other expected files are created. **Setup**. - Operating system: Linux 3.10.0-1160.71.1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method: singularity pull from docker, LSF as batch system. - Type of data: ""DeepTrio quick start"" data. **Steps to reproduce:**. - Command: additional flag. `--output_gvcf_merged ""${OUTPUT_DIR}""/ALL.g.vcf.gz`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/544
https://github.com/google/deepvariant/issues/544:610,deployability,Instal,Installation,610,"DeepTrio : using the flag --output_gvcf_merged does not create a merged file; Hello,. Please find below the description of an issue with the flag --output_gvcf_merged in DeepTrio. Thanks for considering this request. Fred-07. **Describe the issue:**. Running the ""DeepTrio quick start"" commands with the additional flag --output_gvcf_merged produces no error but the""merged"" file is not created. https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-quick-start.md. All other expected files are created. **Setup**. - Operating system: Linux 3.10.0-1160.71.1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method: singularity pull from docker, LSF as batch system. - Type of data: ""DeepTrio quick start"" data. **Steps to reproduce:**. - Command: additional flag. `--output_gvcf_merged ""${OUTPUT_DIR}""/ALL.g.vcf.gz`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/544
https://github.com/google/deepvariant/issues/544:592,integrability,version,version,592,"DeepTrio : using the flag --output_gvcf_merged does not create a merged file; Hello,. Please find below the description of an issue with the flag --output_gvcf_merged in DeepTrio. Thanks for considering this request. Fred-07. **Describe the issue:**. Running the ""DeepTrio quick start"" commands with the additional flag --output_gvcf_merged produces no error but the""merged"" file is not created. https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-quick-start.md. All other expected files are created. **Setup**. - Operating system: Linux 3.10.0-1160.71.1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method: singularity pull from docker, LSF as batch system. - Type of data: ""DeepTrio quick start"" data. **Steps to reproduce:**. - Command: additional flag. `--output_gvcf_merged ""${OUTPUT_DIR}""/ALL.g.vcf.gz`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/544
https://github.com/google/deepvariant/issues/544:668,integrability,batch,batch,668,"DeepTrio : using the flag --output_gvcf_merged does not create a merged file; Hello,. Please find below the description of an issue with the flag --output_gvcf_merged in DeepTrio. Thanks for considering this request. Fred-07. **Describe the issue:**. Running the ""DeepTrio quick start"" commands with the additional flag --output_gvcf_merged produces no error but the""merged"" file is not created. https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-quick-start.md. All other expected files are created. **Setup**. - Operating system: Linux 3.10.0-1160.71.1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method: singularity pull from docker, LSF as batch system. - Type of data: ""DeepTrio quick start"" data. **Steps to reproduce:**. - Command: additional flag. `--output_gvcf_merged ""${OUTPUT_DIR}""/ALL.g.vcf.gz`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/544
https://github.com/google/deepvariant/issues/544:592,modifiability,version,version,592,"DeepTrio : using the flag --output_gvcf_merged does not create a merged file; Hello,. Please find below the description of an issue with the flag --output_gvcf_merged in DeepTrio. Thanks for considering this request. Fred-07. **Describe the issue:**. Running the ""DeepTrio quick start"" commands with the additional flag --output_gvcf_merged produces no error but the""merged"" file is not created. https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-quick-start.md. All other expected files are created. **Setup**. - Operating system: Linux 3.10.0-1160.71.1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method: singularity pull from docker, LSF as batch system. - Type of data: ""DeepTrio quick start"" data. **Steps to reproduce:**. - Command: additional flag. `--output_gvcf_merged ""${OUTPUT_DIR}""/ALL.g.vcf.gz`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/544
https://github.com/google/deepvariant/issues/544:353,performance,error,error,353,"DeepTrio : using the flag --output_gvcf_merged does not create a merged file; Hello,. Please find below the description of an issue with the flag --output_gvcf_merged in DeepTrio. Thanks for considering this request. Fred-07. **Describe the issue:**. Running the ""DeepTrio quick start"" commands with the additional flag --output_gvcf_merged produces no error but the""merged"" file is not created. https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-quick-start.md. All other expected files are created. **Setup**. - Operating system: Linux 3.10.0-1160.71.1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method: singularity pull from docker, LSF as batch system. - Type of data: ""DeepTrio quick start"" data. **Steps to reproduce:**. - Command: additional flag. `--output_gvcf_merged ""${OUTPUT_DIR}""/ALL.g.vcf.gz`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/544
https://github.com/google/deepvariant/issues/544:668,performance,batch,batch,668,"DeepTrio : using the flag --output_gvcf_merged does not create a merged file; Hello,. Please find below the description of an issue with the flag --output_gvcf_merged in DeepTrio. Thanks for considering this request. Fred-07. **Describe the issue:**. Running the ""DeepTrio quick start"" commands with the additional flag --output_gvcf_merged produces no error but the""merged"" file is not created. https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-quick-start.md. All other expected files are created. **Setup**. - Operating system: Linux 3.10.0-1160.71.1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method: singularity pull from docker, LSF as batch system. - Type of data: ""DeepTrio quick start"" data. **Steps to reproduce:**. - Command: additional flag. `--output_gvcf_merged ""${OUTPUT_DIR}""/ALL.g.vcf.gz`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/544
https://github.com/google/deepvariant/issues/544:47,reliability,doe,does,47,"DeepTrio : using the flag --output_gvcf_merged does not create a merged file; Hello,. Please find below the description of an issue with the flag --output_gvcf_merged in DeepTrio. Thanks for considering this request. Fred-07. **Describe the issue:**. Running the ""DeepTrio quick start"" commands with the additional flag --output_gvcf_merged produces no error but the""merged"" file is not created. https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-quick-start.md. All other expected files are created. **Setup**. - Operating system: Linux 3.10.0-1160.71.1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method: singularity pull from docker, LSF as batch system. - Type of data: ""DeepTrio quick start"" data. **Steps to reproduce:**. - Command: additional flag. `--output_gvcf_merged ""${OUTPUT_DIR}""/ALL.g.vcf.gz`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/544
https://github.com/google/deepvariant/issues/544:353,safety,error,error,353,"DeepTrio : using the flag --output_gvcf_merged does not create a merged file; Hello,. Please find below the description of an issue with the flag --output_gvcf_merged in DeepTrio. Thanks for considering this request. Fred-07. **Describe the issue:**. Running the ""DeepTrio quick start"" commands with the additional flag --output_gvcf_merged produces no error but the""merged"" file is not created. https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-quick-start.md. All other expected files are created. **Setup**. - Operating system: Linux 3.10.0-1160.71.1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method: singularity pull from docker, LSF as batch system. - Type of data: ""DeepTrio quick start"" data. **Steps to reproduce:**. - Command: additional flag. `--output_gvcf_merged ""${OUTPUT_DIR}""/ALL.g.vcf.gz`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/544
https://github.com/google/deepvariant/issues/544:286,usability,command,commands,286,"DeepTrio : using the flag --output_gvcf_merged does not create a merged file; Hello,. Please find below the description of an issue with the flag --output_gvcf_merged in DeepTrio. Thanks for considering this request. Fred-07. **Describe the issue:**. Running the ""DeepTrio quick start"" commands with the additional flag --output_gvcf_merged produces no error but the""merged"" file is not created. https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-quick-start.md. All other expected files are created. **Setup**. - Operating system: Linux 3.10.0-1160.71.1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method: singularity pull from docker, LSF as batch system. - Type of data: ""DeepTrio quick start"" data. **Steps to reproduce:**. - Command: additional flag. `--output_gvcf_merged ""${OUTPUT_DIR}""/ALL.g.vcf.gz`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/544
https://github.com/google/deepvariant/issues/544:353,usability,error,error,353,"DeepTrio : using the flag --output_gvcf_merged does not create a merged file; Hello,. Please find below the description of an issue with the flag --output_gvcf_merged in DeepTrio. Thanks for considering this request. Fred-07. **Describe the issue:**. Running the ""DeepTrio quick start"" commands with the additional flag --output_gvcf_merged produces no error but the""merged"" file is not created. https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-quick-start.md. All other expected files are created. **Setup**. - Operating system: Linux 3.10.0-1160.71.1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method: singularity pull from docker, LSF as batch system. - Type of data: ""DeepTrio quick start"" data. **Steps to reproduce:**. - Command: additional flag. `--output_gvcf_merged ""${OUTPUT_DIR}""/ALL.g.vcf.gz`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/544
https://github.com/google/deepvariant/issues/544:754,usability,Command,Command,754,"DeepTrio : using the flag --output_gvcf_merged does not create a merged file; Hello,. Please find below the description of an issue with the flag --output_gvcf_merged in DeepTrio. Thanks for considering this request. Fred-07. **Describe the issue:**. Running the ""DeepTrio quick start"" commands with the additional flag --output_gvcf_merged produces no error but the""merged"" file is not created. https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-quick-start.md. All other expected files are created. **Setup**. - Operating system: Linux 3.10.0-1160.71.1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method: singularity pull from docker, LSF as batch system. - Type of data: ""DeepTrio quick start"" data. **Steps to reproduce:**. - Command: additional flag. `--output_gvcf_merged ""${OUTPUT_DIR}""/ALL.g.vcf.gz`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/544
https://github.com/google/deepvariant/issues/545:412,availability,avail,available,412,"Issue with running using docker; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). Issue encountered during running with Docker, thinking it is possibly due to tf not supported by m1 chip, here is the issue. . The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. qemu: uncaught target signal 6 (Aborted) - core dumped. **Setup**. - Operating system: MacOs (Mac mini/ m1 chip). - DeepVariant version:1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). The test data from GitHub. **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/545
https://github.com/google/deepvariant/issues/545:508,availability,Operat,Operating,508,"Issue with running using docker; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). Issue encountered during running with Docker, thinking it is possibly due to tf not supported by m1 chip, here is the issue. . The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. qemu: uncaught target signal 6 (Aborted) - core dumped. **Setup**. - Operating system: MacOs (Mac mini/ m1 chip). - DeepVariant version:1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). The test data from GitHub. **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/545
https://github.com/google/deepvariant/issues/545:823,availability,Error,Error,823,"Issue with running using docker; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). Issue encountered during running with Docker, thinking it is possibly due to tf not supported by m1 chip, here is the issue. . The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. qemu: uncaught target signal 6 (Aborted) - core dumped. **Setup**. - Operating system: MacOs (Mac mini/ m1 chip). - DeepVariant version:1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). The test data from GitHub. **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/545
https://github.com/google/deepvariant/issues/545:567,deployability,version,version,567,"Issue with running using docker; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). Issue encountered during running with Docker, thinking it is possibly due to tf not supported by m1 chip, here is the issue. . The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. qemu: uncaught target signal 6 (Aborted) - core dumped. **Setup**. - Operating system: MacOs (Mac mini/ m1 chip). - DeepVariant version:1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). The test data from GitHub. **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/545
https://github.com/google/deepvariant/issues/545:584,deployability,Instal,Installation,584,"Issue with running using docker; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). Issue encountered during running with Docker, thinking it is possibly due to tf not supported by m1 chip, here is the issue. . The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. qemu: uncaught target signal 6 (Aborted) - core dumped. **Setup**. - Operating system: MacOs (Mac mini/ m1 chip). - DeepVariant version:1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). The test data from GitHub. **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/545
https://github.com/google/deepvariant/issues/545:482,energy efficiency,core,core,482,"Issue with running using docker; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). Issue encountered during running with Docker, thinking it is possibly due to tf not supported by m1 chip, here is the issue. . The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. qemu: uncaught target signal 6 (Aborted) - core dumped. **Setup**. - Operating system: MacOs (Mac mini/ m1 chip). - DeepVariant version:1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). The test data from GitHub. **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/545
https://github.com/google/deepvariant/issues/545:567,integrability,version,version,567,"Issue with running using docker; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). Issue encountered during running with Docker, thinking it is possibly due to tf not supported by m1 chip, here is the issue. . The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. qemu: uncaught target signal 6 (Aborted) - core dumped. **Setup**. - Operating system: MacOs (Mac mini/ m1 chip). - DeepVariant version:1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). The test data from GitHub. **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/545
https://github.com/google/deepvariant/issues/545:567,modifiability,version,version,567,"Issue with running using docker; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). Issue encountered during running with Docker, thinking it is possibly due to tf not supported by m1 chip, here is the issue. . The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. qemu: uncaught target signal 6 (Aborted) - core dumped. **Setup**. - Operating system: MacOs (Mac mini/ m1 chip). - DeepVariant version:1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). The test data from GitHub. **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/545
https://github.com/google/deepvariant/issues/545:823,performance,Error,Error,823,"Issue with running using docker; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). Issue encountered during running with Docker, thinking it is possibly due to tf not supported by m1 chip, here is the issue. . The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. qemu: uncaught target signal 6 (Aborted) - core dumped. **Setup**. - Operating system: MacOs (Mac mini/ m1 chip). - DeepVariant version:1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). The test data from GitHub. **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/545
https://github.com/google/deepvariant/issues/545:412,reliability,availab,available,412,"Issue with running using docker; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). Issue encountered during running with Docker, thinking it is possibly due to tf not supported by m1 chip, here is the issue. . The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. qemu: uncaught target signal 6 (Aborted) - core dumped. **Setup**. - Operating system: MacOs (Mac mini/ m1 chip). - DeepVariant version:1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). The test data from GitHub. **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/545
https://github.com/google/deepvariant/issues/545:855,reliability,Doe,Does,855,"Issue with running using docker; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). Issue encountered during running with Docker, thinking it is possibly due to tf not supported by m1 chip, here is the issue. . The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. qemu: uncaught target signal 6 (Aborted) - core dumped. **Setup**. - Operating system: MacOs (Mac mini/ m1 chip). - DeepVariant version:1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). The test data from GitHub. **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/545
https://github.com/google/deepvariant/issues/545:412,safety,avail,available,412,"Issue with running using docker; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). Issue encountered during running with Docker, thinking it is possibly due to tf not supported by m1 chip, here is the issue. . The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. qemu: uncaught target signal 6 (Aborted) - core dumped. **Setup**. - Operating system: MacOs (Mac mini/ m1 chip). - DeepVariant version:1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). The test data from GitHub. **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/545
https://github.com/google/deepvariant/issues/545:761,safety,test,test,761,"Issue with running using docker; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). Issue encountered during running with Docker, thinking it is possibly due to tf not supported by m1 chip, here is the issue. . The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. qemu: uncaught target signal 6 (Aborted) - core dumped. **Setup**. - Operating system: MacOs (Mac mini/ m1 chip). - DeepVariant version:1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). The test data from GitHub. **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/545
https://github.com/google/deepvariant/issues/545:823,safety,Error,Error,823,"Issue with running using docker; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). Issue encountered during running with Docker, thinking it is possibly due to tf not supported by m1 chip, here is the issue. . The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. qemu: uncaught target signal 6 (Aborted) - core dumped. **Setup**. - Operating system: MacOs (Mac mini/ m1 chip). - DeepVariant version:1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). The test data from GitHub. **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/545
https://github.com/google/deepvariant/issues/545:876,safety,test,test,876,"Issue with running using docker; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). Issue encountered during running with Docker, thinking it is possibly due to tf not supported by m1 chip, here is the issue. . The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. qemu: uncaught target signal 6 (Aborted) - core dumped. **Setup**. - Operating system: MacOs (Mac mini/ m1 chip). - DeepVariant version:1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). The test data from GitHub. **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/545
https://github.com/google/deepvariant/issues/545:912,safety,test,test,912,"Issue with running using docker; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). Issue encountered during running with Docker, thinking it is possibly due to tf not supported by m1 chip, here is the issue. . The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. qemu: uncaught target signal 6 (Aborted) - core dumped. **Setup**. - Operating system: MacOs (Mac mini/ m1 chip). - DeepVariant version:1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). The test data from GitHub. **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/545
https://github.com/google/deepvariant/issues/545:412,security,availab,available,412,"Issue with running using docker; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). Issue encountered during running with Docker, thinking it is possibly due to tf not supported by m1 chip, here is the issue. . The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. qemu: uncaught target signal 6 (Aborted) - core dumped. **Setup**. - Operating system: MacOs (Mac mini/ m1 chip). - DeepVariant version:1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). The test data from GitHub. **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/545
https://github.com/google/deepvariant/issues/545:461,security,sign,signal,461,"Issue with running using docker; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). Issue encountered during running with Docker, thinking it is possibly due to tf not supported by m1 chip, here is the issue. . The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. qemu: uncaught target signal 6 (Aborted) - core dumped. **Setup**. - Operating system: MacOs (Mac mini/ m1 chip). - DeepVariant version:1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). The test data from GitHub. **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/545
https://github.com/google/deepvariant/issues/545:675,testability,instrument,instrument,675,"Issue with running using docker; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). Issue encountered during running with Docker, thinking it is possibly due to tf not supported by m1 chip, here is the issue. . The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. qemu: uncaught target signal 6 (Aborted) - core dumped. **Setup**. - Operating system: MacOs (Mac mini/ m1 chip). - DeepVariant version:1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). The test data from GitHub. **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/545
https://github.com/google/deepvariant/issues/545:761,testability,test,test,761,"Issue with running using docker; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). Issue encountered during running with Docker, thinking it is possibly due to tf not supported by m1 chip, here is the issue. . The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. qemu: uncaught target signal 6 (Aborted) - core dumped. **Setup**. - Operating system: MacOs (Mac mini/ m1 chip). - DeepVariant version:1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). The test data from GitHub. **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/545
https://github.com/google/deepvariant/issues/545:829,testability,trace,trace,829,"Issue with running using docker; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). Issue encountered during running with Docker, thinking it is possibly due to tf not supported by m1 chip, here is the issue. . The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. qemu: uncaught target signal 6 (Aborted) - core dumped. **Setup**. - Operating system: MacOs (Mac mini/ m1 chip). - DeepVariant version:1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). The test data from GitHub. **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/545
https://github.com/google/deepvariant/issues/545:876,testability,test,test,876,"Issue with running using docker; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). Issue encountered during running with Docker, thinking it is possibly due to tf not supported by m1 chip, here is the issue. . The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. qemu: uncaught target signal 6 (Aborted) - core dumped. **Setup**. - Operating system: MacOs (Mac mini/ m1 chip). - DeepVariant version:1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). The test data from GitHub. **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/545
https://github.com/google/deepvariant/issues/545:912,testability,test,test,912,"Issue with running using docker; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). Issue encountered during running with Docker, thinking it is possibly due to tf not supported by m1 chip, here is the issue. . The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. qemu: uncaught target signal 6 (Aborted) - core dumped. **Setup**. - Operating system: MacOs (Mac mini/ m1 chip). - DeepVariant version:1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). The test data from GitHub. **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/545
https://github.com/google/deepvariant/issues/545:1087,testability,context,context,1087,"Issue with running using docker; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). Issue encountered during running with Docker, thinking it is possibly due to tf not supported by m1 chip, here is the issue. . The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. qemu: uncaught target signal 6 (Aborted) - core dumped. **Setup**. - Operating system: MacOs (Mac mini/ m1 chip). - DeepVariant version:1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). The test data from GitHub. **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/545
https://github.com/google/deepvariant/issues/545:153,usability,clear,clear,153,"Issue with running using docker; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). Issue encountered during running with Docker, thinking it is possibly due to tf not supported by m1 chip, here is the issue. . The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. qemu: uncaught target signal 6 (Aborted) - core dumped. **Setup**. - Operating system: MacOs (Mac mini/ m1 chip). - DeepVariant version:1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). The test data from GitHub. **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/545
https://github.com/google/deepvariant/issues/545:291,usability,support,supported,291,"Issue with running using docker; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). Issue encountered during running with Docker, thinking it is possibly due to tf not supported by m1 chip, here is the issue. . The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. qemu: uncaught target signal 6 (Aborted) - core dumped. **Setup**. - Operating system: MacOs (Mac mini/ m1 chip). - DeepVariant version:1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). The test data from GitHub. **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/545
https://github.com/google/deepvariant/issues/545:811,usability,Command,Command,811,"Issue with running using docker; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). Issue encountered during running with Docker, thinking it is possibly due to tf not supported by m1 chip, here is the issue. . The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. qemu: uncaught target signal 6 (Aborted) - core dumped. **Setup**. - Operating system: MacOs (Mac mini/ m1 chip). - DeepVariant version:1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). The test data from GitHub. **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/545
https://github.com/google/deepvariant/issues/545:823,usability,Error,Error,823,"Issue with running using docker; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). Issue encountered during running with Docker, thinking it is possibly due to tf not supported by m1 chip, here is the issue. . The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. qemu: uncaught target signal 6 (Aborted) - core dumped. **Setup**. - Operating system: MacOs (Mac mini/ m1 chip). - DeepVariant version:1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). The test data from GitHub. **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/545
https://github.com/google/deepvariant/issues/546:393,availability,Operat,Operating,393,"Issues with parallelization of singularity-gpu image; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. When using the singularity-gpu version, the make_examples step will only run sequentially (i.e., one shard processed at a time using only a single CPU) no matter what value I supply to ```--num_shards```. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). WES data from the tutorial. **Steps to reproduce:**. - Command:. . ```. #!/usr/bin/env bash. INPUT_DIR=""input"". OUTPUT_DIR=""output"". BIN_VERSION=1.4.0. export TMPDIR=""$PWD/tmp_dir"". singularity run \. --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}""-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=reference/GRCh38_no_alt_analysis_set.fasta \. --reads=""${INPUT_DIR}""/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions=""${INPUT_DIR}""/idt_capture_novogene.grch38.bed \. --output_vcf=""${OUTPUT_DIR}""/HG003.output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/HG003.output.g.vcf.gz \. --intermediate_results_dir=""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=28. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Yes, though only sequentially. **Any additional context:**. Based on the the documentation and by looking at the code, I _assume_ that the value for ```--num_shards``` is supposed to indicate how many chunks of sequence should be processed in parallel by the ```make_examples``` command, but this does not seem to be working for me. Any suggestions or ideas? Thanks! Dave.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/546
https://github.com/google/deepvariant/issues/546:206,deployability,version,version,206,"Issues with parallelization of singularity-gpu image; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. When using the singularity-gpu version, the make_examples step will only run sequentially (i.e., one shard processed at a time using only a single CPU) no matter what value I supply to ```--num_shards```. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). WES data from the tutorial. **Steps to reproduce:**. - Command:. . ```. #!/usr/bin/env bash. INPUT_DIR=""input"". OUTPUT_DIR=""output"". BIN_VERSION=1.4.0. export TMPDIR=""$PWD/tmp_dir"". singularity run \. --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}""-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=reference/GRCh38_no_alt_analysis_set.fasta \. --reads=""${INPUT_DIR}""/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions=""${INPUT_DIR}""/idt_capture_novogene.grch38.bed \. --output_vcf=""${OUTPUT_DIR}""/HG003.output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/HG003.output.g.vcf.gz \. --intermediate_results_dir=""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=28. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Yes, though only sequentially. **Any additional context:**. Based on the the documentation and by looking at the code, I _assume_ that the value for ```--num_shards``` is supposed to indicate how many chunks of sequence should be processed in parallel by the ```make_examples``` command, but this does not seem to be working for me. Any suggestions or ideas? Thanks! Dave.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/546
https://github.com/google/deepvariant/issues/546:435,deployability,version,version,435,"Issues with parallelization of singularity-gpu image; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. When using the singularity-gpu version, the make_examples step will only run sequentially (i.e., one shard processed at a time using only a single CPU) no matter what value I supply to ```--num_shards```. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). WES data from the tutorial. **Steps to reproduce:**. - Command:. . ```. #!/usr/bin/env bash. INPUT_DIR=""input"". OUTPUT_DIR=""output"". BIN_VERSION=1.4.0. export TMPDIR=""$PWD/tmp_dir"". singularity run \. --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}""-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=reference/GRCh38_no_alt_analysis_set.fasta \. --reads=""${INPUT_DIR}""/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions=""${INPUT_DIR}""/idt_capture_novogene.grch38.bed \. --output_vcf=""${OUTPUT_DIR}""/HG003.output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/HG003.output.g.vcf.gz \. --intermediate_results_dir=""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=28. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Yes, though only sequentially. **Any additional context:**. Based on the the documentation and by looking at the code, I _assume_ that the value for ```--num_shards``` is supposed to indicate how many chunks of sequence should be processed in parallel by the ```make_examples``` command, but this does not seem to be working for me. Any suggestions or ideas? Thanks! Dave.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/546
https://github.com/google/deepvariant/issues/546:453,deployability,Instal,Installation,453,"Issues with parallelization of singularity-gpu image; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. When using the singularity-gpu version, the make_examples step will only run sequentially (i.e., one shard processed at a time using only a single CPU) no matter what value I supply to ```--num_shards```. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). WES data from the tutorial. **Steps to reproduce:**. - Command:. . ```. #!/usr/bin/env bash. INPUT_DIR=""input"". OUTPUT_DIR=""output"". BIN_VERSION=1.4.0. export TMPDIR=""$PWD/tmp_dir"". singularity run \. --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}""-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=reference/GRCh38_no_alt_analysis_set.fasta \. --reads=""${INPUT_DIR}""/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions=""${INPUT_DIR}""/idt_capture_novogene.grch38.bed \. --output_vcf=""${OUTPUT_DIR}""/HG003.output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/HG003.output.g.vcf.gz \. --intermediate_results_dir=""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=28. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Yes, though only sequentially. **Any additional context:**. Based on the the documentation and by looking at the code, I _assume_ that the value for ```--num_shards``` is supposed to indicate how many chunks of sequence should be processed in parallel by the ```make_examples``` command, but this does not seem to be working for me. Any suggestions or ideas? Thanks! Dave.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/546
https://github.com/google/deepvariant/issues/546:43,energy efficiency,gpu,gpu,43,"Issues with parallelization of singularity-gpu image; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. When using the singularity-gpu version, the make_examples step will only run sequentially (i.e., one shard processed at a time using only a single CPU) no matter what value I supply to ```--num_shards```. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). WES data from the tutorial. **Steps to reproduce:**. - Command:. . ```. #!/usr/bin/env bash. INPUT_DIR=""input"". OUTPUT_DIR=""output"". BIN_VERSION=1.4.0. export TMPDIR=""$PWD/tmp_dir"". singularity run \. --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}""-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=reference/GRCh38_no_alt_analysis_set.fasta \. --reads=""${INPUT_DIR}""/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions=""${INPUT_DIR}""/idt_capture_novogene.grch38.bed \. --output_vcf=""${OUTPUT_DIR}""/HG003.output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/HG003.output.g.vcf.gz \. --intermediate_results_dir=""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=28. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Yes, though only sequentially. **Any additional context:**. Based on the the documentation and by looking at the code, I _assume_ that the value for ```--num_shards``` is supposed to indicate how many chunks of sequence should be processed in parallel by the ```make_examples``` command, but this does not seem to be working for me. Any suggestions or ideas? Thanks! Dave.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/546
https://github.com/google/deepvariant/issues/546:202,energy efficiency,gpu,gpu,202,"Issues with parallelization of singularity-gpu image; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. When using the singularity-gpu version, the make_examples step will only run sequentially (i.e., one shard processed at a time using only a single CPU) no matter what value I supply to ```--num_shards```. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). WES data from the tutorial. **Steps to reproduce:**. - Command:. . ```. #!/usr/bin/env bash. INPUT_DIR=""input"". OUTPUT_DIR=""output"". BIN_VERSION=1.4.0. export TMPDIR=""$PWD/tmp_dir"". singularity run \. --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}""-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=reference/GRCh38_no_alt_analysis_set.fasta \. --reads=""${INPUT_DIR}""/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions=""${INPUT_DIR}""/idt_capture_novogene.grch38.bed \. --output_vcf=""${OUTPUT_DIR}""/HG003.output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/HG003.output.g.vcf.gz \. --intermediate_results_dir=""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=28. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Yes, though only sequentially. **Any additional context:**. Based on the the documentation and by looking at the code, I _assume_ that the value for ```--num_shards``` is supposed to indicate how many chunks of sequence should be processed in parallel by the ```make_examples``` command, but this does not seem to be working for me. Any suggestions or ideas? Thanks! Dave.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/546
https://github.com/google/deepvariant/issues/546:322,energy efficiency,CPU,CPU,322,"Issues with parallelization of singularity-gpu image; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. When using the singularity-gpu version, the make_examples step will only run sequentially (i.e., one shard processed at a time using only a single CPU) no matter what value I supply to ```--num_shards```. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). WES data from the tutorial. **Steps to reproduce:**. - Command:. . ```. #!/usr/bin/env bash. INPUT_DIR=""input"". OUTPUT_DIR=""output"". BIN_VERSION=1.4.0. export TMPDIR=""$PWD/tmp_dir"". singularity run \. --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}""-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=reference/GRCh38_no_alt_analysis_set.fasta \. --reads=""${INPUT_DIR}""/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions=""${INPUT_DIR}""/idt_capture_novogene.grch38.bed \. --output_vcf=""${OUTPUT_DIR}""/HG003.output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/HG003.output.g.vcf.gz \. --intermediate_results_dir=""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=28. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Yes, though only sequentially. **Any additional context:**. Based on the the documentation and by looking at the code, I _assume_ that the value for ```--num_shards``` is supposed to indicate how many chunks of sequence should be processed in parallel by the ```make_examples``` command, but this does not seem to be working for me. Any suggestions or ideas? Thanks! Dave.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/546
https://github.com/google/deepvariant/issues/546:922,energy efficiency,gpu,gpu,922,"Issues with parallelization of singularity-gpu image; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. When using the singularity-gpu version, the make_examples step will only run sequentially (i.e., one shard processed at a time using only a single CPU) no matter what value I supply to ```--num_shards```. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). WES data from the tutorial. **Steps to reproduce:**. - Command:. . ```. #!/usr/bin/env bash. INPUT_DIR=""input"". OUTPUT_DIR=""output"". BIN_VERSION=1.4.0. export TMPDIR=""$PWD/tmp_dir"". singularity run \. --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}""-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=reference/GRCh38_no_alt_analysis_set.fasta \. --reads=""${INPUT_DIR}""/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions=""${INPUT_DIR}""/idt_capture_novogene.grch38.bed \. --output_vcf=""${OUTPUT_DIR}""/HG003.output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/HG003.output.g.vcf.gz \. --intermediate_results_dir=""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=28. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Yes, though only sequentially. **Any additional context:**. Based on the the documentation and by looking at the code, I _assume_ that the value for ```--num_shards``` is supposed to indicate how many chunks of sequence should be processed in parallel by the ```make_examples``` command, but this does not seem to be working for me. Any suggestions or ideas? Thanks! Dave.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/546
https://github.com/google/deepvariant/issues/546:206,integrability,version,version,206,"Issues with parallelization of singularity-gpu image; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. When using the singularity-gpu version, the make_examples step will only run sequentially (i.e., one shard processed at a time using only a single CPU) no matter what value I supply to ```--num_shards```. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). WES data from the tutorial. **Steps to reproduce:**. - Command:. . ```. #!/usr/bin/env bash. INPUT_DIR=""input"". OUTPUT_DIR=""output"". BIN_VERSION=1.4.0. export TMPDIR=""$PWD/tmp_dir"". singularity run \. --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}""-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=reference/GRCh38_no_alt_analysis_set.fasta \. --reads=""${INPUT_DIR}""/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions=""${INPUT_DIR}""/idt_capture_novogene.grch38.bed \. --output_vcf=""${OUTPUT_DIR}""/HG003.output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/HG003.output.g.vcf.gz \. --intermediate_results_dir=""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=28. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Yes, though only sequentially. **Any additional context:**. Based on the the documentation and by looking at the code, I _assume_ that the value for ```--num_shards``` is supposed to indicate how many chunks of sequence should be processed in parallel by the ```make_examples``` command, but this does not seem to be working for me. Any suggestions or ideas? Thanks! Dave.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/546
https://github.com/google/deepvariant/issues/546:435,integrability,version,version,435,"Issues with parallelization of singularity-gpu image; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. When using the singularity-gpu version, the make_examples step will only run sequentially (i.e., one shard processed at a time using only a single CPU) no matter what value I supply to ```--num_shards```. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). WES data from the tutorial. **Steps to reproduce:**. - Command:. . ```. #!/usr/bin/env bash. INPUT_DIR=""input"". OUTPUT_DIR=""output"". BIN_VERSION=1.4.0. export TMPDIR=""$PWD/tmp_dir"". singularity run \. --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}""-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=reference/GRCh38_no_alt_analysis_set.fasta \. --reads=""${INPUT_DIR}""/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions=""${INPUT_DIR}""/idt_capture_novogene.grch38.bed \. --output_vcf=""${OUTPUT_DIR}""/HG003.output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/HG003.output.g.vcf.gz \. --intermediate_results_dir=""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=28. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Yes, though only sequentially. **Any additional context:**. Based on the the documentation and by looking at the code, I _assume_ that the value for ```--num_shards``` is supposed to indicate how many chunks of sequence should be processed in parallel by the ```make_examples``` command, but this does not seem to be working for me. Any suggestions or ideas? Thanks! Dave.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/546
https://github.com/google/deepvariant/issues/546:206,modifiability,version,version,206,"Issues with parallelization of singularity-gpu image; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. When using the singularity-gpu version, the make_examples step will only run sequentially (i.e., one shard processed at a time using only a single CPU) no matter what value I supply to ```--num_shards```. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). WES data from the tutorial. **Steps to reproduce:**. - Command:. . ```. #!/usr/bin/env bash. INPUT_DIR=""input"". OUTPUT_DIR=""output"". BIN_VERSION=1.4.0. export TMPDIR=""$PWD/tmp_dir"". singularity run \. --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}""-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=reference/GRCh38_no_alt_analysis_set.fasta \. --reads=""${INPUT_DIR}""/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions=""${INPUT_DIR}""/idt_capture_novogene.grch38.bed \. --output_vcf=""${OUTPUT_DIR}""/HG003.output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/HG003.output.g.vcf.gz \. --intermediate_results_dir=""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=28. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Yes, though only sequentially. **Any additional context:**. Based on the the documentation and by looking at the code, I _assume_ that the value for ```--num_shards``` is supposed to indicate how many chunks of sequence should be processed in parallel by the ```make_examples``` command, but this does not seem to be working for me. Any suggestions or ideas? Thanks! Dave.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/546
https://github.com/google/deepvariant/issues/546:435,modifiability,version,version,435,"Issues with parallelization of singularity-gpu image; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. When using the singularity-gpu version, the make_examples step will only run sequentially (i.e., one shard processed at a time using only a single CPU) no matter what value I supply to ```--num_shards```. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). WES data from the tutorial. **Steps to reproduce:**. - Command:. . ```. #!/usr/bin/env bash. INPUT_DIR=""input"". OUTPUT_DIR=""output"". BIN_VERSION=1.4.0. export TMPDIR=""$PWD/tmp_dir"". singularity run \. --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}""-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=reference/GRCh38_no_alt_analysis_set.fasta \. --reads=""${INPUT_DIR}""/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions=""${INPUT_DIR}""/idt_capture_novogene.grch38.bed \. --output_vcf=""${OUTPUT_DIR}""/HG003.output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/HG003.output.g.vcf.gz \. --intermediate_results_dir=""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=28. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Yes, though only sequentially. **Any additional context:**. Based on the the documentation and by looking at the code, I _assume_ that the value for ```--num_shards``` is supposed to indicate how many chunks of sequence should be processed in parallel by the ```make_examples``` command, but this does not seem to be working for me. Any suggestions or ideas? Thanks! Dave.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/546
https://github.com/google/deepvariant/issues/546:12,performance,parallel,parallelization,12,"Issues with parallelization of singularity-gpu image; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. When using the singularity-gpu version, the make_examples step will only run sequentially (i.e., one shard processed at a time using only a single CPU) no matter what value I supply to ```--num_shards```. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). WES data from the tutorial. **Steps to reproduce:**. - Command:. . ```. #!/usr/bin/env bash. INPUT_DIR=""input"". OUTPUT_DIR=""output"". BIN_VERSION=1.4.0. export TMPDIR=""$PWD/tmp_dir"". singularity run \. --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}""-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=reference/GRCh38_no_alt_analysis_set.fasta \. --reads=""${INPUT_DIR}""/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions=""${INPUT_DIR}""/idt_capture_novogene.grch38.bed \. --output_vcf=""${OUTPUT_DIR}""/HG003.output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/HG003.output.g.vcf.gz \. --intermediate_results_dir=""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=28. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Yes, though only sequentially. **Any additional context:**. Based on the the documentation and by looking at the code, I _assume_ that the value for ```--num_shards``` is supposed to indicate how many chunks of sequence should be processed in parallel by the ```make_examples``` command, but this does not seem to be working for me. Any suggestions or ideas? Thanks! Dave.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/546
https://github.com/google/deepvariant/issues/546:43,performance,gpu,gpu,43,"Issues with parallelization of singularity-gpu image; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. When using the singularity-gpu version, the make_examples step will only run sequentially (i.e., one shard processed at a time using only a single CPU) no matter what value I supply to ```--num_shards```. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). WES data from the tutorial. **Steps to reproduce:**. - Command:. . ```. #!/usr/bin/env bash. INPUT_DIR=""input"". OUTPUT_DIR=""output"". BIN_VERSION=1.4.0. export TMPDIR=""$PWD/tmp_dir"". singularity run \. --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}""-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=reference/GRCh38_no_alt_analysis_set.fasta \. --reads=""${INPUT_DIR}""/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions=""${INPUT_DIR}""/idt_capture_novogene.grch38.bed \. --output_vcf=""${OUTPUT_DIR}""/HG003.output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/HG003.output.g.vcf.gz \. --intermediate_results_dir=""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=28. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Yes, though only sequentially. **Any additional context:**. Based on the the documentation and by looking at the code, I _assume_ that the value for ```--num_shards``` is supposed to indicate how many chunks of sequence should be processed in parallel by the ```make_examples``` command, but this does not seem to be working for me. Any suggestions or ideas? Thanks! Dave.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/546
https://github.com/google/deepvariant/issues/546:202,performance,gpu,gpu,202,"Issues with parallelization of singularity-gpu image; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. When using the singularity-gpu version, the make_examples step will only run sequentially (i.e., one shard processed at a time using only a single CPU) no matter what value I supply to ```--num_shards```. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). WES data from the tutorial. **Steps to reproduce:**. - Command:. . ```. #!/usr/bin/env bash. INPUT_DIR=""input"". OUTPUT_DIR=""output"". BIN_VERSION=1.4.0. export TMPDIR=""$PWD/tmp_dir"". singularity run \. --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}""-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=reference/GRCh38_no_alt_analysis_set.fasta \. --reads=""${INPUT_DIR}""/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions=""${INPUT_DIR}""/idt_capture_novogene.grch38.bed \. --output_vcf=""${OUTPUT_DIR}""/HG003.output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/HG003.output.g.vcf.gz \. --intermediate_results_dir=""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=28. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Yes, though only sequentially. **Any additional context:**. Based on the the documentation and by looking at the code, I _assume_ that the value for ```--num_shards``` is supposed to indicate how many chunks of sequence should be processed in parallel by the ```make_examples``` command, but this does not seem to be working for me. Any suggestions or ideas? Thanks! Dave.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/546
https://github.com/google/deepvariant/issues/546:297,performance,time,time,297,"Issues with parallelization of singularity-gpu image; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. When using the singularity-gpu version, the make_examples step will only run sequentially (i.e., one shard processed at a time using only a single CPU) no matter what value I supply to ```--num_shards```. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). WES data from the tutorial. **Steps to reproduce:**. - Command:. . ```. #!/usr/bin/env bash. INPUT_DIR=""input"". OUTPUT_DIR=""output"". BIN_VERSION=1.4.0. export TMPDIR=""$PWD/tmp_dir"". singularity run \. --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}""-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=reference/GRCh38_no_alt_analysis_set.fasta \. --reads=""${INPUT_DIR}""/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions=""${INPUT_DIR}""/idt_capture_novogene.grch38.bed \. --output_vcf=""${OUTPUT_DIR}""/HG003.output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/HG003.output.g.vcf.gz \. --intermediate_results_dir=""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=28. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Yes, though only sequentially. **Any additional context:**. Based on the the documentation and by looking at the code, I _assume_ that the value for ```--num_shards``` is supposed to indicate how many chunks of sequence should be processed in parallel by the ```make_examples``` command, but this does not seem to be working for me. Any suggestions or ideas? Thanks! Dave.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/546
https://github.com/google/deepvariant/issues/546:322,performance,CPU,CPU,322,"Issues with parallelization of singularity-gpu image; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. When using the singularity-gpu version, the make_examples step will only run sequentially (i.e., one shard processed at a time using only a single CPU) no matter what value I supply to ```--num_shards```. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). WES data from the tutorial. **Steps to reproduce:**. - Command:. . ```. #!/usr/bin/env bash. INPUT_DIR=""input"". OUTPUT_DIR=""output"". BIN_VERSION=1.4.0. export TMPDIR=""$PWD/tmp_dir"". singularity run \. --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}""-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=reference/GRCh38_no_alt_analysis_set.fasta \. --reads=""${INPUT_DIR}""/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions=""${INPUT_DIR}""/idt_capture_novogene.grch38.bed \. --output_vcf=""${OUTPUT_DIR}""/HG003.output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/HG003.output.g.vcf.gz \. --intermediate_results_dir=""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=28. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Yes, though only sequentially. **Any additional context:**. Based on the the documentation and by looking at the code, I _assume_ that the value for ```--num_shards``` is supposed to indicate how many chunks of sequence should be processed in parallel by the ```make_examples``` command, but this does not seem to be working for me. Any suggestions or ideas? Thanks! Dave.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/546
https://github.com/google/deepvariant/issues/546:922,performance,gpu,gpu,922,"Issues with parallelization of singularity-gpu image; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. When using the singularity-gpu version, the make_examples step will only run sequentially (i.e., one shard processed at a time using only a single CPU) no matter what value I supply to ```--num_shards```. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). WES data from the tutorial. **Steps to reproduce:**. - Command:. . ```. #!/usr/bin/env bash. INPUT_DIR=""input"". OUTPUT_DIR=""output"". BIN_VERSION=1.4.0. export TMPDIR=""$PWD/tmp_dir"". singularity run \. --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}""-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=reference/GRCh38_no_alt_analysis_set.fasta \. --reads=""${INPUT_DIR}""/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions=""${INPUT_DIR}""/idt_capture_novogene.grch38.bed \. --output_vcf=""${OUTPUT_DIR}""/HG003.output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/HG003.output.g.vcf.gz \. --intermediate_results_dir=""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=28. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Yes, though only sequentially. **Any additional context:**. Based on the the documentation and by looking at the code, I _assume_ that the value for ```--num_shards``` is supposed to indicate how many chunks of sequence should be processed in parallel by the ```make_examples``` command, but this does not seem to be working for me. Any suggestions or ideas? Thanks! Dave.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/546
https://github.com/google/deepvariant/issues/546:1758,performance,parallel,parallel,1758,"Issues with parallelization of singularity-gpu image; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. When using the singularity-gpu version, the make_examples step will only run sequentially (i.e., one shard processed at a time using only a single CPU) no matter what value I supply to ```--num_shards```. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). WES data from the tutorial. **Steps to reproduce:**. - Command:. . ```. #!/usr/bin/env bash. INPUT_DIR=""input"". OUTPUT_DIR=""output"". BIN_VERSION=1.4.0. export TMPDIR=""$PWD/tmp_dir"". singularity run \. --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}""-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=reference/GRCh38_no_alt_analysis_set.fasta \. --reads=""${INPUT_DIR}""/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions=""${INPUT_DIR}""/idt_capture_novogene.grch38.bed \. --output_vcf=""${OUTPUT_DIR}""/HG003.output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/HG003.output.g.vcf.gz \. --intermediate_results_dir=""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=28. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Yes, though only sequentially. **Any additional context:**. Based on the the documentation and by looking at the code, I _assume_ that the value for ```--num_shards``` is supposed to indicate how many chunks of sequence should be processed in parallel by the ```make_examples``` command, but this does not seem to be working for me. Any suggestions or ideas? Thanks! Dave.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/546
https://github.com/google/deepvariant/issues/546:1366,reliability,Doe,Does,1366,"Issues with parallelization of singularity-gpu image; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. When using the singularity-gpu version, the make_examples step will only run sequentially (i.e., one shard processed at a time using only a single CPU) no matter what value I supply to ```--num_shards```. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). WES data from the tutorial. **Steps to reproduce:**. - Command:. . ```. #!/usr/bin/env bash. INPUT_DIR=""input"". OUTPUT_DIR=""output"". BIN_VERSION=1.4.0. export TMPDIR=""$PWD/tmp_dir"". singularity run \. --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}""-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=reference/GRCh38_no_alt_analysis_set.fasta \. --reads=""${INPUT_DIR}""/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions=""${INPUT_DIR}""/idt_capture_novogene.grch38.bed \. --output_vcf=""${OUTPUT_DIR}""/HG003.output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/HG003.output.g.vcf.gz \. --intermediate_results_dir=""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=28. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Yes, though only sequentially. **Any additional context:**. Based on the the documentation and by looking at the code, I _assume_ that the value for ```--num_shards``` is supposed to indicate how many chunks of sequence should be processed in parallel by the ```make_examples``` command, but this does not seem to be working for me. Any suggestions or ideas? Thanks! Dave.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/546
https://github.com/google/deepvariant/issues/546:1812,reliability,doe,does,1812,"Issues with parallelization of singularity-gpu image; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. When using the singularity-gpu version, the make_examples step will only run sequentially (i.e., one shard processed at a time using only a single CPU) no matter what value I supply to ```--num_shards```. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). WES data from the tutorial. **Steps to reproduce:**. - Command:. . ```. #!/usr/bin/env bash. INPUT_DIR=""input"". OUTPUT_DIR=""output"". BIN_VERSION=1.4.0. export TMPDIR=""$PWD/tmp_dir"". singularity run \. --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}""-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=reference/GRCh38_no_alt_analysis_set.fasta \. --reads=""${INPUT_DIR}""/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions=""${INPUT_DIR}""/idt_capture_novogene.grch38.bed \. --output_vcf=""${OUTPUT_DIR}""/HG003.output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/HG003.output.g.vcf.gz \. --intermediate_results_dir=""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=28. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Yes, though only sequentially. **Any additional context:**. Based on the the documentation and by looking at the code, I _assume_ that the value for ```--num_shards``` is supposed to indicate how many chunks of sequence should be processed in parallel by the ```make_examples``` command, but this does not seem to be working for me. Any suggestions or ideas? Thanks! Dave.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/546
https://github.com/google/deepvariant/issues/546:735,safety,input,input,735,"Issues with parallelization of singularity-gpu image; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. When using the singularity-gpu version, the make_examples step will only run sequentially (i.e., one shard processed at a time using only a single CPU) no matter what value I supply to ```--num_shards```. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). WES data from the tutorial. **Steps to reproduce:**. - Command:. . ```. #!/usr/bin/env bash. INPUT_DIR=""input"". OUTPUT_DIR=""output"". BIN_VERSION=1.4.0. export TMPDIR=""$PWD/tmp_dir"". singularity run \. --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}""-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=reference/GRCh38_no_alt_analysis_set.fasta \. --reads=""${INPUT_DIR}""/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions=""${INPUT_DIR}""/idt_capture_novogene.grch38.bed \. --output_vcf=""${OUTPUT_DIR}""/HG003.output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/HG003.output.g.vcf.gz \. --intermediate_results_dir=""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=28. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Yes, though only sequentially. **Any additional context:**. Based on the the documentation and by looking at the code, I _assume_ that the value for ```--num_shards``` is supposed to indicate how many chunks of sequence should be processed in parallel by the ```make_examples``` command, but this does not seem to be working for me. Any suggestions or ideas? Thanks! Dave.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/546
https://github.com/google/deepvariant/issues/546:1387,safety,test,test,1387,"Issues with parallelization of singularity-gpu image; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. When using the singularity-gpu version, the make_examples step will only run sequentially (i.e., one shard processed at a time using only a single CPU) no matter what value I supply to ```--num_shards```. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). WES data from the tutorial. **Steps to reproduce:**. - Command:. . ```. #!/usr/bin/env bash. INPUT_DIR=""input"". OUTPUT_DIR=""output"". BIN_VERSION=1.4.0. export TMPDIR=""$PWD/tmp_dir"". singularity run \. --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}""-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=reference/GRCh38_no_alt_analysis_set.fasta \. --reads=""${INPUT_DIR}""/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions=""${INPUT_DIR}""/idt_capture_novogene.grch38.bed \. --output_vcf=""${OUTPUT_DIR}""/HG003.output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/HG003.output.g.vcf.gz \. --intermediate_results_dir=""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=28. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Yes, though only sequentially. **Any additional context:**. Based on the the documentation and by looking at the code, I _assume_ that the value for ```--num_shards``` is supposed to indicate how many chunks of sequence should be processed in parallel by the ```make_examples``` command, but this does not seem to be working for me. Any suggestions or ideas? Thanks! Dave.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/546
https://github.com/google/deepvariant/issues/546:1423,safety,test,test,1423,"Issues with parallelization of singularity-gpu image; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. When using the singularity-gpu version, the make_examples step will only run sequentially (i.e., one shard processed at a time using only a single CPU) no matter what value I supply to ```--num_shards```. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). WES data from the tutorial. **Steps to reproduce:**. - Command:. . ```. #!/usr/bin/env bash. INPUT_DIR=""input"". OUTPUT_DIR=""output"". BIN_VERSION=1.4.0. export TMPDIR=""$PWD/tmp_dir"". singularity run \. --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}""-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=reference/GRCh38_no_alt_analysis_set.fasta \. --reads=""${INPUT_DIR}""/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions=""${INPUT_DIR}""/idt_capture_novogene.grch38.bed \. --output_vcf=""${OUTPUT_DIR}""/HG003.output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/HG003.output.g.vcf.gz \. --intermediate_results_dir=""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=28. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Yes, though only sequentially. **Any additional context:**. Based on the the documentation and by looking at the code, I _assume_ that the value for ```--num_shards``` is supposed to indicate how many chunks of sequence should be processed in parallel by the ```make_examples``` command, but this does not seem to be working for me. Any suggestions or ideas? Thanks! Dave.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/546
https://github.com/google/deepvariant/issues/546:549,testability,instrument,instrument,549,"Issues with parallelization of singularity-gpu image; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. When using the singularity-gpu version, the make_examples step will only run sequentially (i.e., one shard processed at a time using only a single CPU) no matter what value I supply to ```--num_shards```. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). WES data from the tutorial. **Steps to reproduce:**. - Command:. . ```. #!/usr/bin/env bash. INPUT_DIR=""input"". OUTPUT_DIR=""output"". BIN_VERSION=1.4.0. export TMPDIR=""$PWD/tmp_dir"". singularity run \. --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}""-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=reference/GRCh38_no_alt_analysis_set.fasta \. --reads=""${INPUT_DIR}""/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions=""${INPUT_DIR}""/idt_capture_novogene.grch38.bed \. --output_vcf=""${OUTPUT_DIR}""/HG003.output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/HG003.output.g.vcf.gz \. --intermediate_results_dir=""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=28. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Yes, though only sequentially. **Any additional context:**. Based on the the documentation and by looking at the code, I _assume_ that the value for ```--num_shards``` is supposed to indicate how many chunks of sequence should be processed in parallel by the ```make_examples``` command, but this does not seem to be working for me. Any suggestions or ideas? Thanks! Dave.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/546
https://github.com/google/deepvariant/issues/546:1387,testability,test,test,1387,"Issues with parallelization of singularity-gpu image; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. When using the singularity-gpu version, the make_examples step will only run sequentially (i.e., one shard processed at a time using only a single CPU) no matter what value I supply to ```--num_shards```. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). WES data from the tutorial. **Steps to reproduce:**. - Command:. . ```. #!/usr/bin/env bash. INPUT_DIR=""input"". OUTPUT_DIR=""output"". BIN_VERSION=1.4.0. export TMPDIR=""$PWD/tmp_dir"". singularity run \. --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}""-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=reference/GRCh38_no_alt_analysis_set.fasta \. --reads=""${INPUT_DIR}""/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions=""${INPUT_DIR}""/idt_capture_novogene.grch38.bed \. --output_vcf=""${OUTPUT_DIR}""/HG003.output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/HG003.output.g.vcf.gz \. --intermediate_results_dir=""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=28. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Yes, though only sequentially. **Any additional context:**. Based on the the documentation and by looking at the code, I _assume_ that the value for ```--num_shards``` is supposed to indicate how many chunks of sequence should be processed in parallel by the ```make_examples``` command, but this does not seem to be working for me. Any suggestions or ideas? Thanks! Dave.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/546
https://github.com/google/deepvariant/issues/546:1423,testability,test,test,1423,"Issues with parallelization of singularity-gpu image; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. When using the singularity-gpu version, the make_examples step will only run sequentially (i.e., one shard processed at a time using only a single CPU) no matter what value I supply to ```--num_shards```. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). WES data from the tutorial. **Steps to reproduce:**. - Command:. . ```. #!/usr/bin/env bash. INPUT_DIR=""input"". OUTPUT_DIR=""output"". BIN_VERSION=1.4.0. export TMPDIR=""$PWD/tmp_dir"". singularity run \. --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}""-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=reference/GRCh38_no_alt_analysis_set.fasta \. --reads=""${INPUT_DIR}""/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions=""${INPUT_DIR}""/idt_capture_novogene.grch38.bed \. --output_vcf=""${OUTPUT_DIR}""/HG003.output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/HG003.output.g.vcf.gz \. --intermediate_results_dir=""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=28. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Yes, though only sequentially. **Any additional context:**. Based on the the documentation and by looking at the code, I _assume_ that the value for ```--num_shards``` is supposed to indicate how many chunks of sequence should be processed in parallel by the ```make_examples``` command, but this does not seem to be working for me. Any suggestions or ideas? Thanks! Dave.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/546
https://github.com/google/deepvariant/issues/546:1563,testability,context,context,1563,"Issues with parallelization of singularity-gpu image; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. When using the singularity-gpu version, the make_examples step will only run sequentially (i.e., one shard processed at a time using only a single CPU) no matter what value I supply to ```--num_shards```. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). WES data from the tutorial. **Steps to reproduce:**. - Command:. . ```. #!/usr/bin/env bash. INPUT_DIR=""input"". OUTPUT_DIR=""output"". BIN_VERSION=1.4.0. export TMPDIR=""$PWD/tmp_dir"". singularity run \. --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}""-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=reference/GRCh38_no_alt_analysis_set.fasta \. --reads=""${INPUT_DIR}""/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions=""${INPUT_DIR}""/idt_capture_novogene.grch38.bed \. --output_vcf=""${OUTPUT_DIR}""/HG003.output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/HG003.output.g.vcf.gz \. --intermediate_results_dir=""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=28. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Yes, though only sequentially. **Any additional context:**. Based on the the documentation and by looking at the code, I _assume_ that the value for ```--num_shards``` is supposed to indicate how many chunks of sequence should be processed in parallel by the ```make_examples``` command, but this does not seem to be working for me. Any suggestions or ideas? Thanks! Dave.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/546
https://github.com/google/deepvariant/issues/546:686,usability,Command,Command,686,"Issues with parallelization of singularity-gpu image; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. When using the singularity-gpu version, the make_examples step will only run sequentially (i.e., one shard processed at a time using only a single CPU) no matter what value I supply to ```--num_shards```. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). WES data from the tutorial. **Steps to reproduce:**. - Command:. . ```. #!/usr/bin/env bash. INPUT_DIR=""input"". OUTPUT_DIR=""output"". BIN_VERSION=1.4.0. export TMPDIR=""$PWD/tmp_dir"". singularity run \. --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}""-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=reference/GRCh38_no_alt_analysis_set.fasta \. --reads=""${INPUT_DIR}""/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions=""${INPUT_DIR}""/idt_capture_novogene.grch38.bed \. --output_vcf=""${OUTPUT_DIR}""/HG003.output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/HG003.output.g.vcf.gz \. --intermediate_results_dir=""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=28. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Yes, though only sequentially. **Any additional context:**. Based on the the documentation and by looking at the code, I _assume_ that the value for ```--num_shards``` is supposed to indicate how many chunks of sequence should be processed in parallel by the ```make_examples``` command, but this does not seem to be working for me. Any suggestions or ideas? Thanks! Dave.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/546
https://github.com/google/deepvariant/issues/546:735,usability,input,input,735,"Issues with parallelization of singularity-gpu image; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. When using the singularity-gpu version, the make_examples step will only run sequentially (i.e., one shard processed at a time using only a single CPU) no matter what value I supply to ```--num_shards```. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). WES data from the tutorial. **Steps to reproduce:**. - Command:. . ```. #!/usr/bin/env bash. INPUT_DIR=""input"". OUTPUT_DIR=""output"". BIN_VERSION=1.4.0. export TMPDIR=""$PWD/tmp_dir"". singularity run \. --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}""-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=reference/GRCh38_no_alt_analysis_set.fasta \. --reads=""${INPUT_DIR}""/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions=""${INPUT_DIR}""/idt_capture_novogene.grch38.bed \. --output_vcf=""${OUTPUT_DIR}""/HG003.output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/HG003.output.g.vcf.gz \. --intermediate_results_dir=""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=28. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Yes, though only sequentially. **Any additional context:**. Based on the the documentation and by looking at the code, I _assume_ that the value for ```--num_shards``` is supposed to indicate how many chunks of sequence should be processed in parallel by the ```make_examples``` command, but this does not seem to be working for me. Any suggestions or ideas? Thanks! Dave.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/546
https://github.com/google/deepvariant/issues/546:1592,usability,document,documentation,1592,"Issues with parallelization of singularity-gpu image; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. When using the singularity-gpu version, the make_examples step will only run sequentially (i.e., one shard processed at a time using only a single CPU) no matter what value I supply to ```--num_shards```. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). WES data from the tutorial. **Steps to reproduce:**. - Command:. . ```. #!/usr/bin/env bash. INPUT_DIR=""input"". OUTPUT_DIR=""output"". BIN_VERSION=1.4.0. export TMPDIR=""$PWD/tmp_dir"". singularity run \. --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}""-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=reference/GRCh38_no_alt_analysis_set.fasta \. --reads=""${INPUT_DIR}""/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions=""${INPUT_DIR}""/idt_capture_novogene.grch38.bed \. --output_vcf=""${OUTPUT_DIR}""/HG003.output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/HG003.output.g.vcf.gz \. --intermediate_results_dir=""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=28. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Yes, though only sequentially. **Any additional context:**. Based on the the documentation and by looking at the code, I _assume_ that the value for ```--num_shards``` is supposed to indicate how many chunks of sequence should be processed in parallel by the ```make_examples``` command, but this does not seem to be working for me. Any suggestions or ideas? Thanks! Dave.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/546
https://github.com/google/deepvariant/issues/546:1698,usability,indicat,indicate,1698,"Issues with parallelization of singularity-gpu image; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. When using the singularity-gpu version, the make_examples step will only run sequentially (i.e., one shard processed at a time using only a single CPU) no matter what value I supply to ```--num_shards```. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). WES data from the tutorial. **Steps to reproduce:**. - Command:. . ```. #!/usr/bin/env bash. INPUT_DIR=""input"". OUTPUT_DIR=""output"". BIN_VERSION=1.4.0. export TMPDIR=""$PWD/tmp_dir"". singularity run \. --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}""-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=reference/GRCh38_no_alt_analysis_set.fasta \. --reads=""${INPUT_DIR}""/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions=""${INPUT_DIR}""/idt_capture_novogene.grch38.bed \. --output_vcf=""${OUTPUT_DIR}""/HG003.output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/HG003.output.g.vcf.gz \. --intermediate_results_dir=""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=28. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Yes, though only sequentially. **Any additional context:**. Based on the the documentation and by looking at the code, I _assume_ that the value for ```--num_shards``` is supposed to indicate how many chunks of sequence should be processed in parallel by the ```make_examples``` command, but this does not seem to be working for me. Any suggestions or ideas? Thanks! Dave.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/546
https://github.com/google/deepvariant/issues/546:1794,usability,command,command,1794,"Issues with parallelization of singularity-gpu image; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. When using the singularity-gpu version, the make_examples step will only run sequentially (i.e., one shard processed at a time using only a single CPU) no matter what value I supply to ```--num_shards```. **Setup**. - Operating system: CentOS 7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). WES data from the tutorial. **Steps to reproduce:**. - Command:. . ```. #!/usr/bin/env bash. INPUT_DIR=""input"". OUTPUT_DIR=""output"". BIN_VERSION=1.4.0. export TMPDIR=""$PWD/tmp_dir"". singularity run \. --nv -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}""-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=reference/GRCh38_no_alt_analysis_set.fasta \. --reads=""${INPUT_DIR}""/HG003.novaseq.wes_idt.100x.dedup.bam \. --regions=""${INPUT_DIR}""/idt_capture_novogene.grch38.bed \. --output_vcf=""${OUTPUT_DIR}""/HG003.output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/HG003.output.g.vcf.gz \. --intermediate_results_dir=""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=28. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Yes, though only sequentially. **Any additional context:**. Based on the the documentation and by looking at the code, I _assume_ that the value for ```--num_shards``` is supposed to indicate how many chunks of sequence should be processed in parallel by the ```make_examples``` command, but this does not seem to be working for me. Any suggestions or ideas? Thanks! Dave.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/546
https://github.com/google/deepvariant/issues/547:253,availability,consist,consistently,253,"QUAL is systematically lower for heterozygote than homozygote sites; I am running the docker image of deepvariant 1.1.0 to find mutations in PacBio HiFi reads aligned with NGMLR (see command below). The issue I am finding is that QUAL values seem to be consistently lower for heterozygote than homozygote sites (around 3x lower median QUAL for heterozygote sites), which results in differential filtering of these types of variants. Why this happen? . # Command to run docker. docker run -v ""/myfolder/"":/input -v ""/myfolder/"":/output google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/input/pool_founders.bp.p_ctg.fa --reads=/input/sample.ngmlr.sort.bam --output_vcf=/output/sample.deepvariant.vcf --num_shards=16",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/547
https://github.com/google/deepvariant/issues/547:395,integrability,filter,filtering,395,"QUAL is systematically lower for heterozygote than homozygote sites; I am running the docker image of deepvariant 1.1.0 to find mutations in PacBio HiFi reads aligned with NGMLR (see command below). The issue I am finding is that QUAL values seem to be consistently lower for heterozygote than homozygote sites (around 3x lower median QUAL for heterozygote sites), which results in differential filtering of these types of variants. Why this happen? . # Command to run docker. docker run -v ""/myfolder/"":/input -v ""/myfolder/"":/output google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/input/pool_founders.bp.p_ctg.fa --reads=/input/sample.ngmlr.sort.bam --output_vcf=/output/sample.deepvariant.vcf --num_shards=16",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/547
https://github.com/google/deepvariant/issues/547:141,modifiability,Pac,PacBio,141,"QUAL is systematically lower for heterozygote than homozygote sites; I am running the docker image of deepvariant 1.1.0 to find mutations in PacBio HiFi reads aligned with NGMLR (see command below). The issue I am finding is that QUAL values seem to be consistently lower for heterozygote than homozygote sites (around 3x lower median QUAL for heterozygote sites), which results in differential filtering of these types of variants. Why this happen? . # Command to run docker. docker run -v ""/myfolder/"":/input -v ""/myfolder/"":/output google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/input/pool_founders.bp.p_ctg.fa --reads=/input/sample.ngmlr.sort.bam --output_vcf=/output/sample.deepvariant.vcf --num_shards=16",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/547
https://github.com/google/deepvariant/issues/547:610,modifiability,PAC,PACBIO,610,"QUAL is systematically lower for heterozygote than homozygote sites; I am running the docker image of deepvariant 1.1.0 to find mutations in PacBio HiFi reads aligned with NGMLR (see command below). The issue I am finding is that QUAL values seem to be consistently lower for heterozygote than homozygote sites (around 3x lower median QUAL for heterozygote sites), which results in differential filtering of these types of variants. Why this happen? . # Command to run docker. docker run -v ""/myfolder/"":/input -v ""/myfolder/"":/output google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/input/pool_founders.bp.p_ctg.fa --reads=/input/sample.ngmlr.sort.bam --output_vcf=/output/sample.deepvariant.vcf --num_shards=16",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/547
https://github.com/google/deepvariant/issues/547:505,safety,input,input,505,"QUAL is systematically lower for heterozygote than homozygote sites; I am running the docker image of deepvariant 1.1.0 to find mutations in PacBio HiFi reads aligned with NGMLR (see command below). The issue I am finding is that QUAL values seem to be consistently lower for heterozygote than homozygote sites (around 3x lower median QUAL for heterozygote sites), which results in differential filtering of these types of variants. Why this happen? . # Command to run docker. docker run -v ""/myfolder/"":/input -v ""/myfolder/"":/output google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/input/pool_founders.bp.p_ctg.fa --reads=/input/sample.ngmlr.sort.bam --output_vcf=/output/sample.deepvariant.vcf --num_shards=16",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/547
https://github.com/google/deepvariant/issues/547:624,safety,input,input,624,"QUAL is systematically lower for heterozygote than homozygote sites; I am running the docker image of deepvariant 1.1.0 to find mutations in PacBio HiFi reads aligned with NGMLR (see command below). The issue I am finding is that QUAL values seem to be consistently lower for heterozygote than homozygote sites (around 3x lower median QUAL for heterozygote sites), which results in differential filtering of these types of variants. Why this happen? . # Command to run docker. docker run -v ""/myfolder/"":/input -v ""/myfolder/"":/output google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/input/pool_founders.bp.p_ctg.fa --reads=/input/sample.ngmlr.sort.bam --output_vcf=/output/sample.deepvariant.vcf --num_shards=16",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/547
https://github.com/google/deepvariant/issues/547:665,safety,input,input,665,"QUAL is systematically lower for heterozygote than homozygote sites; I am running the docker image of deepvariant 1.1.0 to find mutations in PacBio HiFi reads aligned with NGMLR (see command below). The issue I am finding is that QUAL values seem to be consistently lower for heterozygote than homozygote sites (around 3x lower median QUAL for heterozygote sites), which results in differential filtering of these types of variants. Why this happen? . # Command to run docker. docker run -v ""/myfolder/"":/input -v ""/myfolder/"":/output google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/input/pool_founders.bp.p_ctg.fa --reads=/input/sample.ngmlr.sort.bam --output_vcf=/output/sample.deepvariant.vcf --num_shards=16",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/547
https://github.com/google/deepvariant/issues/547:183,usability,command,command,183,"QUAL is systematically lower for heterozygote than homozygote sites; I am running the docker image of deepvariant 1.1.0 to find mutations in PacBio HiFi reads aligned with NGMLR (see command below). The issue I am finding is that QUAL values seem to be consistently lower for heterozygote than homozygote sites (around 3x lower median QUAL for heterozygote sites), which results in differential filtering of these types of variants. Why this happen? . # Command to run docker. docker run -v ""/myfolder/"":/input -v ""/myfolder/"":/output google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/input/pool_founders.bp.p_ctg.fa --reads=/input/sample.ngmlr.sort.bam --output_vcf=/output/sample.deepvariant.vcf --num_shards=16",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/547
https://github.com/google/deepvariant/issues/547:253,usability,consist,consistently,253,"QUAL is systematically lower for heterozygote than homozygote sites; I am running the docker image of deepvariant 1.1.0 to find mutations in PacBio HiFi reads aligned with NGMLR (see command below). The issue I am finding is that QUAL values seem to be consistently lower for heterozygote than homozygote sites (around 3x lower median QUAL for heterozygote sites), which results in differential filtering of these types of variants. Why this happen? . # Command to run docker. docker run -v ""/myfolder/"":/input -v ""/myfolder/"":/output google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/input/pool_founders.bp.p_ctg.fa --reads=/input/sample.ngmlr.sort.bam --output_vcf=/output/sample.deepvariant.vcf --num_shards=16",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/547
https://github.com/google/deepvariant/issues/547:454,usability,Command,Command,454,"QUAL is systematically lower for heterozygote than homozygote sites; I am running the docker image of deepvariant 1.1.0 to find mutations in PacBio HiFi reads aligned with NGMLR (see command below). The issue I am finding is that QUAL values seem to be consistently lower for heterozygote than homozygote sites (around 3x lower median QUAL for heterozygote sites), which results in differential filtering of these types of variants. Why this happen? . # Command to run docker. docker run -v ""/myfolder/"":/input -v ""/myfolder/"":/output google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/input/pool_founders.bp.p_ctg.fa --reads=/input/sample.ngmlr.sort.bam --output_vcf=/output/sample.deepvariant.vcf --num_shards=16",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/547
https://github.com/google/deepvariant/issues/547:505,usability,input,input,505,"QUAL is systematically lower for heterozygote than homozygote sites; I am running the docker image of deepvariant 1.1.0 to find mutations in PacBio HiFi reads aligned with NGMLR (see command below). The issue I am finding is that QUAL values seem to be consistently lower for heterozygote than homozygote sites (around 3x lower median QUAL for heterozygote sites), which results in differential filtering of these types of variants. Why this happen? . # Command to run docker. docker run -v ""/myfolder/"":/input -v ""/myfolder/"":/output google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/input/pool_founders.bp.p_ctg.fa --reads=/input/sample.ngmlr.sort.bam --output_vcf=/output/sample.deepvariant.vcf --num_shards=16",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/547
https://github.com/google/deepvariant/issues/547:624,usability,input,input,624,"QUAL is systematically lower for heterozygote than homozygote sites; I am running the docker image of deepvariant 1.1.0 to find mutations in PacBio HiFi reads aligned with NGMLR (see command below). The issue I am finding is that QUAL values seem to be consistently lower for heterozygote than homozygote sites (around 3x lower median QUAL for heterozygote sites), which results in differential filtering of these types of variants. Why this happen? . # Command to run docker. docker run -v ""/myfolder/"":/input -v ""/myfolder/"":/output google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/input/pool_founders.bp.p_ctg.fa --reads=/input/sample.ngmlr.sort.bam --output_vcf=/output/sample.deepvariant.vcf --num_shards=16",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/547
https://github.com/google/deepvariant/issues/547:665,usability,input,input,665,"QUAL is systematically lower for heterozygote than homozygote sites; I am running the docker image of deepvariant 1.1.0 to find mutations in PacBio HiFi reads aligned with NGMLR (see command below). The issue I am finding is that QUAL values seem to be consistently lower for heterozygote than homozygote sites (around 3x lower median QUAL for heterozygote sites), which results in differential filtering of these types of variants. Why this happen? . # Command to run docker. docker run -v ""/myfolder/"":/input -v ""/myfolder/"":/output google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/input/pool_founders.bp.p_ctg.fa --reads=/input/sample.ngmlr.sort.bam --output_vcf=/output/sample.deepvariant.vcf --num_shards=16",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/547
https://github.com/google/deepvariant/issues/548:286,availability,Operat,Operating,286,"The same script runs successfully on Chr5 but not on the other 4 chromosomes; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. The same script runs successfully on Chr5 but not on the other 4 chromosomes. **Setup**. - Operating system: Debian GNU/Linux 9. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.):. - Type of data: hybrid of Illumina and HiFi data, the reference is the assembly based on the hifi reads. **Steps to reproduce:**. - Command: singularity run --bind ${PWD} \. /software/deepvariant/deepvariant.img \. /opt/deepvariant/bin/run_deepvariant \. --model_type HYBRID_PACBIO_ILLUMINA \. --ref ragtag.fasta \. --reads hifi_illu.bam \. --intermediate_results_dir ./tmp \. --output_vcf rep1.hifi-illu.Chr1.vcf.gz \. --num_shards 4 \. --regions Chr1_RagTag. - Error trace: Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1380, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1363, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1456, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: inflate() failed with error -3: invalid literal/length code. [[{{node IteratorGetNext}}]].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/548
https://github.com/google/deepvariant/issues/548:875,availability,Error,Error,875,"The same script runs successfully on Chr5 but not on the other 4 chromosomes; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. The same script runs successfully on Chr5 but not on the other 4 chromosomes. **Setup**. - Operating system: Debian GNU/Linux 9. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.):. - Type of data: hybrid of Illumina and HiFi data, the reference is the assembly based on the hifi reads. **Steps to reproduce:**. - Command: singularity run --bind ${PWD} \. /software/deepvariant/deepvariant.img \. /opt/deepvariant/bin/run_deepvariant \. --model_type HYBRID_PACBIO_ILLUMINA \. --ref ragtag.fasta \. --reads hifi_illu.bam \. --intermediate_results_dir ./tmp \. --output_vcf rep1.hifi-illu.Chr1.vcf.gz \. --num_shards 4 \. --regions Chr1_RagTag. - Error trace: Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1380, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1363, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1456, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: inflate() failed with error -3: invalid literal/length code. [[{{node IteratorGetNext}}]].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/548
https://github.com/google/deepvariant/issues/548:1491,availability,error,error,1491,"The same script runs successfully on Chr5 but not on the other 4 chromosomes; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. The same script runs successfully on Chr5 but not on the other 4 chromosomes. **Setup**. - Operating system: Debian GNU/Linux 9. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.):. - Type of data: hybrid of Illumina and HiFi data, the reference is the assembly based on the hifi reads. **Steps to reproduce:**. - Command: singularity run --bind ${PWD} \. /software/deepvariant/deepvariant.img \. /opt/deepvariant/bin/run_deepvariant \. --model_type HYBRID_PACBIO_ILLUMINA \. --ref ragtag.fasta \. --reads hifi_illu.bam \. --intermediate_results_dir ./tmp \. --output_vcf rep1.hifi-illu.Chr1.vcf.gz \. --num_shards 4 \. --regions Chr1_RagTag. - Error trace: Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1380, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1363, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1456, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: inflate() failed with error -3: invalid literal/length code. [[{{node IteratorGetNext}}]].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/548
https://github.com/google/deepvariant/issues/548:338,deployability,version,version,338,"The same script runs successfully on Chr5 but not on the other 4 chromosomes; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. The same script runs successfully on Chr5 but not on the other 4 chromosomes. **Setup**. - Operating system: Debian GNU/Linux 9. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.):. - Type of data: hybrid of Illumina and HiFi data, the reference is the assembly based on the hifi reads. **Steps to reproduce:**. - Command: singularity run --bind ${PWD} \. /software/deepvariant/deepvariant.img \. /opt/deepvariant/bin/run_deepvariant \. --model_type HYBRID_PACBIO_ILLUMINA \. --ref ragtag.fasta \. --reads hifi_illu.bam \. --intermediate_results_dir ./tmp \. --output_vcf rep1.hifi-illu.Chr1.vcf.gz \. --num_shards 4 \. --regions Chr1_RagTag. - Error trace: Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1380, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1363, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1456, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: inflate() failed with error -3: invalid literal/length code. [[{{node IteratorGetNext}}]].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/548
https://github.com/google/deepvariant/issues/548:356,deployability,Instal,Installation,356,"The same script runs successfully on Chr5 but not on the other 4 chromosomes; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. The same script runs successfully on Chr5 but not on the other 4 chromosomes. **Setup**. - Operating system: Debian GNU/Linux 9. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.):. - Type of data: hybrid of Illumina and HiFi data, the reference is the assembly based on the hifi reads. **Steps to reproduce:**. - Command: singularity run --bind ${PWD} \. /software/deepvariant/deepvariant.img \. /opt/deepvariant/bin/run_deepvariant \. --model_type HYBRID_PACBIO_ILLUMINA \. --ref ragtag.fasta \. --reads hifi_illu.bam \. --intermediate_results_dir ./tmp \. --output_vcf rep1.hifi-illu.Chr1.vcf.gz \. --num_shards 4 \. --regions Chr1_RagTag. - Error trace: Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1380, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1363, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1456, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: inflate() failed with error -3: invalid literal/length code. [[{{node IteratorGetNext}}]].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/548
https://github.com/google/deepvariant/issues/548:1479,deployability,fail,failed,1479,"The same script runs successfully on Chr5 but not on the other 4 chromosomes; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. The same script runs successfully on Chr5 but not on the other 4 chromosomes. **Setup**. - Operating system: Debian GNU/Linux 9. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.):. - Type of data: hybrid of Illumina and HiFi data, the reference is the assembly based on the hifi reads. **Steps to reproduce:**. - Command: singularity run --bind ${PWD} \. /software/deepvariant/deepvariant.img \. /opt/deepvariant/bin/run_deepvariant \. --model_type HYBRID_PACBIO_ILLUMINA \. --ref ragtag.fasta \. --reads hifi_illu.bam \. --intermediate_results_dir ./tmp \. --output_vcf rep1.hifi-illu.Chr1.vcf.gz \. --num_shards 4 \. --regions Chr1_RagTag. - Error trace: Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1380, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1363, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1456, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: inflate() failed with error -3: invalid literal/length code. [[{{node IteratorGetNext}}]].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/548
https://github.com/google/deepvariant/issues/548:338,integrability,version,version,338,"The same script runs successfully on Chr5 but not on the other 4 chromosomes; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. The same script runs successfully on Chr5 but not on the other 4 chromosomes. **Setup**. - Operating system: Debian GNU/Linux 9. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.):. - Type of data: hybrid of Illumina and HiFi data, the reference is the assembly based on the hifi reads. **Steps to reproduce:**. - Command: singularity run --bind ${PWD} \. /software/deepvariant/deepvariant.img \. /opt/deepvariant/bin/run_deepvariant \. --model_type HYBRID_PACBIO_ILLUMINA \. --ref ragtag.fasta \. --reads hifi_illu.bam \. --intermediate_results_dir ./tmp \. --output_vcf rep1.hifi-illu.Chr1.vcf.gz \. --num_shards 4 \. --regions Chr1_RagTag. - Error trace: Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1380, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1363, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1456, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: inflate() failed with error -3: invalid literal/length code. [[{{node IteratorGetNext}}]].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/548
https://github.com/google/deepvariant/issues/548:571,interoperability,bind,bind,571,"The same script runs successfully on Chr5 but not on the other 4 chromosomes; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. The same script runs successfully on Chr5 but not on the other 4 chromosomes. **Setup**. - Operating system: Debian GNU/Linux 9. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.):. - Type of data: hybrid of Illumina and HiFi data, the reference is the assembly based on the hifi reads. **Steps to reproduce:**. - Command: singularity run --bind ${PWD} \. /software/deepvariant/deepvariant.img \. /opt/deepvariant/bin/run_deepvariant \. --model_type HYBRID_PACBIO_ILLUMINA \. --ref ragtag.fasta \. --reads hifi_illu.bam \. --intermediate_results_dir ./tmp \. --output_vcf rep1.hifi-illu.Chr1.vcf.gz \. --num_shards 4 \. --regions Chr1_RagTag. - Error trace: Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1380, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1363, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1456, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: inflate() failed with error -3: invalid literal/length code. [[{{node IteratorGetNext}}]].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/548
https://github.com/google/deepvariant/issues/548:338,modifiability,version,version,338,"The same script runs successfully on Chr5 but not on the other 4 chromosomes; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. The same script runs successfully on Chr5 but not on the other 4 chromosomes. **Setup**. - Operating system: Debian GNU/Linux 9. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.):. - Type of data: hybrid of Illumina and HiFi data, the reference is the assembly based on the hifi reads. **Steps to reproduce:**. - Command: singularity run --bind ${PWD} \. /software/deepvariant/deepvariant.img \. /opt/deepvariant/bin/run_deepvariant \. --model_type HYBRID_PACBIO_ILLUMINA \. --ref ragtag.fasta \. --reads hifi_illu.bam \. --intermediate_results_dir ./tmp \. --output_vcf rep1.hifi-illu.Chr1.vcf.gz \. --num_shards 4 \. --regions Chr1_RagTag. - Error trace: Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1380, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1363, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1456, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: inflate() failed with error -3: invalid literal/length code. [[{{node IteratorGetNext}}]].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/548
https://github.com/google/deepvariant/issues/548:571,modifiability,bind,bind,571,"The same script runs successfully on Chr5 but not on the other 4 chromosomes; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. The same script runs successfully on Chr5 but not on the other 4 chromosomes. **Setup**. - Operating system: Debian GNU/Linux 9. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.):. - Type of data: hybrid of Illumina and HiFi data, the reference is the assembly based on the hifi reads. **Steps to reproduce:**. - Command: singularity run --bind ${PWD} \. /software/deepvariant/deepvariant.img \. /opt/deepvariant/bin/run_deepvariant \. --model_type HYBRID_PACBIO_ILLUMINA \. --ref ragtag.fasta \. --reads hifi_illu.bam \. --intermediate_results_dir ./tmp \. --output_vcf rep1.hifi-illu.Chr1.vcf.gz \. --num_shards 4 \. --regions Chr1_RagTag. - Error trace: Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1380, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1363, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1456, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: inflate() failed with error -3: invalid literal/length code. [[{{node IteratorGetNext}}]].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/548
https://github.com/google/deepvariant/issues/548:960,modifiability,pac,packages,960,"The same script runs successfully on Chr5 but not on the other 4 chromosomes; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. The same script runs successfully on Chr5 but not on the other 4 chromosomes. **Setup**. - Operating system: Debian GNU/Linux 9. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.):. - Type of data: hybrid of Illumina and HiFi data, the reference is the assembly based on the hifi reads. **Steps to reproduce:**. - Command: singularity run --bind ${PWD} \. /software/deepvariant/deepvariant.img \. /opt/deepvariant/bin/run_deepvariant \. --model_type HYBRID_PACBIO_ILLUMINA \. --ref ragtag.fasta \. --reads hifi_illu.bam \. --intermediate_results_dir ./tmp \. --output_vcf rep1.hifi-illu.Chr1.vcf.gz \. --num_shards 4 \. --regions Chr1_RagTag. - Error trace: Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1380, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1363, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1456, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: inflate() failed with error -3: invalid literal/length code. [[{{node IteratorGetNext}}]].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/548
https://github.com/google/deepvariant/issues/548:1085,modifiability,pac,packages,1085,"The same script runs successfully on Chr5 but not on the other 4 chromosomes; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. The same script runs successfully on Chr5 but not on the other 4 chromosomes. **Setup**. - Operating system: Debian GNU/Linux 9. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.):. - Type of data: hybrid of Illumina and HiFi data, the reference is the assembly based on the hifi reads. **Steps to reproduce:**. - Command: singularity run --bind ${PWD} \. /software/deepvariant/deepvariant.img \. /opt/deepvariant/bin/run_deepvariant \. --model_type HYBRID_PACBIO_ILLUMINA \. --ref ragtag.fasta \. --reads hifi_illu.bam \. --intermediate_results_dir ./tmp \. --output_vcf rep1.hifi-illu.Chr1.vcf.gz \. --num_shards 4 \. --regions Chr1_RagTag. - Error trace: Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1380, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1363, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1456, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: inflate() failed with error -3: invalid literal/length code. [[{{node IteratorGetNext}}]].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/548
https://github.com/google/deepvariant/issues/548:1256,modifiability,pac,packages,1256,"The same script runs successfully on Chr5 but not on the other 4 chromosomes; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. The same script runs successfully on Chr5 but not on the other 4 chromosomes. **Setup**. - Operating system: Debian GNU/Linux 9. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.):. - Type of data: hybrid of Illumina and HiFi data, the reference is the assembly based on the hifi reads. **Steps to reproduce:**. - Command: singularity run --bind ${PWD} \. /software/deepvariant/deepvariant.img \. /opt/deepvariant/bin/run_deepvariant \. --model_type HYBRID_PACBIO_ILLUMINA \. --ref ragtag.fasta \. --reads hifi_illu.bam \. --intermediate_results_dir ./tmp \. --output_vcf rep1.hifi-illu.Chr1.vcf.gz \. --num_shards 4 \. --regions Chr1_RagTag. - Error trace: Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1380, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1363, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1456, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: inflate() failed with error -3: invalid literal/length code. [[{{node IteratorGetNext}}]].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/548
https://github.com/google/deepvariant/issues/548:875,performance,Error,Error,875,"The same script runs successfully on Chr5 but not on the other 4 chromosomes; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. The same script runs successfully on Chr5 but not on the other 4 chromosomes. **Setup**. - Operating system: Debian GNU/Linux 9. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.):. - Type of data: hybrid of Illumina and HiFi data, the reference is the assembly based on the hifi reads. **Steps to reproduce:**. - Command: singularity run --bind ${PWD} \. /software/deepvariant/deepvariant.img \. /opt/deepvariant/bin/run_deepvariant \. --model_type HYBRID_PACBIO_ILLUMINA \. --ref ragtag.fasta \. --reads hifi_illu.bam \. --intermediate_results_dir ./tmp \. --output_vcf rep1.hifi-illu.Chr1.vcf.gz \. --num_shards 4 \. --regions Chr1_RagTag. - Error trace: Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1380, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1363, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1456, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: inflate() failed with error -3: invalid literal/length code. [[{{node IteratorGetNext}}]].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/548
https://github.com/google/deepvariant/issues/548:1491,performance,error,error,1491,"The same script runs successfully on Chr5 but not on the other 4 chromosomes; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. The same script runs successfully on Chr5 but not on the other 4 chromosomes. **Setup**. - Operating system: Debian GNU/Linux 9. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.):. - Type of data: hybrid of Illumina and HiFi data, the reference is the assembly based on the hifi reads. **Steps to reproduce:**. - Command: singularity run --bind ${PWD} \. /software/deepvariant/deepvariant.img \. /opt/deepvariant/bin/run_deepvariant \. --model_type HYBRID_PACBIO_ILLUMINA \. --ref ragtag.fasta \. --reads hifi_illu.bam \. --intermediate_results_dir ./tmp \. --output_vcf rep1.hifi-illu.Chr1.vcf.gz \. --num_shards 4 \. --regions Chr1_RagTag. - Error trace: Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1380, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1363, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1456, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: inflate() failed with error -3: invalid literal/length code. [[{{node IteratorGetNext}}]].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/548
https://github.com/google/deepvariant/issues/548:1479,reliability,fail,failed,1479,"The same script runs successfully on Chr5 but not on the other 4 chromosomes; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. The same script runs successfully on Chr5 but not on the other 4 chromosomes. **Setup**. - Operating system: Debian GNU/Linux 9. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.):. - Type of data: hybrid of Illumina and HiFi data, the reference is the assembly based on the hifi reads. **Steps to reproduce:**. - Command: singularity run --bind ${PWD} \. /software/deepvariant/deepvariant.img \. /opt/deepvariant/bin/run_deepvariant \. --model_type HYBRID_PACBIO_ILLUMINA \. --ref ragtag.fasta \. --reads hifi_illu.bam \. --intermediate_results_dir ./tmp \. --output_vcf rep1.hifi-illu.Chr1.vcf.gz \. --num_shards 4 \. --regions Chr1_RagTag. - Error trace: Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1380, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1363, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1456, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: inflate() failed with error -3: invalid literal/length code. [[{{node IteratorGetNext}}]].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/548
https://github.com/google/deepvariant/issues/548:875,safety,Error,Error,875,"The same script runs successfully on Chr5 but not on the other 4 chromosomes; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. The same script runs successfully on Chr5 but not on the other 4 chromosomes. **Setup**. - Operating system: Debian GNU/Linux 9. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.):. - Type of data: hybrid of Illumina and HiFi data, the reference is the assembly based on the hifi reads. **Steps to reproduce:**. - Command: singularity run --bind ${PWD} \. /software/deepvariant/deepvariant.img \. /opt/deepvariant/bin/run_deepvariant \. --model_type HYBRID_PACBIO_ILLUMINA \. --ref ragtag.fasta \. --reads hifi_illu.bam \. --intermediate_results_dir ./tmp \. --output_vcf rep1.hifi-illu.Chr1.vcf.gz \. --num_shards 4 \. --regions Chr1_RagTag. - Error trace: Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1380, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1363, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1456, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: inflate() failed with error -3: invalid literal/length code. [[{{node IteratorGetNext}}]].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/548
https://github.com/google/deepvariant/issues/548:1491,safety,error,error,1491,"The same script runs successfully on Chr5 but not on the other 4 chromosomes; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. The same script runs successfully on Chr5 but not on the other 4 chromosomes. **Setup**. - Operating system: Debian GNU/Linux 9. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.):. - Type of data: hybrid of Illumina and HiFi data, the reference is the assembly based on the hifi reads. **Steps to reproduce:**. - Command: singularity run --bind ${PWD} \. /software/deepvariant/deepvariant.img \. /opt/deepvariant/bin/run_deepvariant \. --model_type HYBRID_PACBIO_ILLUMINA \. --ref ragtag.fasta \. --reads hifi_illu.bam \. --intermediate_results_dir ./tmp \. --output_vcf rep1.hifi-illu.Chr1.vcf.gz \. --num_shards 4 \. --regions Chr1_RagTag. - Error trace: Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1380, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1363, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1456, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: inflate() failed with error -3: invalid literal/length code. [[{{node IteratorGetNext}}]].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/548
https://github.com/google/deepvariant/issues/548:994,security,session,session,994,"The same script runs successfully on Chr5 but not on the other 4 chromosomes; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. The same script runs successfully on Chr5 but not on the other 4 chromosomes. **Setup**. - Operating system: Debian GNU/Linux 9. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.):. - Type of data: hybrid of Illumina and HiFi data, the reference is the assembly based on the hifi reads. **Steps to reproduce:**. - Command: singularity run --bind ${PWD} \. /software/deepvariant/deepvariant.img \. /opt/deepvariant/bin/run_deepvariant \. --model_type HYBRID_PACBIO_ILLUMINA \. --ref ragtag.fasta \. --reads hifi_illu.bam \. --intermediate_results_dir ./tmp \. --output_vcf rep1.hifi-illu.Chr1.vcf.gz \. --num_shards 4 \. --regions Chr1_RagTag. - Error trace: Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1380, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1363, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1456, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: inflate() failed with error -3: invalid literal/length code. [[{{node IteratorGetNext}}]].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/548
https://github.com/google/deepvariant/issues/548:1119,security,session,session,1119,"The same script runs successfully on Chr5 but not on the other 4 chromosomes; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. The same script runs successfully on Chr5 but not on the other 4 chromosomes. **Setup**. - Operating system: Debian GNU/Linux 9. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.):. - Type of data: hybrid of Illumina and HiFi data, the reference is the assembly based on the hifi reads. **Steps to reproduce:**. - Command: singularity run --bind ${PWD} \. /software/deepvariant/deepvariant.img \. /opt/deepvariant/bin/run_deepvariant \. --model_type HYBRID_PACBIO_ILLUMINA \. --ref ragtag.fasta \. --reads hifi_illu.bam \. --intermediate_results_dir ./tmp \. --output_vcf rep1.hifi-illu.Chr1.vcf.gz \. --num_shards 4 \. --regions Chr1_RagTag. - Error trace: Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1380, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1363, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1456, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: inflate() failed with error -3: invalid literal/length code. [[{{node IteratorGetNext}}]].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/548
https://github.com/google/deepvariant/issues/548:1290,security,session,session,1290,"The same script runs successfully on Chr5 but not on the other 4 chromosomes; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. The same script runs successfully on Chr5 but not on the other 4 chromosomes. **Setup**. - Operating system: Debian GNU/Linux 9. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.):. - Type of data: hybrid of Illumina and HiFi data, the reference is the assembly based on the hifi reads. **Steps to reproduce:**. - Command: singularity run --bind ${PWD} \. /software/deepvariant/deepvariant.img \. /opt/deepvariant/bin/run_deepvariant \. --model_type HYBRID_PACBIO_ILLUMINA \. --ref ragtag.fasta \. --reads hifi_illu.bam \. --intermediate_results_dir ./tmp \. --output_vcf rep1.hifi-illu.Chr1.vcf.gz \. --num_shards 4 \. --regions Chr1_RagTag. - Error trace: Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1380, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1363, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1456, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: inflate() failed with error -3: invalid literal/length code. [[{{node IteratorGetNext}}]].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/548
https://github.com/google/deepvariant/issues/548:881,testability,trace,trace,881,"The same script runs successfully on Chr5 but not on the other 4 chromosomes; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. The same script runs successfully on Chr5 but not on the other 4 chromosomes. **Setup**. - Operating system: Debian GNU/Linux 9. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.):. - Type of data: hybrid of Illumina and HiFi data, the reference is the assembly based on the hifi reads. **Steps to reproduce:**. - Command: singularity run --bind ${PWD} \. /software/deepvariant/deepvariant.img \. /opt/deepvariant/bin/run_deepvariant \. --model_type HYBRID_PACBIO_ILLUMINA \. --ref ragtag.fasta \. --reads hifi_illu.bam \. --intermediate_results_dir ./tmp \. --output_vcf rep1.hifi-illu.Chr1.vcf.gz \. --num_shards 4 \. --regions Chr1_RagTag. - Error trace: Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1380, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1363, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1456, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: inflate() failed with error -3: invalid literal/length code. [[{{node IteratorGetNext}}]].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/548
https://github.com/google/deepvariant/issues/548:888,testability,Trace,Traceback,888,"The same script runs successfully on Chr5 but not on the other 4 chromosomes; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. The same script runs successfully on Chr5 but not on the other 4 chromosomes. **Setup**. - Operating system: Debian GNU/Linux 9. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.):. - Type of data: hybrid of Illumina and HiFi data, the reference is the assembly based on the hifi reads. **Steps to reproduce:**. - Command: singularity run --bind ${PWD} \. /software/deepvariant/deepvariant.img \. /opt/deepvariant/bin/run_deepvariant \. --model_type HYBRID_PACBIO_ILLUMINA \. --ref ragtag.fasta \. --reads hifi_illu.bam \. --intermediate_results_dir ./tmp \. --output_vcf rep1.hifi-illu.Chr1.vcf.gz \. --num_shards 4 \. --regions Chr1_RagTag. - Error trace: Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1380, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1363, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1456, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: inflate() failed with error -3: invalid literal/length code. [[{{node IteratorGetNext}}]].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/548
https://github.com/google/deepvariant/issues/548:544,usability,Command,Command,544,"The same script runs successfully on Chr5 but not on the other 4 chromosomes; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. The same script runs successfully on Chr5 but not on the other 4 chromosomes. **Setup**. - Operating system: Debian GNU/Linux 9. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.):. - Type of data: hybrid of Illumina and HiFi data, the reference is the assembly based on the hifi reads. **Steps to reproduce:**. - Command: singularity run --bind ${PWD} \. /software/deepvariant/deepvariant.img \. /opt/deepvariant/bin/run_deepvariant \. --model_type HYBRID_PACBIO_ILLUMINA \. --ref ragtag.fasta \. --reads hifi_illu.bam \. --intermediate_results_dir ./tmp \. --output_vcf rep1.hifi-illu.Chr1.vcf.gz \. --num_shards 4 \. --regions Chr1_RagTag. - Error trace: Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1380, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1363, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1456, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: inflate() failed with error -3: invalid literal/length code. [[{{node IteratorGetNext}}]].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/548
https://github.com/google/deepvariant/issues/548:875,usability,Error,Error,875,"The same script runs successfully on Chr5 but not on the other 4 chromosomes; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. The same script runs successfully on Chr5 but not on the other 4 chromosomes. **Setup**. - Operating system: Debian GNU/Linux 9. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.):. - Type of data: hybrid of Illumina and HiFi data, the reference is the assembly based on the hifi reads. **Steps to reproduce:**. - Command: singularity run --bind ${PWD} \. /software/deepvariant/deepvariant.img \. /opt/deepvariant/bin/run_deepvariant \. --model_type HYBRID_PACBIO_ILLUMINA \. --ref ragtag.fasta \. --reads hifi_illu.bam \. --intermediate_results_dir ./tmp \. --output_vcf rep1.hifi-illu.Chr1.vcf.gz \. --num_shards 4 \. --regions Chr1_RagTag. - Error trace: Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1380, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1363, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1456, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: inflate() failed with error -3: invalid literal/length code. [[{{node IteratorGetNext}}]].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/548
https://github.com/google/deepvariant/issues/548:1491,usability,error,error,1491,"The same script runs successfully on Chr5 but not on the other 4 chromosomes; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. The same script runs successfully on Chr5 but not on the other 4 chromosomes. **Setup**. - Operating system: Debian GNU/Linux 9. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.):. - Type of data: hybrid of Illumina and HiFi data, the reference is the assembly based on the hifi reads. **Steps to reproduce:**. - Command: singularity run --bind ${PWD} \. /software/deepvariant/deepvariant.img \. /opt/deepvariant/bin/run_deepvariant \. --model_type HYBRID_PACBIO_ILLUMINA \. --ref ragtag.fasta \. --reads hifi_illu.bam \. --intermediate_results_dir ./tmp \. --output_vcf rep1.hifi-illu.Chr1.vcf.gz \. --num_shards 4 \. --regions Chr1_RagTag. - Error trace: Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1380, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1363, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1456, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: inflate() failed with error -3: invalid literal/length code. [[{{node IteratorGetNext}}]].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/548
https://github.com/google/deepvariant/issues/550:248,availability,Operat,Operating,248,"Quick Start Creates Files as Root, Now I can't delete them.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS Linux release 7.9.2009. - DeepVariant version: deepvariant:0.9.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - Illumina, HG38, standard capture panel. **Steps to reproduce:**. - Command: Snakemake command:. - docker --rm -v {params.input_dir}/:/input -v {params.output_dir}/{params.sample}_DeepVariant:/output -v /data:/data -v {params.bed_dir}:/bed --user $CURRENT_UID google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/{params.sample}.bam --regions=/bed/{params.primary_bed} --output_vcf=/output/{params.sample}_DeepVariant.vcf.gz --output_gvcf=/output/{params.sample}_DeepVariant.gvcf.gz --num_shards=12. - actual command (XXXXX = removed for security purposes) . - docker --rm -v XXXXXXXXX/gatk_align_metrics_t/:/input -v XXXXXXXXX/deep_variant2/xGENIDTn2_DeepVariant:/output -v /XXXXXXXXX/deepvariant/data:/data -v XXXXXXXXX/bed:/bed google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quickstart creates files as root. As it's a high performance computing cluster, I am no longer able to delete these files. How",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/550
https://github.com/google/deepvariant/issues/550:1618,availability,Error,Error,1618," checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS Linux release 7.9.2009. - DeepVariant version: deepvariant:0.9.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - Illumina, HG38, standard capture panel. **Steps to reproduce:**. - Command: Snakemake command:. - docker --rm -v {params.input_dir}/:/input -v {params.output_dir}/{params.sample}_DeepVariant:/output -v /data:/data -v {params.bed_dir}:/bed --user $CURRENT_UID google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/{params.sample}.bam --regions=/bed/{params.primary_bed} --output_vcf=/output/{params.sample}_DeepVariant.vcf.gz --output_gvcf=/output/{params.sample}_DeepVariant.gvcf.gz --num_shards=12. - actual command (XXXXX = removed for security purposes) . - docker --rm -v XXXXXXXXX/gatk_align_metrics_t/:/input -v XXXXXXXXX/deep_variant2/xGENIDTn2_DeepVariant:/output -v /XXXXXXXXX/deepvariant/data:/data -v XXXXXXXXX/bed:/bed google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quickstart creates files as root. As it's a high performance computing cluster, I am no longer able to delete these files. How do I stop it from creating files as root? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/550
https://github.com/google/deepvariant/issues/550:1945,availability,cluster,cluster,1945," checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS Linux release 7.9.2009. - DeepVariant version: deepvariant:0.9.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - Illumina, HG38, standard capture panel. **Steps to reproduce:**. - Command: Snakemake command:. - docker --rm -v {params.input_dir}/:/input -v {params.output_dir}/{params.sample}_DeepVariant:/output -v /data:/data -v {params.bed_dir}:/bed --user $CURRENT_UID google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/{params.sample}.bam --regions=/bed/{params.primary_bed} --output_vcf=/output/{params.sample}_DeepVariant.vcf.gz --output_gvcf=/output/{params.sample}_DeepVariant.gvcf.gz --num_shards=12. - actual command (XXXXX = removed for security purposes) . - docker --rm -v XXXXXXXXX/gatk_align_metrics_t/:/input -v XXXXXXXXX/deep_variant2/xGENIDTn2_DeepVariant:/output -v /XXXXXXXXX/deepvariant/data:/data -v XXXXXXXXX/bed:/bed google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quickstart creates files as root. As it's a high performance computing cluster, I am no longer able to delete these files. How do I stop it from creating files as root? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/550
https://github.com/google/deepvariant/issues/550:279,deployability,releas,release,279,"Quick Start Creates Files as Root, Now I can't delete them.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS Linux release 7.9.2009. - DeepVariant version: deepvariant:0.9.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - Illumina, HG38, standard capture panel. **Steps to reproduce:**. - Command: Snakemake command:. - docker --rm -v {params.input_dir}/:/input -v {params.output_dir}/{params.sample}_DeepVariant:/output -v /data:/data -v {params.bed_dir}:/bed --user $CURRENT_UID google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/{params.sample}.bam --regions=/bed/{params.primary_bed} --output_vcf=/output/{params.sample}_DeepVariant.vcf.gz --output_gvcf=/output/{params.sample}_DeepVariant.gvcf.gz --num_shards=12. - actual command (XXXXX = removed for security purposes) . - docker --rm -v XXXXXXXXX/gatk_align_metrics_t/:/input -v XXXXXXXXX/deep_variant2/xGENIDTn2_DeepVariant:/output -v /XXXXXXXXX/deepvariant/data:/data -v XXXXXXXXX/bed:/bed google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quickstart creates files as root. As it's a high performance computing cluster, I am no longer able to delete these files. How",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/550
https://github.com/google/deepvariant/issues/550:311,deployability,version,version,311,"Quick Start Creates Files as Root, Now I can't delete them.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS Linux release 7.9.2009. - DeepVariant version: deepvariant:0.9.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - Illumina, HG38, standard capture panel. **Steps to reproduce:**. - Command: Snakemake command:. - docker --rm -v {params.input_dir}/:/input -v {params.output_dir}/{params.sample}_DeepVariant:/output -v /data:/data -v {params.bed_dir}:/bed --user $CURRENT_UID google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/{params.sample}.bam --regions=/bed/{params.primary_bed} --output_vcf=/output/{params.sample}_DeepVariant.vcf.gz --output_gvcf=/output/{params.sample}_DeepVariant.gvcf.gz --num_shards=12. - actual command (XXXXX = removed for security purposes) . - docker --rm -v XXXXXXXXX/gatk_align_metrics_t/:/input -v XXXXXXXXX/deep_variant2/xGENIDTn2_DeepVariant:/output -v /XXXXXXXXX/deepvariant/data:/data -v XXXXXXXXX/bed:/bed google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quickstart creates files as root. As it's a high performance computing cluster, I am no longer able to delete these files. How",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/550
https://github.com/google/deepvariant/issues/550:341,deployability,Instal,Installation,341,"Quick Start Creates Files as Root, Now I can't delete them.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS Linux release 7.9.2009. - DeepVariant version: deepvariant:0.9.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - Illumina, HG38, standard capture panel. **Steps to reproduce:**. - Command: Snakemake command:. - docker --rm -v {params.input_dir}/:/input -v {params.output_dir}/{params.sample}_DeepVariant:/output -v /data:/data -v {params.bed_dir}:/bed --user $CURRENT_UID google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/{params.sample}.bam --regions=/bed/{params.primary_bed} --output_vcf=/output/{params.sample}_DeepVariant.vcf.gz --output_gvcf=/output/{params.sample}_DeepVariant.gvcf.gz --num_shards=12. - actual command (XXXXX = removed for security purposes) . - docker --rm -v XXXXXXXXX/gatk_align_metrics_t/:/input -v XXXXXXXXX/deep_variant2/xGENIDTn2_DeepVariant:/output -v /XXXXXXXXX/deepvariant/data:/data -v XXXXXXXXX/bed:/bed google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quickstart creates files as root. As it's a high performance computing cluster, I am no longer able to delete these files. How",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/550
https://github.com/google/deepvariant/issues/550:1945,deployability,cluster,cluster,1945," checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS Linux release 7.9.2009. - DeepVariant version: deepvariant:0.9.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - Illumina, HG38, standard capture panel. **Steps to reproduce:**. - Command: Snakemake command:. - docker --rm -v {params.input_dir}/:/input -v {params.output_dir}/{params.sample}_DeepVariant:/output -v /data:/data -v {params.bed_dir}:/bed --user $CURRENT_UID google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/{params.sample}.bam --regions=/bed/{params.primary_bed} --output_vcf=/output/{params.sample}_DeepVariant.vcf.gz --output_gvcf=/output/{params.sample}_DeepVariant.gvcf.gz --num_shards=12. - actual command (XXXXX = removed for security purposes) . - docker --rm -v XXXXXXXXX/gatk_align_metrics_t/:/input -v XXXXXXXXX/deep_variant2/xGENIDTn2_DeepVariant:/output -v /XXXXXXXXX/deepvariant/data:/data -v XXXXXXXXX/bed:/bed google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quickstart creates files as root. As it's a high performance computing cluster, I am no longer able to delete these files. How do I stop it from creating files as root? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/550
https://github.com/google/deepvariant/issues/550:311,integrability,version,version,311,"Quick Start Creates Files as Root, Now I can't delete them.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS Linux release 7.9.2009. - DeepVariant version: deepvariant:0.9.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - Illumina, HG38, standard capture panel. **Steps to reproduce:**. - Command: Snakemake command:. - docker --rm -v {params.input_dir}/:/input -v {params.output_dir}/{params.sample}_DeepVariant:/output -v /data:/data -v {params.bed_dir}:/bed --user $CURRENT_UID google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/{params.sample}.bam --regions=/bed/{params.primary_bed} --output_vcf=/output/{params.sample}_DeepVariant.vcf.gz --output_gvcf=/output/{params.sample}_DeepVariant.gvcf.gz --num_shards=12. - actual command (XXXXX = removed for security purposes) . - docker --rm -v XXXXXXXXX/gatk_align_metrics_t/:/input -v XXXXXXXXX/deep_variant2/xGENIDTn2_DeepVariant:/output -v /XXXXXXXXX/deepvariant/data:/data -v XXXXXXXXX/bed:/bed google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quickstart creates files as root. As it's a high performance computing cluster, I am no longer able to delete these files. How",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/550
https://github.com/google/deepvariant/issues/550:532,interoperability,standard,standard,532,"Quick Start Creates Files as Root, Now I can't delete them.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS Linux release 7.9.2009. - DeepVariant version: deepvariant:0.9.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - Illumina, HG38, standard capture panel. **Steps to reproduce:**. - Command: Snakemake command:. - docker --rm -v {params.input_dir}/:/input -v {params.output_dir}/{params.sample}_DeepVariant:/output -v /data:/data -v {params.bed_dir}:/bed --user $CURRENT_UID google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/{params.sample}.bam --regions=/bed/{params.primary_bed} --output_vcf=/output/{params.sample}_DeepVariant.vcf.gz --output_gvcf=/output/{params.sample}_DeepVariant.gvcf.gz --num_shards=12. - actual command (XXXXX = removed for security purposes) . - docker --rm -v XXXXXXXXX/gatk_align_metrics_t/:/input -v XXXXXXXXX/deep_variant2/xGENIDTn2_DeepVariant:/output -v /XXXXXXXXX/deepvariant/data:/data -v XXXXXXXXX/bed:/bed google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quickstart creates files as root. As it's a high performance computing cluster, I am no longer able to delete these files. How",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/550
https://github.com/google/deepvariant/issues/550:311,modifiability,version,version,311,"Quick Start Creates Files as Root, Now I can't delete them.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS Linux release 7.9.2009. - DeepVariant version: deepvariant:0.9.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - Illumina, HG38, standard capture panel. **Steps to reproduce:**. - Command: Snakemake command:. - docker --rm -v {params.input_dir}/:/input -v {params.output_dir}/{params.sample}_DeepVariant:/output -v /data:/data -v {params.bed_dir}:/bed --user $CURRENT_UID google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/{params.sample}.bam --regions=/bed/{params.primary_bed} --output_vcf=/output/{params.sample}_DeepVariant.vcf.gz --output_gvcf=/output/{params.sample}_DeepVariant.gvcf.gz --num_shards=12. - actual command (XXXXX = removed for security purposes) . - docker --rm -v XXXXXXXXX/gatk_align_metrics_t/:/input -v XXXXXXXXX/deep_variant2/xGENIDTn2_DeepVariant:/output -v /XXXXXXXXX/deepvariant/data:/data -v XXXXXXXXX/bed:/bed google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quickstart creates files as root. As it's a high performance computing cluster, I am no longer able to delete these files. How",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/550
https://github.com/google/deepvariant/issues/550:1618,performance,Error,Error,1618," checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS Linux release 7.9.2009. - DeepVariant version: deepvariant:0.9.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - Illumina, HG38, standard capture panel. **Steps to reproduce:**. - Command: Snakemake command:. - docker --rm -v {params.input_dir}/:/input -v {params.output_dir}/{params.sample}_DeepVariant:/output -v /data:/data -v {params.bed_dir}:/bed --user $CURRENT_UID google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/{params.sample}.bam --regions=/bed/{params.primary_bed} --output_vcf=/output/{params.sample}_DeepVariant.vcf.gz --output_gvcf=/output/{params.sample}_DeepVariant.gvcf.gz --num_shards=12. - actual command (XXXXX = removed for security purposes) . - docker --rm -v XXXXXXXXX/gatk_align_metrics_t/:/input -v XXXXXXXXX/deep_variant2/xGENIDTn2_DeepVariant:/output -v /XXXXXXXXX/deepvariant/data:/data -v XXXXXXXXX/bed:/bed google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quickstart creates files as root. As it's a high performance computing cluster, I am no longer able to delete these files. How do I stop it from creating files as root? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/550
https://github.com/google/deepvariant/issues/550:1923,performance,perform,performance,1923," checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS Linux release 7.9.2009. - DeepVariant version: deepvariant:0.9.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - Illumina, HG38, standard capture panel. **Steps to reproduce:**. - Command: Snakemake command:. - docker --rm -v {params.input_dir}/:/input -v {params.output_dir}/{params.sample}_DeepVariant:/output -v /data:/data -v {params.bed_dir}:/bed --user $CURRENT_UID google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/{params.sample}.bam --regions=/bed/{params.primary_bed} --output_vcf=/output/{params.sample}_DeepVariant.vcf.gz --output_gvcf=/output/{params.sample}_DeepVariant.gvcf.gz --num_shards=12. - actual command (XXXXX = removed for security purposes) . - docker --rm -v XXXXXXXXX/gatk_align_metrics_t/:/input -v XXXXXXXXX/deep_variant2/xGENIDTn2_DeepVariant:/output -v /XXXXXXXXX/deepvariant/data:/data -v XXXXXXXXX/bed:/bed google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quickstart creates files as root. As it's a high performance computing cluster, I am no longer able to delete these files. How do I stop it from creating files as root? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/550
https://github.com/google/deepvariant/issues/550:1650,reliability,Doe,Does,1650," checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS Linux release 7.9.2009. - DeepVariant version: deepvariant:0.9.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - Illumina, HG38, standard capture panel. **Steps to reproduce:**. - Command: Snakemake command:. - docker --rm -v {params.input_dir}/:/input -v {params.output_dir}/{params.sample}_DeepVariant:/output -v /data:/data -v {params.bed_dir}:/bed --user $CURRENT_UID google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/{params.sample}.bam --regions=/bed/{params.primary_bed} --output_vcf=/output/{params.sample}_DeepVariant.vcf.gz --output_gvcf=/output/{params.sample}_DeepVariant.gvcf.gz --num_shards=12. - actual command (XXXXX = removed for security purposes) . - docker --rm -v XXXXXXXXX/gatk_align_metrics_t/:/input -v XXXXXXXXX/deep_variant2/xGENIDTn2_DeepVariant:/output -v /XXXXXXXXX/deepvariant/data:/data -v XXXXXXXXX/bed:/bed google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quickstart creates files as root. As it's a high performance computing cluster, I am no longer able to delete these files. How do I stop it from creating files as root? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/550
https://github.com/google/deepvariant/issues/550:650,safety,input,input,650,"Quick Start Creates Files as Root, Now I can't delete them.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS Linux release 7.9.2009. - DeepVariant version: deepvariant:0.9.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - Illumina, HG38, standard capture panel. **Steps to reproduce:**. - Command: Snakemake command:. - docker --rm -v {params.input_dir}/:/input -v {params.output_dir}/{params.sample}_DeepVariant:/output -v /data:/data -v {params.bed_dir}:/bed --user $CURRENT_UID google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/{params.sample}.bam --regions=/bed/{params.primary_bed} --output_vcf=/output/{params.sample}_DeepVariant.vcf.gz --output_gvcf=/output/{params.sample}_DeepVariant.gvcf.gz --num_shards=12. - actual command (XXXXX = removed for security purposes) . - docker --rm -v XXXXXXXXX/gatk_align_metrics_t/:/input -v XXXXXXXXX/deep_variant2/xGENIDTn2_DeepVariant:/output -v /XXXXXXXXX/deepvariant/data:/data -v XXXXXXXXX/bed:/bed google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quickstart creates files as root. As it's a high performance computing cluster, I am no longer able to delete these files. How",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/550
https://github.com/google/deepvariant/issues/550:886,safety,input,input,886,"Quick Start Creates Files as Root, Now I can't delete them.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS Linux release 7.9.2009. - DeepVariant version: deepvariant:0.9.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - Illumina, HG38, standard capture panel. **Steps to reproduce:**. - Command: Snakemake command:. - docker --rm -v {params.input_dir}/:/input -v {params.output_dir}/{params.sample}_DeepVariant:/output -v /data:/data -v {params.bed_dir}:/bed --user $CURRENT_UID google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/{params.sample}.bam --regions=/bed/{params.primary_bed} --output_vcf=/output/{params.sample}_DeepVariant.vcf.gz --output_gvcf=/output/{params.sample}_DeepVariant.gvcf.gz --num_shards=12. - actual command (XXXXX = removed for security purposes) . - docker --rm -v XXXXXXXXX/gatk_align_metrics_t/:/input -v XXXXXXXXX/deep_variant2/xGENIDTn2_DeepVariant:/output -v /XXXXXXXXX/deepvariant/data:/data -v XXXXXXXXX/bed:/bed google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quickstart creates files as root. As it's a high performance computing cluster, I am no longer able to delete these files. How",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/550
https://github.com/google/deepvariant/issues/550:1188,safety,input,input,1188," checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS Linux release 7.9.2009. - DeepVariant version: deepvariant:0.9.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - Illumina, HG38, standard capture panel. **Steps to reproduce:**. - Command: Snakemake command:. - docker --rm -v {params.input_dir}/:/input -v {params.output_dir}/{params.sample}_DeepVariant:/output -v /data:/data -v {params.bed_dir}:/bed --user $CURRENT_UID google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/{params.sample}.bam --regions=/bed/{params.primary_bed} --output_vcf=/output/{params.sample}_DeepVariant.vcf.gz --output_gvcf=/output/{params.sample}_DeepVariant.gvcf.gz --num_shards=12. - actual command (XXXXX = removed for security purposes) . - docker --rm -v XXXXXXXXX/gatk_align_metrics_t/:/input -v XXXXXXXXX/deep_variant2/xGENIDTn2_DeepVariant:/output -v /XXXXXXXXX/deepvariant/data:/data -v XXXXXXXXX/bed:/bed google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quickstart creates files as root. As it's a high performance computing cluster, I am no longer able to delete these files. How do I stop it from creating files as root? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/550
https://github.com/google/deepvariant/issues/550:1421,safety,input,input,1421," checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS Linux release 7.9.2009. - DeepVariant version: deepvariant:0.9.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - Illumina, HG38, standard capture panel. **Steps to reproduce:**. - Command: Snakemake command:. - docker --rm -v {params.input_dir}/:/input -v {params.output_dir}/{params.sample}_DeepVariant:/output -v /data:/data -v {params.bed_dir}:/bed --user $CURRENT_UID google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/{params.sample}.bam --regions=/bed/{params.primary_bed} --output_vcf=/output/{params.sample}_DeepVariant.vcf.gz --output_gvcf=/output/{params.sample}_DeepVariant.gvcf.gz --num_shards=12. - actual command (XXXXX = removed for security purposes) . - docker --rm -v XXXXXXXXX/gatk_align_metrics_t/:/input -v XXXXXXXXX/deep_variant2/xGENIDTn2_DeepVariant:/output -v /XXXXXXXXX/deepvariant/data:/data -v XXXXXXXXX/bed:/bed google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quickstart creates files as root. As it's a high performance computing cluster, I am no longer able to delete these files. How do I stop it from creating files as root? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/550
https://github.com/google/deepvariant/issues/550:1618,safety,Error,Error,1618," checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS Linux release 7.9.2009. - DeepVariant version: deepvariant:0.9.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - Illumina, HG38, standard capture panel. **Steps to reproduce:**. - Command: Snakemake command:. - docker --rm -v {params.input_dir}/:/input -v {params.output_dir}/{params.sample}_DeepVariant:/output -v /data:/data -v {params.bed_dir}:/bed --user $CURRENT_UID google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/{params.sample}.bam --regions=/bed/{params.primary_bed} --output_vcf=/output/{params.sample}_DeepVariant.vcf.gz --output_gvcf=/output/{params.sample}_DeepVariant.gvcf.gz --num_shards=12. - actual command (XXXXX = removed for security purposes) . - docker --rm -v XXXXXXXXX/gatk_align_metrics_t/:/input -v XXXXXXXXX/deep_variant2/xGENIDTn2_DeepVariant:/output -v /XXXXXXXXX/deepvariant/data:/data -v XXXXXXXXX/bed:/bed google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quickstart creates files as root. As it's a high performance computing cluster, I am no longer able to delete these files. How do I stop it from creating files as root? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/550
https://github.com/google/deepvariant/issues/550:1671,safety,test,test,1671," checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS Linux release 7.9.2009. - DeepVariant version: deepvariant:0.9.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - Illumina, HG38, standard capture panel. **Steps to reproduce:**. - Command: Snakemake command:. - docker --rm -v {params.input_dir}/:/input -v {params.output_dir}/{params.sample}_DeepVariant:/output -v /data:/data -v {params.bed_dir}:/bed --user $CURRENT_UID google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/{params.sample}.bam --regions=/bed/{params.primary_bed} --output_vcf=/output/{params.sample}_DeepVariant.vcf.gz --output_gvcf=/output/{params.sample}_DeepVariant.gvcf.gz --num_shards=12. - actual command (XXXXX = removed for security purposes) . - docker --rm -v XXXXXXXXX/gatk_align_metrics_t/:/input -v XXXXXXXXX/deep_variant2/xGENIDTn2_DeepVariant:/output -v /XXXXXXXXX/deepvariant/data:/data -v XXXXXXXXX/bed:/bed google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quickstart creates files as root. As it's a high performance computing cluster, I am no longer able to delete these files. How do I stop it from creating files as root? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/550
https://github.com/google/deepvariant/issues/550:1707,safety,test,test,1707," checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS Linux release 7.9.2009. - DeepVariant version: deepvariant:0.9.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - Illumina, HG38, standard capture panel. **Steps to reproduce:**. - Command: Snakemake command:. - docker --rm -v {params.input_dir}/:/input -v {params.output_dir}/{params.sample}_DeepVariant:/output -v /data:/data -v {params.bed_dir}:/bed --user $CURRENT_UID google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/{params.sample}.bam --regions=/bed/{params.primary_bed} --output_vcf=/output/{params.sample}_DeepVariant.vcf.gz --output_gvcf=/output/{params.sample}_DeepVariant.gvcf.gz --num_shards=12. - actual command (XXXXX = removed for security purposes) . - docker --rm -v XXXXXXXXX/gatk_align_metrics_t/:/input -v XXXXXXXXX/deep_variant2/xGENIDTn2_DeepVariant:/output -v /XXXXXXXXX/deepvariant/data:/data -v XXXXXXXXX/bed:/bed google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quickstart creates files as root. As it's a high performance computing cluster, I am no longer able to delete these files. How do I stop it from creating files as root? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/550
https://github.com/google/deepvariant/issues/550:1117,security,secur,security,1117," checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS Linux release 7.9.2009. - DeepVariant version: deepvariant:0.9.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - Illumina, HG38, standard capture panel. **Steps to reproduce:**. - Command: Snakemake command:. - docker --rm -v {params.input_dir}/:/input -v {params.output_dir}/{params.sample}_DeepVariant:/output -v /data:/data -v {params.bed_dir}:/bed --user $CURRENT_UID google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/{params.sample}.bam --regions=/bed/{params.primary_bed} --output_vcf=/output/{params.sample}_DeepVariant.vcf.gz --output_gvcf=/output/{params.sample}_DeepVariant.gvcf.gz --num_shards=12. - actual command (XXXXX = removed for security purposes) . - docker --rm -v XXXXXXXXX/gatk_align_metrics_t/:/input -v XXXXXXXXX/deep_variant2/xGENIDTn2_DeepVariant:/output -v /XXXXXXXXX/deepvariant/data:/data -v XXXXXXXXX/bed:/bed google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quickstart creates files as root. As it's a high performance computing cluster, I am no longer able to delete these files. How do I stop it from creating files as root? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/550
https://github.com/google/deepvariant/issues/550:432,testability,instrument,instrument,432,"Quick Start Creates Files as Root, Now I can't delete them.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS Linux release 7.9.2009. - DeepVariant version: deepvariant:0.9.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - Illumina, HG38, standard capture panel. **Steps to reproduce:**. - Command: Snakemake command:. - docker --rm -v {params.input_dir}/:/input -v {params.output_dir}/{params.sample}_DeepVariant:/output -v /data:/data -v {params.bed_dir}:/bed --user $CURRENT_UID google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/{params.sample}.bam --regions=/bed/{params.primary_bed} --output_vcf=/output/{params.sample}_DeepVariant.vcf.gz --output_gvcf=/output/{params.sample}_DeepVariant.gvcf.gz --num_shards=12. - actual command (XXXXX = removed for security purposes) . - docker --rm -v XXXXXXXXX/gatk_align_metrics_t/:/input -v XXXXXXXXX/deep_variant2/xGENIDTn2_DeepVariant:/output -v /XXXXXXXXX/deepvariant/data:/data -v XXXXXXXXX/bed:/bed google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quickstart creates files as root. As it's a high performance computing cluster, I am no longer able to delete these files. How",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/550
https://github.com/google/deepvariant/issues/550:1624,testability,trace,trace,1624," checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS Linux release 7.9.2009. - DeepVariant version: deepvariant:0.9.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - Illumina, HG38, standard capture panel. **Steps to reproduce:**. - Command: Snakemake command:. - docker --rm -v {params.input_dir}/:/input -v {params.output_dir}/{params.sample}_DeepVariant:/output -v /data:/data -v {params.bed_dir}:/bed --user $CURRENT_UID google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/{params.sample}.bam --regions=/bed/{params.primary_bed} --output_vcf=/output/{params.sample}_DeepVariant.vcf.gz --output_gvcf=/output/{params.sample}_DeepVariant.gvcf.gz --num_shards=12. - actual command (XXXXX = removed for security purposes) . - docker --rm -v XXXXXXXXX/gatk_align_metrics_t/:/input -v XXXXXXXXX/deep_variant2/xGENIDTn2_DeepVariant:/output -v /XXXXXXXXX/deepvariant/data:/data -v XXXXXXXXX/bed:/bed google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quickstart creates files as root. As it's a high performance computing cluster, I am no longer able to delete these files. How do I stop it from creating files as root? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/550
https://github.com/google/deepvariant/issues/550:1671,testability,test,test,1671," checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS Linux release 7.9.2009. - DeepVariant version: deepvariant:0.9.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - Illumina, HG38, standard capture panel. **Steps to reproduce:**. - Command: Snakemake command:. - docker --rm -v {params.input_dir}/:/input -v {params.output_dir}/{params.sample}_DeepVariant:/output -v /data:/data -v {params.bed_dir}:/bed --user $CURRENT_UID google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/{params.sample}.bam --regions=/bed/{params.primary_bed} --output_vcf=/output/{params.sample}_DeepVariant.vcf.gz --output_gvcf=/output/{params.sample}_DeepVariant.gvcf.gz --num_shards=12. - actual command (XXXXX = removed for security purposes) . - docker --rm -v XXXXXXXXX/gatk_align_metrics_t/:/input -v XXXXXXXXX/deep_variant2/xGENIDTn2_DeepVariant:/output -v /XXXXXXXXX/deepvariant/data:/data -v XXXXXXXXX/bed:/bed google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quickstart creates files as root. As it's a high performance computing cluster, I am no longer able to delete these files. How do I stop it from creating files as root? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/550
https://github.com/google/deepvariant/issues/550:1707,testability,test,test,1707," checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS Linux release 7.9.2009. - DeepVariant version: deepvariant:0.9.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - Illumina, HG38, standard capture panel. **Steps to reproduce:**. - Command: Snakemake command:. - docker --rm -v {params.input_dir}/:/input -v {params.output_dir}/{params.sample}_DeepVariant:/output -v /data:/data -v {params.bed_dir}:/bed --user $CURRENT_UID google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/{params.sample}.bam --regions=/bed/{params.primary_bed} --output_vcf=/output/{params.sample}_DeepVariant.vcf.gz --output_gvcf=/output/{params.sample}_DeepVariant.gvcf.gz --num_shards=12. - actual command (XXXXX = removed for security purposes) . - docker --rm -v XXXXXXXXX/gatk_align_metrics_t/:/input -v XXXXXXXXX/deep_variant2/xGENIDTn2_DeepVariant:/output -v /XXXXXXXXX/deepvariant/data:/data -v XXXXXXXXX/bed:/bed google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quickstart creates files as root. As it's a high performance computing cluster, I am no longer able to delete these files. How do I stop it from creating files as root? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/550
https://github.com/google/deepvariant/issues/550:2060,testability,context,context,2060," checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS Linux release 7.9.2009. - DeepVariant version: deepvariant:0.9.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - Illumina, HG38, standard capture panel. **Steps to reproduce:**. - Command: Snakemake command:. - docker --rm -v {params.input_dir}/:/input -v {params.output_dir}/{params.sample}_DeepVariant:/output -v /data:/data -v {params.bed_dir}:/bed --user $CURRENT_UID google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/{params.sample}.bam --regions=/bed/{params.primary_bed} --output_vcf=/output/{params.sample}_DeepVariant.vcf.gz --output_gvcf=/output/{params.sample}_DeepVariant.gvcf.gz --num_shards=12. - actual command (XXXXX = removed for security purposes) . - docker --rm -v XXXXXXXXX/gatk_align_metrics_t/:/input -v XXXXXXXXX/deep_variant2/xGENIDTn2_DeepVariant:/output -v /XXXXXXXXX/deepvariant/data:/data -v XXXXXXXXX/bed:/bed google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quickstart creates files as root. As it's a high performance computing cluster, I am no longer able to delete these files. How do I stop it from creating files as root? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/550
https://github.com/google/deepvariant/issues/550:181,usability,clear,clear,181,"Quick Start Creates Files as Root, Now I can't delete them.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS Linux release 7.9.2009. - DeepVariant version: deepvariant:0.9.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - Illumina, HG38, standard capture panel. **Steps to reproduce:**. - Command: Snakemake command:. - docker --rm -v {params.input_dir}/:/input -v {params.output_dir}/{params.sample}_DeepVariant:/output -v /data:/data -v {params.bed_dir}:/bed --user $CURRENT_UID google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/{params.sample}.bam --regions=/bed/{params.primary_bed} --output_vcf=/output/{params.sample}_DeepVariant.vcf.gz --output_gvcf=/output/{params.sample}_DeepVariant.gvcf.gz --num_shards=12. - actual command (XXXXX = removed for security purposes) . - docker --rm -v XXXXXXXXX/gatk_align_metrics_t/:/input -v XXXXXXXXX/deep_variant2/xGENIDTn2_DeepVariant:/output -v /XXXXXXXXX/deepvariant/data:/data -v XXXXXXXXX/bed:/bed google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quickstart creates files as root. As it's a high performance computing cluster, I am no longer able to delete these files. How",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/550
https://github.com/google/deepvariant/issues/550:583,usability,Command,Command,583,"Quick Start Creates Files as Root, Now I can't delete them.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS Linux release 7.9.2009. - DeepVariant version: deepvariant:0.9.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - Illumina, HG38, standard capture panel. **Steps to reproduce:**. - Command: Snakemake command:. - docker --rm -v {params.input_dir}/:/input -v {params.output_dir}/{params.sample}_DeepVariant:/output -v /data:/data -v {params.bed_dir}:/bed --user $CURRENT_UID google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/{params.sample}.bam --regions=/bed/{params.primary_bed} --output_vcf=/output/{params.sample}_DeepVariant.vcf.gz --output_gvcf=/output/{params.sample}_DeepVariant.gvcf.gz --num_shards=12. - actual command (XXXXX = removed for security purposes) . - docker --rm -v XXXXXXXXX/gatk_align_metrics_t/:/input -v XXXXXXXXX/deep_variant2/xGENIDTn2_DeepVariant:/output -v /XXXXXXXXX/deepvariant/data:/data -v XXXXXXXXX/bed:/bed google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quickstart creates files as root. As it's a high performance computing cluster, I am no longer able to delete these files. How",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/550
https://github.com/google/deepvariant/issues/550:602,usability,command,command,602,"Quick Start Creates Files as Root, Now I can't delete them.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS Linux release 7.9.2009. - DeepVariant version: deepvariant:0.9.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - Illumina, HG38, standard capture panel. **Steps to reproduce:**. - Command: Snakemake command:. - docker --rm -v {params.input_dir}/:/input -v {params.output_dir}/{params.sample}_DeepVariant:/output -v /data:/data -v {params.bed_dir}:/bed --user $CURRENT_UID google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/{params.sample}.bam --regions=/bed/{params.primary_bed} --output_vcf=/output/{params.sample}_DeepVariant.vcf.gz --output_gvcf=/output/{params.sample}_DeepVariant.gvcf.gz --num_shards=12. - actual command (XXXXX = removed for security purposes) . - docker --rm -v XXXXXXXXX/gatk_align_metrics_t/:/input -v XXXXXXXXX/deep_variant2/xGENIDTn2_DeepVariant:/output -v /XXXXXXXXX/deepvariant/data:/data -v XXXXXXXXX/bed:/bed google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quickstart creates files as root. As it's a high performance computing cluster, I am no longer able to delete these files. How",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/550
https://github.com/google/deepvariant/issues/550:650,usability,input,input,650,"Quick Start Creates Files as Root, Now I can't delete them.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS Linux release 7.9.2009. - DeepVariant version: deepvariant:0.9.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - Illumina, HG38, standard capture panel. **Steps to reproduce:**. - Command: Snakemake command:. - docker --rm -v {params.input_dir}/:/input -v {params.output_dir}/{params.sample}_DeepVariant:/output -v /data:/data -v {params.bed_dir}:/bed --user $CURRENT_UID google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/{params.sample}.bam --regions=/bed/{params.primary_bed} --output_vcf=/output/{params.sample}_DeepVariant.vcf.gz --output_gvcf=/output/{params.sample}_DeepVariant.gvcf.gz --num_shards=12. - actual command (XXXXX = removed for security purposes) . - docker --rm -v XXXXXXXXX/gatk_align_metrics_t/:/input -v XXXXXXXXX/deep_variant2/xGENIDTn2_DeepVariant:/output -v /XXXXXXXXX/deepvariant/data:/data -v XXXXXXXXX/bed:/bed google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quickstart creates files as root. As it's a high performance computing cluster, I am no longer able to delete these files. How",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/550
https://github.com/google/deepvariant/issues/550:757,usability,user,user,757,"Quick Start Creates Files as Root, Now I can't delete them.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS Linux release 7.9.2009. - DeepVariant version: deepvariant:0.9.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - Illumina, HG38, standard capture panel. **Steps to reproduce:**. - Command: Snakemake command:. - docker --rm -v {params.input_dir}/:/input -v {params.output_dir}/{params.sample}_DeepVariant:/output -v /data:/data -v {params.bed_dir}:/bed --user $CURRENT_UID google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/{params.sample}.bam --regions=/bed/{params.primary_bed} --output_vcf=/output/{params.sample}_DeepVariant.vcf.gz --output_gvcf=/output/{params.sample}_DeepVariant.gvcf.gz --num_shards=12. - actual command (XXXXX = removed for security purposes) . - docker --rm -v XXXXXXXXX/gatk_align_metrics_t/:/input -v XXXXXXXXX/deep_variant2/xGENIDTn2_DeepVariant:/output -v /XXXXXXXXX/deepvariant/data:/data -v XXXXXXXXX/bed:/bed google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quickstart creates files as root. As it's a high performance computing cluster, I am no longer able to delete these files. How",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/550
https://github.com/google/deepvariant/issues/550:886,usability,input,input,886,"Quick Start Creates Files as Root, Now I can't delete them.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS Linux release 7.9.2009. - DeepVariant version: deepvariant:0.9.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - Illumina, HG38, standard capture panel. **Steps to reproduce:**. - Command: Snakemake command:. - docker --rm -v {params.input_dir}/:/input -v {params.output_dir}/{params.sample}_DeepVariant:/output -v /data:/data -v {params.bed_dir}:/bed --user $CURRENT_UID google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/{params.sample}.bam --regions=/bed/{params.primary_bed} --output_vcf=/output/{params.sample}_DeepVariant.vcf.gz --output_gvcf=/output/{params.sample}_DeepVariant.gvcf.gz --num_shards=12. - actual command (XXXXX = removed for security purposes) . - docker --rm -v XXXXXXXXX/gatk_align_metrics_t/:/input -v XXXXXXXXX/deep_variant2/xGENIDTn2_DeepVariant:/output -v /XXXXXXXXX/deepvariant/data:/data -v XXXXXXXXX/bed:/bed google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quickstart creates files as root. As it's a high performance computing cluster, I am no longer able to delete these files. How",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/550
https://github.com/google/deepvariant/issues/550:1088,usability,command,command,1088," checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS Linux release 7.9.2009. - DeepVariant version: deepvariant:0.9.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - Illumina, HG38, standard capture panel. **Steps to reproduce:**. - Command: Snakemake command:. - docker --rm -v {params.input_dir}/:/input -v {params.output_dir}/{params.sample}_DeepVariant:/output -v /data:/data -v {params.bed_dir}:/bed --user $CURRENT_UID google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/{params.sample}.bam --regions=/bed/{params.primary_bed} --output_vcf=/output/{params.sample}_DeepVariant.vcf.gz --output_gvcf=/output/{params.sample}_DeepVariant.gvcf.gz --num_shards=12. - actual command (XXXXX = removed for security purposes) . - docker --rm -v XXXXXXXXX/gatk_align_metrics_t/:/input -v XXXXXXXXX/deep_variant2/xGENIDTn2_DeepVariant:/output -v /XXXXXXXXX/deepvariant/data:/data -v XXXXXXXXX/bed:/bed google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quickstart creates files as root. As it's a high performance computing cluster, I am no longer able to delete these files. How do I stop it from creating files as root? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/550
https://github.com/google/deepvariant/issues/550:1188,usability,input,input,1188," checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS Linux release 7.9.2009. - DeepVariant version: deepvariant:0.9.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - Illumina, HG38, standard capture panel. **Steps to reproduce:**. - Command: Snakemake command:. - docker --rm -v {params.input_dir}/:/input -v {params.output_dir}/{params.sample}_DeepVariant:/output -v /data:/data -v {params.bed_dir}:/bed --user $CURRENT_UID google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/{params.sample}.bam --regions=/bed/{params.primary_bed} --output_vcf=/output/{params.sample}_DeepVariant.vcf.gz --output_gvcf=/output/{params.sample}_DeepVariant.gvcf.gz --num_shards=12. - actual command (XXXXX = removed for security purposes) . - docker --rm -v XXXXXXXXX/gatk_align_metrics_t/:/input -v XXXXXXXXX/deep_variant2/xGENIDTn2_DeepVariant:/output -v /XXXXXXXXX/deepvariant/data:/data -v XXXXXXXXX/bed:/bed google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quickstart creates files as root. As it's a high performance computing cluster, I am no longer able to delete these files. How do I stop it from creating files as root? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/550
https://github.com/google/deepvariant/issues/550:1421,usability,input,input,1421," checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS Linux release 7.9.2009. - DeepVariant version: deepvariant:0.9.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - Illumina, HG38, standard capture panel. **Steps to reproduce:**. - Command: Snakemake command:. - docker --rm -v {params.input_dir}/:/input -v {params.output_dir}/{params.sample}_DeepVariant:/output -v /data:/data -v {params.bed_dir}:/bed --user $CURRENT_UID google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/{params.sample}.bam --regions=/bed/{params.primary_bed} --output_vcf=/output/{params.sample}_DeepVariant.vcf.gz --output_gvcf=/output/{params.sample}_DeepVariant.gvcf.gz --num_shards=12. - actual command (XXXXX = removed for security purposes) . - docker --rm -v XXXXXXXXX/gatk_align_metrics_t/:/input -v XXXXXXXXX/deep_variant2/xGENIDTn2_DeepVariant:/output -v /XXXXXXXXX/deepvariant/data:/data -v XXXXXXXXX/bed:/bed google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quickstart creates files as root. As it's a high performance computing cluster, I am no longer able to delete these files. How do I stop it from creating files as root? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/550
https://github.com/google/deepvariant/issues/550:1618,usability,Error,Error,1618," checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS Linux release 7.9.2009. - DeepVariant version: deepvariant:0.9.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - Illumina, HG38, standard capture panel. **Steps to reproduce:**. - Command: Snakemake command:. - docker --rm -v {params.input_dir}/:/input -v {params.output_dir}/{params.sample}_DeepVariant:/output -v /data:/data -v {params.bed_dir}:/bed --user $CURRENT_UID google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/{params.sample}.bam --regions=/bed/{params.primary_bed} --output_vcf=/output/{params.sample}_DeepVariant.vcf.gz --output_gvcf=/output/{params.sample}_DeepVariant.gvcf.gz --num_shards=12. - actual command (XXXXX = removed for security purposes) . - docker --rm -v XXXXXXXXX/gatk_align_metrics_t/:/input -v XXXXXXXXX/deep_variant2/xGENIDTn2_DeepVariant:/output -v /XXXXXXXXX/deepvariant/data:/data -v XXXXXXXXX/bed:/bed google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quickstart creates files as root. As it's a high performance computing cluster, I am no longer able to delete these files. How do I stop it from creating files as root? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/550
https://github.com/google/deepvariant/issues/550:1923,usability,perform,performance,1923," checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS Linux release 7.9.2009. - DeepVariant version: deepvariant:0.9.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - Illumina, HG38, standard capture panel. **Steps to reproduce:**. - Command: Snakemake command:. - docker --rm -v {params.input_dir}/:/input -v {params.output_dir}/{params.sample}_DeepVariant:/output -v /data:/data -v {params.bed_dir}:/bed --user $CURRENT_UID google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/{params.sample}.bam --regions=/bed/{params.primary_bed} --output_vcf=/output/{params.sample}_DeepVariant.vcf.gz --output_gvcf=/output/{params.sample}_DeepVariant.gvcf.gz --num_shards=12. - actual command (XXXXX = removed for security purposes) . - docker --rm -v XXXXXXXXX/gatk_align_metrics_t/:/input -v XXXXXXXXX/deep_variant2/xGENIDTn2_DeepVariant:/output -v /XXXXXXXXX/deepvariant/data:/data -v XXXXXXXXX/bed:/bed google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quickstart creates files as root. As it's a high performance computing cluster, I am no longer able to delete these files. How do I stop it from creating files as root? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/550
https://github.com/google/deepvariant/issues/550:2006,usability,stop,stop,2006," checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS Linux release 7.9.2009. - DeepVariant version: deepvariant:0.9.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - Illumina, HG38, standard capture panel. **Steps to reproduce:**. - Command: Snakemake command:. - docker --rm -v {params.input_dir}/:/input -v {params.output_dir}/{params.sample}_DeepVariant:/output -v /data:/data -v {params.bed_dir}:/bed --user $CURRENT_UID google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/{params.sample}.bam --regions=/bed/{params.primary_bed} --output_vcf=/output/{params.sample}_DeepVariant.vcf.gz --output_gvcf=/output/{params.sample}_DeepVariant.gvcf.gz --num_shards=12. - actual command (XXXXX = removed for security purposes) . - docker --rm -v XXXXXXXXX/gatk_align_metrics_t/:/input -v XXXXXXXXX/deep_variant2/xGENIDTn2_DeepVariant:/output -v /XXXXXXXXX/deepvariant/data:/data -v XXXXXXXXX/bed:/bed google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quickstart creates files as root. As it's a high performance computing cluster, I am no longer able to delete these files. How do I stop it from creating files as root? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/550
https://github.com/google/deepvariant/issues/552:81,availability,avail,available,81,"The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine; When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:. `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**. - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa). - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: docker run google/deepvariant:1.4.0. - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. CPU information from /proc/cpuinfo. product: Common KVM processor. vendor: Intel Corp. physical id: 2. bus info: cpu@1. width: 64 bits. capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552
https://github.com/google/deepvariant/issues/552:185,availability,error,error,185,"The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine; When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:. `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**. - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa). - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: docker run google/deepvariant:1.4.0. - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. CPU information from /proc/cpuinfo. product: Common KVM processor. vendor: Intel Corp. physical id: 2. bus info: cpu@1. width: 64 bits. capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552
https://github.com/google/deepvariant/issues/552:283,availability,avail,available,283,"The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine; When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:. `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**. - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa). - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: docker run google/deepvariant:1.4.0. - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. CPU information from /proc/cpuinfo. product: Common KVM processor. vendor: Intel Corp. physical id: 2. bus info: cpu@1. width: 64 bits. capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552
https://github.com/google/deepvariant/issues/552:451,availability,Operat,Operating,451,"The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine; When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:. `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**. - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa). - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: docker run google/deepvariant:1.4.0. - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. CPU information from /proc/cpuinfo. product: Common KVM processor. vendor: Intel Corp. physical id: 2. bus info: cpu@1. width: 64 bits. capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552
https://github.com/google/deepvariant/issues/552:783,availability,Error,Error,783,"The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine; When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:. `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**. - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa). - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: docker run google/deepvariant:1.4.0. - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. CPU information from /proc/cpuinfo. product: Common KVM processor. vendor: Intel Corp. physical id: 2. bus info: cpu@1. width: 64 bits. capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552
https://github.com/google/deepvariant/issues/552:877,availability,avail,available,877,"The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine; When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:. `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**. - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa). - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: docker run google/deepvariant:1.4.0. - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. CPU information from /proc/cpuinfo. product: Common KVM processor. vendor: Intel Corp. physical id: 2. bus info: cpu@1. width: 64 bits. capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552
https://github.com/google/deepvariant/issues/552:517,deployability,version,version,517,"The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine; When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:. `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**. - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa). - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: docker run google/deepvariant:1.4.0. - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. CPU information from /proc/cpuinfo. product: Common KVM processor. vendor: Intel Corp. physical id: 2. bus info: cpu@1. width: 64 bits. capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552
https://github.com/google/deepvariant/issues/552:535,deployability,Instal,Installation,535,"The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine; When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:. `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**. - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa). - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: docker run google/deepvariant:1.4.0. - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. CPU information from /proc/cpuinfo. product: Common KVM processor. vendor: Intel Corp. physical id: 2. bus info: cpu@1. width: 64 bits. capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552
https://github.com/google/deepvariant/issues/552:1479,deployability,api,apic,1479,"The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine; When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:. `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**. - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa). - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: docker run google/deepvariant:1.4.0. - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. CPU information from /proc/cpuinfo. product: Common KVM processor. vendor: Intel Corp. physical id: 2. bus info: cpu@1. width: 64 bits. capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552
https://github.com/google/deepvariant/issues/552:367,energy efficiency,core,core,367,"The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine; When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:. `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**. - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa). - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: docker run google/deepvariant:1.4.0. - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. CPU information from /proc/cpuinfo. product: Common KVM processor. vendor: Intel Corp. physical id: 2. bus info: cpu@1. width: 64 bits. capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552
https://github.com/google/deepvariant/issues/552:961,energy efficiency,core,core,961,"The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine; When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:. `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**. - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa). - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: docker run google/deepvariant:1.4.0. - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. CPU information from /proc/cpuinfo. product: Common KVM processor. vendor: Intel Corp. physical id: 2. bus info: cpu@1. width: 64 bits. capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552
https://github.com/google/deepvariant/issues/552:1277,energy efficiency,CPU,CPU,1277,"The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine; When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:. `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**. - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa). - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: docker run google/deepvariant:1.4.0. - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. CPU information from /proc/cpuinfo. product: Common KVM processor. vendor: Intel Corp. physical id: 2. bus info: cpu@1. width: 64 bits. capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552
https://github.com/google/deepvariant/issues/552:1304,energy efficiency,cpu,cpuinfo,1304,"The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine; When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:. `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**. - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa). - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: docker run google/deepvariant:1.4.0. - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. CPU information from /proc/cpuinfo. product: Common KVM processor. vendor: Intel Corp. physical id: 2. bus info: cpu@1. width: 64 bits. capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552
https://github.com/google/deepvariant/issues/552:1390,energy efficiency,cpu,cpu,1390,"The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine; When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:. `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**. - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa). - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: docker run google/deepvariant:1.4.0. - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. CPU information from /proc/cpuinfo. product: Common KVM processor. vendor: Intel Corp. physical id: 2. bus info: cpu@1. width: 64 bits. capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552
https://github.com/google/deepvariant/issues/552:1591,energy efficiency,cpu,cpuid,1591,"The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine; When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:. `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**. - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa). - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: docker run google/deepvariant:1.4.0. - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. CPU information from /proc/cpuinfo. product: Common KVM processor. vendor: Intel Corp. physical id: 2. bus info: cpu@1. width: 64 bits. capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552
https://github.com/google/deepvariant/issues/552:191,integrability,messag,message,191,"The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine; When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:. `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**. - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa). - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: docker run google/deepvariant:1.4.0. - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. CPU information from /proc/cpuinfo. product: Common KVM processor. vendor: Intel Corp. physical id: 2. bus info: cpu@1. width: 64 bits. capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552
https://github.com/google/deepvariant/issues/552:517,integrability,version,version,517,"The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine; When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:. `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**. - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa). - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: docker run google/deepvariant:1.4.0. - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. CPU information from /proc/cpuinfo. product: Common KVM processor. vendor: Intel Corp. physical id: 2. bus info: cpu@1. width: 64 bits. capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552
https://github.com/google/deepvariant/issues/552:1479,integrability,api,apic,1479,"The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine; When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:. `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**. - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa). - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: docker run google/deepvariant:1.4.0. - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. CPU information from /proc/cpuinfo. product: Common KVM processor. vendor: Intel Corp. physical id: 2. bus info: cpu@1. width: 64 bits. capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552
https://github.com/google/deepvariant/issues/552:191,interoperability,messag,message,191,"The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine; When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:. `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**. - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa). - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: docker run google/deepvariant:1.4.0. - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. CPU information from /proc/cpuinfo. product: Common KVM processor. vendor: Intel Corp. physical id: 2. bus info: cpu@1. width: 64 bits. capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552
https://github.com/google/deepvariant/issues/552:1479,interoperability,api,apic,1479,"The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine; When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:. `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**. - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa). - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: docker run google/deepvariant:1.4.0. - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. CPU information from /proc/cpuinfo. product: Common KVM processor. vendor: Intel Corp. physical id: 2. bus info: cpu@1. width: 64 bits. capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552
https://github.com/google/deepvariant/issues/552:517,modifiability,version,version,517,"The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine; When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:. `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**. - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa). - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: docker run google/deepvariant:1.4.0. - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. CPU information from /proc/cpuinfo. product: Common KVM processor. vendor: Intel Corp. physical id: 2. bus info: cpu@1. width: 64 bits. capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552
https://github.com/google/deepvariant/issues/552:185,performance,error,error,185,"The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine; When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:. `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**. - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa). - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: docker run google/deepvariant:1.4.0. - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. CPU information from /proc/cpuinfo. product: Common KVM processor. vendor: Intel Corp. physical id: 2. bus info: cpu@1. width: 64 bits. capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552
https://github.com/google/deepvariant/issues/552:783,performance,Error,Error,783,"The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine; When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:. `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**. - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa). - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: docker run google/deepvariant:1.4.0. - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. CPU information from /proc/cpuinfo. product: Common KVM processor. vendor: Intel Corp. physical id: 2. bus info: cpu@1. width: 64 bits. capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552
https://github.com/google/deepvariant/issues/552:1277,performance,CPU,CPU,1277,"The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine; When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:. `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**. - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa). - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: docker run google/deepvariant:1.4.0. - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. CPU information from /proc/cpuinfo. product: Common KVM processor. vendor: Intel Corp. physical id: 2. bus info: cpu@1. width: 64 bits. capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552
https://github.com/google/deepvariant/issues/552:1304,performance,cpu,cpuinfo,1304,"The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine; When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:. `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**. - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa). - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: docker run google/deepvariant:1.4.0. - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. CPU information from /proc/cpuinfo. product: Common KVM processor. vendor: Intel Corp. physical id: 2. bus info: cpu@1. width: 64 bits. capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552
https://github.com/google/deepvariant/issues/552:1390,performance,cpu,cpu,1390,"The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine; When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:. `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**. - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa). - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: docker run google/deepvariant:1.4.0. - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. CPU information from /proc/cpuinfo. product: Common KVM processor. vendor: Intel Corp. physical id: 2. bus info: cpu@1. width: 64 bits. capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552
https://github.com/google/deepvariant/issues/552:1591,performance,cpu,cpuid,1591,"The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine; When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:. `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**. - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa). - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: docker run google/deepvariant:1.4.0. - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. CPU information from /proc/cpuinfo. product: Common KVM processor. vendor: Intel Corp. physical id: 2. bus info: cpu@1. width: 64 bits. capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552
https://github.com/google/deepvariant/issues/552:81,reliability,availab,available,81,"The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine; When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:. `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**. - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa). - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: docker run google/deepvariant:1.4.0. - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. CPU information from /proc/cpuinfo. product: Common KVM processor. vendor: Intel Corp. physical id: 2. bus info: cpu@1. width: 64 bits. capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552
https://github.com/google/deepvariant/issues/552:283,reliability,availab,available,283,"The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine; When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:. `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**. - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa). - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: docker run google/deepvariant:1.4.0. - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. CPU information from /proc/cpuinfo. product: Common KVM processor. vendor: Intel Corp. physical id: 2. bus info: cpu@1. width: 64 bits. capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552
https://github.com/google/deepvariant/issues/552:877,reliability,availab,available,877,"The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine; When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:. `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**. - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa). - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: docker run google/deepvariant:1.4.0. - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. CPU information from /proc/cpuinfo. product: Common KVM processor. vendor: Intel Corp. physical id: 2. bus info: cpu@1. width: 64 bits. capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552
https://github.com/google/deepvariant/issues/552:1033,reliability,Doe,Does,1033,"The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine; When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:. `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**. - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa). - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: docker run google/deepvariant:1.4.0. - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. CPU information from /proc/cpuinfo. product: Common KVM processor. vendor: Intel Corp. physical id: 2. bus info: cpu@1. width: 64 bits. capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552
https://github.com/google/deepvariant/issues/552:81,safety,avail,available,81,"The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine; When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:. `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**. - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa). - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: docker run google/deepvariant:1.4.0. - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. CPU information from /proc/cpuinfo. product: Common KVM processor. vendor: Intel Corp. physical id: 2. bus info: cpu@1. width: 64 bits. capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552
https://github.com/google/deepvariant/issues/552:185,safety,error,error,185,"The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine; When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:. `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**. - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa). - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: docker run google/deepvariant:1.4.0. - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. CPU information from /proc/cpuinfo. product: Common KVM processor. vendor: Intel Corp. physical id: 2. bus info: cpu@1. width: 64 bits. capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552
https://github.com/google/deepvariant/issues/552:283,safety,avail,available,283,"The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine; When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:. `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**. - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa). - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: docker run google/deepvariant:1.4.0. - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. CPU information from /proc/cpuinfo. product: Common KVM processor. vendor: Intel Corp. physical id: 2. bus info: cpu@1. width: 64 bits. capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552
https://github.com/google/deepvariant/issues/552:783,safety,Error,Error,783,"The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine; When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:. `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**. - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa). - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: docker run google/deepvariant:1.4.0. - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. CPU information from /proc/cpuinfo. product: Common KVM processor. vendor: Intel Corp. physical id: 2. bus info: cpu@1. width: 64 bits. capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552
https://github.com/google/deepvariant/issues/552:877,safety,avail,available,877,"The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine; When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:. `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**. - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa). - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: docker run google/deepvariant:1.4.0. - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. CPU information from /proc/cpuinfo. product: Common KVM processor. vendor: Intel Corp. physical id: 2. bus info: cpu@1. width: 64 bits. capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552
https://github.com/google/deepvariant/issues/552:1054,safety,test,test,1054,"The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine; When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:. `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**. - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa). - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: docker run google/deepvariant:1.4.0. - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. CPU information from /proc/cpuinfo. product: Common KVM processor. vendor: Intel Corp. physical id: 2. bus info: cpu@1. width: 64 bits. capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552
https://github.com/google/deepvariant/issues/552:1090,safety,test,test,1090,"The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine; When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:. `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**. - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa). - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: docker run google/deepvariant:1.4.0. - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. CPU information from /proc/cpuinfo. product: Common KVM processor. vendor: Intel Corp. physical id: 2. bus info: cpu@1. width: 64 bits. capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552
https://github.com/google/deepvariant/issues/552:81,security,availab,available,81,"The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine; When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:. `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**. - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa). - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: docker run google/deepvariant:1.4.0. - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. CPU information from /proc/cpuinfo. product: Common KVM processor. vendor: Intel Corp. physical id: 2. bus info: cpu@1. width: 64 bits. capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552
https://github.com/google/deepvariant/issues/552:283,security,availab,available,283,"The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine; When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:. `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**. - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa). - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: docker run google/deepvariant:1.4.0. - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. CPU information from /proc/cpuinfo. product: Common KVM processor. vendor: Intel Corp. physical id: 2. bus info: cpu@1. width: 64 bits. capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552
https://github.com/google/deepvariant/issues/552:877,security,availab,available,877,"The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine; When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:. `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**. - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa). - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: docker run google/deepvariant:1.4.0. - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. CPU information from /proc/cpuinfo. product: Common KVM processor. vendor: Intel Corp. physical id: 2. bus info: cpu@1. width: 64 bits. capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552
https://github.com/google/deepvariant/issues/552:626,testability,instrument,instrument,626,"The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine; When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:. `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**. - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa). - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: docker run google/deepvariant:1.4.0. - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. CPU information from /proc/cpuinfo. product: Common KVM processor. vendor: Intel Corp. physical id: 2. bus info: cpu@1. width: 64 bits. capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552
https://github.com/google/deepvariant/issues/552:789,testability,trace,trace,789,"The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine; When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:. `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**. - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa). - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: docker run google/deepvariant:1.4.0. - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. CPU information from /proc/cpuinfo. product: Common KVM processor. vendor: Intel Corp. physical id: 2. bus info: cpu@1. width: 64 bits. capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552
https://github.com/google/deepvariant/issues/552:1054,testability,test,test,1054,"The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine; When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:. `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**. - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa). - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: docker run google/deepvariant:1.4.0. - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. CPU information from /proc/cpuinfo. product: Common KVM processor. vendor: Intel Corp. physical id: 2. bus info: cpu@1. width: 64 bits. capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552
https://github.com/google/deepvariant/issues/552:1090,testability,test,test,1090,"The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine; When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:. `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**. - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa). - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: docker run google/deepvariant:1.4.0. - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. CPU information from /proc/cpuinfo. product: Common KVM processor. vendor: Intel Corp. physical id: 2. bus info: cpu@1. width: 64 bits. capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552
https://github.com/google/deepvariant/issues/552:1265,testability,context,context,1265,"The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine; When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:. `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**. - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa). - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: docker run google/deepvariant:1.4.0. - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. CPU information from /proc/cpuinfo. product: Common KVM processor. vendor: Intel Corp. physical id: 2. bus info: cpu@1. width: 64 bits. capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552
https://github.com/google/deepvariant/issues/552:185,usability,error,error,185,"The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine; When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:. `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**. - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa). - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: docker run google/deepvariant:1.4.0. - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. CPU information from /proc/cpuinfo. product: Common KVM processor. vendor: Intel Corp. physical id: 2. bus info: cpu@1. width: 64 bits. capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552
https://github.com/google/deepvariant/issues/552:735,usability,Command,Command,735,"The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine; When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:. `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**. - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa). - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: docker run google/deepvariant:1.4.0. - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. CPU information from /proc/cpuinfo. product: Common KVM processor. vendor: Intel Corp. physical id: 2. bus info: cpu@1. width: 64 bits. capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552
https://github.com/google/deepvariant/issues/552:783,usability,Error,Error,783,"The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine; When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:. `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**. - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa). - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: docker run google/deepvariant:1.4.0. - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. CPU information from /proc/cpuinfo. product: Common KVM processor. vendor: Intel Corp. physical id: 2. bus info: cpu@1. width: 64 bits. capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552
https://github.com/google/deepvariant/issues/554:7,energy efficiency,model,model,7,Hybrid model with ONT + Illumina; Hello! Thank you for such a wonderful tool. We have ONT and Illumina reads for the same samples. Can I use DeepVariant Hybrid model but with ONT reads instead of Pacbio? Has someone tried this?,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/554
https://github.com/google/deepvariant/issues/554:160,energy efficiency,model,model,160,Hybrid model with ONT + Illumina; Hello! Thank you for such a wonderful tool. We have ONT and Illumina reads for the same samples. Can I use DeepVariant Hybrid model but with ONT reads instead of Pacbio? Has someone tried this?,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/554
https://github.com/google/deepvariant/issues/554:196,modifiability,Pac,Pacbio,196,Hybrid model with ONT + Illumina; Hello! Thank you for such a wonderful tool. We have ONT and Illumina reads for the same samples. Can I use DeepVariant Hybrid model but with ONT reads instead of Pacbio? Has someone tried this?,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/554
https://github.com/google/deepvariant/issues/554:7,security,model,model,7,Hybrid model with ONT + Illumina; Hello! Thank you for such a wonderful tool. We have ONT and Illumina reads for the same samples. Can I use DeepVariant Hybrid model but with ONT reads instead of Pacbio? Has someone tried this?,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/554
https://github.com/google/deepvariant/issues/554:160,security,model,model,160,Hybrid model with ONT + Illumina; Hello! Thank you for such a wonderful tool. We have ONT and Illumina reads for the same samples. Can I use DeepVariant Hybrid model but with ONT reads instead of Pacbio? Has someone tried this?,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/554
https://github.com/google/deepvariant/issues/554:72,usability,tool,tool,72,Hybrid model with ONT + Illumina; Hello! Thank you for such a wonderful tool. We have ONT and Illumina reads for the same samples. Can I use DeepVariant Hybrid model but with ONT reads instead of Pacbio? Has someone tried this?,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/554
https://github.com/google/deepvariant/issues/555:20,deployability,version,version,20,"Issue related to TF version in DeepVariant ; Is this issue related to TF version? Any help to fix this issue? Thanks. ```. (base) [tahmad@gcn35 tests]$ BIN_VERSION=""1.4.0"". (base) [tahmad@gcn35 tests]$ singularity run --nv -B /usr/lib/locale/ docker://google/deepvariant:${BIN_VERSION}-gpu /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref reference/GRCh38_no_alt_analysis_set.fasta. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --output_vcf deepvariant_output/output.vcf.gz --num_shards $(nproc) --regions chr20. INFO: Using cached SIF image. 2022-08-20 12:59:52.389461: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow14kernel_factory17OpKernelRegistrar12InitInternalEPKNS_9KernelDefEN4absl12lts_2021032411string_viewESt10unique_ptrINS0_15OpKernelFactoryESt14default_deleteIS9_EE. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/555
https://github.com/google/deepvariant/issues/555:73,deployability,version,version,73,"Issue related to TF version in DeepVariant ; Is this issue related to TF version? Any help to fix this issue? Thanks. ```. (base) [tahmad@gcn35 tests]$ BIN_VERSION=""1.4.0"". (base) [tahmad@gcn35 tests]$ singularity run --nv -B /usr/lib/locale/ docker://google/deepvariant:${BIN_VERSION}-gpu /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref reference/GRCh38_no_alt_analysis_set.fasta. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --output_vcf deepvariant_output/output.vcf.gz --num_shards $(nproc) --regions chr20. INFO: Using cached SIF image. 2022-08-20 12:59:52.389461: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow14kernel_factory17OpKernelRegistrar12InitInternalEPKNS_9KernelDefEN4absl12lts_2021032411string_viewESt10unique_ptrINS0_15OpKernelFactoryESt14default_deleteIS9_EE. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/555
https://github.com/google/deepvariant/issues/555:806,deployability,modul,module,806,"Issue related to TF version in DeepVariant ; Is this issue related to TF version? Any help to fix this issue? Thanks. ```. (base) [tahmad@gcn35 tests]$ BIN_VERSION=""1.4.0"". (base) [tahmad@gcn35 tests]$ singularity run --nv -B /usr/lib/locale/ docker://google/deepvariant:${BIN_VERSION}-gpu /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref reference/GRCh38_no_alt_analysis_set.fasta. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --output_vcf deepvariant_output/output.vcf.gz --num_shards $(nproc) --regions chr20. INFO: Using cached SIF image. 2022-08-20 12:59:52.389461: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow14kernel_factory17OpKernelRegistrar12InitInternalEPKNS_9KernelDefEN4absl12lts_2021032411string_viewESt10unique_ptrINS0_15OpKernelFactoryESt14default_deleteIS9_EE. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/555
https://github.com/google/deepvariant/issues/555:933,deployability,modul,module,933,"Issue related to TF version in DeepVariant ; Is this issue related to TF version? Any help to fix this issue? Thanks. ```. (base) [tahmad@gcn35 tests]$ BIN_VERSION=""1.4.0"". (base) [tahmad@gcn35 tests]$ singularity run --nv -B /usr/lib/locale/ docker://google/deepvariant:${BIN_VERSION}-gpu /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref reference/GRCh38_no_alt_analysis_set.fasta. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --output_vcf deepvariant_output/output.vcf.gz --num_shards $(nproc) --regions chr20. INFO: Using cached SIF image. 2022-08-20 12:59:52.389461: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow14kernel_factory17OpKernelRegistrar12InitInternalEPKNS_9KernelDefEN4absl12lts_2021032411string_viewESt10unique_ptrINS0_15OpKernelFactoryESt14default_deleteIS9_EE. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/555
https://github.com/google/deepvariant/issues/555:286,energy efficiency,gpu,gpu,286,"Issue related to TF version in DeepVariant ; Is this issue related to TF version? Any help to fix this issue? Thanks. ```. (base) [tahmad@gcn35 tests]$ BIN_VERSION=""1.4.0"". (base) [tahmad@gcn35 tests]$ singularity run --nv -B /usr/lib/locale/ docker://google/deepvariant:${BIN_VERSION}-gpu /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref reference/GRCh38_no_alt_analysis_set.fasta. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --output_vcf deepvariant_output/output.vcf.gz --num_shards $(nproc) --regions chr20. INFO: Using cached SIF image. 2022-08-20 12:59:52.389461: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow14kernel_factory17OpKernelRegistrar12InitInternalEPKNS_9KernelDefEN4absl12lts_2021032411string_viewESt10unique_ptrINS0_15OpKernelFactoryESt14default_deleteIS9_EE. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/555
https://github.com/google/deepvariant/issues/555:1230,energy efficiency,core,core,1230,"Issue related to TF version in DeepVariant ; Is this issue related to TF version? Any help to fix this issue? Thanks. ```. (base) [tahmad@gcn35 tests]$ BIN_VERSION=""1.4.0"". (base) [tahmad@gcn35 tests]$ singularity run --nv -B /usr/lib/locale/ docker://google/deepvariant:${BIN_VERSION}-gpu /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref reference/GRCh38_no_alt_analysis_set.fasta. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --output_vcf deepvariant_output/output.vcf.gz --num_shards $(nproc) --regions chr20. INFO: Using cached SIF image. 2022-08-20 12:59:52.389461: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow14kernel_factory17OpKernelRegistrar12InitInternalEPKNS_9KernelDefEN4absl12lts_2021032411string_viewESt10unique_ptrINS0_15OpKernelFactoryESt14default_deleteIS9_EE. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/555
https://github.com/google/deepvariant/issues/555:20,integrability,version,version,20,"Issue related to TF version in DeepVariant ; Is this issue related to TF version? Any help to fix this issue? Thanks. ```. (base) [tahmad@gcn35 tests]$ BIN_VERSION=""1.4.0"". (base) [tahmad@gcn35 tests]$ singularity run --nv -B /usr/lib/locale/ docker://google/deepvariant:${BIN_VERSION}-gpu /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref reference/GRCh38_no_alt_analysis_set.fasta. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --output_vcf deepvariant_output/output.vcf.gz --num_shards $(nproc) --regions chr20. INFO: Using cached SIF image. 2022-08-20 12:59:52.389461: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow14kernel_factory17OpKernelRegistrar12InitInternalEPKNS_9KernelDefEN4absl12lts_2021032411string_viewESt10unique_ptrINS0_15OpKernelFactoryESt14default_deleteIS9_EE. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/555
https://github.com/google/deepvariant/issues/555:73,integrability,version,version,73,"Issue related to TF version in DeepVariant ; Is this issue related to TF version? Any help to fix this issue? Thanks. ```. (base) [tahmad@gcn35 tests]$ BIN_VERSION=""1.4.0"". (base) [tahmad@gcn35 tests]$ singularity run --nv -B /usr/lib/locale/ docker://google/deepvariant:${BIN_VERSION}-gpu /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref reference/GRCh38_no_alt_analysis_set.fasta. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --output_vcf deepvariant_output/output.vcf.gz --num_shards $(nproc) --regions chr20. INFO: Using cached SIF image. 2022-08-20 12:59:52.389461: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow14kernel_factory17OpKernelRegistrar12InitInternalEPKNS_9KernelDefEN4absl12lts_2021032411string_viewESt10unique_ptrINS0_15OpKernelFactoryESt14default_deleteIS9_EE. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/555
https://github.com/google/deepvariant/issues/555:619,interoperability,platform,platform,619,"Issue related to TF version in DeepVariant ; Is this issue related to TF version? Any help to fix this issue? Thanks. ```. (base) [tahmad@gcn35 tests]$ BIN_VERSION=""1.4.0"". (base) [tahmad@gcn35 tests]$ singularity run --nv -B /usr/lib/locale/ docker://google/deepvariant:${BIN_VERSION}-gpu /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref reference/GRCh38_no_alt_analysis_set.fasta. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --output_vcf deepvariant_output/output.vcf.gz --num_shards $(nproc) --regions chr20. INFO: Using cached SIF image. 2022-08-20 12:59:52.389461: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow14kernel_factory17OpKernelRegistrar12InitInternalEPKNS_9KernelDefEN4absl12lts_2021032411string_viewESt10unique_ptrINS0_15OpKernelFactoryESt14default_deleteIS9_EE. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/555
https://github.com/google/deepvariant/issues/555:20,modifiability,version,version,20,"Issue related to TF version in DeepVariant ; Is this issue related to TF version? Any help to fix this issue? Thanks. ```. (base) [tahmad@gcn35 tests]$ BIN_VERSION=""1.4.0"". (base) [tahmad@gcn35 tests]$ singularity run --nv -B /usr/lib/locale/ docker://google/deepvariant:${BIN_VERSION}-gpu /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref reference/GRCh38_no_alt_analysis_set.fasta. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --output_vcf deepvariant_output/output.vcf.gz --num_shards $(nproc) --regions chr20. INFO: Using cached SIF image. 2022-08-20 12:59:52.389461: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow14kernel_factory17OpKernelRegistrar12InitInternalEPKNS_9KernelDefEN4absl12lts_2021032411string_viewESt10unique_ptrINS0_15OpKernelFactoryESt14default_deleteIS9_EE. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/555
https://github.com/google/deepvariant/issues/555:73,modifiability,version,version,73,"Issue related to TF version in DeepVariant ; Is this issue related to TF version? Any help to fix this issue? Thanks. ```. (base) [tahmad@gcn35 tests]$ BIN_VERSION=""1.4.0"". (base) [tahmad@gcn35 tests]$ singularity run --nv -B /usr/lib/locale/ docker://google/deepvariant:${BIN_VERSION}-gpu /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref reference/GRCh38_no_alt_analysis_set.fasta. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --output_vcf deepvariant_output/output.vcf.gz --num_shards $(nproc) --regions chr20. INFO: Using cached SIF image. 2022-08-20 12:59:52.389461: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow14kernel_factory17OpKernelRegistrar12InitInternalEPKNS_9KernelDefEN4absl12lts_2021032411string_viewESt10unique_ptrINS0_15OpKernelFactoryESt14default_deleteIS9_EE. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/555
https://github.com/google/deepvariant/issues/555:340,modifiability,PAC,PACBIO,340,"Issue related to TF version in DeepVariant ; Is this issue related to TF version? Any help to fix this issue? Thanks. ```. (base) [tahmad@gcn35 tests]$ BIN_VERSION=""1.4.0"". (base) [tahmad@gcn35 tests]$ singularity run --nv -B /usr/lib/locale/ docker://google/deepvariant:${BIN_VERSION}-gpu /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref reference/GRCh38_no_alt_analysis_set.fasta. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --output_vcf deepvariant_output/output.vcf.gz --num_shards $(nproc) --regions chr20. INFO: Using cached SIF image. 2022-08-20 12:59:52.389461: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow14kernel_factory17OpKernelRegistrar12InitInternalEPKNS_9KernelDefEN4absl12lts_2021032411string_viewESt10unique_ptrINS0_15OpKernelFactoryESt14default_deleteIS9_EE. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/555
https://github.com/google/deepvariant/issues/555:806,modifiability,modul,module,806,"Issue related to TF version in DeepVariant ; Is this issue related to TF version? Any help to fix this issue? Thanks. ```. (base) [tahmad@gcn35 tests]$ BIN_VERSION=""1.4.0"". (base) [tahmad@gcn35 tests]$ singularity run --nv -B /usr/lib/locale/ docker://google/deepvariant:${BIN_VERSION}-gpu /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref reference/GRCh38_no_alt_analysis_set.fasta. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --output_vcf deepvariant_output/output.vcf.gz --num_shards $(nproc) --regions chr20. INFO: Using cached SIF image. 2022-08-20 12:59:52.389461: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow14kernel_factory17OpKernelRegistrar12InitInternalEPKNS_9KernelDefEN4absl12lts_2021032411string_viewESt10unique_ptrINS0_15OpKernelFactoryESt14default_deleteIS9_EE. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/555
https://github.com/google/deepvariant/issues/555:885,modifiability,pac,packages,885,"Issue related to TF version in DeepVariant ; Is this issue related to TF version? Any help to fix this issue? Thanks. ```. (base) [tahmad@gcn35 tests]$ BIN_VERSION=""1.4.0"". (base) [tahmad@gcn35 tests]$ singularity run --nv -B /usr/lib/locale/ docker://google/deepvariant:${BIN_VERSION}-gpu /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref reference/GRCh38_no_alt_analysis_set.fasta. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --output_vcf deepvariant_output/output.vcf.gz --num_shards $(nproc) --regions chr20. INFO: Using cached SIF image. 2022-08-20 12:59:52.389461: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow14kernel_factory17OpKernelRegistrar12InitInternalEPKNS_9KernelDefEN4absl12lts_2021032411string_viewESt10unique_ptrINS0_15OpKernelFactoryESt14default_deleteIS9_EE. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/555
https://github.com/google/deepvariant/issues/555:933,modifiability,modul,module,933,"Issue related to TF version in DeepVariant ; Is this issue related to TF version? Any help to fix this issue? Thanks. ```. (base) [tahmad@gcn35 tests]$ BIN_VERSION=""1.4.0"". (base) [tahmad@gcn35 tests]$ singularity run --nv -B /usr/lib/locale/ docker://google/deepvariant:${BIN_VERSION}-gpu /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref reference/GRCh38_no_alt_analysis_set.fasta. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --output_vcf deepvariant_output/output.vcf.gz --num_shards $(nproc) --regions chr20. INFO: Using cached SIF image. 2022-08-20 12:59:52.389461: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow14kernel_factory17OpKernelRegistrar12InitInternalEPKNS_9KernelDefEN4absl12lts_2021032411string_viewESt10unique_ptrINS0_15OpKernelFactoryESt14default_deleteIS9_EE. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/555
https://github.com/google/deepvariant/issues/555:1016,modifiability,pac,packages,1016,"Issue related to TF version in DeepVariant ; Is this issue related to TF version? Any help to fix this issue? Thanks. ```. (base) [tahmad@gcn35 tests]$ BIN_VERSION=""1.4.0"". (base) [tahmad@gcn35 tests]$ singularity run --nv -B /usr/lib/locale/ docker://google/deepvariant:${BIN_VERSION}-gpu /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref reference/GRCh38_no_alt_analysis_set.fasta. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --output_vcf deepvariant_output/output.vcf.gz --num_shards $(nproc) --regions chr20. INFO: Using cached SIF image. 2022-08-20 12:59:52.389461: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow14kernel_factory17OpKernelRegistrar12InitInternalEPKNS_9KernelDefEN4absl12lts_2021032411string_viewESt10unique_ptrINS0_15OpKernelFactoryESt14default_deleteIS9_EE. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/555
https://github.com/google/deepvariant/issues/555:1210,modifiability,pac,packages,1210,"Issue related to TF version in DeepVariant ; Is this issue related to TF version? Any help to fix this issue? Thanks. ```. (base) [tahmad@gcn35 tests]$ BIN_VERSION=""1.4.0"". (base) [tahmad@gcn35 tests]$ singularity run --nv -B /usr/lib/locale/ docker://google/deepvariant:${BIN_VERSION}-gpu /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref reference/GRCh38_no_alt_analysis_set.fasta. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --output_vcf deepvariant_output/output.vcf.gz --num_shards $(nproc) --regions chr20. INFO: Using cached SIF image. 2022-08-20 12:59:52.389461: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow14kernel_factory17OpKernelRegistrar12InitInternalEPKNS_9KernelDefEN4absl12lts_2021032411string_viewESt10unique_ptrINS0_15OpKernelFactoryESt14default_deleteIS9_EE. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/555
https://github.com/google/deepvariant/issues/555:286,performance,gpu,gpu,286,"Issue related to TF version in DeepVariant ; Is this issue related to TF version? Any help to fix this issue? Thanks. ```. (base) [tahmad@gcn35 tests]$ BIN_VERSION=""1.4.0"". (base) [tahmad@gcn35 tests]$ singularity run --nv -B /usr/lib/locale/ docker://google/deepvariant:${BIN_VERSION}-gpu /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref reference/GRCh38_no_alt_analysis_set.fasta. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --output_vcf deepvariant_output/output.vcf.gz --num_shards $(nproc) --regions chr20. INFO: Using cached SIF image. 2022-08-20 12:59:52.389461: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow14kernel_factory17OpKernelRegistrar12InitInternalEPKNS_9KernelDefEN4absl12lts_2021032411string_viewESt10unique_ptrINS0_15OpKernelFactoryESt14default_deleteIS9_EE. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/555
https://github.com/google/deepvariant/issues/555:544,performance,cach,cached,544,"Issue related to TF version in DeepVariant ; Is this issue related to TF version? Any help to fix this issue? Thanks. ```. (base) [tahmad@gcn35 tests]$ BIN_VERSION=""1.4.0"". (base) [tahmad@gcn35 tests]$ singularity run --nv -B /usr/lib/locale/ docker://google/deepvariant:${BIN_VERSION}-gpu /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref reference/GRCh38_no_alt_analysis_set.fasta. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --output_vcf deepvariant_output/output.vcf.gz --num_shards $(nproc) --regions chr20. INFO: Using cached SIF image. 2022-08-20 12:59:52.389461: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow14kernel_factory17OpKernelRegistrar12InitInternalEPKNS_9KernelDefEN4absl12lts_2021032411string_viewESt10unique_ptrINS0_15OpKernelFactoryESt14default_deleteIS9_EE. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/555
https://github.com/google/deepvariant/issues/555:144,safety,test,tests,144,"Issue related to TF version in DeepVariant ; Is this issue related to TF version? Any help to fix this issue? Thanks. ```. (base) [tahmad@gcn35 tests]$ BIN_VERSION=""1.4.0"". (base) [tahmad@gcn35 tests]$ singularity run --nv -B /usr/lib/locale/ docker://google/deepvariant:${BIN_VERSION}-gpu /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref reference/GRCh38_no_alt_analysis_set.fasta. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --output_vcf deepvariant_output/output.vcf.gz --num_shards $(nproc) --regions chr20. INFO: Using cached SIF image. 2022-08-20 12:59:52.389461: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow14kernel_factory17OpKernelRegistrar12InitInternalEPKNS_9KernelDefEN4absl12lts_2021032411string_viewESt10unique_ptrINS0_15OpKernelFactoryESt14default_deleteIS9_EE. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/555
https://github.com/google/deepvariant/issues/555:194,safety,test,tests,194,"Issue related to TF version in DeepVariant ; Is this issue related to TF version? Any help to fix this issue? Thanks. ```. (base) [tahmad@gcn35 tests]$ BIN_VERSION=""1.4.0"". (base) [tahmad@gcn35 tests]$ singularity run --nv -B /usr/lib/locale/ docker://google/deepvariant:${BIN_VERSION}-gpu /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref reference/GRCh38_no_alt_analysis_set.fasta. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --output_vcf deepvariant_output/output.vcf.gz --num_shards $(nproc) --regions chr20. INFO: Using cached SIF image. 2022-08-20 12:59:52.389461: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow14kernel_factory17OpKernelRegistrar12InitInternalEPKNS_9KernelDefEN4absl12lts_2021032411string_viewESt10unique_ptrINS0_15OpKernelFactoryESt14default_deleteIS9_EE. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/555
https://github.com/google/deepvariant/issues/555:405,safety,input,input,405,"Issue related to TF version in DeepVariant ; Is this issue related to TF version? Any help to fix this issue? Thanks. ```. (base) [tahmad@gcn35 tests]$ BIN_VERSION=""1.4.0"". (base) [tahmad@gcn35 tests]$ singularity run --nv -B /usr/lib/locale/ docker://google/deepvariant:${BIN_VERSION}-gpu /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref reference/GRCh38_no_alt_analysis_set.fasta. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --output_vcf deepvariant_output/output.vcf.gz --num_shards $(nproc) --regions chr20. INFO: Using cached SIF image. 2022-08-20 12:59:52.389461: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow14kernel_factory17OpKernelRegistrar12InitInternalEPKNS_9KernelDefEN4absl12lts_2021032411string_viewESt10unique_ptrINS0_15OpKernelFactoryESt14default_deleteIS9_EE. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/555
https://github.com/google/deepvariant/issues/555:806,safety,modul,module,806,"Issue related to TF version in DeepVariant ; Is this issue related to TF version? Any help to fix this issue? Thanks. ```. (base) [tahmad@gcn35 tests]$ BIN_VERSION=""1.4.0"". (base) [tahmad@gcn35 tests]$ singularity run --nv -B /usr/lib/locale/ docker://google/deepvariant:${BIN_VERSION}-gpu /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref reference/GRCh38_no_alt_analysis_set.fasta. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --output_vcf deepvariant_output/output.vcf.gz --num_shards $(nproc) --regions chr20. INFO: Using cached SIF image. 2022-08-20 12:59:52.389461: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow14kernel_factory17OpKernelRegistrar12InitInternalEPKNS_9KernelDefEN4absl12lts_2021032411string_viewESt10unique_ptrINS0_15OpKernelFactoryESt14default_deleteIS9_EE. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/555
https://github.com/google/deepvariant/issues/555:933,safety,modul,module,933,"Issue related to TF version in DeepVariant ; Is this issue related to TF version? Any help to fix this issue? Thanks. ```. (base) [tahmad@gcn35 tests]$ BIN_VERSION=""1.4.0"". (base) [tahmad@gcn35 tests]$ singularity run --nv -B /usr/lib/locale/ docker://google/deepvariant:${BIN_VERSION}-gpu /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref reference/GRCh38_no_alt_analysis_set.fasta. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --output_vcf deepvariant_output/output.vcf.gz --num_shards $(nproc) --regions chr20. INFO: Using cached SIF image. 2022-08-20 12:59:52.389461: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow14kernel_factory17OpKernelRegistrar12InitInternalEPKNS_9KernelDefEN4absl12lts_2021032411string_viewESt10unique_ptrINS0_15OpKernelFactoryESt14default_deleteIS9_EE. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/555
https://github.com/google/deepvariant/issues/555:144,testability,test,tests,144,"Issue related to TF version in DeepVariant ; Is this issue related to TF version? Any help to fix this issue? Thanks. ```. (base) [tahmad@gcn35 tests]$ BIN_VERSION=""1.4.0"". (base) [tahmad@gcn35 tests]$ singularity run --nv -B /usr/lib/locale/ docker://google/deepvariant:${BIN_VERSION}-gpu /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref reference/GRCh38_no_alt_analysis_set.fasta. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --output_vcf deepvariant_output/output.vcf.gz --num_shards $(nproc) --regions chr20. INFO: Using cached SIF image. 2022-08-20 12:59:52.389461: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow14kernel_factory17OpKernelRegistrar12InitInternalEPKNS_9KernelDefEN4absl12lts_2021032411string_viewESt10unique_ptrINS0_15OpKernelFactoryESt14default_deleteIS9_EE. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/555
https://github.com/google/deepvariant/issues/555:194,testability,test,tests,194,"Issue related to TF version in DeepVariant ; Is this issue related to TF version? Any help to fix this issue? Thanks. ```. (base) [tahmad@gcn35 tests]$ BIN_VERSION=""1.4.0"". (base) [tahmad@gcn35 tests]$ singularity run --nv -B /usr/lib/locale/ docker://google/deepvariant:${BIN_VERSION}-gpu /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref reference/GRCh38_no_alt_analysis_set.fasta. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --output_vcf deepvariant_output/output.vcf.gz --num_shards $(nproc) --regions chr20. INFO: Using cached SIF image. 2022-08-20 12:59:52.389461: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow14kernel_factory17OpKernelRegistrar12InitInternalEPKNS_9KernelDefEN4absl12lts_2021032411string_viewESt10unique_ptrINS0_15OpKernelFactoryESt14default_deleteIS9_EE. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/555
https://github.com/google/deepvariant/issues/555:709,testability,Trace,Traceback,709,"Issue related to TF version in DeepVariant ; Is this issue related to TF version? Any help to fix this issue? Thanks. ```. (base) [tahmad@gcn35 tests]$ BIN_VERSION=""1.4.0"". (base) [tahmad@gcn35 tests]$ singularity run --nv -B /usr/lib/locale/ docker://google/deepvariant:${BIN_VERSION}-gpu /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref reference/GRCh38_no_alt_analysis_set.fasta. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --output_vcf deepvariant_output/output.vcf.gz --num_shards $(nproc) --regions chr20. INFO: Using cached SIF image. 2022-08-20 12:59:52.389461: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow14kernel_factory17OpKernelRegistrar12InitInternalEPKNS_9KernelDefEN4absl12lts_2021032411string_viewESt10unique_ptrINS0_15OpKernelFactoryESt14default_deleteIS9_EE. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/555
https://github.com/google/deepvariant/issues/555:86,usability,help,help,86,"Issue related to TF version in DeepVariant ; Is this issue related to TF version? Any help to fix this issue? Thanks. ```. (base) [tahmad@gcn35 tests]$ BIN_VERSION=""1.4.0"". (base) [tahmad@gcn35 tests]$ singularity run --nv -B /usr/lib/locale/ docker://google/deepvariant:${BIN_VERSION}-gpu /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref reference/GRCh38_no_alt_analysis_set.fasta. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --output_vcf deepvariant_output/output.vcf.gz --num_shards $(nproc) --regions chr20. INFO: Using cached SIF image. 2022-08-20 12:59:52.389461: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow14kernel_factory17OpKernelRegistrar12InitInternalEPKNS_9KernelDefEN4absl12lts_2021032411string_viewESt10unique_ptrINS0_15OpKernelFactoryESt14default_deleteIS9_EE. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/555
https://github.com/google/deepvariant/issues/555:405,usability,input,input,405,"Issue related to TF version in DeepVariant ; Is this issue related to TF version? Any help to fix this issue? Thanks. ```. (base) [tahmad@gcn35 tests]$ BIN_VERSION=""1.4.0"". (base) [tahmad@gcn35 tests]$ singularity run --nv -B /usr/lib/locale/ docker://google/deepvariant:${BIN_VERSION}-gpu /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref reference/GRCh38_no_alt_analysis_set.fasta. --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --output_vcf deepvariant_output/output.vcf.gz --num_shards $(nproc) --regions chr20. INFO: Using cached SIF image. 2022-08-20 12:59:52.389461: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>. _ll.load_library(_main_dir). File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library. py_tf.TF_LoadLibrary(lib). tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow14kernel_factory17OpKernelRegistrar12InitInternalEPKNS_9KernelDefEN4absl12lts_2021032411string_viewESt10unique_ptrINS0_15OpKernelFactoryESt14default_deleteIS9_EE. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/555
https://github.com/google/deepvariant/issues/556:32,availability,Error,Error,32,"--regions results in 'parallel: Error: Command line too long (<num> >= 65524)'; Running DeepVariant/DeepTrio 1.4.0 using singularity, based on the provided docker images, with many regions results in a `'parallel: Error: Command line too long (<num> >= 65524)` error. My use case is processing .bam files aligned against `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz` (containing 2500+ contigs) in parallel. Splitting by contig would result in 2500+ chunks which introduces too much overhead so I group contigs together based on the number of reads. In practice this results in chunks for chr1, ... chr22, chrX, chrY, chrM and one chunk for the other ~2500 contigs. This results in a very long command-line arg for `--regions` and thus the error. What would you think of adding an additional `--regions-file` command-line argument similar to e.g. [bcftools](https://samtools.github.io/bcftools/bcftools.html) which accepts a text file containing one region per line?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/556
https://github.com/google/deepvariant/issues/556:214,availability,Error,Error,214,"--regions results in 'parallel: Error: Command line too long (<num> >= 65524)'; Running DeepVariant/DeepTrio 1.4.0 using singularity, based on the provided docker images, with many regions results in a `'parallel: Error: Command line too long (<num> >= 65524)` error. My use case is processing .bam files aligned against `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz` (containing 2500+ contigs) in parallel. Splitting by contig would result in 2500+ chunks which introduces too much overhead so I group contigs together based on the number of reads. In practice this results in chunks for chr1, ... chr22, chrX, chrY, chrM and one chunk for the other ~2500 contigs. This results in a very long command-line arg for `--regions` and thus the error. What would you think of adding an additional `--regions-file` command-line argument similar to e.g. [bcftools](https://samtools.github.io/bcftools/bcftools.html) which accepts a text file containing one region per line?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/556
https://github.com/google/deepvariant/issues/556:261,availability,error,error,261,"--regions results in 'parallel: Error: Command line too long (<num> >= 65524)'; Running DeepVariant/DeepTrio 1.4.0 using singularity, based on the provided docker images, with many regions results in a `'parallel: Error: Command line too long (<num> >= 65524)` error. My use case is processing .bam files aligned against `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz` (containing 2500+ contigs) in parallel. Splitting by contig would result in 2500+ chunks which introduces too much overhead so I group contigs together based on the number of reads. In practice this results in chunks for chr1, ... chr22, chrX, chrY, chrM and one chunk for the other ~2500 contigs. This results in a very long command-line arg for `--regions` and thus the error. What would you think of adding an additional `--regions-file` command-line argument similar to e.g. [bcftools](https://samtools.github.io/bcftools/bcftools.html) which accepts a text file containing one region per line?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/556
https://github.com/google/deepvariant/issues/556:746,availability,error,error,746,"--regions results in 'parallel: Error: Command line too long (<num> >= 65524)'; Running DeepVariant/DeepTrio 1.4.0 using singularity, based on the provided docker images, with many regions results in a `'parallel: Error: Command line too long (<num> >= 65524)` error. My use case is processing .bam files aligned against `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz` (containing 2500+ contigs) in parallel. Splitting by contig would result in 2500+ chunks which introduces too much overhead so I group contigs together based on the number of reads. In practice this results in chunks for chr1, ... chr22, chrX, chrY, chrM and one chunk for the other ~2500 contigs. This results in a very long command-line arg for `--regions` and thus the error. What would you think of adding an additional `--regions-file` command-line argument similar to e.g. [bcftools](https://samtools.github.io/bcftools/bcftools.html) which accepts a text file containing one region per line?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/556
https://github.com/google/deepvariant/issues/556:375,deployability,contain,containing,375,"--regions results in 'parallel: Error: Command line too long (<num> >= 65524)'; Running DeepVariant/DeepTrio 1.4.0 using singularity, based on the provided docker images, with many regions results in a `'parallel: Error: Command line too long (<num> >= 65524)` error. My use case is processing .bam files aligned against `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz` (containing 2500+ contigs) in parallel. Splitting by contig would result in 2500+ chunks which introduces too much overhead so I group contigs together based on the number of reads. In practice this results in chunks for chr1, ... chr22, chrX, chrY, chrM and one chunk for the other ~2500 contigs. This results in a very long command-line arg for `--regions` and thus the error. What would you think of adding an additional `--regions-file` command-line argument similar to e.g. [bcftools](https://samtools.github.io/bcftools/bcftools.html) which accepts a text file containing one region per line?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/556
https://github.com/google/deepvariant/issues/556:941,deployability,contain,containing,941,"--regions results in 'parallel: Error: Command line too long (<num> >= 65524)'; Running DeepVariant/DeepTrio 1.4.0 using singularity, based on the provided docker images, with many regions results in a `'parallel: Error: Command line too long (<num> >= 65524)` error. My use case is processing .bam files aligned against `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz` (containing 2500+ contigs) in parallel. Splitting by contig would result in 2500+ chunks which introduces too much overhead so I group contigs together based on the number of reads. In practice this results in chunks for chr1, ... chr22, chrX, chrY, chrM and one chunk for the other ~2500 contigs. This results in a very long command-line arg for `--regions` and thus the error. What would you think of adding an additional `--regions-file` command-line argument similar to e.g. [bcftools](https://samtools.github.io/bcftools/bcftools.html) which accepts a text file containing one region per line?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/556
https://github.com/google/deepvariant/issues/556:22,performance,parallel,parallel,22,"--regions results in 'parallel: Error: Command line too long (<num> >= 65524)'; Running DeepVariant/DeepTrio 1.4.0 using singularity, based on the provided docker images, with many regions results in a `'parallel: Error: Command line too long (<num> >= 65524)` error. My use case is processing .bam files aligned against `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz` (containing 2500+ contigs) in parallel. Splitting by contig would result in 2500+ chunks which introduces too much overhead so I group contigs together based on the number of reads. In practice this results in chunks for chr1, ... chr22, chrX, chrY, chrM and one chunk for the other ~2500 contigs. This results in a very long command-line arg for `--regions` and thus the error. What would you think of adding an additional `--regions-file` command-line argument similar to e.g. [bcftools](https://samtools.github.io/bcftools/bcftools.html) which accepts a text file containing one region per line?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/556
https://github.com/google/deepvariant/issues/556:32,performance,Error,Error,32,"--regions results in 'parallel: Error: Command line too long (<num> >= 65524)'; Running DeepVariant/DeepTrio 1.4.0 using singularity, based on the provided docker images, with many regions results in a `'parallel: Error: Command line too long (<num> >= 65524)` error. My use case is processing .bam files aligned against `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz` (containing 2500+ contigs) in parallel. Splitting by contig would result in 2500+ chunks which introduces too much overhead so I group contigs together based on the number of reads. In practice this results in chunks for chr1, ... chr22, chrX, chrY, chrM and one chunk for the other ~2500 contigs. This results in a very long command-line arg for `--regions` and thus the error. What would you think of adding an additional `--regions-file` command-line argument similar to e.g. [bcftools](https://samtools.github.io/bcftools/bcftools.html) which accepts a text file containing one region per line?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/556
https://github.com/google/deepvariant/issues/556:204,performance,parallel,parallel,204,"--regions results in 'parallel: Error: Command line too long (<num> >= 65524)'; Running DeepVariant/DeepTrio 1.4.0 using singularity, based on the provided docker images, with many regions results in a `'parallel: Error: Command line too long (<num> >= 65524)` error. My use case is processing .bam files aligned against `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz` (containing 2500+ contigs) in parallel. Splitting by contig would result in 2500+ chunks which introduces too much overhead so I group contigs together based on the number of reads. In practice this results in chunks for chr1, ... chr22, chrX, chrY, chrM and one chunk for the other ~2500 contigs. This results in a very long command-line arg for `--regions` and thus the error. What would you think of adding an additional `--regions-file` command-line argument similar to e.g. [bcftools](https://samtools.github.io/bcftools/bcftools.html) which accepts a text file containing one region per line?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/556
https://github.com/google/deepvariant/issues/556:214,performance,Error,Error,214,"--regions results in 'parallel: Error: Command line too long (<num> >= 65524)'; Running DeepVariant/DeepTrio 1.4.0 using singularity, based on the provided docker images, with many regions results in a `'parallel: Error: Command line too long (<num> >= 65524)` error. My use case is processing .bam files aligned against `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz` (containing 2500+ contigs) in parallel. Splitting by contig would result in 2500+ chunks which introduces too much overhead so I group contigs together based on the number of reads. In practice this results in chunks for chr1, ... chr22, chrX, chrY, chrM and one chunk for the other ~2500 contigs. This results in a very long command-line arg for `--regions` and thus the error. What would you think of adding an additional `--regions-file` command-line argument similar to e.g. [bcftools](https://samtools.github.io/bcftools/bcftools.html) which accepts a text file containing one region per line?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/556
https://github.com/google/deepvariant/issues/556:261,performance,error,error,261,"--regions results in 'parallel: Error: Command line too long (<num> >= 65524)'; Running DeepVariant/DeepTrio 1.4.0 using singularity, based on the provided docker images, with many regions results in a `'parallel: Error: Command line too long (<num> >= 65524)` error. My use case is processing .bam files aligned against `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz` (containing 2500+ contigs) in parallel. Splitting by contig would result in 2500+ chunks which introduces too much overhead so I group contigs together based on the number of reads. In practice this results in chunks for chr1, ... chr22, chrX, chrY, chrM and one chunk for the other ~2500 contigs. This results in a very long command-line arg for `--regions` and thus the error. What would you think of adding an additional `--regions-file` command-line argument similar to e.g. [bcftools](https://samtools.github.io/bcftools/bcftools.html) which accepts a text file containing one region per line?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/556
https://github.com/google/deepvariant/issues/556:404,performance,parallel,parallel,404,"--regions results in 'parallel: Error: Command line too long (<num> >= 65524)'; Running DeepVariant/DeepTrio 1.4.0 using singularity, based on the provided docker images, with many regions results in a `'parallel: Error: Command line too long (<num> >= 65524)` error. My use case is processing .bam files aligned against `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz` (containing 2500+ contigs) in parallel. Splitting by contig would result in 2500+ chunks which introduces too much overhead so I group contigs together based on the number of reads. In practice this results in chunks for chr1, ... chr22, chrX, chrY, chrM and one chunk for the other ~2500 contigs. This results in a very long command-line arg for `--regions` and thus the error. What would you think of adding an additional `--regions-file` command-line argument similar to e.g. [bcftools](https://samtools.github.io/bcftools/bcftools.html) which accepts a text file containing one region per line?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/556
https://github.com/google/deepvariant/issues/556:489,performance,overhead,overhead,489,"--regions results in 'parallel: Error: Command line too long (<num> >= 65524)'; Running DeepVariant/DeepTrio 1.4.0 using singularity, based on the provided docker images, with many regions results in a `'parallel: Error: Command line too long (<num> >= 65524)` error. My use case is processing .bam files aligned against `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz` (containing 2500+ contigs) in parallel. Splitting by contig would result in 2500+ chunks which introduces too much overhead so I group contigs together based on the number of reads. In practice this results in chunks for chr1, ... chr22, chrX, chrY, chrM and one chunk for the other ~2500 contigs. This results in a very long command-line arg for `--regions` and thus the error. What would you think of adding an additional `--regions-file` command-line argument similar to e.g. [bcftools](https://samtools.github.io/bcftools/bcftools.html) which accepts a text file containing one region per line?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/556
https://github.com/google/deepvariant/issues/556:746,performance,error,error,746,"--regions results in 'parallel: Error: Command line too long (<num> >= 65524)'; Running DeepVariant/DeepTrio 1.4.0 using singularity, based on the provided docker images, with many regions results in a `'parallel: Error: Command line too long (<num> >= 65524)` error. My use case is processing .bam files aligned against `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz` (containing 2500+ contigs) in parallel. Splitting by contig would result in 2500+ chunks which introduces too much overhead so I group contigs together based on the number of reads. In practice this results in chunks for chr1, ... chr22, chrX, chrY, chrM and one chunk for the other ~2500 contigs. This results in a very long command-line arg for `--regions` and thus the error. What would you think of adding an additional `--regions-file` command-line argument similar to e.g. [bcftools](https://samtools.github.io/bcftools/bcftools.html) which accepts a text file containing one region per line?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/556
https://github.com/google/deepvariant/issues/556:559,reliability,pra,practice,559,"--regions results in 'parallel: Error: Command line too long (<num> >= 65524)'; Running DeepVariant/DeepTrio 1.4.0 using singularity, based on the provided docker images, with many regions results in a `'parallel: Error: Command line too long (<num> >= 65524)` error. My use case is processing .bam files aligned against `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz` (containing 2500+ contigs) in parallel. Splitting by contig would result in 2500+ chunks which introduces too much overhead so I group contigs together based on the number of reads. In practice this results in chunks for chr1, ... chr22, chrX, chrY, chrM and one chunk for the other ~2500 contigs. This results in a very long command-line arg for `--regions` and thus the error. What would you think of adding an additional `--regions-file` command-line argument similar to e.g. [bcftools](https://samtools.github.io/bcftools/bcftools.html) which accepts a text file containing one region per line?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/556
https://github.com/google/deepvariant/issues/556:32,safety,Error,Error,32,"--regions results in 'parallel: Error: Command line too long (<num> >= 65524)'; Running DeepVariant/DeepTrio 1.4.0 using singularity, based on the provided docker images, with many regions results in a `'parallel: Error: Command line too long (<num> >= 65524)` error. My use case is processing .bam files aligned against `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz` (containing 2500+ contigs) in parallel. Splitting by contig would result in 2500+ chunks which introduces too much overhead so I group contigs together based on the number of reads. In practice this results in chunks for chr1, ... chr22, chrX, chrY, chrM and one chunk for the other ~2500 contigs. This results in a very long command-line arg for `--regions` and thus the error. What would you think of adding an additional `--regions-file` command-line argument similar to e.g. [bcftools](https://samtools.github.io/bcftools/bcftools.html) which accepts a text file containing one region per line?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/556
https://github.com/google/deepvariant/issues/556:214,safety,Error,Error,214,"--regions results in 'parallel: Error: Command line too long (<num> >= 65524)'; Running DeepVariant/DeepTrio 1.4.0 using singularity, based on the provided docker images, with many regions results in a `'parallel: Error: Command line too long (<num> >= 65524)` error. My use case is processing .bam files aligned against `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz` (containing 2500+ contigs) in parallel. Splitting by contig would result in 2500+ chunks which introduces too much overhead so I group contigs together based on the number of reads. In practice this results in chunks for chr1, ... chr22, chrX, chrY, chrM and one chunk for the other ~2500 contigs. This results in a very long command-line arg for `--regions` and thus the error. What would you think of adding an additional `--regions-file` command-line argument similar to e.g. [bcftools](https://samtools.github.io/bcftools/bcftools.html) which accepts a text file containing one region per line?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/556
https://github.com/google/deepvariant/issues/556:261,safety,error,error,261,"--regions results in 'parallel: Error: Command line too long (<num> >= 65524)'; Running DeepVariant/DeepTrio 1.4.0 using singularity, based on the provided docker images, with many regions results in a `'parallel: Error: Command line too long (<num> >= 65524)` error. My use case is processing .bam files aligned against `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz` (containing 2500+ contigs) in parallel. Splitting by contig would result in 2500+ chunks which introduces too much overhead so I group contigs together based on the number of reads. In practice this results in chunks for chr1, ... chr22, chrX, chrY, chrM and one chunk for the other ~2500 contigs. This results in a very long command-line arg for `--regions` and thus the error. What would you think of adding an additional `--regions-file` command-line argument similar to e.g. [bcftools](https://samtools.github.io/bcftools/bcftools.html) which accepts a text file containing one region per line?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/556
https://github.com/google/deepvariant/issues/556:746,safety,error,error,746,"--regions results in 'parallel: Error: Command line too long (<num> >= 65524)'; Running DeepVariant/DeepTrio 1.4.0 using singularity, based on the provided docker images, with many regions results in a `'parallel: Error: Command line too long (<num> >= 65524)` error. My use case is processing .bam files aligned against `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz` (containing 2500+ contigs) in parallel. Splitting by contig would result in 2500+ chunks which introduces too much overhead so I group contigs together based on the number of reads. In practice this results in chunks for chr1, ... chr22, chrX, chrY, chrM and one chunk for the other ~2500 contigs. This results in a very long command-line arg for `--regions` and thus the error. What would you think of adding an additional `--regions-file` command-line argument similar to e.g. [bcftools](https://samtools.github.io/bcftools/bcftools.html) which accepts a text file containing one region per line?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/556
https://github.com/google/deepvariant/issues/556:700,security,command-lin,command-line,700,"--regions results in 'parallel: Error: Command line too long (<num> >= 65524)'; Running DeepVariant/DeepTrio 1.4.0 using singularity, based on the provided docker images, with many regions results in a `'parallel: Error: Command line too long (<num> >= 65524)` error. My use case is processing .bam files aligned against `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz` (containing 2500+ contigs) in parallel. Splitting by contig would result in 2500+ chunks which introduces too much overhead so I group contigs together based on the number of reads. In practice this results in chunks for chr1, ... chr22, chrX, chrY, chrM and one chunk for the other ~2500 contigs. This results in a very long command-line arg for `--regions` and thus the error. What would you think of adding an additional `--regions-file` command-line argument similar to e.g. [bcftools](https://samtools.github.io/bcftools/bcftools.html) which accepts a text file containing one region per line?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/556
https://github.com/google/deepvariant/issues/556:815,security,command-lin,command-line,815,"--regions results in 'parallel: Error: Command line too long (<num> >= 65524)'; Running DeepVariant/DeepTrio 1.4.0 using singularity, based on the provided docker images, with many regions results in a `'parallel: Error: Command line too long (<num> >= 65524)` error. My use case is processing .bam files aligned against `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz` (containing 2500+ contigs) in parallel. Splitting by contig would result in 2500+ chunks which introduces too much overhead so I group contigs together based on the number of reads. In practice this results in chunks for chr1, ... chr22, chrX, chrY, chrM and one chunk for the other ~2500 contigs. This results in a very long command-line arg for `--regions` and thus the error. What would you think of adding an additional `--regions-file` command-line argument similar to e.g. [bcftools](https://samtools.github.io/bcftools/bcftools.html) which accepts a text file containing one region per line?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/556
https://github.com/google/deepvariant/issues/556:32,usability,Error,Error,32,"--regions results in 'parallel: Error: Command line too long (<num> >= 65524)'; Running DeepVariant/DeepTrio 1.4.0 using singularity, based on the provided docker images, with many regions results in a `'parallel: Error: Command line too long (<num> >= 65524)` error. My use case is processing .bam files aligned against `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz` (containing 2500+ contigs) in parallel. Splitting by contig would result in 2500+ chunks which introduces too much overhead so I group contigs together based on the number of reads. In practice this results in chunks for chr1, ... chr22, chrX, chrY, chrM and one chunk for the other ~2500 contigs. This results in a very long command-line arg for `--regions` and thus the error. What would you think of adding an additional `--regions-file` command-line argument similar to e.g. [bcftools](https://samtools.github.io/bcftools/bcftools.html) which accepts a text file containing one region per line?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/556
https://github.com/google/deepvariant/issues/556:39,usability,Command,Command,39,"--regions results in 'parallel: Error: Command line too long (<num> >= 65524)'; Running DeepVariant/DeepTrio 1.4.0 using singularity, based on the provided docker images, with many regions results in a `'parallel: Error: Command line too long (<num> >= 65524)` error. My use case is processing .bam files aligned against `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz` (containing 2500+ contigs) in parallel. Splitting by contig would result in 2500+ chunks which introduces too much overhead so I group contigs together based on the number of reads. In practice this results in chunks for chr1, ... chr22, chrX, chrY, chrM and one chunk for the other ~2500 contigs. This results in a very long command-line arg for `--regions` and thus the error. What would you think of adding an additional `--regions-file` command-line argument similar to e.g. [bcftools](https://samtools.github.io/bcftools/bcftools.html) which accepts a text file containing one region per line?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/556
https://github.com/google/deepvariant/issues/556:214,usability,Error,Error,214,"--regions results in 'parallel: Error: Command line too long (<num> >= 65524)'; Running DeepVariant/DeepTrio 1.4.0 using singularity, based on the provided docker images, with many regions results in a `'parallel: Error: Command line too long (<num> >= 65524)` error. My use case is processing .bam files aligned against `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz` (containing 2500+ contigs) in parallel. Splitting by contig would result in 2500+ chunks which introduces too much overhead so I group contigs together based on the number of reads. In practice this results in chunks for chr1, ... chr22, chrX, chrY, chrM and one chunk for the other ~2500 contigs. This results in a very long command-line arg for `--regions` and thus the error. What would you think of adding an additional `--regions-file` command-line argument similar to e.g. [bcftools](https://samtools.github.io/bcftools/bcftools.html) which accepts a text file containing one region per line?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/556
https://github.com/google/deepvariant/issues/556:221,usability,Command,Command,221,"--regions results in 'parallel: Error: Command line too long (<num> >= 65524)'; Running DeepVariant/DeepTrio 1.4.0 using singularity, based on the provided docker images, with many regions results in a `'parallel: Error: Command line too long (<num> >= 65524)` error. My use case is processing .bam files aligned against `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz` (containing 2500+ contigs) in parallel. Splitting by contig would result in 2500+ chunks which introduces too much overhead so I group contigs together based on the number of reads. In practice this results in chunks for chr1, ... chr22, chrX, chrY, chrM and one chunk for the other ~2500 contigs. This results in a very long command-line arg for `--regions` and thus the error. What would you think of adding an additional `--regions-file` command-line argument similar to e.g. [bcftools](https://samtools.github.io/bcftools/bcftools.html) which accepts a text file containing one region per line?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/556
https://github.com/google/deepvariant/issues/556:261,usability,error,error,261,"--regions results in 'parallel: Error: Command line too long (<num> >= 65524)'; Running DeepVariant/DeepTrio 1.4.0 using singularity, based on the provided docker images, with many regions results in a `'parallel: Error: Command line too long (<num> >= 65524)` error. My use case is processing .bam files aligned against `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz` (containing 2500+ contigs) in parallel. Splitting by contig would result in 2500+ chunks which introduces too much overhead so I group contigs together based on the number of reads. In practice this results in chunks for chr1, ... chr22, chrX, chrY, chrM and one chunk for the other ~2500 contigs. This results in a very long command-line arg for `--regions` and thus the error. What would you think of adding an additional `--regions-file` command-line argument similar to e.g. [bcftools](https://samtools.github.io/bcftools/bcftools.html) which accepts a text file containing one region per line?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/556
https://github.com/google/deepvariant/issues/556:700,usability,command,command-line,700,"--regions results in 'parallel: Error: Command line too long (<num> >= 65524)'; Running DeepVariant/DeepTrio 1.4.0 using singularity, based on the provided docker images, with many regions results in a `'parallel: Error: Command line too long (<num> >= 65524)` error. My use case is processing .bam files aligned against `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz` (containing 2500+ contigs) in parallel. Splitting by contig would result in 2500+ chunks which introduces too much overhead so I group contigs together based on the number of reads. In practice this results in chunks for chr1, ... chr22, chrX, chrY, chrM and one chunk for the other ~2500 contigs. This results in a very long command-line arg for `--regions` and thus the error. What would you think of adding an additional `--regions-file` command-line argument similar to e.g. [bcftools](https://samtools.github.io/bcftools/bcftools.html) which accepts a text file containing one region per line?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/556
https://github.com/google/deepvariant/issues/556:746,usability,error,error,746,"--regions results in 'parallel: Error: Command line too long (<num> >= 65524)'; Running DeepVariant/DeepTrio 1.4.0 using singularity, based on the provided docker images, with many regions results in a `'parallel: Error: Command line too long (<num> >= 65524)` error. My use case is processing .bam files aligned against `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz` (containing 2500+ contigs) in parallel. Splitting by contig would result in 2500+ chunks which introduces too much overhead so I group contigs together based on the number of reads. In practice this results in chunks for chr1, ... chr22, chrX, chrY, chrM and one chunk for the other ~2500 contigs. This results in a very long command-line arg for `--regions` and thus the error. What would you think of adding an additional `--regions-file` command-line argument similar to e.g. [bcftools](https://samtools.github.io/bcftools/bcftools.html) which accepts a text file containing one region per line?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/556
https://github.com/google/deepvariant/issues/556:815,usability,command,command-line,815,"--regions results in 'parallel: Error: Command line too long (<num> >= 65524)'; Running DeepVariant/DeepTrio 1.4.0 using singularity, based on the provided docker images, with many regions results in a `'parallel: Error: Command line too long (<num> >= 65524)` error. My use case is processing .bam files aligned against `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz` (containing 2500+ contigs) in parallel. Splitting by contig would result in 2500+ chunks which introduces too much overhead so I group contigs together based on the number of reads. In practice this results in chunks for chr1, ... chr22, chrX, chrY, chrM and one chunk for the other ~2500 contigs. This results in a very long command-line arg for `--regions` and thus the error. What would you think of adding an additional `--regions-file` command-line argument similar to e.g. [bcftools](https://samtools.github.io/bcftools/bcftools.html) which accepts a text file containing one region per line?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/556
https://github.com/google/deepvariant/issues/557:92,availability,avail,available,92,"Additional documentation on single-step phasing; Hi,. Is there any additional documentation available on the new single-step phasing method? It would be very helpful to have an overview of how that compares to the previous WhatsHap-based process in terms of how the phasing information is generated.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/557
https://github.com/google/deepvariant/issues/557:92,reliability,availab,available,92,"Additional documentation on single-step phasing; Hi,. Is there any additional documentation available on the new single-step phasing method? It would be very helpful to have an overview of how that compares to the previous WhatsHap-based process in terms of how the phasing information is generated.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/557
https://github.com/google/deepvariant/issues/557:92,safety,avail,available,92,"Additional documentation on single-step phasing; Hi,. Is there any additional documentation available on the new single-step phasing method? It would be very helpful to have an overview of how that compares to the previous WhatsHap-based process in terms of how the phasing information is generated.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/557
https://github.com/google/deepvariant/issues/557:92,security,availab,available,92,"Additional documentation on single-step phasing; Hi,. Is there any additional documentation available on the new single-step phasing method? It would be very helpful to have an overview of how that compares to the previous WhatsHap-based process in terms of how the phasing information is generated.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/557
https://github.com/google/deepvariant/issues/557:11,usability,document,documentation,11,"Additional documentation on single-step phasing; Hi,. Is there any additional documentation available on the new single-step phasing method? It would be very helpful to have an overview of how that compares to the previous WhatsHap-based process in terms of how the phasing information is generated.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/557
https://github.com/google/deepvariant/issues/557:78,usability,document,documentation,78,"Additional documentation on single-step phasing; Hi,. Is there any additional documentation available on the new single-step phasing method? It would be very helpful to have an overview of how that compares to the previous WhatsHap-based process in terms of how the phasing information is generated.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/557
https://github.com/google/deepvariant/issues/557:158,usability,help,helpful,158,"Additional documentation on single-step phasing; Hi,. Is there any additional documentation available on the new single-step phasing method? It would be very helpful to have an overview of how that compares to the previous WhatsHap-based process in terms of how the phasing information is generated.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/557
https://github.com/google/deepvariant/issues/558:1017,availability,error,error,1017," not found when running test-data; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Running singularity on the test data I get the following:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \ **Optional. --num_shards=20 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. My error: . ... ... Try --helpfull to get a list of all flags. deepvariant.sing.sh: line 13: --ref=/mnt/scratch/username/software/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory. deepvariant.sing.sh: line 18: make_examples: command not found. deepvariant.sing.sh: line 18: --num_shards=20: command not found. ```. I have checked and these paths and files exist and can be opened used the above links. . **Setup**. - Operating system: linux. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) test data tutorial. **Steps to reproduce:**. - Command: . - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional co",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/558
https://github.com/google/deepvariant/issues/558:1460,availability,Operat,Operating,1460,"d when running test-data; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Running singularity on the test data I get the following:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \ **Optional. --num_shards=20 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. My error: . ... ... Try --helpfull to get a list of all flags. deepvariant.sing.sh: line 13: --ref=/mnt/scratch/username/software/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory. deepvariant.sing.sh: line 18: make_examples: command not found. deepvariant.sing.sh: line 18: --num_shards=20: command not found. ```. I have checked and these paths and files exist and can be opened used the above links. . **Setup**. - Operating system: linux. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) test data tutorial. **Steps to reproduce:**. - Command: . - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/558
https://github.com/google/deepvariant/issues/558:1754,availability,Error,Error,1754,"d when running test-data; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Running singularity on the test data I get the following:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \ **Optional. --num_shards=20 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. My error: . ... ... Try --helpfull to get a list of all flags. deepvariant.sing.sh: line 13: --ref=/mnt/scratch/username/software/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory. deepvariant.sing.sh: line 18: make_examples: command not found. deepvariant.sing.sh: line 18: --num_shards=20: command not found. ```. I have checked and these paths and files exist and can be opened used the above links. . **Setup**. - Operating system: linux. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) test data tutorial. **Steps to reproduce:**. - Command: . - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/558
https://github.com/google/deepvariant/issues/558:1499,deployability,version,version,1499,"d when running test-data; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Running singularity on the test data I get the following:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \ **Optional. --num_shards=20 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. My error: . ... ... Try --helpfull to get a list of all flags. deepvariant.sing.sh: line 13: --ref=/mnt/scratch/username/software/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory. deepvariant.sing.sh: line 18: make_examples: command not found. deepvariant.sing.sh: line 18: --num_shards=20: command not found. ```. I have checked and these paths and files exist and can be opened used the above links. . **Setup**. - Operating system: linux. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) test data tutorial. **Steps to reproduce:**. - Command: . - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/558
https://github.com/google/deepvariant/issues/558:1517,deployability,Instal,Installation,1517,"d when running test-data; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Running singularity on the test data I get the following:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \ **Optional. --num_shards=20 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. My error: . ... ... Try --helpfull to get a list of all flags. deepvariant.sing.sh: line 13: --ref=/mnt/scratch/username/software/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory. deepvariant.sing.sh: line 18: make_examples: command not found. deepvariant.sing.sh: line 18: --num_shards=20: command not found. ```. I have checked and these paths and files exist and can be opened used the above links. . **Setup**. - Operating system: linux. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) test data tutorial. **Steps to reproduce:**. - Command: . - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/558
https://github.com/google/deepvariant/issues/558:927,energy efficiency,core,cores,927,"files and parameters not found when running test-data; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Running singularity on the test data I get the following:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \ **Optional. --num_shards=20 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. My error: . ... ... Try --helpfull to get a list of all flags. deepvariant.sing.sh: line 13: --ref=/mnt/scratch/username/software/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory. deepvariant.sing.sh: line 18: make_examples: command not found. deepvariant.sing.sh: line 18: --num_shards=20: command not found. ```. I have checked and these paths and files exist and can be opened used the above links. . **Setup**. - Operating system: linux. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) test data tutorial. **Steps to reproduce:**. - Command: . - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/558
https://github.com/google/deepvariant/issues/558:991,energy efficiency,CPU,CPU,991,"files and parameters not found when running test-data; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Running singularity on the test data I get the following:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \ **Optional. --num_shards=20 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. My error: . ... ... Try --helpfull to get a list of all flags. deepvariant.sing.sh: line 13: --ref=/mnt/scratch/username/software/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory. deepvariant.sing.sh: line 18: make_examples: command not found. deepvariant.sing.sh: line 18: --num_shards=20: command not found. ```. I have checked and these paths and files exist and can be opened used the above links. . **Setup**. - Operating system: linux. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) test data tutorial. **Steps to reproduce:**. - Command: . - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/558
https://github.com/google/deepvariant/issues/558:995,energy efficiency,core,cores,995,"files and parameters not found when running test-data; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Running singularity on the test data I get the following:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \ **Optional. --num_shards=20 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. My error: . ... ... Try --helpfull to get a list of all flags. deepvariant.sing.sh: line 13: --ref=/mnt/scratch/username/software/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory. deepvariant.sing.sh: line 18: make_examples: command not found. deepvariant.sing.sh: line 18: --num_shards=20: command not found. ```. I have checked and these paths and files exist and can be opened used the above links. . **Setup**. - Operating system: linux. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) test data tutorial. **Steps to reproduce:**. - Command: . - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/558
https://github.com/google/deepvariant/issues/558:1499,integrability,version,version,1499,"d when running test-data; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Running singularity on the test data I get the following:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \ **Optional. --num_shards=20 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. My error: . ... ... Try --helpfull to get a list of all flags. deepvariant.sing.sh: line 13: --ref=/mnt/scratch/username/software/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory. deepvariant.sing.sh: line 18: make_examples: command not found. deepvariant.sing.sh: line 18: --num_shards=20: command not found. ```. I have checked and these paths and files exist and can be opened used the above links. . **Setup**. - Operating system: linux. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) test data tutorial. **Steps to reproduce:**. - Command: . - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/558
https://github.com/google/deepvariant/issues/558:10,modifiability,paramet,parameters,10,"files and parameters not found when running test-data; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Running singularity on the test data I get the following:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \ **Optional. --num_shards=20 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. My error: . ... ... Try --helpfull to get a list of all flags. deepvariant.sing.sh: line 13: --ref=/mnt/scratch/username/software/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory. deepvariant.sing.sh: line 18: make_examples: command not found. deepvariant.sing.sh: line 18: --num_shards=20: command not found. ```. I have checked and these paths and files exist and can be opened used the above links. . **Setup**. - Operating system: linux. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) test data tutorial. **Steps to reproduce:**. - Command: . - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/558
https://github.com/google/deepvariant/issues/558:532,modifiability,PAC,PACBIO,532,"files and parameters not found when running test-data; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Running singularity on the test data I get the following:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \ **Optional. --num_shards=20 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. My error: . ... ... Try --helpfull to get a list of all flags. deepvariant.sing.sh: line 13: --ref=/mnt/scratch/username/software/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory. deepvariant.sing.sh: line 18: make_examples: command not found. deepvariant.sing.sh: line 18: --num_shards=20: command not found. ```. I have checked and these paths and files exist and can be opened used the above links. . **Setup**. - Operating system: linux. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) test data tutorial. **Steps to reproduce:**. - Command: . - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/558
https://github.com/google/deepvariant/issues/558:1499,modifiability,version,version,1499,"d when running test-data; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Running singularity on the test data I get the following:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \ **Optional. --num_shards=20 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. My error: . ... ... Try --helpfull to get a list of all flags. deepvariant.sing.sh: line 13: --ref=/mnt/scratch/username/software/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory. deepvariant.sing.sh: line 18: make_examples: command not found. deepvariant.sing.sh: line 18: --num_shards=20: command not found. ```. I have checked and these paths and files exist and can be opened used the above links. . **Setup**. - Operating system: linux. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) test data tutorial. **Steps to reproduce:**. - Command: . - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/558
https://github.com/google/deepvariant/issues/558:991,performance,CPU,CPU,991,"files and parameters not found when running test-data; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Running singularity on the test data I get the following:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \ **Optional. --num_shards=20 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. My error: . ... ... Try --helpfull to get a list of all flags. deepvariant.sing.sh: line 13: --ref=/mnt/scratch/username/software/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory. deepvariant.sing.sh: line 18: make_examples: command not found. deepvariant.sing.sh: line 18: --num_shards=20: command not found. ```. I have checked and these paths and files exist and can be opened used the above links. . **Setup**. - Operating system: linux. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) test data tutorial. **Steps to reproduce:**. - Command: . - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/558
https://github.com/google/deepvariant/issues/558:1017,performance,error,error,1017," not found when running test-data; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Running singularity on the test data I get the following:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \ **Optional. --num_shards=20 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. My error: . ... ... Try --helpfull to get a list of all flags. deepvariant.sing.sh: line 13: --ref=/mnt/scratch/username/software/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory. deepvariant.sing.sh: line 18: make_examples: command not found. deepvariant.sing.sh: line 18: --num_shards=20: command not found. ```. I have checked and these paths and files exist and can be opened used the above links. . **Setup**. - Operating system: linux. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) test data tutorial. **Steps to reproduce:**. - Command: . - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional co",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/558
https://github.com/google/deepvariant/issues/558:1754,performance,Error,Error,1754,"d when running test-data; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Running singularity on the test data I get the following:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \ **Optional. --num_shards=20 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. My error: . ... ... Try --helpfull to get a list of all flags. deepvariant.sing.sh: line 13: --ref=/mnt/scratch/username/software/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory. deepvariant.sing.sh: line 18: make_examples: command not found. deepvariant.sing.sh: line 18: --num_shards=20: command not found. ```. I have checked and these paths and files exist and can be opened used the above links. . **Setup**. - Operating system: linux. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) test data tutorial. **Steps to reproduce:**. - Command: . - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/558
https://github.com/google/deepvariant/issues/558:1786,reliability,Doe,Does,1786,"d when running test-data; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Running singularity on the test data I get the following:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \ **Optional. --num_shards=20 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. My error: . ... ... Try --helpfull to get a list of all flags. deepvariant.sing.sh: line 13: --ref=/mnt/scratch/username/software/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory. deepvariant.sing.sh: line 18: make_examples: command not found. deepvariant.sing.sh: line 18: --num_shards=20: command not found. ```. I have checked and these paths and files exist and can be opened used the above links. . **Setup**. - Operating system: linux. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) test data tutorial. **Steps to reproduce:**. - Command: . - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/558
https://github.com/google/deepvariant/issues/558:44,safety,test,test-data,44,"files and parameters not found when running test-data; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Running singularity on the test data I get the following:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \ **Optional. --num_shards=20 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. My error: . ... ... Try --helpfull to get a list of all flags. deepvariant.sing.sh: line 13: --ref=/mnt/scratch/username/software/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory. deepvariant.sing.sh: line 18: make_examples: command not found. deepvariant.sing.sh: line 18: --num_shards=20: command not found. ```. I have checked and these paths and files exist and can be opened used the above links. . **Setup**. - Operating system: linux. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) test data tutorial. **Steps to reproduce:**. - Command: . - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/558
https://github.com/google/deepvariant/issues/558:199,safety,test,test,199,"files and parameters not found when running test-data; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Running singularity on the test data I get the following:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \ **Optional. --num_shards=20 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. My error: . ... ... Try --helpfull to get a list of all flags. deepvariant.sing.sh: line 13: --ref=/mnt/scratch/username/software/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory. deepvariant.sing.sh: line 18: make_examples: command not found. deepvariant.sing.sh: line 18: --num_shards=20: command not found. ```. I have checked and these paths and files exist and can be opened used the above links. . **Setup**. - Operating system: linux. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) test data tutorial. **Steps to reproduce:**. - Command: . - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/558
https://github.com/google/deepvariant/issues/558:304,safety,test,testdata,304,"files and parameters not found when running test-data; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Running singularity on the test data I get the following:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \ **Optional. --num_shards=20 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. My error: . ... ... Try --helpfull to get a list of all flags. deepvariant.sing.sh: line 13: --ref=/mnt/scratch/username/software/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory. deepvariant.sing.sh: line 18: make_examples: command not found. deepvariant.sing.sh: line 18: --num_shards=20: command not found. ```. I have checked and these paths and files exist and can be opened used the above links. . **Setup**. - Operating system: linux. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) test data tutorial. **Steps to reproduce:**. - Command: . - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/558
https://github.com/google/deepvariant/issues/558:1017,safety,error,error,1017," not found when running test-data; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Running singularity on the test data I get the following:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \ **Optional. --num_shards=20 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. My error: . ... ... Try --helpfull to get a list of all flags. deepvariant.sing.sh: line 13: --ref=/mnt/scratch/username/software/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory. deepvariant.sing.sh: line 18: make_examples: command not found. deepvariant.sing.sh: line 18: --num_shards=20: command not found. ```. I have checked and these paths and files exist and can be opened used the above links. . **Setup**. - Operating system: linux. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) test data tutorial. **Steps to reproduce:**. - Command: . - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional co",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/558
https://github.com/google/deepvariant/issues/558:1155,safety,test,testdata,1155,"d when running test-data; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Running singularity on the test data I get the following:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \ **Optional. --num_shards=20 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. My error: . ... ... Try --helpfull to get a list of all flags. deepvariant.sing.sh: line 13: --ref=/mnt/scratch/username/software/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory. deepvariant.sing.sh: line 18: make_examples: command not found. deepvariant.sing.sh: line 18: --num_shards=20: command not found. ```. I have checked and these paths and files exist and can be opened used the above links. . **Setup**. - Operating system: linux. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) test data tutorial. **Steps to reproduce:**. - Command: . - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/558
https://github.com/google/deepvariant/issues/558:1694,safety,test,test,1694,"d when running test-data; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Running singularity on the test data I get the following:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \ **Optional. --num_shards=20 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. My error: . ... ... Try --helpfull to get a list of all flags. deepvariant.sing.sh: line 13: --ref=/mnt/scratch/username/software/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory. deepvariant.sing.sh: line 18: make_examples: command not found. deepvariant.sing.sh: line 18: --num_shards=20: command not found. ```. I have checked and these paths and files exist and can be opened used the above links. . **Setup**. - Operating system: linux. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) test data tutorial. **Steps to reproduce:**. - Command: . - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/558
https://github.com/google/deepvariant/issues/558:1754,safety,Error,Error,1754,"d when running test-data; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Running singularity on the test data I get the following:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \ **Optional. --num_shards=20 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. My error: . ... ... Try --helpfull to get a list of all flags. deepvariant.sing.sh: line 13: --ref=/mnt/scratch/username/software/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory. deepvariant.sing.sh: line 18: make_examples: command not found. deepvariant.sing.sh: line 18: --num_shards=20: command not found. ```. I have checked and these paths and files exist and can be opened used the above links. . **Setup**. - Operating system: linux. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) test data tutorial. **Steps to reproduce:**. - Command: . - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/558
https://github.com/google/deepvariant/issues/558:1807,safety,test,test,1807,"d when running test-data; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Running singularity on the test data I get the following:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \ **Optional. --num_shards=20 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. My error: . ... ... Try --helpfull to get a list of all flags. deepvariant.sing.sh: line 13: --ref=/mnt/scratch/username/software/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory. deepvariant.sing.sh: line 18: make_examples: command not found. deepvariant.sing.sh: line 18: --num_shards=20: command not found. ```. I have checked and these paths and files exist and can be opened used the above links. . **Setup**. - Operating system: linux. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) test data tutorial. **Steps to reproduce:**. - Command: . - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/558
https://github.com/google/deepvariant/issues/558:1843,safety,test,test,1843,"d when running test-data; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Running singularity on the test data I get the following:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \ **Optional. --num_shards=20 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. My error: . ... ... Try --helpfull to get a list of all flags. deepvariant.sing.sh: line 13: --ref=/mnt/scratch/username/software/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory. deepvariant.sing.sh: line 18: make_examples: command not found. deepvariant.sing.sh: line 18: --num_shards=20: command not found. ```. I have checked and these paths and files exist and can be opened used the above links. . **Setup**. - Operating system: linux. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) test data tutorial. **Steps to reproduce:**. - Command: . - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/558
https://github.com/google/deepvariant/issues/558:44,testability,test,test-data,44,"files and parameters not found when running test-data; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Running singularity on the test data I get the following:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \ **Optional. --num_shards=20 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. My error: . ... ... Try --helpfull to get a list of all flags. deepvariant.sing.sh: line 13: --ref=/mnt/scratch/username/software/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory. deepvariant.sing.sh: line 18: make_examples: command not found. deepvariant.sing.sh: line 18: --num_shards=20: command not found. ```. I have checked and these paths and files exist and can be opened used the above links. . **Setup**. - Operating system: linux. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) test data tutorial. **Steps to reproduce:**. - Command: . - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/558
https://github.com/google/deepvariant/issues/558:199,testability,test,test,199,"files and parameters not found when running test-data; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Running singularity on the test data I get the following:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \ **Optional. --num_shards=20 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. My error: . ... ... Try --helpfull to get a list of all flags. deepvariant.sing.sh: line 13: --ref=/mnt/scratch/username/software/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory. deepvariant.sing.sh: line 18: make_examples: command not found. deepvariant.sing.sh: line 18: --num_shards=20: command not found. ```. I have checked and these paths and files exist and can be opened used the above links. . **Setup**. - Operating system: linux. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) test data tutorial. **Steps to reproduce:**. - Command: . - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/558
https://github.com/google/deepvariant/issues/558:304,testability,test,testdata,304,"files and parameters not found when running test-data; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Running singularity on the test data I get the following:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \ **Optional. --num_shards=20 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. My error: . ... ... Try --helpfull to get a list of all flags. deepvariant.sing.sh: line 13: --ref=/mnt/scratch/username/software/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory. deepvariant.sing.sh: line 18: make_examples: command not found. deepvariant.sing.sh: line 18: --num_shards=20: command not found. ```. I have checked and these paths and files exist and can be opened used the above links. . **Setup**. - Operating system: linux. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) test data tutorial. **Steps to reproduce:**. - Command: . - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/558
https://github.com/google/deepvariant/issues/558:603,testability,unit,unittest,603,"files and parameters not found when running test-data; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Running singularity on the test data I get the following:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \ **Optional. --num_shards=20 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. My error: . ... ... Try --helpfull to get a list of all flags. deepvariant.sing.sh: line 13: --ref=/mnt/scratch/username/software/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory. deepvariant.sing.sh: line 18: make_examples: command not found. deepvariant.sing.sh: line 18: --num_shards=20: command not found. ```. I have checked and these paths and files exist and can be opened used the above links. . **Setup**. - Operating system: linux. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) test data tutorial. **Steps to reproduce:**. - Command: . - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/558
https://github.com/google/deepvariant/issues/558:1155,testability,test,testdata,1155,"d when running test-data; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Running singularity on the test data I get the following:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \ **Optional. --num_shards=20 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. My error: . ... ... Try --helpfull to get a list of all flags. deepvariant.sing.sh: line 13: --ref=/mnt/scratch/username/software/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory. deepvariant.sing.sh: line 18: make_examples: command not found. deepvariant.sing.sh: line 18: --num_shards=20: command not found. ```. I have checked and these paths and files exist and can be opened used the above links. . **Setup**. - Operating system: linux. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) test data tutorial. **Steps to reproduce:**. - Command: . - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/558
https://github.com/google/deepvariant/issues/558:1180,testability,unit,unittest,1180,"d when running test-data; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Running singularity on the test data I get the following:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \ **Optional. --num_shards=20 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. My error: . ... ... Try --helpfull to get a list of all flags. deepvariant.sing.sh: line 13: --ref=/mnt/scratch/username/software/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory. deepvariant.sing.sh: line 18: make_examples: command not found. deepvariant.sing.sh: line 18: --num_shards=20: command not found. ```. I have checked and these paths and files exist and can be opened used the above links. . **Setup**. - Operating system: linux. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) test data tutorial. **Steps to reproduce:**. - Command: . - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/558
https://github.com/google/deepvariant/issues/558:1613,testability,instrument,instrument,1613,"d when running test-data; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Running singularity on the test data I get the following:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \ **Optional. --num_shards=20 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. My error: . ... ... Try --helpfull to get a list of all flags. deepvariant.sing.sh: line 13: --ref=/mnt/scratch/username/software/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory. deepvariant.sing.sh: line 18: make_examples: command not found. deepvariant.sing.sh: line 18: --num_shards=20: command not found. ```. I have checked and these paths and files exist and can be opened used the above links. . **Setup**. - Operating system: linux. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) test data tutorial. **Steps to reproduce:**. - Command: . - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/558
https://github.com/google/deepvariant/issues/558:1694,testability,test,test,1694,"d when running test-data; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Running singularity on the test data I get the following:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \ **Optional. --num_shards=20 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. My error: . ... ... Try --helpfull to get a list of all flags. deepvariant.sing.sh: line 13: --ref=/mnt/scratch/username/software/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory. deepvariant.sing.sh: line 18: make_examples: command not found. deepvariant.sing.sh: line 18: --num_shards=20: command not found. ```. I have checked and these paths and files exist and can be opened used the above links. . **Setup**. - Operating system: linux. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) test data tutorial. **Steps to reproduce:**. - Command: . - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/558
https://github.com/google/deepvariant/issues/558:1760,testability,trace,trace,1760,"d when running test-data; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Running singularity on the test data I get the following:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \ **Optional. --num_shards=20 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. My error: . ... ... Try --helpfull to get a list of all flags. deepvariant.sing.sh: line 13: --ref=/mnt/scratch/username/software/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory. deepvariant.sing.sh: line 18: make_examples: command not found. deepvariant.sing.sh: line 18: --num_shards=20: command not found. ```. I have checked and these paths and files exist and can be opened used the above links. . **Setup**. - Operating system: linux. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) test data tutorial. **Steps to reproduce:**. - Command: . - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/558
https://github.com/google/deepvariant/issues/558:1807,testability,test,test,1807,"d when running test-data; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Running singularity on the test data I get the following:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \ **Optional. --num_shards=20 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. My error: . ... ... Try --helpfull to get a list of all flags. deepvariant.sing.sh: line 13: --ref=/mnt/scratch/username/software/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory. deepvariant.sing.sh: line 18: make_examples: command not found. deepvariant.sing.sh: line 18: --num_shards=20: command not found. ```. I have checked and these paths and files exist and can be opened used the above links. . **Setup**. - Operating system: linux. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) test data tutorial. **Steps to reproduce:**. - Command: . - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/558
https://github.com/google/deepvariant/issues/558:1843,testability,test,test,1843,"d when running test-data; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Running singularity on the test data I get the following:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \ **Optional. --num_shards=20 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. My error: . ... ... Try --helpfull to get a list of all flags. deepvariant.sing.sh: line 13: --ref=/mnt/scratch/username/software/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory. deepvariant.sing.sh: line 18: make_examples: command not found. deepvariant.sing.sh: line 18: --num_shards=20: command not found. ```. I have checked and these paths and files exist and can be opened used the above links. . **Setup**. - Operating system: linux. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) test data tutorial. **Steps to reproduce:**. - Command: . - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/558
https://github.com/google/deepvariant/issues/558:2018,testability,context,context,2018,"d when running test-data; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Running singularity on the test data I get the following:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \ **Optional. --num_shards=20 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. My error: . ... ... Try --helpfull to get a list of all flags. deepvariant.sing.sh: line 13: --ref=/mnt/scratch/username/software/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory. deepvariant.sing.sh: line 18: make_examples: command not found. deepvariant.sing.sh: line 18: --num_shards=20: command not found. ```. I have checked and these paths and files exist and can be opened used the above links. . **Setup**. - Operating system: linux. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) test data tutorial. **Steps to reproduce:**. - Command: . - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/558
https://github.com/google/deepvariant/issues/558:1017,usability,error,error,1017," not found when running test-data; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Running singularity on the test data I get the following:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \ **Optional. --num_shards=20 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. My error: . ... ... Try --helpfull to get a list of all flags. deepvariant.sing.sh: line 13: --ref=/mnt/scratch/username/software/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory. deepvariant.sing.sh: line 18: make_examples: command not found. deepvariant.sing.sh: line 18: --num_shards=20: command not found. ```. I have checked and these paths and files exist and can be opened used the above links. . **Setup**. - Operating system: linux. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) test data tutorial. **Steps to reproduce:**. - Command: . - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional co",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/558
https://github.com/google/deepvariant/issues/558:1040,usability,help,helpfull,1040,"d when running test-data; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Running singularity on the test data I get the following:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \ **Optional. --num_shards=20 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. My error: . ... ... Try --helpfull to get a list of all flags. deepvariant.sing.sh: line 13: --ref=/mnt/scratch/username/software/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory. deepvariant.sing.sh: line 18: make_examples: command not found. deepvariant.sing.sh: line 18: --num_shards=20: command not found. ```. I have checked and these paths and files exist and can be opened used the above links. . **Setup**. - Operating system: linux. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) test data tutorial. **Steps to reproduce:**. - Command: . - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/558
https://github.com/google/deepvariant/issues/558:1126,usability,user,username,1126,"d when running test-data; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Running singularity on the test data I get the following:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \ **Optional. --num_shards=20 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. My error: . ... ... Try --helpfull to get a list of all flags. deepvariant.sing.sh: line 13: --ref=/mnt/scratch/username/software/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory. deepvariant.sing.sh: line 18: make_examples: command not found. deepvariant.sing.sh: line 18: --num_shards=20: command not found. ```. I have checked and these paths and files exist and can be opened used the above links. . **Setup**. - Operating system: linux. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) test data tutorial. **Steps to reproduce:**. - Command: . - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/558
https://github.com/google/deepvariant/issues/558:1268,usability,command,command,1268,"d when running test-data; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Running singularity on the test data I get the following:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \ **Optional. --num_shards=20 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. My error: . ... ... Try --helpfull to get a list of all flags. deepvariant.sing.sh: line 13: --ref=/mnt/scratch/username/software/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory. deepvariant.sing.sh: line 18: make_examples: command not found. deepvariant.sing.sh: line 18: --num_shards=20: command not found. ```. I have checked and these paths and files exist and can be opened used the above links. . **Setup**. - Operating system: linux. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) test data tutorial. **Steps to reproduce:**. - Command: . - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/558
https://github.com/google/deepvariant/issues/558:1334,usability,command,command,1334,"d when running test-data; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Running singularity on the test data I get the following:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \ **Optional. --num_shards=20 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. My error: . ... ... Try --helpfull to get a list of all flags. deepvariant.sing.sh: line 13: --ref=/mnt/scratch/username/software/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory. deepvariant.sing.sh: line 18: make_examples: command not found. deepvariant.sing.sh: line 18: --num_shards=20: command not found. ```. I have checked and these paths and files exist and can be opened used the above links. . **Setup**. - Operating system: linux. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) test data tutorial. **Steps to reproduce:**. - Command: . - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/558
https://github.com/google/deepvariant/issues/558:1741,usability,Command,Command,1741,"d when running test-data; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Running singularity on the test data I get the following:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \ **Optional. --num_shards=20 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. My error: . ... ... Try --helpfull to get a list of all flags. deepvariant.sing.sh: line 13: --ref=/mnt/scratch/username/software/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory. deepvariant.sing.sh: line 18: make_examples: command not found. deepvariant.sing.sh: line 18: --num_shards=20: command not found. ```. I have checked and these paths and files exist and can be opened used the above links. . **Setup**. - Operating system: linux. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) test data tutorial. **Steps to reproduce:**. - Command: . - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/558
https://github.com/google/deepvariant/issues/558:1754,usability,Error,Error,1754,"d when running test-data; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Running singularity on the test data I get the following:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". INPUT_DIR=""${PWD}/quickstart-testdata"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \ **Optional. --num_shards=20 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. My error: . ... ... Try --helpfull to get a list of all flags. deepvariant.sing.sh: line 13: --ref=/mnt/scratch/username/software/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory. deepvariant.sing.sh: line 18: make_examples: command not found. deepvariant.sing.sh: line 18: --num_shards=20: command not found. ```. I have checked and these paths and files exist and can be opened used the above links. . **Setup**. - Operating system: linux. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) test data tutorial. **Steps to reproduce:**. - Command: . - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/558
https://github.com/google/deepvariant/issues/559:87,availability,error,error,87,"Dynamic cast failed; I am trying to run this PacBio use case and encountered following error: . https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md. Any hint/info to solve this issue? . ```. INFO: Using cached SIF image. I0828 10:16:33.630316 22957909862208 run_deepvariant.py:342] Re-using the directory for intermediate results in /scratch-local/tahmad.1459036/tmpcy60f694. ***** Intermediate results will be written to /scratch-local/tahmad.1459036/tmpcy60f694 in docker. ****. ***** Running the command:*****. time seq 0 71 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord@72.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. 2022-08-28 10:16:41.685530: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685937 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693562 22858289674048 make_examples_core.py:243] Task 0/72: Preparing inputs. 2022-08-28 10:16:41.685378: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685891 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693572 23090607179584 make_examples_core.py:243] Task 51/72: Preparing inputs. 2022-08-28 10:16:41.910178: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:13,deployability,fail,failed,13,"Dynamic cast failed; I am trying to run this PacBio use case and encountered following error: . https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md. Any hint/info to solve this issue? . ```. INFO: Using cached SIF image. I0828 10:16:33.630316 22957909862208 run_deepvariant.py:342] Re-using the directory for intermediate results in /scratch-local/tahmad.1459036/tmpcy60f694. ***** Intermediate results will be written to /scratch-local/tahmad.1459036/tmpcy60f694 in docker. ****. ***** Running the command:*****. time seq 0 71 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord@72.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. 2022-08-28 10:16:41.685530: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685937 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693562 22858289674048 make_examples_core.py:243] Task 0/72: Preparing inputs. 2022-08-28 10:16:41.685378: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685891 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693572 23090607179584 make_examples_core.py:243] Task 51/72: Preparing inputs. 2022-08-28 10:16:41.910178: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:7392,deployability,Fail,Failed,7392,"der.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.621521: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.621896 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. 2022-08-28 10:16:42.702562: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.702797 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.727535 22858289674048 make_examples_core.py:243] Task 0/72: Writing examples to /scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord-00000-of-00072.gz. I0828 10:16:42.727609 22858289674048 make_examples_core.py:243] Task 0/72: Overhead for preparing inputs: 1 seconds. 2022-08-28 10:16:42.748975: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 170, in main. make_examples_core.make_examples_runner(options). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795, in make_examples_runner. runtimes) = region_processor.process(region). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:7621,deployability,modul,module,7621,"289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. 2022-08-28 10:16:42.702562: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.702797 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.727535 22858289674048 make_examples_core.py:243] Task 0/72: Writing examples to /scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord-00000-of-00072.gz. I0828 10:16:42.727609 22858289674048 make_examples_core.py:243] Task 0/72: Overhead for preparing inputs: 1 seconds. 2022-08-28 10:16:42.748975: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 170, in main. make_examples_core.make_examples_runner(options). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795, in make_examples_runner. runtimes) = region_processor.process(region). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1123, in process. reads = self.region_reads(. File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", l",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:9159,deployability,fail,failed,9159,"_2e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795, in make_examples_runner. runtimes) = region_processor.process(region). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1123, in process. reads = self.region_reads(. File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1200, in region_reads. reads.extend(sam_reader.query(region)). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. 2022-08-28 10:16:42.621554: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.621905 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. 2022-08-28 10:16:42.705882: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.706136 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.727798 23090607179584 make_examples_core.py:243] Task 51/72: Writing examples to /scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord-00051-of-00072.gz. I0828 10:16:42.727886 23090607179584 make_examples_core.py:243] Task 51/72: Overhead for preparing inputs: 1 seconds. 2022-08-28 10:16:42.768980: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMes",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:10113,deployability,Fail,Failed,10113,") argument read is not valid: Dynamic cast failed. 2022-08-28 10:16:42.621554: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.621905 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. 2022-08-28 10:16:42.705882: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.706136 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.727798 23090607179584 make_examples_core.py:243] Task 51/72: Writing examples to /scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord-00051-of-00072.gz. I0828 10:16:42.727886 23090607179584 make_examples_core.py:243] Task 51/72: Overhead for preparing inputs: 1 seconds. 2022-08-28 10:16:42.768980: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 170, in main. make_examples_core.make_examples_runner(options). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795, in make_examples_runner. runtimes) = region_processor.process(region). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:10342,deployability,modul,module,10342,"7179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. 2022-08-28 10:16:42.705882: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.706136 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.727798 23090607179584 make_examples_core.py:243] Task 51/72: Writing examples to /scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord-00051-of-00072.gz. I0828 10:16:42.727886 23090607179584 make_examples_core.py:243] Task 51/72: Overhead for preparing inputs: 1 seconds. 2022-08-28 10:16:42.768980: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 170, in main. make_examples_core.make_examples_runner(options). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795, in make_examples_runner. runtimes) = region_processor.process(region). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1123, in process. reads = self.region_reads(. File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", l",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:11880,deployability,fail,failed,11880,"vkm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795, in make_examples_runner. runtimes) = region_processor.process(region). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1123, in process. reads = self.region_reads(. File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1200, in region_reads. reads.extend(sam_reader.query(region)). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. 2022-08-28 10:16:42.783298: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.783742 22429787215680 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.790578 22429787215680 make_examples_core.py:243] Task 35/72: Preparing inputs. 2022-08-28 10:16:42.806732: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.806946 22429787215680 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.814701 22429787215680 make_examples_core.py:243] Task 35/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:42.824596 22429",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:168,energy efficiency,model,model-case-study,168,"Dynamic cast failed; I am trying to run this PacBio use case and encountered following error: . https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md. Any hint/info to solve this issue? . ```. INFO: Using cached SIF image. I0828 10:16:33.630316 22957909862208 run_deepvariant.py:342] Re-using the directory for intermediate results in /scratch-local/tahmad.1459036/tmpcy60f694. ***** Intermediate results will be written to /scratch-local/tahmad.1459036/tmpcy60f694 in docker. ****. ***** Running the command:*****. time seq 0 71 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord@72.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. 2022-08-28 10:16:41.685530: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685937 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693562 22858289674048 make_examples_core.py:243] Task 0/72: Preparing inputs. 2022-08-28 10:16:41.685378: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685891 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693572 23090607179584 make_examples_core.py:243] Task 51/72: Preparing inputs. 2022-08-28 10:16:41.910178: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:598,integrability,buffer,buffer,598,"Dynamic cast failed; I am trying to run this PacBio use case and encountered following error: . https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md. Any hint/info to solve this issue? . ```. INFO: Using cached SIF image. I0828 10:16:33.630316 22957909862208 run_deepvariant.py:342] Re-using the directory for intermediate results in /scratch-local/tahmad.1459036/tmpcy60f694. ***** Intermediate results will be written to /scratch-local/tahmad.1459036/tmpcy60f694 in docker. ****. ***** Running the command:*****. time seq 0 71 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord@72.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. 2022-08-28 10:16:41.685530: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685937 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693562 22858289674048 make_examples_core.py:243] Task 0/72: Preparing inputs. 2022-08-28 10:16:41.685378: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685891 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693572 23090607179584 make_examples_core.py:243] Task 51/72: Preparing inputs. 2022-08-28 10:16:41.910178: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:1290,interoperability,coordinat,coordinate,1290,"08 run_deepvariant.py:342] Re-using the directory for intermediate results in /scratch-local/tahmad.1459036/tmpcy60f694. ***** Intermediate results will be written to /scratch-local/tahmad.1459036/tmpcy60f694 in docker. ****. ***** Running the command:*****. time seq 0 71 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord@72.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. 2022-08-28 10:16:41.685530: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685937 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693562 22858289674048 make_examples_core.py:243] Task 0/72: Preparing inputs. 2022-08-28 10:16:41.685378: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685891 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693572 23090607179584 make_examples_core.py:243] Task 51/72: Preparing inputs. 2022-08-28 10:16:41.910178: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.910383 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.917916 22858289674048 make_examples_core.py:243] Task 0/72: Common contigs are ['chr1",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:1665,interoperability,coordinat,coordinate,1665,"e/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord@72.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. 2022-08-28 10:16:41.685530: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685937 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693562 22858289674048 make_examples_core.py:243] Task 0/72: Preparing inputs. 2022-08-28 10:16:41.685378: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685891 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693572 23090607179584 make_examples_core.py:243] Task 51/72: Preparing inputs. 2022-08-28 10:16:41.910178: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.910383 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.917916 22858289674048 make_examples_core.py:243] Task 0/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:41.927865 22858289674048 make_examples_core.py:243] Task 0/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, n",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:2041,interoperability,coordinat,coordinate,2041,"orealign_reads --regions ""chr20"" --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. 2022-08-28 10:16:41.685530: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685937 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693562 22858289674048 make_examples_core.py:243] Task 0/72: Preparing inputs. 2022-08-28 10:16:41.685378: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685891 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693572 23090607179584 make_examples_core.py:243] Task 51/72: Preparing inputs. 2022-08-28 10:16:41.910178: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.910383 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.917916 22858289674048 make_examples_core.py:243] Task 0/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:41.927865 22858289674048 make_examples_core.py:243] Task 0/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:41.928017: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:41.910192: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.910406 23090607179584 ge",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:2986,interoperability,coordinat,coordinate,2986,"n tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.910383 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.917916 22858289674048 make_examples_core.py:243] Task 0/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:41.927865 22858289674048 make_examples_core.py:243] Task 0/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:41.928017: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:41.910192: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.910406 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.918358 23090607179584 make_examples_core.py:243] Task 51/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:41.928208 23090607179584 make_examples_core.py:243] Task 51/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:41.928362: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.297998: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.298424 22358796187456 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:3933,interoperability,coordinat,coordinate,3933,"tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.910406 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.918358 23090607179584 make_examples_core.py:243] Task 51/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:41.928208 23090607179584 make_examples_core.py:243] Task 51/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:41.928362: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.297998: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.298424 22358796187456 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.304855 22358796187456 make_examples_core.py:243] Task 9/72: Preparing inputs. 2022-08-28 10:16:42.306635: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.306837 22358796187456 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.314611 22358796187456 make_examples_core.py:243] Task 9/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:42.324295 22358796187456 make_examples_core.py:243] Task 9/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, no",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:4308,interoperability,coordinat,coordinate,4308,"0', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:41.928208 23090607179584 make_examples_core.py:243] Task 51/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:41.928362: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.297998: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.298424 22358796187456 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.304855 22358796187456 make_examples_core.py:243] Task 9/72: Preparing inputs. 2022-08-28 10:16:42.306635: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.306837 22358796187456 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.314611 22358796187456 make_examples_core.py:243] Task 9/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:42.324295 22358796187456 make_examples_core.py:243] Task 9/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:42.324449: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.229368: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.229775 22823217350464 ge",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:5253,interoperability,coordinat,coordinate,5253,"n tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.306837 22358796187456 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.314611 22358796187456 make_examples_core.py:243] Task 9/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:42.324295 22358796187456 make_examples_core.py:243] Task 9/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:42.324449: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.229368: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.229775 22823217350464 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.236391 22823217350464 make_examples_core.py:243] Task 16/72: Preparing inputs. 2022-08-28 10:16:42.257786: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.257981 22823217350464 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.265606 22823217350464 make_examples_core.py:243] Task 16/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:42.275274 22823217350464 make_examples_core.py:243] Task 16/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:5629,interoperability,coordinat,coordinate,5629,"0', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:42.324295 22358796187456 make_examples_core.py:243] Task 9/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:42.324449: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.229368: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.229775 22823217350464 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.236391 22823217350464 make_examples_core.py:243] Task 16/72: Preparing inputs. 2022-08-28 10:16:42.257786: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.257981 22823217350464 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.265606 22823217350464 make_examples_core.py:243] Task 16/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:42.275274 22823217350464 make_examples_core.py:243] Task 16/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:42.275424: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.621521: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.621896 22858289674048 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:6576,interoperability,coordinat,coordinate,6576,"tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.257981 22823217350464 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.265606 22823217350464 make_examples_core.py:243] Task 16/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:42.275274 22823217350464 make_examples_core.py:243] Task 16/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:42.275424: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.621521: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.621896 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. 2022-08-28 10:16:42.702562: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.702797 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.727535 22858289674048 make_examples_core.py:243] Task 0/72: Writing examples to /scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord-00000-of-00072.gz. I0828 10:16:42.727609 22858289674048 make_examples_core.py:243] Task 0/72: Overhead for preparing inputs: 1 seconds. 2022-08-28 10:16:42.748975: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepv",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:6858,interoperability,coordinat,coordinate,6858,"on contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:42.275274 22823217350464 make_examples_core.py:243] Task 16/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:42.275424: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.621521: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.621896 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. 2022-08-28 10:16:42.702562: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.702797 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.727535 22858289674048 make_examples_core.py:243] Task 0/72: Writing examples to /scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord-00000-of-00072.gz. I0828 10:16:42.727609 22858289674048 make_examples_core.py:243] Task 0/72: Overhead for preparing inputs: 1 seconds. 2022-08-28 10:16:42.748975: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/absl_py/absl/app.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:9295,interoperability,coordinat,coordinate,9295,"rocess(region). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1123, in process. reads = self.region_reads(. File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1200, in region_reads. reads.extend(sam_reader.query(region)). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. 2022-08-28 10:16:42.621554: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.621905 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. 2022-08-28 10:16:42.705882: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.706136 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.727798 23090607179584 make_examples_core.py:243] Task 51/72: Writing examples to /scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord-00051-of-00072.gz. I0828 10:16:42.727886 23090607179584 make_examples_core.py:243] Task 51/72: Overhead for preparing inputs: 1 seconds. 2022-08-28 10:16:42.768980: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/dee",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:9577,interoperability,coordinat,coordinate,9577,"iant/deepvariant/make_examples_core.py"", line 1200, in region_reads. reads.extend(sam_reader.query(region)). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. 2022-08-28 10:16:42.621554: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.621905 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. 2022-08-28 10:16:42.705882: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.706136 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.727798 23090607179584 make_examples_core.py:243] Task 51/72: Writing examples to /scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord-00051-of-00072.gz. I0828 10:16:42.727886 23090607179584 make_examples_core.py:243] Task 51/72: Overhead for preparing inputs: 1 seconds. 2022-08-28 10:16:42.768980: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/absl_py/absl/ap",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:12016,interoperability,coordinat,coordinate,12016,"rocess(region). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1123, in process. reads = self.region_reads(. File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1200, in region_reads. reads.extend(sam_reader.query(region)). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. 2022-08-28 10:16:42.783298: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.783742 22429787215680 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.790578 22429787215680 make_examples_core.py:243] Task 35/72: Preparing inputs. 2022-08-28 10:16:42.806732: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.806946 22429787215680 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.814701 22429787215680 make_examples_core.py:243] Task 35/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:42.824596 22429787215680 make_examples_core.py:243] Task 35/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:12392,interoperability,coordinat,coordinate,12392,"uery(region)). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. 2022-08-28 10:16:42.783298: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.783742 22429787215680 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.790578 22429787215680 make_examples_core.py:243] Task 35/72: Preparing inputs. 2022-08-28 10:16:42.806732: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.806946 22429787215680 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.814701 22429787215680 make_examples_core.py:243] Task 35/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:42.824596 22429787215680 make_examples_core.py:243] Task 35/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:42.824762: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.621537: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.621887 22358796187456 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:13339,interoperability,coordinat,coordinate,13339,"l.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. 2022-08-28 10:16:42.783298: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.783742 22429787215680 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.790578 22429787215680 make_examples_core.py:243] Task 35/72: Preparing inputs. 2022-08-28 10:16:42.806732: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.806946 22429787215680 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.814701 22429787215680 make_examples_core.py:243] Task 35/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:42.824596 22429787215680 make_examples_core.py:243] Task 35/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:42.824762: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.621537: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.621887 22358796187456 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. 2022-08-28 10:16:42.702981: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:13621,interoperability,coordinat,coordinate,13621,"l.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. 2022-08-28 10:16:42.783298: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.783742 22429787215680 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.790578 22429787215680 make_examples_core.py:243] Task 35/72: Preparing inputs. 2022-08-28 10:16:42.806732: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.806946 22429787215680 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.814701 22429787215680 make_examples_core.py:243] Task 35/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:42.824596 22429787215680 make_examples_core.py:243] Task 35/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:42.824762: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.621537: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.621887 22358796187456 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. 2022-08-28 10:16:42.702981: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:45,modifiability,Pac,PacBio,45,"Dynamic cast failed; I am trying to run this PacBio use case and encountered following error: . https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md. Any hint/info to solve this issue? . ```. INFO: Using cached SIF image. I0828 10:16:33.630316 22957909862208 run_deepvariant.py:342] Re-using the directory for intermediate results in /scratch-local/tahmad.1459036/tmpcy60f694. ***** Intermediate results will be written to /scratch-local/tahmad.1459036/tmpcy60f694 in docker. ****. ***** Running the command:*****. time seq 0 71 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord@72.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. 2022-08-28 10:16:41.685530: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685937 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693562 22858289674048 make_examples_core.py:243] Task 0/72: Preparing inputs. 2022-08-28 10:16:41.685378: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685891 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693572 23090607179584 make_examples_core.py:243] Task 51/72: Preparing inputs. 2022-08-28 10:16:41.910178: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:161,modifiability,pac,pacbio-model-case-study,161,"Dynamic cast failed; I am trying to run this PacBio use case and encountered following error: . https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md. Any hint/info to solve this issue? . ```. INFO: Using cached SIF image. I0828 10:16:33.630316 22957909862208 run_deepvariant.py:342] Re-using the directory for intermediate results in /scratch-local/tahmad.1459036/tmpcy60f694. ***** Intermediate results will be written to /scratch-local/tahmad.1459036/tmpcy60f694 in docker. ****. ***** Running the command:*****. time seq 0 71 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord@72.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. 2022-08-28 10:16:41.685530: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685937 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693562 22858289674048 make_examples_core.py:243] Task 0/72: Preparing inputs. 2022-08-28 10:16:41.685378: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685891 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693572 23090607179584 make_examples_core.py:243] Task 51/72: Preparing inputs. 2022-08-28 10:16:41.910178: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:349,modifiability,interm,intermediate,349,"Dynamic cast failed; I am trying to run this PacBio use case and encountered following error: . https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md. Any hint/info to solve this issue? . ```. INFO: Using cached SIF image. I0828 10:16:33.630316 22957909862208 run_deepvariant.py:342] Re-using the directory for intermediate results in /scratch-local/tahmad.1459036/tmpcy60f694. ***** Intermediate results will be written to /scratch-local/tahmad.1459036/tmpcy60f694 in docker. ****. ***** Running the command:*****. time seq 0 71 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord@72.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. 2022-08-28 10:16:41.685530: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685937 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693562 22858289674048 make_examples_core.py:243] Task 0/72: Preparing inputs. 2022-08-28 10:16:41.685378: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685891 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693572 23090607179584 make_examples_core.py:243] Task 51/72: Preparing inputs. 2022-08-28 10:16:41.910178: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:422,modifiability,Interm,Intermediate,422,"Dynamic cast failed; I am trying to run this PacBio use case and encountered following error: . https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md. Any hint/info to solve this issue? . ```. INFO: Using cached SIF image. I0828 10:16:33.630316 22957909862208 run_deepvariant.py:342] Re-using the directory for intermediate results in /scratch-local/tahmad.1459036/tmpcy60f694. ***** Intermediate results will be written to /scratch-local/tahmad.1459036/tmpcy60f694 in docker. ****. ***** Running the command:*****. time seq 0 71 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord@72.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. 2022-08-28 10:16:41.685530: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685937 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693562 22858289674048 make_examples_core.py:243] Task 0/72: Preparing inputs. 2022-08-28 10:16:41.685378: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685891 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693572 23090607179584 make_examples_core.py:243] Task 51/72: Preparing inputs. 2022-08-28 10:16:41.910178: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:2687,modifiability,deco,decode,2687,"8 10:16:41.685891 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693572 23090607179584 make_examples_core.py:243] Task 51/72: Preparing inputs. 2022-08-28 10:16:41.910178: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.910383 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.917916 22858289674048 make_examples_core.py:243] Task 0/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:41.927865 22858289674048 make_examples_core.py:243] Task 0/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:41.928017: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:41.910192: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.910406 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.918358 23090607179584 make_examples_core.py:243] Task 51/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:41.928208 23090607179584 make_examples_core.py:243] Task 51/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:3634,modifiability,deco,decode,3634,"e. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:41.928017: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:41.910192: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.910406 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.918358 23090607179584 make_examples_core.py:243] Task 51/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:41.928208 23090607179584 make_examples_core.py:243] Task 51/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:41.928362: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.297998: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.298424 22358796187456 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.304855 22358796187456 make_examples_core.py:243] Task 9/72: Preparing inputs. 2022-08-28 10:16:42.306635: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.306837 22358796187456 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.314611 22358796187456 make_examples_core.py:243] Task 9/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10',",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:4954,modifiability,deco,decode,4954,"28 10:16:42.298424 22358796187456 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.304855 22358796187456 make_examples_core.py:243] Task 9/72: Preparing inputs. 2022-08-28 10:16:42.306635: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.306837 22358796187456 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.314611 22358796187456 make_examples_core.py:243] Task 9/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:42.324295 22358796187456 make_examples_core.py:243] Task 9/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:42.324449: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.229368: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.229775 22823217350464 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.236391 22823217350464 make_examples_core.py:243] Task 16/72: Preparing inputs. 2022-08-28 10:16:42.257786: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.257981 22823217350464 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.265606 22823217350464 make_examples_core.py:243] Task 16/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:6277,modifiability,deco,decode,6277,"10:16:42.229775 22823217350464 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.236391 22823217350464 make_examples_core.py:243] Task 16/72: Preparing inputs. 2022-08-28 10:16:42.257786: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.257981 22823217350464 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.265606 22823217350464 make_examples_core.py:243] Task 16/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:42.275274 22823217350464 make_examples_core.py:243] Task 16/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:42.275424: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.621521: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.621896 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. 2022-08-28 10:16:42.702562: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.702797 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.727535 22858289674048 make_examples_core.py:243] Task 0/72: Writing examples to /scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord-00000-of-00072.gz. I0828 10:16:42.727609 22858289674048 make_examples_core.py:243] Task 0/72: Overhead for p",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:7621,modifiability,modul,module,7621,"289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. 2022-08-28 10:16:42.702562: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.702797 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.727535 22858289674048 make_examples_core.py:243] Task 0/72: Writing examples to /scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord-00000-of-00072.gz. I0828 10:16:42.727609 22858289674048 make_examples_core.py:243] Task 0/72: Overhead for preparing inputs: 1 seconds. 2022-08-28 10:16:42.748975: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 170, in main. make_examples_core.make_examples_runner(options). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795, in make_examples_runner. runtimes) = region_processor.process(region). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1123, in process. reads = self.region_reads(. File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", l",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:8657,modifiability,exten,extend,8657,"local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 170, in main. make_examples_core.make_examples_runner(options). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795, in make_examples_runner. runtimes) = region_processor.process(region). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1123, in process. reads = self.region_reads(. File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1200, in region_reads. reads.extend(sam_reader.query(region)). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. 2022-08-28 10:16:42.621554: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.621905 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. 2022-08-28 10:16:42.705882: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.706136 23090607179584 genomics_reader.py:222] R",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:10342,modifiability,modul,module,10342,"7179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. 2022-08-28 10:16:42.705882: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.706136 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.727798 23090607179584 make_examples_core.py:243] Task 51/72: Writing examples to /scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord-00051-of-00072.gz. I0828 10:16:42.727886 23090607179584 make_examples_core.py:243] Task 51/72: Overhead for preparing inputs: 1 seconds. 2022-08-28 10:16:42.768980: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 170, in main. make_examples_core.make_examples_runner(options). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795, in make_examples_runner. runtimes) = region_processor.process(region). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1123, in process. reads = self.region_reads(. File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", l",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:11378,modifiability,exten,extend,11378,"local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 170, in main. make_examples_core.make_examples_runner(options). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795, in make_examples_runner. runtimes) = region_processor.process(region). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1123, in process. reads = self.region_reads(. File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1200, in region_reads. reads.extend(sam_reader.query(region)). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. 2022-08-28 10:16:42.783298: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.783742 22429787215680 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.790578 22429787215680 make_examples_core.py:243] Task 35/72: Preparing inputs. 2022-08-28 10:16:42.806732: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:13040,modifiability,deco,decode,13040,"l.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. 2022-08-28 10:16:42.783298: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.783742 22429787215680 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.790578 22429787215680 make_examples_core.py:243] Task 35/72: Preparing inputs. 2022-08-28 10:16:42.806732: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.806946 22429787215680 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.814701 22429787215680 make_examples_core.py:243] Task 35/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:42.824596 22429787215680 make_examples_core.py:243] Task 35/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:42.824762: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.621537: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.621887 22358796187456 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. 2022-08-28 10:16:42.702981: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:87,performance,error,error,87,"Dynamic cast failed; I am trying to run this PacBio use case and encountered following error: . https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md. Any hint/info to solve this issue? . ```. INFO: Using cached SIF image. I0828 10:16:33.630316 22957909862208 run_deepvariant.py:342] Re-using the directory for intermediate results in /scratch-local/tahmad.1459036/tmpcy60f694. ***** Intermediate results will be written to /scratch-local/tahmad.1459036/tmpcy60f694 in docker. ****. ***** Running the command:*****. time seq 0 71 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord@72.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. 2022-08-28 10:16:41.685530: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685937 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693562 22858289674048 make_examples_core.py:243] Task 0/72: Preparing inputs. 2022-08-28 10:16:41.685378: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685891 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693572 23090607179584 make_examples_core.py:243] Task 51/72: Preparing inputs. 2022-08-28 10:16:41.910178: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:243,performance,cach,cached,243,"Dynamic cast failed; I am trying to run this PacBio use case and encountered following error: . https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md. Any hint/info to solve this issue? . ```. INFO: Using cached SIF image. I0828 10:16:33.630316 22957909862208 run_deepvariant.py:342] Re-using the directory for intermediate results in /scratch-local/tahmad.1459036/tmpcy60f694. ***** Intermediate results will be written to /scratch-local/tahmad.1459036/tmpcy60f694 in docker. ****. ***** Running the command:*****. time seq 0 71 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord@72.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. 2022-08-28 10:16:41.685530: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685937 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693562 22858289674048 make_examples_core.py:243] Task 0/72: Preparing inputs. 2022-08-28 10:16:41.685378: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685891 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693572 23090607179584 make_examples_core.py:243] Task 51/72: Preparing inputs. 2022-08-28 10:16:41.910178: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:554,performance,time,time,554,"Dynamic cast failed; I am trying to run this PacBio use case and encountered following error: . https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md. Any hint/info to solve this issue? . ```. INFO: Using cached SIF image. I0828 10:16:33.630316 22957909862208 run_deepvariant.py:342] Re-using the directory for intermediate results in /scratch-local/tahmad.1459036/tmpcy60f694. ***** Intermediate results will be written to /scratch-local/tahmad.1459036/tmpcy60f694 in docker. ****. ***** Running the command:*****. time seq 0 71 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord@72.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. 2022-08-28 10:16:41.685530: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685937 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693562 22858289674048 make_examples_core.py:243] Task 0/72: Preparing inputs. 2022-08-28 10:16:41.685378: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685891 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693572 23090607179584 make_examples_core.py:243] Task 51/72: Preparing inputs. 2022-08-28 10:16:41.910178: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:570,performance,parallel,parallel,570,"Dynamic cast failed; I am trying to run this PacBio use case and encountered following error: . https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md. Any hint/info to solve this issue? . ```. INFO: Using cached SIF image. I0828 10:16:33.630316 22957909862208 run_deepvariant.py:342] Re-using the directory for intermediate results in /scratch-local/tahmad.1459036/tmpcy60f694. ***** Intermediate results will be written to /scratch-local/tahmad.1459036/tmpcy60f694 in docker. ****. ***** Running the command:*****. time seq 0 71 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord@72.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. 2022-08-28 10:16:41.685530: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685937 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693562 22858289674048 make_examples_core.py:243] Task 0/72: Preparing inputs. 2022-08-28 10:16:41.685378: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685891 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693572 23090607179584 make_examples_core.py:243] Task 51/72: Preparing inputs. 2022-08-28 10:16:41.910178: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:7266,performance,Overhead,Overhead,7266,"e will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:42.275424: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.621521: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.621896 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. 2022-08-28 10:16:42.702562: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.702797 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.727535 22858289674048 make_examples_core.py:243] Task 0/72: Writing examples to /scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord-00000-of-00072.gz. I0828 10:16:42.727609 22858289674048 make_examples_core.py:243] Task 0/72: Overhead for preparing inputs: 1 seconds. 2022-08-28 10:16:42.748975: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 170, in main. make_examples_core.make_examples_runner(options). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795, in make_examples_runner. ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:9987,performance,Overhead,Overhead,9987,"leus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. 2022-08-28 10:16:42.621554: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.621905 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. 2022-08-28 10:16:42.705882: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.706136 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.727798 23090607179584 make_examples_core.py:243] Task 51/72: Writing examples to /scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord-00051-of-00072.gz. I0828 10:16:42.727886 23090607179584 make_examples_core.py:243] Task 51/72: Overhead for preparing inputs: 1 seconds. 2022-08-28 10:16:42.768980: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 170, in main. make_examples_core.make_examples_runner(options). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795, in make_examples_runner. ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:13,reliability,fail,failed,13,"Dynamic cast failed; I am trying to run this PacBio use case and encountered following error: . https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md. Any hint/info to solve this issue? . ```. INFO: Using cached SIF image. I0828 10:16:33.630316 22957909862208 run_deepvariant.py:342] Re-using the directory for intermediate results in /scratch-local/tahmad.1459036/tmpcy60f694. ***** Intermediate results will be written to /scratch-local/tahmad.1459036/tmpcy60f694 in docker. ****. ***** Running the command:*****. time seq 0 71 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord@72.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. 2022-08-28 10:16:41.685530: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685937 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693562 22858289674048 make_examples_core.py:243] Task 0/72: Preparing inputs. 2022-08-28 10:16:41.685378: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685891 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693572 23090607179584 make_examples_core.py:243] Task 51/72: Preparing inputs. 2022-08-28 10:16:41.910178: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:7392,reliability,Fail,Failed,7392,"der.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.621521: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.621896 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. 2022-08-28 10:16:42.702562: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.702797 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.727535 22858289674048 make_examples_core.py:243] Task 0/72: Writing examples to /scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord-00000-of-00072.gz. I0828 10:16:42.727609 22858289674048 make_examples_core.py:243] Task 0/72: Overhead for preparing inputs: 1 seconds. 2022-08-28 10:16:42.748975: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 170, in main. make_examples_core.make_examples_runner(options). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795, in make_examples_runner. runtimes) = region_processor.process(region). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:9159,reliability,fail,failed,9159,"_2e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795, in make_examples_runner. runtimes) = region_processor.process(region). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1123, in process. reads = self.region_reads(. File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1200, in region_reads. reads.extend(sam_reader.query(region)). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. 2022-08-28 10:16:42.621554: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.621905 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. 2022-08-28 10:16:42.705882: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.706136 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.727798 23090607179584 make_examples_core.py:243] Task 51/72: Writing examples to /scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord-00051-of-00072.gz. I0828 10:16:42.727886 23090607179584 make_examples_core.py:243] Task 51/72: Overhead for preparing inputs: 1 seconds. 2022-08-28 10:16:42.768980: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMes",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:10113,reliability,Fail,Failed,10113,") argument read is not valid: Dynamic cast failed. 2022-08-28 10:16:42.621554: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.621905 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. 2022-08-28 10:16:42.705882: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.706136 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.727798 23090607179584 make_examples_core.py:243] Task 51/72: Writing examples to /scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord-00051-of-00072.gz. I0828 10:16:42.727886 23090607179584 make_examples_core.py:243] Task 51/72: Overhead for preparing inputs: 1 seconds. 2022-08-28 10:16:42.768980: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 170, in main. make_examples_core.make_examples_runner(options). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795, in make_examples_runner. runtimes) = region_processor.process(region). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:11880,reliability,fail,failed,11880,"vkm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795, in make_examples_runner. runtimes) = region_processor.process(region). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1123, in process. reads = self.region_reads(. File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1200, in region_reads. reads.extend(sam_reader.query(region)). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. 2022-08-28 10:16:42.783298: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.783742 22429787215680 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.790578 22429787215680 make_examples_core.py:243] Task 35/72: Preparing inputs. 2022-08-28 10:16:42.806732: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.806946 22429787215680 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.814701 22429787215680 make_examples_core.py:243] Task 35/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:42.824596 22429",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:87,safety,error,error,87,"Dynamic cast failed; I am trying to run this PacBio use case and encountered following error: . https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md. Any hint/info to solve this issue? . ```. INFO: Using cached SIF image. I0828 10:16:33.630316 22957909862208 run_deepvariant.py:342] Re-using the directory for intermediate results in /scratch-local/tahmad.1459036/tmpcy60f694. ***** Intermediate results will be written to /scratch-local/tahmad.1459036/tmpcy60f694 in docker. ****. ***** Running the command:*****. time seq 0 71 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord@72.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. 2022-08-28 10:16:41.685530: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685937 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693562 22858289674048 make_examples_core.py:243] Task 0/72: Preparing inputs. 2022-08-28 10:16:41.685378: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685891 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693572 23090607179584 make_examples_core.py:243] Task 51/72: Preparing inputs. 2022-08-28 10:16:41.910178: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:715,safety,input,input,715,"Dynamic cast failed; I am trying to run this PacBio use case and encountered following error: . https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md. Any hint/info to solve this issue? . ```. INFO: Using cached SIF image. I0828 10:16:33.630316 22957909862208 run_deepvariant.py:342] Re-using the directory for intermediate results in /scratch-local/tahmad.1459036/tmpcy60f694. ***** Intermediate results will be written to /scratch-local/tahmad.1459036/tmpcy60f694 in docker. ****. ***** Running the command:*****. time seq 0 71 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord@72.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. 2022-08-28 10:16:41.685530: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685937 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693562 22858289674048 make_examples_core.py:243] Task 0/72: Preparing inputs. 2022-08-28 10:16:41.685378: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685891 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693572 23090607179584 make_examples_core.py:243] Task 51/72: Preparing inputs. 2022-08-28 10:16:41.910178: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:1380,safety,input,input,1380,"ocal/tahmad.1459036/tmpcy60f694. ***** Intermediate results will be written to /scratch-local/tahmad.1459036/tmpcy60f694 in docker. ****. ***** Running the command:*****. time seq 0 71 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord@72.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. 2022-08-28 10:16:41.685530: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685937 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693562 22858289674048 make_examples_core.py:243] Task 0/72: Preparing inputs. 2022-08-28 10:16:41.685378: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685891 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693572 23090607179584 make_examples_core.py:243] Task 51/72: Preparing inputs. 2022-08-28 10:16:41.910178: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.910383 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.917916 22858289674048 make_examples_core.py:243] Task 0/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'ch",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:1529,safety,input,inputs,1529,"ng the command:*****. time seq 0 71 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord@72.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. 2022-08-28 10:16:41.685530: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685937 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693562 22858289674048 make_examples_core.py:243] Task 0/72: Preparing inputs. 2022-08-28 10:16:41.685378: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685891 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693572 23090607179584 make_examples_core.py:243] Task 51/72: Preparing inputs. 2022-08-28 10:16:41.910178: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.910383 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.917916 22858289674048 make_examples_core.py:243] Task 0/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:41.927865 228582",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:1755,safety,input,input,1755,"--examples ""/scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord@72.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. 2022-08-28 10:16:41.685530: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685937 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693562 22858289674048 make_examples_core.py:243] Task 0/72: Preparing inputs. 2022-08-28 10:16:41.685378: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685891 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693572 23090607179584 make_examples_core.py:243] Task 51/72: Preparing inputs. 2022-08-28 10:16:41.910178: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.910383 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.917916 22858289674048 make_examples_core.py:243] Task 0/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:41.927865 22858289674048 make_examples_core.py:243] Task 0/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:1905,safety,input,inputs,1905,"per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. 2022-08-28 10:16:41.685530: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685937 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693562 22858289674048 make_examples_core.py:243] Task 0/72: Preparing inputs. 2022-08-28 10:16:41.685378: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685891 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693572 23090607179584 make_examples_core.py:243] Task 51/72: Preparing inputs. 2022-08-28 10:16:41.910178: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.910383 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.917916 22858289674048 make_examples_core.py:243] Task 0/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:41.927865 22858289674048 make_examples_core.py:243] Task 0/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:41.928017: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:41.910192: W third_party/nucleus/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:2131,safety,input,input,2131,"on_indels ""0.12"" --task {}. 2022-08-28 10:16:41.685530: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685937 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693562 22858289674048 make_examples_core.py:243] Task 0/72: Preparing inputs. 2022-08-28 10:16:41.685378: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685891 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693572 23090607179584 make_examples_core.py:243] Task 51/72: Preparing inputs. 2022-08-28 10:16:41.910178: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.910383 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.917916 22858289674048 make_examples_core.py:243] Task 0/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:41.927865 22858289674048 make_examples_core.py:243] Task 0/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:41.928017: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:41.910192: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.910406 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamRe",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:2662,safety,input,input,2662,"coordinate pb:3.0.7. I0828 10:16:41.685891 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693572 23090607179584 make_examples_core.py:243] Task 51/72: Preparing inputs. 2022-08-28 10:16:41.910178: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.910383 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.917916 22858289674048 make_examples_core.py:243] Task 0/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:41.927865 22858289674048 make_examples_core.py:243] Task 0/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:41.928017: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:41.910192: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.910406 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.918358 23090607179584 make_examples_core.py:243] Task 51/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:41.928208 23090607179584 make_examples_core.py:243] Task 51/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:3076,safety,input,input,3076,"0383 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.917916 22858289674048 make_examples_core.py:243] Task 0/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:41.927865 22858289674048 make_examples_core.py:243] Task 0/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:41.928017: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:41.910192: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.910406 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.918358 23090607179584 make_examples_core.py:243] Task 51/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:41.928208 23090607179584 make_examples_core.py:243] Task 51/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:41.928362: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.297998: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.298424 22358796187456 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSam",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:3609,safety,input,input,3609,"or_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:41.928017: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:41.910192: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.910406 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.918358 23090607179584 make_examples_core.py:243] Task 51/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:41.928208 23090607179584 make_examples_core.py:243] Task 51/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:41.928362: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.297998: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.298424 22358796187456 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.304855 22358796187456 make_examples_core.py:243] Task 9/72: Preparing inputs. 2022-08-28 10:16:42.306635: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.306837 22358796187456 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.314611 22358796187456 make_examples_core.py:243] Task 9/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7',",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:4023,safety,input,input,4023,"06 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.918358 23090607179584 make_examples_core.py:243] Task 51/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:41.928208 23090607179584 make_examples_core.py:243] Task 51/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:41.928362: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.297998: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.298424 22358796187456 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.304855 22358796187456 make_examples_core.py:243] Task 9/72: Preparing inputs. 2022-08-28 10:16:42.306635: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.306837 22358796187456 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.314611 22358796187456 make_examples_core.py:243] Task 9/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:42.324295 22358796187456 make_examples_core.py:243] Task 9/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:4172,safety,input,inputs,4172,"4 make_examples_core.py:243] Task 51/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:41.928208 23090607179584 make_examples_core.py:243] Task 51/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:41.928362: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.297998: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.298424 22358796187456 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.304855 22358796187456 make_examples_core.py:243] Task 9/72: Preparing inputs. 2022-08-28 10:16:42.306635: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.306837 22358796187456 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.314611 22358796187456 make_examples_core.py:243] Task 9/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:42.324295 22358796187456 make_examples_core.py:243] Task 9/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:42.324449: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.229368: W third_party/nucleus/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:4398,safety,input,input,4398,"r20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:41.928208 23090607179584 make_examples_core.py:243] Task 51/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:41.928362: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.297998: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.298424 22358796187456 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.304855 22358796187456 make_examples_core.py:243] Task 9/72: Preparing inputs. 2022-08-28 10:16:42.306635: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.306837 22358796187456 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.314611 22358796187456 make_examples_core.py:243] Task 9/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:42.324295 22358796187456 make_examples_core.py:243] Task 9/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:42.324449: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.229368: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.229775 22823217350464 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamRe",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:4929,safety,input,input,4929,":coordinate pb:3.0.7. I0828 10:16:42.298424 22358796187456 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.304855 22358796187456 make_examples_core.py:243] Task 9/72: Preparing inputs. 2022-08-28 10:16:42.306635: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.306837 22358796187456 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.314611 22358796187456 make_examples_core.py:243] Task 9/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:42.324295 22358796187456 make_examples_core.py:243] Task 9/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:42.324449: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.229368: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.229775 22823217350464 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.236391 22823217350464 make_examples_core.py:243] Task 16/72: Preparing inputs. 2022-08-28 10:16:42.257786: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.257981 22823217350464 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.265606 22823217350464 make_examples_core.py:243] Task 16/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:5343,safety,input,input,5343,"6837 22358796187456 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.314611 22358796187456 make_examples_core.py:243] Task 9/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:42.324295 22358796187456 make_examples_core.py:243] Task 9/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:42.324449: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.229368: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.229775 22823217350464 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.236391 22823217350464 make_examples_core.py:243] Task 16/72: Preparing inputs. 2022-08-28 10:16:42.257786: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.257981 22823217350464 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.265606 22823217350464 make_examples_core.py:243] Task 16/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:42.275274 22823217350464 make_examples_core.py:243] Task 16/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:5493,safety,input,inputs,5493,"56 make_examples_core.py:243] Task 9/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:42.324295 22358796187456 make_examples_core.py:243] Task 9/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:42.324449: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.229368: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.229775 22823217350464 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.236391 22823217350464 make_examples_core.py:243] Task 16/72: Preparing inputs. 2022-08-28 10:16:42.257786: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.257981 22823217350464 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.265606 22823217350464 make_examples_core.py:243] Task 16/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:42.275274 22823217350464 make_examples_core.py:243] Task 16/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:42.275424: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.621521: W third_party/nucleu",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:5719,safety,input,input,5719,"r20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:42.324295 22358796187456 make_examples_core.py:243] Task 9/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:42.324449: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.229368: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.229775 22823217350464 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.236391 22823217350464 make_examples_core.py:243] Task 16/72: Preparing inputs. 2022-08-28 10:16:42.257786: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.257981 22823217350464 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.265606 22823217350464 make_examples_core.py:243] Task 16/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:42.275274 22823217350464 make_examples_core.py:243] Task 16/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:42.275424: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.621521: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.621896 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSam",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:6252,safety,input,input,6252,"ordinate pb:3.0.7. I0828 10:16:42.229775 22823217350464 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.236391 22823217350464 make_examples_core.py:243] Task 16/72: Preparing inputs. 2022-08-28 10:16:42.257786: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.257981 22823217350464 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.265606 22823217350464 make_examples_core.py:243] Task 16/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:42.275274 22823217350464 make_examples_core.py:243] Task 16/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:42.275424: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.621521: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.621896 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. 2022-08-28 10:16:42.702562: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.702797 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.727535 22858289674048 make_examples_core.py:243] Task 0/72: Writing examples to /scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord-00000-of-00072.gz. I0828 10:16:42.727609 22858289674048 make_examples_core.py:243] ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:6666,safety,input,input,6666,"81 22823217350464 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.265606 22823217350464 make_examples_core.py:243] Task 16/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:42.275274 22823217350464 make_examples_core.py:243] Task 16/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:42.275424: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.621521: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.621896 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. 2022-08-28 10:16:42.702562: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.702797 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.727535 22858289674048 make_examples_core.py:243] Task 0/72: Writing examples to /scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord-00000-of-00072.gz. I0828 10:16:42.727609 22858289674048 make_examples_core.py:243] Task 0/72: Overhead for preparing inputs: 1 seconds. 2022-08-28 10:16:42.748975: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch-local/tah",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:6948,safety,input,input,6948,"'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:42.275274 22823217350464 make_examples_core.py:243] Task 16/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:42.275424: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.621521: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.621896 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. 2022-08-28 10:16:42.702562: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.702797 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.727535 22858289674048 make_examples_core.py:243] Task 0/72: Writing examples to /scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord-00000-of-00072.gz. I0828 10:16:42.727609 22858289674048 make_examples_core.py:243] Task 0/72: Overhead for preparing inputs: 1 seconds. 2022-08-28 10:16:42.748975: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/scratch-local/tahmad.1459036/B",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:7289,safety,input,inputs,7289,"ng the reference you passed in with --ref. 2022-08-28 10:16:42.275424: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.621521: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.621896 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. 2022-08-28 10:16:42.702562: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.702797 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.727535 22858289674048 make_examples_core.py:243] Task 0/72: Writing examples to /scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord-00000-of-00072.gz. I0828 10:16:42.727609 22858289674048 make_examples_core.py:243] Task 0/72: Overhead for preparing inputs: 1 seconds. 2022-08-28 10:16:42.748975: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 170, in main. make_examples_core.make_examples_runner(options). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795, in make_examples_runner. runtimes) = region_pro",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:7621,safety,modul,module,7621,"289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. 2022-08-28 10:16:42.702562: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.702797 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.727535 22858289674048 make_examples_core.py:243] Task 0/72: Writing examples to /scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord-00000-of-00072.gz. I0828 10:16:42.727609 22858289674048 make_examples_core.py:243] Task 0/72: Overhead for preparing inputs: 1 seconds. 2022-08-28 10:16:42.748975: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 170, in main. make_examples_core.make_examples_runner(options). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795, in make_examples_runner. runtimes) = region_processor.process(region). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1123, in process. reads = self.region_reads(. File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", l",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:9139,safety,valid,valid,9139,"Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795, in make_examples_runner. runtimes) = region_processor.process(region). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1123, in process. reads = self.region_reads(. File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1200, in region_reads. reads.extend(sam_reader.query(region)). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. 2022-08-28 10:16:42.621554: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.621905 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. 2022-08-28 10:16:42.705882: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.706136 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.727798 23090607179584 make_examples_core.py:243] Task 51/72: Writing examples to /scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord-00051-of-00072.gz. I0828 10:16:42.727886 23090607179584 make_examples_core.py:243] Task 51/72: Overhead for preparing inputs: 1 seconds. 2022-08-28 10:16:42.768980: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:9385,safety,input,input,9385,"_google_deepvariant/deepvariant/make_examples_core.py"", line 1123, in process. reads = self.region_reads(. File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1200, in region_reads. reads.extend(sam_reader.query(region)). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. 2022-08-28 10:16:42.621554: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.621905 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. 2022-08-28 10:16:42.705882: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.706136 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.727798 23090607179584 make_examples_core.py:243] Task 51/72: Writing examples to /scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord-00051-of-00072.gz. I0828 10:16:42.727886 23090607179584 make_examples_core.py:243] Task 51/72: Overhead for preparing inputs: 1 seconds. 2022-08-28 10:16:42.768980: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch-local/t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:9667,safety,input,input,9667,"ader.query(region)). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. 2022-08-28 10:16:42.621554: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.621905 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. 2022-08-28 10:16:42.705882: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.706136 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.727798 23090607179584 make_examples_core.py:243] Task 51/72: Writing examples to /scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord-00051-of-00072.gz. I0828 10:16:42.727886 23090607179584 make_examples_core.py:243] Task 51/72: Overhead for preparing inputs: 1 seconds. 2022-08-28 10:16:42.768980: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/scratch-local/tahmad.1459036",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:10010,safety,input,inputs,10010,"py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. 2022-08-28 10:16:42.621554: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.621905 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. 2022-08-28 10:16:42.705882: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.706136 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.727798 23090607179584 make_examples_core.py:243] Task 51/72: Writing examples to /scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord-00051-of-00072.gz. I0828 10:16:42.727886 23090607179584 make_examples_core.py:243] Task 51/72: Overhead for preparing inputs: 1 seconds. 2022-08-28 10:16:42.768980: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 170, in main. make_examples_core.make_examples_runner(options). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795, in make_examples_runner. runtimes) = region_pro",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:10342,safety,modul,module,10342,"7179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. 2022-08-28 10:16:42.705882: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.706136 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.727798 23090607179584 make_examples_core.py:243] Task 51/72: Writing examples to /scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord-00051-of-00072.gz. I0828 10:16:42.727886 23090607179584 make_examples_core.py:243] Task 51/72: Overhead for preparing inputs: 1 seconds. 2022-08-28 10:16:42.768980: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 170, in main. make_examples_core.make_examples_runner(options). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795, in make_examples_runner. runtimes) = region_processor.process(region). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1123, in process. reads = self.region_reads(. File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", l",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:11860,safety,valid,valid,11860,"Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795, in make_examples_runner. runtimes) = region_processor.process(region). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1123, in process. reads = self.region_reads(. File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1200, in region_reads. reads.extend(sam_reader.query(region)). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. 2022-08-28 10:16:42.783298: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.783742 22429787215680 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.790578 22429787215680 make_examples_core.py:243] Task 35/72: Preparing inputs. 2022-08-28 10:16:42.806732: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.806946 22429787215680 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.814701 22429787215680 make_examples_core.py:243] Task 35/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 1",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:12106,safety,input,input,12106,"_google_deepvariant/deepvariant/make_examples_core.py"", line 1123, in process. reads = self.region_reads(. File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1200, in region_reads. reads.extend(sam_reader.query(region)). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. 2022-08-28 10:16:42.783298: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.783742 22429787215680 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.790578 22429787215680 make_examples_core.py:243] Task 35/72: Preparing inputs. 2022-08-28 10:16:42.806732: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.806946 22429787215680 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.814701 22429787215680 make_examples_core.py:243] Task 35/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:42.824596 22429787215680 make_examples_core.py:243] Task 35/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:12256,safety,input,inputs,12256,"unfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1200, in region_reads. reads.extend(sam_reader.query(region)). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. 2022-08-28 10:16:42.783298: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.783742 22429787215680 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.790578 22429787215680 make_examples_core.py:243] Task 35/72: Preparing inputs. 2022-08-28 10:16:42.806732: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.806946 22429787215680 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.814701 22429787215680 make_examples_core.py:243] Task 35/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:42.824596 22429787215680 make_examples_core.py:243] Task 35/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:42.824762: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.621537: W third_party/nucleu",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:12482,safety,input,input,12482,"google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. 2022-08-28 10:16:42.783298: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.783742 22429787215680 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.790578 22429787215680 make_examples_core.py:243] Task 35/72: Preparing inputs. 2022-08-28 10:16:42.806732: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.806946 22429787215680 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.814701 22429787215680 make_examples_core.py:243] Task 35/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:42.824596 22429787215680 make_examples_core.py:243] Task 35/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:42.824762: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.621537: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.621887 22358796187456 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSam",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:13015,safety,input,input,13015,"l.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. 2022-08-28 10:16:42.783298: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.783742 22429787215680 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.790578 22429787215680 make_examples_core.py:243] Task 35/72: Preparing inputs. 2022-08-28 10:16:42.806732: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.806946 22429787215680 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.814701 22429787215680 make_examples_core.py:243] Task 35/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:42.824596 22429787215680 make_examples_core.py:243] Task 35/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:42.824762: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.621537: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.621887 22358796187456 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. 2022-08-28 10:16:42.702981: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:13429,safety,input,input,13429,"l.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. 2022-08-28 10:16:42.783298: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.783742 22429787215680 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.790578 22429787215680 make_examples_core.py:243] Task 35/72: Preparing inputs. 2022-08-28 10:16:42.806732: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.806946 22429787215680 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.814701 22429787215680 make_examples_core.py:243] Task 35/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:42.824596 22429787215680 make_examples_core.py:243] Task 35/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:42.824762: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.621537: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.621887 22358796187456 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. 2022-08-28 10:16:42.702981: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:168,security,model,model-case-study,168,"Dynamic cast failed; I am trying to run this PacBio use case and encountered following error: . https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md. Any hint/info to solve this issue? . ```. INFO: Using cached SIF image. I0828 10:16:33.630316 22957909862208 run_deepvariant.py:342] Re-using the directory for intermediate results in /scratch-local/tahmad.1459036/tmpcy60f694. ***** Intermediate results will be written to /scratch-local/tahmad.1459036/tmpcy60f694 in docker. ****. ***** Running the command:*****. time seq 0 71 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord@72.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. 2022-08-28 10:16:41.685530: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685937 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693562 22858289674048 make_examples_core.py:243] Task 0/72: Preparing inputs. 2022-08-28 10:16:41.685378: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685891 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693572 23090607179584 make_examples_core.py:243] Task 51/72: Preparing inputs. 2022-08-28 10:16:41.910178: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:7448,testability,Trace,Traceback,7448,"08-28 10:16:42.621521: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.621896 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. 2022-08-28 10:16:42.702562: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.702797 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.727535 22858289674048 make_examples_core.py:243] Task 0/72: Writing examples to /scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord-00000-of-00072.gz. I0828 10:16:42.727609 22858289674048 make_examples_core.py:243] Task 0/72: Overhead for preparing inputs: 1 seconds. 2022-08-28 10:16:42.748975: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 170, in main. make_examples_core.make_examples_runner(options). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795, in make_examples_runner. runtimes) = region_processor.process(region). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1123",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:10169,testability,Trace,Traceback,10169,"-28 10:16:42.621554: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.621905 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. 2022-08-28 10:16:42.705882: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.706136 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.727798 23090607179584 make_examples_core.py:243] Task 51/72: Writing examples to /scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord-00051-of-00072.gz. I0828 10:16:42.727886 23090607179584 make_examples_core.py:243] Task 51/72: Overhead for preparing inputs: 1 seconds. 2022-08-28 10:16:42.768980: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 170, in main. make_examples_core.make_examples_runner(options). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795, in make_examples_runner. runtimes) = region_processor.process(region). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1123",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:87,usability,error,error,87,"Dynamic cast failed; I am trying to run this PacBio use case and encountered following error: . https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md. Any hint/info to solve this issue? . ```. INFO: Using cached SIF image. I0828 10:16:33.630316 22957909862208 run_deepvariant.py:342] Re-using the directory for intermediate results in /scratch-local/tahmad.1459036/tmpcy60f694. ***** Intermediate results will be written to /scratch-local/tahmad.1459036/tmpcy60f694 in docker. ****. ***** Running the command:*****. time seq 0 71 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord@72.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. 2022-08-28 10:16:41.685530: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685937 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693562 22858289674048 make_examples_core.py:243] Task 0/72: Preparing inputs. 2022-08-28 10:16:41.685378: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685891 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693572 23090607179584 make_examples_core.py:243] Task 51/72: Preparing inputs. 2022-08-28 10:16:41.910178: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:193,usability,hint,hint,193,"Dynamic cast failed; I am trying to run this PacBio use case and encountered following error: . https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md. Any hint/info to solve this issue? . ```. INFO: Using cached SIF image. I0828 10:16:33.630316 22957909862208 run_deepvariant.py:342] Re-using the directory for intermediate results in /scratch-local/tahmad.1459036/tmpcy60f694. ***** Intermediate results will be written to /scratch-local/tahmad.1459036/tmpcy60f694 in docker. ****. ***** Running the command:*****. time seq 0 71 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord@72.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. 2022-08-28 10:16:41.685530: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685937 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693562 22858289674048 make_examples_core.py:243] Task 0/72: Preparing inputs. 2022-08-28 10:16:41.685378: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685891 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693572 23090607179584 make_examples_core.py:243] Task 51/72: Preparing inputs. 2022-08-28 10:16:41.910178: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:539,usability,command,command,539,"Dynamic cast failed; I am trying to run this PacBio use case and encountered following error: . https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md. Any hint/info to solve this issue? . ```. INFO: Using cached SIF image. I0828 10:16:33.630316 22957909862208 run_deepvariant.py:342] Re-using the directory for intermediate results in /scratch-local/tahmad.1459036/tmpcy60f694. ***** Intermediate results will be written to /scratch-local/tahmad.1459036/tmpcy60f694 in docker. ****. ***** Running the command:*****. time seq 0 71 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord@72.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. 2022-08-28 10:16:41.685530: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685937 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693562 22858289674048 make_examples_core.py:243] Task 0/72: Preparing inputs. 2022-08-28 10:16:41.685378: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685891 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693572 23090607179584 make_examples_core.py:243] Task 51/72: Preparing inputs. 2022-08-28 10:16:41.910178: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:715,usability,input,input,715,"Dynamic cast failed; I am trying to run this PacBio use case and encountered following error: . https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md. Any hint/info to solve this issue? . ```. INFO: Using cached SIF image. I0828 10:16:33.630316 22957909862208 run_deepvariant.py:342] Re-using the directory for intermediate results in /scratch-local/tahmad.1459036/tmpcy60f694. ***** Intermediate results will be written to /scratch-local/tahmad.1459036/tmpcy60f694 in docker. ****. ***** Running the command:*****. time seq 0 71 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord@72.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. 2022-08-28 10:16:41.685530: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685937 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693562 22858289674048 make_examples_core.py:243] Task 0/72: Preparing inputs. 2022-08-28 10:16:41.685378: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685891 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693572 23090607179584 make_examples_core.py:243] Task 51/72: Preparing inputs. 2022-08-28 10:16:41.910178: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:1380,usability,input,input,1380,"ocal/tahmad.1459036/tmpcy60f694. ***** Intermediate results will be written to /scratch-local/tahmad.1459036/tmpcy60f694 in docker. ****. ***** Running the command:*****. time seq 0 71 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord@72.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. 2022-08-28 10:16:41.685530: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685937 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693562 22858289674048 make_examples_core.py:243] Task 0/72: Preparing inputs. 2022-08-28 10:16:41.685378: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685891 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693572 23090607179584 make_examples_core.py:243] Task 51/72: Preparing inputs. 2022-08-28 10:16:41.910178: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.910383 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.917916 22858289674048 make_examples_core.py:243] Task 0/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'ch",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:1529,usability,input,inputs,1529,"ng the command:*****. time seq 0 71 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord@72.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. 2022-08-28 10:16:41.685530: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685937 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693562 22858289674048 make_examples_core.py:243] Task 0/72: Preparing inputs. 2022-08-28 10:16:41.685378: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685891 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693572 23090607179584 make_examples_core.py:243] Task 51/72: Preparing inputs. 2022-08-28 10:16:41.910178: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.910383 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.917916 22858289674048 make_examples_core.py:243] Task 0/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:41.927865 228582",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:1755,usability,input,input,1755,"--examples ""/scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord@72.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. 2022-08-28 10:16:41.685530: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685937 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693562 22858289674048 make_examples_core.py:243] Task 0/72: Preparing inputs. 2022-08-28 10:16:41.685378: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685891 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693572 23090607179584 make_examples_core.py:243] Task 51/72: Preparing inputs. 2022-08-28 10:16:41.910178: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.910383 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.917916 22858289674048 make_examples_core.py:243] Task 0/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:41.927865 22858289674048 make_examples_core.py:243] Task 0/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:1905,usability,input,inputs,1905,"per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. 2022-08-28 10:16:41.685530: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685937 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693562 22858289674048 make_examples_core.py:243] Task 0/72: Preparing inputs. 2022-08-28 10:16:41.685378: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685891 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693572 23090607179584 make_examples_core.py:243] Task 51/72: Preparing inputs. 2022-08-28 10:16:41.910178: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.910383 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.917916 22858289674048 make_examples_core.py:243] Task 0/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:41.927865 22858289674048 make_examples_core.py:243] Task 0/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:41.928017: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:41.910192: W third_party/nucleus/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:2131,usability,input,input,2131,"on_indels ""0.12"" --task {}. 2022-08-28 10:16:41.685530: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685937 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693562 22858289674048 make_examples_core.py:243] Task 0/72: Preparing inputs. 2022-08-28 10:16:41.685378: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.685891 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693572 23090607179584 make_examples_core.py:243] Task 51/72: Preparing inputs. 2022-08-28 10:16:41.910178: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.910383 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.917916 22858289674048 make_examples_core.py:243] Task 0/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:41.927865 22858289674048 make_examples_core.py:243] Task 0/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:41.928017: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:41.910192: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.910406 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamRe",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:2662,usability,input,input,2662,"coordinate pb:3.0.7. I0828 10:16:41.685891 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.693572 23090607179584 make_examples_core.py:243] Task 51/72: Preparing inputs. 2022-08-28 10:16:41.910178: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.910383 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.917916 22858289674048 make_examples_core.py:243] Task 0/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:41.927865 22858289674048 make_examples_core.py:243] Task 0/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:41.928017: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:41.910192: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.910406 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.918358 23090607179584 make_examples_core.py:243] Task 51/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:41.928208 23090607179584 make_examples_core.py:243] Task 51/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:3076,usability,input,input,3076,"0383 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.917916 22858289674048 make_examples_core.py:243] Task 0/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:41.927865 22858289674048 make_examples_core.py:243] Task 0/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:41.928017: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:41.910192: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.910406 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.918358 23090607179584 make_examples_core.py:243] Task 51/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:41.928208 23090607179584 make_examples_core.py:243] Task 51/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:41.928362: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.297998: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.298424 22358796187456 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSam",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:3609,usability,input,input,3609,"or_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:41.928017: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:41.910192: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:41.910406 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.918358 23090607179584 make_examples_core.py:243] Task 51/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:41.928208 23090607179584 make_examples_core.py:243] Task 51/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:41.928362: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.297998: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.298424 22358796187456 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.304855 22358796187456 make_examples_core.py:243] Task 9/72: Preparing inputs. 2022-08-28 10:16:42.306635: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.306837 22358796187456 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.314611 22358796187456 make_examples_core.py:243] Task 9/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7',",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:4023,usability,input,input,4023,"06 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:41.918358 23090607179584 make_examples_core.py:243] Task 51/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:41.928208 23090607179584 make_examples_core.py:243] Task 51/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:41.928362: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.297998: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.298424 22358796187456 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.304855 22358796187456 make_examples_core.py:243] Task 9/72: Preparing inputs. 2022-08-28 10:16:42.306635: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.306837 22358796187456 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.314611 22358796187456 make_examples_core.py:243] Task 9/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:42.324295 22358796187456 make_examples_core.py:243] Task 9/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:4172,usability,input,inputs,4172,"4 make_examples_core.py:243] Task 51/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:41.928208 23090607179584 make_examples_core.py:243] Task 51/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:41.928362: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.297998: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.298424 22358796187456 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.304855 22358796187456 make_examples_core.py:243] Task 9/72: Preparing inputs. 2022-08-28 10:16:42.306635: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.306837 22358796187456 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.314611 22358796187456 make_examples_core.py:243] Task 9/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:42.324295 22358796187456 make_examples_core.py:243] Task 9/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:42.324449: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.229368: W third_party/nucleus/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:4398,usability,input,input,4398,"r20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:41.928208 23090607179584 make_examples_core.py:243] Task 51/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:41.928362: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.297998: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.298424 22358796187456 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.304855 22358796187456 make_examples_core.py:243] Task 9/72: Preparing inputs. 2022-08-28 10:16:42.306635: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.306837 22358796187456 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.314611 22358796187456 make_examples_core.py:243] Task 9/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:42.324295 22358796187456 make_examples_core.py:243] Task 9/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:42.324449: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.229368: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.229775 22823217350464 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamRe",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:4929,usability,input,input,4929,":coordinate pb:3.0.7. I0828 10:16:42.298424 22358796187456 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.304855 22358796187456 make_examples_core.py:243] Task 9/72: Preparing inputs. 2022-08-28 10:16:42.306635: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.306837 22358796187456 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.314611 22358796187456 make_examples_core.py:243] Task 9/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:42.324295 22358796187456 make_examples_core.py:243] Task 9/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:42.324449: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.229368: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.229775 22823217350464 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.236391 22823217350464 make_examples_core.py:243] Task 16/72: Preparing inputs. 2022-08-28 10:16:42.257786: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.257981 22823217350464 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.265606 22823217350464 make_examples_core.py:243] Task 16/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:5343,usability,input,input,5343,"6837 22358796187456 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.314611 22358796187456 make_examples_core.py:243] Task 9/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:42.324295 22358796187456 make_examples_core.py:243] Task 9/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:42.324449: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.229368: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.229775 22823217350464 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.236391 22823217350464 make_examples_core.py:243] Task 16/72: Preparing inputs. 2022-08-28 10:16:42.257786: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.257981 22823217350464 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.265606 22823217350464 make_examples_core.py:243] Task 16/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:42.275274 22823217350464 make_examples_core.py:243] Task 16/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:5493,usability,input,inputs,5493,"56 make_examples_core.py:243] Task 9/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:42.324295 22358796187456 make_examples_core.py:243] Task 9/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:42.324449: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.229368: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.229775 22823217350464 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.236391 22823217350464 make_examples_core.py:243] Task 16/72: Preparing inputs. 2022-08-28 10:16:42.257786: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.257981 22823217350464 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.265606 22823217350464 make_examples_core.py:243] Task 16/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:42.275274 22823217350464 make_examples_core.py:243] Task 16/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:42.275424: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.621521: W third_party/nucleu",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:5719,usability,input,input,5719,"r20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:42.324295 22358796187456 make_examples_core.py:243] Task 9/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:42.324449: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.229368: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.229775 22823217350464 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.236391 22823217350464 make_examples_core.py:243] Task 16/72: Preparing inputs. 2022-08-28 10:16:42.257786: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.257981 22823217350464 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.265606 22823217350464 make_examples_core.py:243] Task 16/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:42.275274 22823217350464 make_examples_core.py:243] Task 16/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:42.275424: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.621521: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.621896 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSam",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:6252,usability,input,input,6252,"ordinate pb:3.0.7. I0828 10:16:42.229775 22823217350464 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.236391 22823217350464 make_examples_core.py:243] Task 16/72: Preparing inputs. 2022-08-28 10:16:42.257786: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.257981 22823217350464 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.265606 22823217350464 make_examples_core.py:243] Task 16/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:42.275274 22823217350464 make_examples_core.py:243] Task 16/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:42.275424: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.621521: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.621896 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. 2022-08-28 10:16:42.702562: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.702797 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.727535 22858289674048 make_examples_core.py:243] Task 0/72: Writing examples to /scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord-00000-of-00072.gz. I0828 10:16:42.727609 22858289674048 make_examples_core.py:243] ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:6666,usability,input,input,6666,"81 22823217350464 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.265606 22823217350464 make_examples_core.py:243] Task 16/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:42.275274 22823217350464 make_examples_core.py:243] Task 16/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:42.275424: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.621521: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.621896 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. 2022-08-28 10:16:42.702562: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.702797 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.727535 22858289674048 make_examples_core.py:243] Task 0/72: Writing examples to /scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord-00000-of-00072.gz. I0828 10:16:42.727609 22858289674048 make_examples_core.py:243] Task 0/72: Overhead for preparing inputs: 1 seconds. 2022-08-28 10:16:42.748975: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch-local/tah",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:6948,usability,input,input,6948,"'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:42.275274 22823217350464 make_examples_core.py:243] Task 16/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:42.275424: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.621521: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.621896 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. 2022-08-28 10:16:42.702562: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.702797 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.727535 22858289674048 make_examples_core.py:243] Task 0/72: Writing examples to /scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord-00000-of-00072.gz. I0828 10:16:42.727609 22858289674048 make_examples_core.py:243] Task 0/72: Overhead for preparing inputs: 1 seconds. 2022-08-28 10:16:42.748975: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/scratch-local/tahmad.1459036/B",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:7289,usability,input,inputs,7289,"ng the reference you passed in with --ref. 2022-08-28 10:16:42.275424: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.621521: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.621896 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. 2022-08-28 10:16:42.702562: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.702797 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.727535 22858289674048 make_examples_core.py:243] Task 0/72: Writing examples to /scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord-00000-of-00072.gz. I0828 10:16:42.727609 22858289674048 make_examples_core.py:243] Task 0/72: Overhead for preparing inputs: 1 seconds. 2022-08-28 10:16:42.748975: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 170, in main. make_examples_core.make_examples_runner(options). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795, in make_examples_runner. runtimes) = region_pro",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:9385,usability,input,input,9385,"_google_deepvariant/deepvariant/make_examples_core.py"", line 1123, in process. reads = self.region_reads(. File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1200, in region_reads. reads.extend(sam_reader.query(region)). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. 2022-08-28 10:16:42.621554: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.621905 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. 2022-08-28 10:16:42.705882: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.706136 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.727798 23090607179584 make_examples_core.py:243] Task 51/72: Writing examples to /scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord-00051-of-00072.gz. I0828 10:16:42.727886 23090607179584 make_examples_core.py:243] Task 51/72: Overhead for preparing inputs: 1 seconds. 2022-08-28 10:16:42.768980: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch-local/t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:9667,usability,input,input,9667,"ader.query(region)). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_w_myh_2e/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. 2022-08-28 10:16:42.621554: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.621905 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. 2022-08-28 10:16:42.705882: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.706136 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.727798 23090607179584 make_examples_core.py:243] Task 51/72: Writing examples to /scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord-00051-of-00072.gz. I0828 10:16:42.727886 23090607179584 make_examples_core.py:243] Task 51/72: Overhead for preparing inputs: 1 seconds. 2022-08-28 10:16:42.768980: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/scratch-local/tahmad.1459036",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:10010,usability,input,inputs,10010,"py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. 2022-08-28 10:16:42.621554: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.621905 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. 2022-08-28 10:16:42.705882: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.706136 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.727798 23090607179584 make_examples_core.py:243] Task 51/72: Writing examples to /scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord-00051-of-00072.gz. I0828 10:16:42.727886 23090607179584 make_examples_core.py:243] Task 51/72: Overhead for preparing inputs: 1 seconds. 2022-08-28 10:16:42.768980: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>. app.run(main). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 170, in main. make_examples_core.make_examples_runner(options). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795, in make_examples_runner. runtimes) = region_pro",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:12106,usability,input,input,12106,"_google_deepvariant/deepvariant/make_examples_core.py"", line 1123, in process. reads = self.region_reads(. File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1200, in region_reads. reads.extend(sam_reader.query(region)). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. 2022-08-28 10:16:42.783298: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.783742 22429787215680 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.790578 22429787215680 make_examples_core.py:243] Task 35/72: Preparing inputs. 2022-08-28 10:16:42.806732: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.806946 22429787215680 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.814701 22429787215680 make_examples_core.py:243] Task 35/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:42.824596 22429787215680 make_examples_core.py:243] Task 35/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:12256,usability,input,inputs,12256,"unfiles_v_r0zvkm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1200, in region_reads. reads.extend(sam_reader.query(region)). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. 2022-08-28 10:16:42.783298: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.783742 22429787215680 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.790578 22429787215680 make_examples_core.py:243] Task 35/72: Preparing inputs. 2022-08-28 10:16:42.806732: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.806946 22429787215680 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.814701 22429787215680 make_examples_core.py:243] Task 35/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:42.824596 22429787215680 make_examples_core.py:243] Task 35/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:42.824762: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.621537: W third_party/nucleu",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:12482,usability,input,input,12482,"google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/scratch-local/tahmad.1459036/Bazel.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. 2022-08-28 10:16:42.783298: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.783742 22429787215680 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.790578 22429787215680 make_examples_core.py:243] Task 35/72: Preparing inputs. 2022-08-28 10:16:42.806732: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.806946 22429787215680 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.814701 22429787215680 make_examples_core.py:243] Task 35/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:42.824596 22429787215680 make_examples_core.py:243] Task 35/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:42.824762: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.621537: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.621887 22358796187456 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSam",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:13015,usability,input,input,13015,"l.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. 2022-08-28 10:16:42.783298: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.783742 22429787215680 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.790578 22429787215680 make_examples_core.py:243] Task 35/72: Preparing inputs. 2022-08-28 10:16:42.806732: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.806946 22429787215680 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.814701 22429787215680 make_examples_core.py:243] Task 35/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:42.824596 22429787215680 make_examples_core.py:243] Task 35/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:42.824762: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.621537: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.621887 22358796187456 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. 2022-08-28 10:16:42.702981: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/559:13429,usability,input,input,13429,"l.runfiles_v_r0zvkm/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next. not_done = self._cc_iterable.PythonNext(record). RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. 2022-08-28 10:16:42.783298: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.783742 22429787215680 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.790578 22429787215680 make_examples_core.py:243] Task 35/72: Preparing inputs. 2022-08-28 10:16:42.806732: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.806946 22429787215680 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. I0828 10:16:42.814701 22429787215680 make_examples_core.py:243] Task 35/72: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0828 10:16:42.824596 22429787215680 make_examples_core.py:243] Task 35/72: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2022-08-28 10:16:42.824762: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2022-08-28 10:16:42.621537: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. I0828 10:16:42.621887 22358796187456 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader. 2022-08-28 10:16:42.702981: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559
https://github.com/google/deepvariant/issues/560:330,deployability,contain,contains,330,"Mishandling of IUPAC ambiguous bases sometimes causes malformed VCF outputs; Hi,. I have a few VCFs on ONT data called with DV (technically [Pepper](https://github.com/kishwarshafin/pepper) but I suspect this is an upstream issue). On a number of these files, it just happens that a deletion is called over a region of HG38 which contains one of the [IUPAC ambiguous base codes](https://www.bioinformatics.org/sms/iupac.html). This means there is a reference allele which looks like e.g. `TW` and the variant is `T`, so there is a deletion of a `W` base. However, because `W` is not an allowed base by htsjdk (and VCF Spec 4.2 / 4.3), this causes a ton of genomics tools to throw an exception when they get to those lines. . There are fewer than 100 places in HG38 where a non-standard base (not A/C/G/T/N) is placed (see e.g. `chr3:16902883`), but for some reason it seems the long reads DV data I've been using hits deletions on these sites somewhat frequently. I don't know what the best solution might be, whether replacing them with `N` bases, or substituting them as recommended in [VCF Spec 4.3](https://samtools.github.io/hts-specs/VCFv4.3.pdf), or something else. But it would be great to have a fix with documentation about how it's resolved.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/560
https://github.com/google/deepvariant/issues/560:1052,integrability,sub,substituting,1052,"Mishandling of IUPAC ambiguous bases sometimes causes malformed VCF outputs; Hi,. I have a few VCFs on ONT data called with DV (technically [Pepper](https://github.com/kishwarshafin/pepper) but I suspect this is an upstream issue). On a number of these files, it just happens that a deletion is called over a region of HG38 which contains one of the [IUPAC ambiguous base codes](https://www.bioinformatics.org/sms/iupac.html). This means there is a reference allele which looks like e.g. `TW` and the variant is `T`, so there is a deletion of a `W` base. However, because `W` is not an allowed base by htsjdk (and VCF Spec 4.2 / 4.3), this causes a ton of genomics tools to throw an exception when they get to those lines. . There are fewer than 100 places in HG38 where a non-standard base (not A/C/G/T/N) is placed (see e.g. `chr3:16902883`), but for some reason it seems the long reads DV data I've been using hits deletions on these sites somewhat frequently. I don't know what the best solution might be, whether replacing them with `N` bases, or substituting them as recommended in [VCF Spec 4.3](https://samtools.github.io/hts-specs/VCFv4.3.pdf), or something else. But it would be great to have a fix with documentation about how it's resolved.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/560
https://github.com/google/deepvariant/issues/560:777,interoperability,standard,standard,777,"Mishandling of IUPAC ambiguous bases sometimes causes malformed VCF outputs; Hi,. I have a few VCFs on ONT data called with DV (technically [Pepper](https://github.com/kishwarshafin/pepper) but I suspect this is an upstream issue). On a number of these files, it just happens that a deletion is called over a region of HG38 which contains one of the [IUPAC ambiguous base codes](https://www.bioinformatics.org/sms/iupac.html). This means there is a reference allele which looks like e.g. `TW` and the variant is `T`, so there is a deletion of a `W` base. However, because `W` is not an allowed base by htsjdk (and VCF Spec 4.2 / 4.3), this causes a ton of genomics tools to throw an exception when they get to those lines. . There are fewer than 100 places in HG38 where a non-standard base (not A/C/G/T/N) is placed (see e.g. `chr3:16902883`), but for some reason it seems the long reads DV data I've been using hits deletions on these sites somewhat frequently. I don't know what the best solution might be, whether replacing them with `N` bases, or substituting them as recommended in [VCF Spec 4.3](https://samtools.github.io/hts-specs/VCFv4.3.pdf), or something else. But it would be great to have a fix with documentation about how it's resolved.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/560
https://github.com/google/deepvariant/issues/560:683,safety,except,exception,683,"Mishandling of IUPAC ambiguous bases sometimes causes malformed VCF outputs; Hi,. I have a few VCFs on ONT data called with DV (technically [Pepper](https://github.com/kishwarshafin/pepper) but I suspect this is an upstream issue). On a number of these files, it just happens that a deletion is called over a region of HG38 which contains one of the [IUPAC ambiguous base codes](https://www.bioinformatics.org/sms/iupac.html). This means there is a reference allele which looks like e.g. `TW` and the variant is `T`, so there is a deletion of a `W` base. However, because `W` is not an allowed base by htsjdk (and VCF Spec 4.2 / 4.3), this causes a ton of genomics tools to throw an exception when they get to those lines. . There are fewer than 100 places in HG38 where a non-standard base (not A/C/G/T/N) is placed (see e.g. `chr3:16902883`), but for some reason it seems the long reads DV data I've been using hits deletions on these sites somewhat frequently. I don't know what the best solution might be, whether replacing them with `N` bases, or substituting them as recommended in [VCF Spec 4.3](https://samtools.github.io/hts-specs/VCFv4.3.pdf), or something else. But it would be great to have a fix with documentation about how it's resolved.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/560
https://github.com/google/deepvariant/issues/560:665,usability,tool,tools,665,"Mishandling of IUPAC ambiguous bases sometimes causes malformed VCF outputs; Hi,. I have a few VCFs on ONT data called with DV (technically [Pepper](https://github.com/kishwarshafin/pepper) but I suspect this is an upstream issue). On a number of these files, it just happens that a deletion is called over a region of HG38 which contains one of the [IUPAC ambiguous base codes](https://www.bioinformatics.org/sms/iupac.html). This means there is a reference allele which looks like e.g. `TW` and the variant is `T`, so there is a deletion of a `W` base. However, because `W` is not an allowed base by htsjdk (and VCF Spec 4.2 / 4.3), this causes a ton of genomics tools to throw an exception when they get to those lines. . There are fewer than 100 places in HG38 where a non-standard base (not A/C/G/T/N) is placed (see e.g. `chr3:16902883`), but for some reason it seems the long reads DV data I've been using hits deletions on these sites somewhat frequently. I don't know what the best solution might be, whether replacing them with `N` bases, or substituting them as recommended in [VCF Spec 4.3](https://samtools.github.io/hts-specs/VCFv4.3.pdf), or something else. But it would be great to have a fix with documentation about how it's resolved.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/560
https://github.com/google/deepvariant/issues/560:1214,usability,document,documentation,1214,"Mishandling of IUPAC ambiguous bases sometimes causes malformed VCF outputs; Hi,. I have a few VCFs on ONT data called with DV (technically [Pepper](https://github.com/kishwarshafin/pepper) but I suspect this is an upstream issue). On a number of these files, it just happens that a deletion is called over a region of HG38 which contains one of the [IUPAC ambiguous base codes](https://www.bioinformatics.org/sms/iupac.html). This means there is a reference allele which looks like e.g. `TW` and the variant is `T`, so there is a deletion of a `W` base. However, because `W` is not an allowed base by htsjdk (and VCF Spec 4.2 / 4.3), this causes a ton of genomics tools to throw an exception when they get to those lines. . There are fewer than 100 places in HG38 where a non-standard base (not A/C/G/T/N) is placed (see e.g. `chr3:16902883`), but for some reason it seems the long reads DV data I've been using hits deletions on these sites somewhat frequently. I don't know what the best solution might be, whether replacing them with `N` bases, or substituting them as recommended in [VCF Spec 4.3](https://samtools.github.io/hts-specs/VCFv4.3.pdf), or something else. But it would be great to have a fix with documentation about how it's resolved.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/560
https://github.com/google/deepvariant/issues/561:348,availability,Operat,Operating,348,"No output file generating in quick start; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** After running the code in the deepvariant docker container (quick start), the output vcf files have not been generated. (A clear and concise description of what the issue is.). **Setup**. - Operating system:Mac OS . - DeepVariant version: Latest. - Installation method (Docker, built from source, etc.): Docker. - Type of data: Test files(sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: sudoa docker run \-v ""${INPUT_DIR}"":""/input"" \-v ""${INPUT_DIR}"":""/output"" \google/deepvariant:""${BIN_VERSION}"" \/opt/deepvariant/bin/run_deepvariant \--model_type=WES \--ref=/input/ucsc.hg19.chr20.unittest.fasta \--reads=/input/NA12878_S1.chr20.10_10p1mb.bam \--regions ""chr20:10,000,000-10,010,000"" \--output_vcf=/output/output.vcf.gz \--output_gvcf=/output/output.g.vcf.gz \--num_shards=1 \--dry_run=true. - Error trace: No error.(if applicable).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/561
https://github.com/google/deepvariant/issues/561:1036,availability,Error,Error,1036,"No output file generating in quick start; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** After running the code in the deepvariant docker container (quick start), the output vcf files have not been generated. (A clear and concise description of what the issue is.). **Setup**. - Operating system:Mac OS . - DeepVariant version: Latest. - Installation method (Docker, built from source, etc.): Docker. - Type of data: Test files(sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: sudoa docker run \-v ""${INPUT_DIR}"":""/input"" \-v ""${INPUT_DIR}"":""/output"" \google/deepvariant:""${BIN_VERSION}"" \/opt/deepvariant/bin/run_deepvariant \--model_type=WES \--ref=/input/ucsc.hg19.chr20.unittest.fasta \--reads=/input/NA12878_S1.chr20.10_10p1mb.bam \--regions ""chr20:10,000,000-10,010,000"" \--output_vcf=/output/output.vcf.gz \--output_gvcf=/output/output.g.vcf.gz \--num_shards=1 \--dry_run=true. - Error trace: No error.(if applicable).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/561
https://github.com/google/deepvariant/issues/561:1052,availability,error,error,1052,"No output file generating in quick start; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** After running the code in the deepvariant docker container (quick start), the output vcf files have not been generated. (A clear and concise description of what the issue is.). **Setup**. - Operating system:Mac OS . - DeepVariant version: Latest. - Installation method (Docker, built from source, etc.): Docker. - Type of data: Test files(sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: sudoa docker run \-v ""${INPUT_DIR}"":""/input"" \-v ""${INPUT_DIR}"":""/output"" \google/deepvariant:""${BIN_VERSION}"" \/opt/deepvariant/bin/run_deepvariant \--model_type=WES \--ref=/input/ucsc.hg19.chr20.unittest.fasta \--reads=/input/NA12878_S1.chr20.10_10p1mb.bam \--regions ""chr20:10,000,000-10,010,000"" \--output_vcf=/output/output.vcf.gz \--output_gvcf=/output/output.g.vcf.gz \--num_shards=1 \--dry_run=true. - Error trace: No error.(if applicable).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/561
https://github.com/google/deepvariant/issues/561:207,deployability,contain,container,207,"No output file generating in quick start; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** After running the code in the deepvariant docker container (quick start), the output vcf files have not been generated. (A clear and concise description of what the issue is.). **Setup**. - Operating system:Mac OS . - DeepVariant version: Latest. - Installation method (Docker, built from source, etc.): Docker. - Type of data: Test files(sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: sudoa docker run \-v ""${INPUT_DIR}"":""/input"" \-v ""${INPUT_DIR}"":""/output"" \google/deepvariant:""${BIN_VERSION}"" \/opt/deepvariant/bin/run_deepvariant \--model_type=WES \--ref=/input/ucsc.hg19.chr20.unittest.fasta \--reads=/input/NA12878_S1.chr20.10_10p1mb.bam \--regions ""chr20:10,000,000-10,010,000"" \--output_vcf=/output/output.vcf.gz \--output_gvcf=/output/output.g.vcf.gz \--num_shards=1 \--dry_run=true. - Error trace: No error.(if applicable).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/561
https://github.com/google/deepvariant/issues/561:388,deployability,version,version,388,"No output file generating in quick start; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** After running the code in the deepvariant docker container (quick start), the output vcf files have not been generated. (A clear and concise description of what the issue is.). **Setup**. - Operating system:Mac OS . - DeepVariant version: Latest. - Installation method (Docker, built from source, etc.): Docker. - Type of data: Test files(sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: sudoa docker run \-v ""${INPUT_DIR}"":""/input"" \-v ""${INPUT_DIR}"":""/output"" \google/deepvariant:""${BIN_VERSION}"" \/opt/deepvariant/bin/run_deepvariant \--model_type=WES \--ref=/input/ucsc.hg19.chr20.unittest.fasta \--reads=/input/NA12878_S1.chr20.10_10p1mb.bam \--regions ""chr20:10,000,000-10,010,000"" \--output_vcf=/output/output.vcf.gz \--output_gvcf=/output/output.g.vcf.gz \--num_shards=1 \--dry_run=true. - Error trace: No error.(if applicable).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/561
https://github.com/google/deepvariant/issues/561:407,deployability,Instal,Installation,407,"No output file generating in quick start; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** After running the code in the deepvariant docker container (quick start), the output vcf files have not been generated. (A clear and concise description of what the issue is.). **Setup**. - Operating system:Mac OS . - DeepVariant version: Latest. - Installation method (Docker, built from source, etc.): Docker. - Type of data: Test files(sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: sudoa docker run \-v ""${INPUT_DIR}"":""/input"" \-v ""${INPUT_DIR}"":""/output"" \google/deepvariant:""${BIN_VERSION}"" \/opt/deepvariant/bin/run_deepvariant \--model_type=WES \--ref=/input/ucsc.hg19.chr20.unittest.fasta \--reads=/input/NA12878_S1.chr20.10_10p1mb.bam \--regions ""chr20:10,000,000-10,010,000"" \--output_vcf=/output/output.vcf.gz \--output_gvcf=/output/output.g.vcf.gz \--num_shards=1 \--dry_run=true. - Error trace: No error.(if applicable).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/561
https://github.com/google/deepvariant/issues/561:388,integrability,version,version,388,"No output file generating in quick start; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** After running the code in the deepvariant docker container (quick start), the output vcf files have not been generated. (A clear and concise description of what the issue is.). **Setup**. - Operating system:Mac OS . - DeepVariant version: Latest. - Installation method (Docker, built from source, etc.): Docker. - Type of data: Test files(sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: sudoa docker run \-v ""${INPUT_DIR}"":""/input"" \-v ""${INPUT_DIR}"":""/output"" \google/deepvariant:""${BIN_VERSION}"" \/opt/deepvariant/bin/run_deepvariant \--model_type=WES \--ref=/input/ucsc.hg19.chr20.unittest.fasta \--reads=/input/NA12878_S1.chr20.10_10p1mb.bam \--regions ""chr20:10,000,000-10,010,000"" \--output_vcf=/output/output.vcf.gz \--output_gvcf=/output/output.g.vcf.gz \--num_shards=1 \--dry_run=true. - Error trace: No error.(if applicable).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/561
https://github.com/google/deepvariant/issues/561:388,modifiability,version,version,388,"No output file generating in quick start; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** After running the code in the deepvariant docker container (quick start), the output vcf files have not been generated. (A clear and concise description of what the issue is.). **Setup**. - Operating system:Mac OS . - DeepVariant version: Latest. - Installation method (Docker, built from source, etc.): Docker. - Type of data: Test files(sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: sudoa docker run \-v ""${INPUT_DIR}"":""/input"" \-v ""${INPUT_DIR}"":""/output"" \google/deepvariant:""${BIN_VERSION}"" \/opt/deepvariant/bin/run_deepvariant \--model_type=WES \--ref=/input/ucsc.hg19.chr20.unittest.fasta \--reads=/input/NA12878_S1.chr20.10_10p1mb.bam \--regions ""chr20:10,000,000-10,010,000"" \--output_vcf=/output/output.vcf.gz \--output_gvcf=/output/output.g.vcf.gz \--num_shards=1 \--dry_run=true. - Error trace: No error.(if applicable).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/561
https://github.com/google/deepvariant/issues/561:1036,performance,Error,Error,1036,"No output file generating in quick start; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** After running the code in the deepvariant docker container (quick start), the output vcf files have not been generated. (A clear and concise description of what the issue is.). **Setup**. - Operating system:Mac OS . - DeepVariant version: Latest. - Installation method (Docker, built from source, etc.): Docker. - Type of data: Test files(sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: sudoa docker run \-v ""${INPUT_DIR}"":""/input"" \-v ""${INPUT_DIR}"":""/output"" \google/deepvariant:""${BIN_VERSION}"" \/opt/deepvariant/bin/run_deepvariant \--model_type=WES \--ref=/input/ucsc.hg19.chr20.unittest.fasta \--reads=/input/NA12878_S1.chr20.10_10p1mb.bam \--regions ""chr20:10,000,000-10,010,000"" \--output_vcf=/output/output.vcf.gz \--output_gvcf=/output/output.g.vcf.gz \--num_shards=1 \--dry_run=true. - Error trace: No error.(if applicable).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/561
https://github.com/google/deepvariant/issues/561:1052,performance,error,error,1052,"No output file generating in quick start; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** After running the code in the deepvariant docker container (quick start), the output vcf files have not been generated. (A clear and concise description of what the issue is.). **Setup**. - Operating system:Mac OS . - DeepVariant version: Latest. - Installation method (Docker, built from source, etc.): Docker. - Type of data: Test files(sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: sudoa docker run \-v ""${INPUT_DIR}"":""/input"" \-v ""${INPUT_DIR}"":""/output"" \google/deepvariant:""${BIN_VERSION}"" \/opt/deepvariant/bin/run_deepvariant \--model_type=WES \--ref=/input/ucsc.hg19.chr20.unittest.fasta \--reads=/input/NA12878_S1.chr20.10_10p1mb.bam \--regions ""chr20:10,000,000-10,010,000"" \--output_vcf=/output/output.vcf.gz \--output_gvcf=/output/output.g.vcf.gz \--num_shards=1 \--dry_run=true. - Error trace: No error.(if applicable).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/561
https://github.com/google/deepvariant/issues/561:486,safety,Test,Test,486,"No output file generating in quick start; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** After running the code in the deepvariant docker container (quick start), the output vcf files have not been generated. (A clear and concise description of what the issue is.). **Setup**. - Operating system:Mac OS . - DeepVariant version: Latest. - Installation method (Docker, built from source, etc.): Docker. - Type of data: Test files(sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: sudoa docker run \-v ""${INPUT_DIR}"":""/input"" \-v ""${INPUT_DIR}"":""/output"" \google/deepvariant:""${BIN_VERSION}"" \/opt/deepvariant/bin/run_deepvariant \--model_type=WES \--ref=/input/ucsc.hg19.chr20.unittest.fasta \--reads=/input/NA12878_S1.chr20.10_10p1mb.bam \--regions ""chr20:10,000,000-10,010,000"" \--output_vcf=/output/output.vcf.gz \--output_gvcf=/output/output.g.vcf.gz \--num_shards=1 \--dry_run=true. - Error trace: No error.(if applicable).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/561
https://github.com/google/deepvariant/issues/561:664,safety,input,input,664,"No output file generating in quick start; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** After running the code in the deepvariant docker container (quick start), the output vcf files have not been generated. (A clear and concise description of what the issue is.). **Setup**. - Operating system:Mac OS . - DeepVariant version: Latest. - Installation method (Docker, built from source, etc.): Docker. - Type of data: Test files(sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: sudoa docker run \-v ""${INPUT_DIR}"":""/input"" \-v ""${INPUT_DIR}"":""/output"" \google/deepvariant:""${BIN_VERSION}"" \/opt/deepvariant/bin/run_deepvariant \--model_type=WES \--ref=/input/ucsc.hg19.chr20.unittest.fasta \--reads=/input/NA12878_S1.chr20.10_10p1mb.bam \--regions ""chr20:10,000,000-10,010,000"" \--output_vcf=/output/output.vcf.gz \--output_gvcf=/output/output.g.vcf.gz \--num_shards=1 \--dry_run=true. - Error trace: No error.(if applicable).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/561
https://github.com/google/deepvariant/issues/561:801,safety,input,input,801,"No output file generating in quick start; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** After running the code in the deepvariant docker container (quick start), the output vcf files have not been generated. (A clear and concise description of what the issue is.). **Setup**. - Operating system:Mac OS . - DeepVariant version: Latest. - Installation method (Docker, built from source, etc.): Docker. - Type of data: Test files(sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: sudoa docker run \-v ""${INPUT_DIR}"":""/input"" \-v ""${INPUT_DIR}"":""/output"" \google/deepvariant:""${BIN_VERSION}"" \/opt/deepvariant/bin/run_deepvariant \--model_type=WES \--ref=/input/ucsc.hg19.chr20.unittest.fasta \--reads=/input/NA12878_S1.chr20.10_10p1mb.bam \--regions ""chr20:10,000,000-10,010,000"" \--output_vcf=/output/output.vcf.gz \--output_gvcf=/output/output.g.vcf.gz \--num_shards=1 \--dry_run=true. - Error trace: No error.(if applicable).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/561
https://github.com/google/deepvariant/issues/561:848,safety,input,input,848,"No output file generating in quick start; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** After running the code in the deepvariant docker container (quick start), the output vcf files have not been generated. (A clear and concise description of what the issue is.). **Setup**. - Operating system:Mac OS . - DeepVariant version: Latest. - Installation method (Docker, built from source, etc.): Docker. - Type of data: Test files(sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: sudoa docker run \-v ""${INPUT_DIR}"":""/input"" \-v ""${INPUT_DIR}"":""/output"" \google/deepvariant:""${BIN_VERSION}"" \/opt/deepvariant/bin/run_deepvariant \--model_type=WES \--ref=/input/ucsc.hg19.chr20.unittest.fasta \--reads=/input/NA12878_S1.chr20.10_10p1mb.bam \--regions ""chr20:10,000,000-10,010,000"" \--output_vcf=/output/output.vcf.gz \--output_gvcf=/output/output.g.vcf.gz \--num_shards=1 \--dry_run=true. - Error trace: No error.(if applicable).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/561
https://github.com/google/deepvariant/issues/561:1036,safety,Error,Error,1036,"No output file generating in quick start; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** After running the code in the deepvariant docker container (quick start), the output vcf files have not been generated. (A clear and concise description of what the issue is.). **Setup**. - Operating system:Mac OS . - DeepVariant version: Latest. - Installation method (Docker, built from source, etc.): Docker. - Type of data: Test files(sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: sudoa docker run \-v ""${INPUT_DIR}"":""/input"" \-v ""${INPUT_DIR}"":""/output"" \google/deepvariant:""${BIN_VERSION}"" \/opt/deepvariant/bin/run_deepvariant \--model_type=WES \--ref=/input/ucsc.hg19.chr20.unittest.fasta \--reads=/input/NA12878_S1.chr20.10_10p1mb.bam \--regions ""chr20:10,000,000-10,010,000"" \--output_vcf=/output/output.vcf.gz \--output_gvcf=/output/output.g.vcf.gz \--num_shards=1 \--dry_run=true. - Error trace: No error.(if applicable).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/561
https://github.com/google/deepvariant/issues/561:1052,safety,error,error,1052,"No output file generating in quick start; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** After running the code in the deepvariant docker container (quick start), the output vcf files have not been generated. (A clear and concise description of what the issue is.). **Setup**. - Operating system:Mac OS . - DeepVariant version: Latest. - Installation method (Docker, built from source, etc.): Docker. - Type of data: Test files(sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: sudoa docker run \-v ""${INPUT_DIR}"":""/input"" \-v ""${INPUT_DIR}"":""/output"" \google/deepvariant:""${BIN_VERSION}"" \/opt/deepvariant/bin/run_deepvariant \--model_type=WES \--ref=/input/ucsc.hg19.chr20.unittest.fasta \--reads=/input/NA12878_S1.chr20.10_10p1mb.bam \--regions ""chr20:10,000,000-10,010,000"" \--output_vcf=/output/output.vcf.gz \--output_gvcf=/output/output.g.vcf.gz \--num_shards=1 \--dry_run=true. - Error trace: No error.(if applicable).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/561
https://github.com/google/deepvariant/issues/561:486,testability,Test,Test,486,"No output file generating in quick start; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** After running the code in the deepvariant docker container (quick start), the output vcf files have not been generated. (A clear and concise description of what the issue is.). **Setup**. - Operating system:Mac OS . - DeepVariant version: Latest. - Installation method (Docker, built from source, etc.): Docker. - Type of data: Test files(sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: sudoa docker run \-v ""${INPUT_DIR}"":""/input"" \-v ""${INPUT_DIR}"":""/output"" \google/deepvariant:""${BIN_VERSION}"" \/opt/deepvariant/bin/run_deepvariant \--model_type=WES \--ref=/input/ucsc.hg19.chr20.unittest.fasta \--reads=/input/NA12878_S1.chr20.10_10p1mb.bam \--regions ""chr20:10,000,000-10,010,000"" \--output_vcf=/output/output.vcf.gz \--output_gvcf=/output/output.g.vcf.gz \--num_shards=1 \--dry_run=true. - Error trace: No error.(if applicable).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/561
https://github.com/google/deepvariant/issues/561:508,testability,instrument,instrument,508,"No output file generating in quick start; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** After running the code in the deepvariant docker container (quick start), the output vcf files have not been generated. (A clear and concise description of what the issue is.). **Setup**. - Operating system:Mac OS . - DeepVariant version: Latest. - Installation method (Docker, built from source, etc.): Docker. - Type of data: Test files(sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: sudoa docker run \-v ""${INPUT_DIR}"":""/input"" \-v ""${INPUT_DIR}"":""/output"" \google/deepvariant:""${BIN_VERSION}"" \/opt/deepvariant/bin/run_deepvariant \--model_type=WES \--ref=/input/ucsc.hg19.chr20.unittest.fasta \--reads=/input/NA12878_S1.chr20.10_10p1mb.bam \--regions ""chr20:10,000,000-10,010,000"" \--output_vcf=/output/output.vcf.gz \--output_gvcf=/output/output.g.vcf.gz \--num_shards=1 \--dry_run=true. - Error trace: No error.(if applicable).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/561
https://github.com/google/deepvariant/issues/561:823,testability,unit,unittest,823,"No output file generating in quick start; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** After running the code in the deepvariant docker container (quick start), the output vcf files have not been generated. (A clear and concise description of what the issue is.). **Setup**. - Operating system:Mac OS . - DeepVariant version: Latest. - Installation method (Docker, built from source, etc.): Docker. - Type of data: Test files(sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: sudoa docker run \-v ""${INPUT_DIR}"":""/input"" \-v ""${INPUT_DIR}"":""/output"" \google/deepvariant:""${BIN_VERSION}"" \/opt/deepvariant/bin/run_deepvariant \--model_type=WES \--ref=/input/ucsc.hg19.chr20.unittest.fasta \--reads=/input/NA12878_S1.chr20.10_10p1mb.bam \--regions ""chr20:10,000,000-10,010,000"" \--output_vcf=/output/output.vcf.gz \--output_gvcf=/output/output.g.vcf.gz \--num_shards=1 \--dry_run=true. - Error trace: No error.(if applicable).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/561
https://github.com/google/deepvariant/issues/561:1042,testability,trace,trace,1042,"No output file generating in quick start; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** After running the code in the deepvariant docker container (quick start), the output vcf files have not been generated. (A clear and concise description of what the issue is.). **Setup**. - Operating system:Mac OS . - DeepVariant version: Latest. - Installation method (Docker, built from source, etc.): Docker. - Type of data: Test files(sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: sudoa docker run \-v ""${INPUT_DIR}"":""/input"" \-v ""${INPUT_DIR}"":""/output"" \google/deepvariant:""${BIN_VERSION}"" \/opt/deepvariant/bin/run_deepvariant \--model_type=WES \--ref=/input/ucsc.hg19.chr20.unittest.fasta \--reads=/input/NA12878_S1.chr20.10_10p1mb.bam \--regions ""chr20:10,000,000-10,010,000"" \--output_vcf=/output/output.vcf.gz \--output_gvcf=/output/output.g.vcf.gz \--num_shards=1 \--dry_run=true. - Error trace: No error.(if applicable).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/561
https://github.com/google/deepvariant/issues/561:281,usability,clear,clear,281,"No output file generating in quick start; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** After running the code in the deepvariant docker container (quick start), the output vcf files have not been generated. (A clear and concise description of what the issue is.). **Setup**. - Operating system:Mac OS . - DeepVariant version: Latest. - Installation method (Docker, built from source, etc.): Docker. - Type of data: Test files(sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: sudoa docker run \-v ""${INPUT_DIR}"":""/input"" \-v ""${INPUT_DIR}"":""/output"" \google/deepvariant:""${BIN_VERSION}"" \/opt/deepvariant/bin/run_deepvariant \--model_type=WES \--ref=/input/ucsc.hg19.chr20.unittest.fasta \--reads=/input/NA12878_S1.chr20.10_10p1mb.bam \--regions ""chr20:10,000,000-10,010,000"" \--output_vcf=/output/output.vcf.gz \--output_gvcf=/output/output.g.vcf.gz \--num_shards=1 \--dry_run=true. - Error trace: No error.(if applicable).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/561
https://github.com/google/deepvariant/issues/561:617,usability,Command,Command,617,"No output file generating in quick start; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** After running the code in the deepvariant docker container (quick start), the output vcf files have not been generated. (A clear and concise description of what the issue is.). **Setup**. - Operating system:Mac OS . - DeepVariant version: Latest. - Installation method (Docker, built from source, etc.): Docker. - Type of data: Test files(sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: sudoa docker run \-v ""${INPUT_DIR}"":""/input"" \-v ""${INPUT_DIR}"":""/output"" \google/deepvariant:""${BIN_VERSION}"" \/opt/deepvariant/bin/run_deepvariant \--model_type=WES \--ref=/input/ucsc.hg19.chr20.unittest.fasta \--reads=/input/NA12878_S1.chr20.10_10p1mb.bam \--regions ""chr20:10,000,000-10,010,000"" \--output_vcf=/output/output.vcf.gz \--output_gvcf=/output/output.g.vcf.gz \--num_shards=1 \--dry_run=true. - Error trace: No error.(if applicable).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/561
https://github.com/google/deepvariant/issues/561:664,usability,input,input,664,"No output file generating in quick start; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** After running the code in the deepvariant docker container (quick start), the output vcf files have not been generated. (A clear and concise description of what the issue is.). **Setup**. - Operating system:Mac OS . - DeepVariant version: Latest. - Installation method (Docker, built from source, etc.): Docker. - Type of data: Test files(sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: sudoa docker run \-v ""${INPUT_DIR}"":""/input"" \-v ""${INPUT_DIR}"":""/output"" \google/deepvariant:""${BIN_VERSION}"" \/opt/deepvariant/bin/run_deepvariant \--model_type=WES \--ref=/input/ucsc.hg19.chr20.unittest.fasta \--reads=/input/NA12878_S1.chr20.10_10p1mb.bam \--regions ""chr20:10,000,000-10,010,000"" \--output_vcf=/output/output.vcf.gz \--output_gvcf=/output/output.g.vcf.gz \--num_shards=1 \--dry_run=true. - Error trace: No error.(if applicable).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/561
https://github.com/google/deepvariant/issues/561:801,usability,input,input,801,"No output file generating in quick start; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** After running the code in the deepvariant docker container (quick start), the output vcf files have not been generated. (A clear and concise description of what the issue is.). **Setup**. - Operating system:Mac OS . - DeepVariant version: Latest. - Installation method (Docker, built from source, etc.): Docker. - Type of data: Test files(sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: sudoa docker run \-v ""${INPUT_DIR}"":""/input"" \-v ""${INPUT_DIR}"":""/output"" \google/deepvariant:""${BIN_VERSION}"" \/opt/deepvariant/bin/run_deepvariant \--model_type=WES \--ref=/input/ucsc.hg19.chr20.unittest.fasta \--reads=/input/NA12878_S1.chr20.10_10p1mb.bam \--regions ""chr20:10,000,000-10,010,000"" \--output_vcf=/output/output.vcf.gz \--output_gvcf=/output/output.g.vcf.gz \--num_shards=1 \--dry_run=true. - Error trace: No error.(if applicable).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/561
https://github.com/google/deepvariant/issues/561:848,usability,input,input,848,"No output file generating in quick start; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** After running the code in the deepvariant docker container (quick start), the output vcf files have not been generated. (A clear and concise description of what the issue is.). **Setup**. - Operating system:Mac OS . - DeepVariant version: Latest. - Installation method (Docker, built from source, etc.): Docker. - Type of data: Test files(sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: sudoa docker run \-v ""${INPUT_DIR}"":""/input"" \-v ""${INPUT_DIR}"":""/output"" \google/deepvariant:""${BIN_VERSION}"" \/opt/deepvariant/bin/run_deepvariant \--model_type=WES \--ref=/input/ucsc.hg19.chr20.unittest.fasta \--reads=/input/NA12878_S1.chr20.10_10p1mb.bam \--regions ""chr20:10,000,000-10,010,000"" \--output_vcf=/output/output.vcf.gz \--output_gvcf=/output/output.g.vcf.gz \--num_shards=1 \--dry_run=true. - Error trace: No error.(if applicable).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/561
https://github.com/google/deepvariant/issues/561:1036,usability,Error,Error,1036,"No output file generating in quick start; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** After running the code in the deepvariant docker container (quick start), the output vcf files have not been generated. (A clear and concise description of what the issue is.). **Setup**. - Operating system:Mac OS . - DeepVariant version: Latest. - Installation method (Docker, built from source, etc.): Docker. - Type of data: Test files(sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: sudoa docker run \-v ""${INPUT_DIR}"":""/input"" \-v ""${INPUT_DIR}"":""/output"" \google/deepvariant:""${BIN_VERSION}"" \/opt/deepvariant/bin/run_deepvariant \--model_type=WES \--ref=/input/ucsc.hg19.chr20.unittest.fasta \--reads=/input/NA12878_S1.chr20.10_10p1mb.bam \--regions ""chr20:10,000,000-10,010,000"" \--output_vcf=/output/output.vcf.gz \--output_gvcf=/output/output.g.vcf.gz \--num_shards=1 \--dry_run=true. - Error trace: No error.(if applicable).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/561
https://github.com/google/deepvariant/issues/561:1052,usability,error,error,1052,"No output file generating in quick start; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** After running the code in the deepvariant docker container (quick start), the output vcf files have not been generated. (A clear and concise description of what the issue is.). **Setup**. - Operating system:Mac OS . - DeepVariant version: Latest. - Installation method (Docker, built from source, etc.): Docker. - Type of data: Test files(sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: sudoa docker run \-v ""${INPUT_DIR}"":""/input"" \-v ""${INPUT_DIR}"":""/output"" \google/deepvariant:""${BIN_VERSION}"" \/opt/deepvariant/bin/run_deepvariant \--model_type=WES \--ref=/input/ucsc.hg19.chr20.unittest.fasta \--reads=/input/NA12878_S1.chr20.10_10p1mb.bam \--regions ""chr20:10,000,000-10,010,000"" \--output_vcf=/output/output.vcf.gz \--output_gvcf=/output/output.g.vcf.gz \--num_shards=1 \--dry_run=true. - Error trace: No error.(if applicable).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/561
https://github.com/google/deepvariant/issues/562:0,energy efficiency,Adapt,Adaptation,0,Adaptation to polyploid organisms.; Is it possible to modify the DeepVariant source code to get correct CNN outputs for polyploid organisms?,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/562
https://github.com/google/deepvariant/issues/562:0,integrability,Adapt,Adaptation,0,Adaptation to polyploid organisms.; Is it possible to modify the DeepVariant source code to get correct CNN outputs for polyploid organisms?,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/562
https://github.com/google/deepvariant/issues/562:0,interoperability,Adapt,Adaptation,0,Adaptation to polyploid organisms.; Is it possible to modify the DeepVariant source code to get correct CNN outputs for polyploid organisms?,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/562
https://github.com/google/deepvariant/issues/562:0,modifiability,Adapt,Adaptation,0,Adaptation to polyploid organisms.; Is it possible to modify the DeepVariant source code to get correct CNN outputs for polyploid organisms?,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/562
https://github.com/google/deepvariant/issues/562:54,security,modif,modify,54,Adaptation to polyploid organisms.; Is it possible to modify the DeepVariant source code to get correct CNN outputs for polyploid organisms?,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/562
https://github.com/google/deepvariant/issues/563:149,availability,error,error,149,"Optionally drop channel 18; **Describe the issue:**. I am attempting to use DeepVariant 1.4 with a model trained on DeepVariant 1.3. I encounter the error:. ""ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 6 channels while the examples have 7."". **Setup**. - Operating system: Linux Ubuntu 20.04. - DeepVariant version: 1.4. - Installation method: Docker. Just regular bam files being called on the T2T reference fasta. **Steps to reproduce:**. /opt/deepvariant/bin/run_deepvariant \. --ref=hprc-jun1-mc-chm13-minaf.0.1.fasta \. --reads=HSB340-CHM13v2.chrY.sorted.deduped.cram \. --customized_model=model.ckpt-364300 \. --output_vcf=HSB340-CHM13v2.chrY.deepvariant.vcf.gz \. --output_gvcf=HSB340-CHM13v2.chrY.deepvariant.g.vcf.gz \. --model_type WGS \. --make_examples_extra_args phase_reads=true,channels=blank \. --regions CHM13v2.chrY \. --num_shards=24. parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""hprc-jun1-mc-chm13-minaf.0.1.fasta"" --reads ""HSB340-CHM13v2.chrY.sorted.deduped.cram"" -examples ""/tmp/tmpwn2kfxca/make_examples.tfrecord@24.gz"" --channels ""blank"" --gvcf ""/tmp/tmpwn2kfxca/gvcf.tfrecord@24.gz"" --phase_reads --regions ""CHM13v2.chrY"" --task {}. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpwn2kfxca/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpwn2kfxca/make_examples.tfrecord@24.gz"" --checkpoint ""model.ckpt-364300"" --openvino_model_dir ""/tmp/tmpwn2kfxca"". I0901 22:59:14.275113 140554215814976 call_variants.py:317] From /tmp/tmpwn2kfxca/make_examples.tfrecord-00000-of-00024.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 18]. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_2ucnuw5e/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/563
https://github.com/google/deepvariant/issues/563:209,availability,checkpoint,checkpoint,209,"Optionally drop channel 18; **Describe the issue:**. I am attempting to use DeepVariant 1.4 with a model trained on DeepVariant 1.3. I encounter the error:. ""ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 6 channels while the examples have 7."". **Setup**. - Operating system: Linux Ubuntu 20.04. - DeepVariant version: 1.4. - Installation method: Docker. Just regular bam files being called on the T2T reference fasta. **Steps to reproduce:**. /opt/deepvariant/bin/run_deepvariant \. --ref=hprc-jun1-mc-chm13-minaf.0.1.fasta \. --reads=HSB340-CHM13v2.chrY.sorted.deduped.cram \. --customized_model=model.ckpt-364300 \. --output_vcf=HSB340-CHM13v2.chrY.deepvariant.vcf.gz \. --output_gvcf=HSB340-CHM13v2.chrY.deepvariant.g.vcf.gz \. --model_type WGS \. --make_examples_extra_args phase_reads=true,channels=blank \. --regions CHM13v2.chrY \. --num_shards=24. parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""hprc-jun1-mc-chm13-minaf.0.1.fasta"" --reads ""HSB340-CHM13v2.chrY.sorted.deduped.cram"" -examples ""/tmp/tmpwn2kfxca/make_examples.tfrecord@24.gz"" --channels ""blank"" --gvcf ""/tmp/tmpwn2kfxca/gvcf.tfrecord@24.gz"" --phase_reads --regions ""CHM13v2.chrY"" --task {}. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpwn2kfxca/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpwn2kfxca/make_examples.tfrecord@24.gz"" --checkpoint ""model.ckpt-364300"" --openvino_model_dir ""/tmp/tmpwn2kfxca"". I0901 22:59:14.275113 140554215814976 call_variants.py:317] From /tmp/tmpwn2kfxca/make_examples.tfrecord-00000-of-00024.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 18]. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_2ucnuw5e/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/563
https://github.com/google/deepvariant/issues/563:242,availability,checkpoint,checkpoint,242,"Optionally drop channel 18; **Describe the issue:**. I am attempting to use DeepVariant 1.4 with a model trained on DeepVariant 1.3. I encounter the error:. ""ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 6 channels while the examples have 7."". **Setup**. - Operating system: Linux Ubuntu 20.04. - DeepVariant version: 1.4. - Installation method: Docker. Just regular bam files being called on the T2T reference fasta. **Steps to reproduce:**. /opt/deepvariant/bin/run_deepvariant \. --ref=hprc-jun1-mc-chm13-minaf.0.1.fasta \. --reads=HSB340-CHM13v2.chrY.sorted.deduped.cram \. --customized_model=model.ckpt-364300 \. --output_vcf=HSB340-CHM13v2.chrY.deepvariant.vcf.gz \. --output_gvcf=HSB340-CHM13v2.chrY.deepvariant.g.vcf.gz \. --model_type WGS \. --make_examples_extra_args phase_reads=true,channels=blank \. --regions CHM13v2.chrY \. --num_shards=24. parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""hprc-jun1-mc-chm13-minaf.0.1.fasta"" --reads ""HSB340-CHM13v2.chrY.sorted.deduped.cram"" -examples ""/tmp/tmpwn2kfxca/make_examples.tfrecord@24.gz"" --channels ""blank"" --gvcf ""/tmp/tmpwn2kfxca/gvcf.tfrecord@24.gz"" --phase_reads --regions ""CHM13v2.chrY"" --task {}. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpwn2kfxca/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpwn2kfxca/make_examples.tfrecord@24.gz"" --checkpoint ""model.ckpt-364300"" --openvino_model_dir ""/tmp/tmpwn2kfxca"". I0901 22:59:14.275113 140554215814976 call_variants.py:317] From /tmp/tmpwn2kfxca/make_examples.tfrecord-00000-of-00024.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 18]. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_2ucnuw5e/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/563
https://github.com/google/deepvariant/issues/563:310,availability,Operat,Operating,310,"Optionally drop channel 18; **Describe the issue:**. I am attempting to use DeepVariant 1.4 with a model trained on DeepVariant 1.3. I encounter the error:. ""ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 6 channels while the examples have 7."". **Setup**. - Operating system: Linux Ubuntu 20.04. - DeepVariant version: 1.4. - Installation method: Docker. Just regular bam files being called on the T2T reference fasta. **Steps to reproduce:**. /opt/deepvariant/bin/run_deepvariant \. --ref=hprc-jun1-mc-chm13-minaf.0.1.fasta \. --reads=HSB340-CHM13v2.chrY.sorted.deduped.cram \. --customized_model=model.ckpt-364300 \. --output_vcf=HSB340-CHM13v2.chrY.deepvariant.vcf.gz \. --output_gvcf=HSB340-CHM13v2.chrY.deepvariant.g.vcf.gz \. --model_type WGS \. --make_examples_extra_args phase_reads=true,channels=blank \. --regions CHM13v2.chrY \. --num_shards=24. parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""hprc-jun1-mc-chm13-minaf.0.1.fasta"" --reads ""HSB340-CHM13v2.chrY.sorted.deduped.cram"" -examples ""/tmp/tmpwn2kfxca/make_examples.tfrecord@24.gz"" --channels ""blank"" --gvcf ""/tmp/tmpwn2kfxca/gvcf.tfrecord@24.gz"" --phase_reads --regions ""CHM13v2.chrY"" --task {}. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpwn2kfxca/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpwn2kfxca/make_examples.tfrecord@24.gz"" --checkpoint ""model.ckpt-364300"" --openvino_model_dir ""/tmp/tmpwn2kfxca"". I0901 22:59:14.275113 140554215814976 call_variants.py:317] From /tmp/tmpwn2kfxca/make_examples.tfrecord-00000-of-00024.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 18]. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_2ucnuw5e/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/563
https://github.com/google/deepvariant/issues/563:1423,availability,checkpoint,checkpoint,1423,"s being called on the T2T reference fasta. **Steps to reproduce:**. /opt/deepvariant/bin/run_deepvariant \. --ref=hprc-jun1-mc-chm13-minaf.0.1.fasta \. --reads=HSB340-CHM13v2.chrY.sorted.deduped.cram \. --customized_model=model.ckpt-364300 \. --output_vcf=HSB340-CHM13v2.chrY.deepvariant.vcf.gz \. --output_gvcf=HSB340-CHM13v2.chrY.deepvariant.g.vcf.gz \. --model_type WGS \. --make_examples_extra_args phase_reads=true,channels=blank \. --regions CHM13v2.chrY \. --num_shards=24. parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""hprc-jun1-mc-chm13-minaf.0.1.fasta"" --reads ""HSB340-CHM13v2.chrY.sorted.deduped.cram"" -examples ""/tmp/tmpwn2kfxca/make_examples.tfrecord@24.gz"" --channels ""blank"" --gvcf ""/tmp/tmpwn2kfxca/gvcf.tfrecord@24.gz"" --phase_reads --regions ""CHM13v2.chrY"" --task {}. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpwn2kfxca/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpwn2kfxca/make_examples.tfrecord@24.gz"" --checkpoint ""model.ckpt-364300"" --openvino_model_dir ""/tmp/tmpwn2kfxca"". I0901 22:59:14.275113 140554215814976 call_variants.py:317] From /tmp/tmpwn2kfxca/make_examples.tfrecord-00000-of-00024.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 18]. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_2ucnuw5e/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_2ucnuw5e/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_2ucnuw5e/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_2ucnuw5e/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/563
https://github.com/google/deepvariant/issues/563:2613,availability,checkpoint,checkpoint,2613,"e_reads --regions ""CHM13v2.chrY"" --task {}. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpwn2kfxca/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpwn2kfxca/make_examples.tfrecord@24.gz"" --checkpoint ""model.ckpt-364300"" --openvino_model_dir ""/tmp/tmpwn2kfxca"". I0901 22:59:14.275113 140554215814976 call_variants.py:317] From /tmp/tmpwn2kfxca/make_examples.tfrecord-00000-of-00024.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 18]. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_2ucnuw5e/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_2ucnuw5e/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_2ucnuw5e/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_2ucnuw5e/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_2ucnuw5e/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 363, in call_variants. raise ValueError('The number of channels in examples and checkpoint '. ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 6 channels while the examples have 7. **Any additional context:**. DeepVariant 1.4 added an additional default channel. This appears to have broken all previously trained models. Using the convenient ""run_deepvariant"" script with the channels=blank options does not result in make_examples generating input examples with only six features. ***Desired Output:***. An option in run_deepvariant that will allow for the creation of example files with the previously standard six input channels.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/563
https://github.com/google/deepvariant/issues/563:2678,availability,checkpoint,checkpoint,2678,"e_reads --regions ""CHM13v2.chrY"" --task {}. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpwn2kfxca/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpwn2kfxca/make_examples.tfrecord@24.gz"" --checkpoint ""model.ckpt-364300"" --openvino_model_dir ""/tmp/tmpwn2kfxca"". I0901 22:59:14.275113 140554215814976 call_variants.py:317] From /tmp/tmpwn2kfxca/make_examples.tfrecord-00000-of-00024.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 18]. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_2ucnuw5e/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_2ucnuw5e/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_2ucnuw5e/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_2ucnuw5e/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_2ucnuw5e/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 363, in call_variants. raise ValueError('The number of channels in examples and checkpoint '. ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 6 channels while the examples have 7. **Any additional context:**. DeepVariant 1.4 added an additional default channel. This appears to have broken all previously trained models. Using the convenient ""run_deepvariant"" script with the channels=blank options does not result in make_examples generating input examples with only six features. ***Desired Output:***. An option in run_deepvariant that will allow for the creation of example files with the previously standard six input channels.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/563
https://github.com/google/deepvariant/issues/563:2711,availability,checkpoint,checkpoint,2711,"e_reads --regions ""CHM13v2.chrY"" --task {}. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpwn2kfxca/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpwn2kfxca/make_examples.tfrecord@24.gz"" --checkpoint ""model.ckpt-364300"" --openvino_model_dir ""/tmp/tmpwn2kfxca"". I0901 22:59:14.275113 140554215814976 call_variants.py:317] From /tmp/tmpwn2kfxca/make_examples.tfrecord-00000-of-00024.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 18]. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_2ucnuw5e/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_2ucnuw5e/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_2ucnuw5e/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_2ucnuw5e/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_2ucnuw5e/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 363, in call_variants. raise ValueError('The number of channels in examples and checkpoint '. ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 6 channels while the examples have 7. **Any additional context:**. DeepVariant 1.4 added an additional default channel. This appears to have broken all previously trained models. Using the convenient ""run_deepvariant"" script with the channels=blank options does not result in make_examples generating input examples with only six features. ***Desired Output:***. An option in run_deepvariant that will allow for the creation of example files with the previously standard six input channels.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/563
https://github.com/google/deepvariant/issues/563:362,deployability,version,version,362,"Optionally drop channel 18; **Describe the issue:**. I am attempting to use DeepVariant 1.4 with a model trained on DeepVariant 1.3. I encounter the error:. ""ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 6 channels while the examples have 7."". **Setup**. - Operating system: Linux Ubuntu 20.04. - DeepVariant version: 1.4. - Installation method: Docker. Just regular bam files being called on the T2T reference fasta. **Steps to reproduce:**. /opt/deepvariant/bin/run_deepvariant \. --ref=hprc-jun1-mc-chm13-minaf.0.1.fasta \. --reads=HSB340-CHM13v2.chrY.sorted.deduped.cram \. --customized_model=model.ckpt-364300 \. --output_vcf=HSB340-CHM13v2.chrY.deepvariant.vcf.gz \. --output_gvcf=HSB340-CHM13v2.chrY.deepvariant.g.vcf.gz \. --model_type WGS \. --make_examples_extra_args phase_reads=true,channels=blank \. --regions CHM13v2.chrY \. --num_shards=24. parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""hprc-jun1-mc-chm13-minaf.0.1.fasta"" --reads ""HSB340-CHM13v2.chrY.sorted.deduped.cram"" -examples ""/tmp/tmpwn2kfxca/make_examples.tfrecord@24.gz"" --channels ""blank"" --gvcf ""/tmp/tmpwn2kfxca/gvcf.tfrecord@24.gz"" --phase_reads --regions ""CHM13v2.chrY"" --task {}. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpwn2kfxca/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpwn2kfxca/make_examples.tfrecord@24.gz"" --checkpoint ""model.ckpt-364300"" --openvino_model_dir ""/tmp/tmpwn2kfxca"". I0901 22:59:14.275113 140554215814976 call_variants.py:317] From /tmp/tmpwn2kfxca/make_examples.tfrecord-00000-of-00024.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 18]. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_2ucnuw5e/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/563
https://github.com/google/deepvariant/issues/563:378,deployability,Instal,Installation,378,"Optionally drop channel 18; **Describe the issue:**. I am attempting to use DeepVariant 1.4 with a model trained on DeepVariant 1.3. I encounter the error:. ""ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 6 channels while the examples have 7."". **Setup**. - Operating system: Linux Ubuntu 20.04. - DeepVariant version: 1.4. - Installation method: Docker. Just regular bam files being called on the T2T reference fasta. **Steps to reproduce:**. /opt/deepvariant/bin/run_deepvariant \. --ref=hprc-jun1-mc-chm13-minaf.0.1.fasta \. --reads=HSB340-CHM13v2.chrY.sorted.deduped.cram \. --customized_model=model.ckpt-364300 \. --output_vcf=HSB340-CHM13v2.chrY.deepvariant.vcf.gz \. --output_gvcf=HSB340-CHM13v2.chrY.deepvariant.g.vcf.gz \. --model_type WGS \. --make_examples_extra_args phase_reads=true,channels=blank \. --regions CHM13v2.chrY \. --num_shards=24. parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""hprc-jun1-mc-chm13-minaf.0.1.fasta"" --reads ""HSB340-CHM13v2.chrY.sorted.deduped.cram"" -examples ""/tmp/tmpwn2kfxca/make_examples.tfrecord@24.gz"" --channels ""blank"" --gvcf ""/tmp/tmpwn2kfxca/gvcf.tfrecord@24.gz"" --phase_reads --regions ""CHM13v2.chrY"" --task {}. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpwn2kfxca/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpwn2kfxca/make_examples.tfrecord@24.gz"" --checkpoint ""model.ckpt-364300"" --openvino_model_dir ""/tmp/tmpwn2kfxca"". I0901 22:59:14.275113 140554215814976 call_variants.py:317] From /tmp/tmpwn2kfxca/make_examples.tfrecord-00000-of-00024.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 18]. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_2ucnuw5e/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/563
https://github.com/google/deepvariant/issues/563:1877,deployability,modul,module,1877,"3v2.chrY \. --num_shards=24. parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""hprc-jun1-mc-chm13-minaf.0.1.fasta"" --reads ""HSB340-CHM13v2.chrY.sorted.deduped.cram"" -examples ""/tmp/tmpwn2kfxca/make_examples.tfrecord@24.gz"" --channels ""blank"" --gvcf ""/tmp/tmpwn2kfxca/gvcf.tfrecord@24.gz"" --phase_reads --regions ""CHM13v2.chrY"" --task {}. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpwn2kfxca/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpwn2kfxca/make_examples.tfrecord@24.gz"" --checkpoint ""model.ckpt-364300"" --openvino_model_dir ""/tmp/tmpwn2kfxca"". I0901 22:59:14.275113 140554215814976 call_variants.py:317] From /tmp/tmpwn2kfxca/make_examples.tfrecord-00000-of-00024.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 18]. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_2ucnuw5e/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_2ucnuw5e/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_2ucnuw5e/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_2ucnuw5e/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_2ucnuw5e/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 363, in call_variants. raise ValueError('The number of channels in examples and checkpoint '. ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 6 channels while the examples have 7. **Any additional context:**. DeepVariant 1.4 added an additional default channel. This appears to have broken all pr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/563
https://github.com/google/deepvariant/issues/563:99,energy efficiency,model,model,99,"Optionally drop channel 18; **Describe the issue:**. I am attempting to use DeepVariant 1.4 with a model trained on DeepVariant 1.3. I encounter the error:. ""ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 6 channels while the examples have 7."". **Setup**. - Operating system: Linux Ubuntu 20.04. - DeepVariant version: 1.4. - Installation method: Docker. Just regular bam files being called on the T2T reference fasta. **Steps to reproduce:**. /opt/deepvariant/bin/run_deepvariant \. --ref=hprc-jun1-mc-chm13-minaf.0.1.fasta \. --reads=HSB340-CHM13v2.chrY.sorted.deduped.cram \. --customized_model=model.ckpt-364300 \. --output_vcf=HSB340-CHM13v2.chrY.deepvariant.vcf.gz \. --output_gvcf=HSB340-CHM13v2.chrY.deepvariant.g.vcf.gz \. --model_type WGS \. --make_examples_extra_args phase_reads=true,channels=blank \. --regions CHM13v2.chrY \. --num_shards=24. parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""hprc-jun1-mc-chm13-minaf.0.1.fasta"" --reads ""HSB340-CHM13v2.chrY.sorted.deduped.cram"" -examples ""/tmp/tmpwn2kfxca/make_examples.tfrecord@24.gz"" --channels ""blank"" --gvcf ""/tmp/tmpwn2kfxca/gvcf.tfrecord@24.gz"" --phase_reads --regions ""CHM13v2.chrY"" --task {}. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpwn2kfxca/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpwn2kfxca/make_examples.tfrecord@24.gz"" --checkpoint ""model.ckpt-364300"" --openvino_model_dir ""/tmp/tmpwn2kfxca"". I0901 22:59:14.275113 140554215814976 call_variants.py:317] From /tmp/tmpwn2kfxca/make_examples.tfrecord-00000-of-00024.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 18]. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_2ucnuw5e/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/563
https://github.com/google/deepvariant/issues/563:650,energy efficiency,model,model,650,"Optionally drop channel 18; **Describe the issue:**. I am attempting to use DeepVariant 1.4 with a model trained on DeepVariant 1.3. I encounter the error:. ""ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 6 channels while the examples have 7."". **Setup**. - Operating system: Linux Ubuntu 20.04. - DeepVariant version: 1.4. - Installation method: Docker. Just regular bam files being called on the T2T reference fasta. **Steps to reproduce:**. /opt/deepvariant/bin/run_deepvariant \. --ref=hprc-jun1-mc-chm13-minaf.0.1.fasta \. --reads=HSB340-CHM13v2.chrY.sorted.deduped.cram \. --customized_model=model.ckpt-364300 \. --output_vcf=HSB340-CHM13v2.chrY.deepvariant.vcf.gz \. --output_gvcf=HSB340-CHM13v2.chrY.deepvariant.g.vcf.gz \. --model_type WGS \. --make_examples_extra_args phase_reads=true,channels=blank \. --regions CHM13v2.chrY \. --num_shards=24. parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""hprc-jun1-mc-chm13-minaf.0.1.fasta"" --reads ""HSB340-CHM13v2.chrY.sorted.deduped.cram"" -examples ""/tmp/tmpwn2kfxca/make_examples.tfrecord@24.gz"" --channels ""blank"" --gvcf ""/tmp/tmpwn2kfxca/gvcf.tfrecord@24.gz"" --phase_reads --regions ""CHM13v2.chrY"" --task {}. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpwn2kfxca/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpwn2kfxca/make_examples.tfrecord@24.gz"" --checkpoint ""model.ckpt-364300"" --openvino_model_dir ""/tmp/tmpwn2kfxca"". I0901 22:59:14.275113 140554215814976 call_variants.py:317] From /tmp/tmpwn2kfxca/make_examples.tfrecord-00000-of-00024.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 18]. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_2ucnuw5e/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/563
https://github.com/google/deepvariant/issues/563:1435,energy efficiency,model,model,1435,"lled on the T2T reference fasta. **Steps to reproduce:**. /opt/deepvariant/bin/run_deepvariant \. --ref=hprc-jun1-mc-chm13-minaf.0.1.fasta \. --reads=HSB340-CHM13v2.chrY.sorted.deduped.cram \. --customized_model=model.ckpt-364300 \. --output_vcf=HSB340-CHM13v2.chrY.deepvariant.vcf.gz \. --output_gvcf=HSB340-CHM13v2.chrY.deepvariant.g.vcf.gz \. --model_type WGS \. --make_examples_extra_args phase_reads=true,channels=blank \. --regions CHM13v2.chrY \. --num_shards=24. parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""hprc-jun1-mc-chm13-minaf.0.1.fasta"" --reads ""HSB340-CHM13v2.chrY.sorted.deduped.cram"" -examples ""/tmp/tmpwn2kfxca/make_examples.tfrecord@24.gz"" --channels ""blank"" --gvcf ""/tmp/tmpwn2kfxca/gvcf.tfrecord@24.gz"" --phase_reads --regions ""CHM13v2.chrY"" --task {}. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpwn2kfxca/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpwn2kfxca/make_examples.tfrecord@24.gz"" --checkpoint ""model.ckpt-364300"" --openvino_model_dir ""/tmp/tmpwn2kfxca"". I0901 22:59:14.275113 140554215814976 call_variants.py:317] From /tmp/tmpwn2kfxca/make_examples.tfrecord-00000-of-00024.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 18]. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_2ucnuw5e/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_2ucnuw5e/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_2ucnuw5e/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_2ucnuw5e/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/563
https://github.com/google/deepvariant/issues/563:2897,energy efficiency,model,models,2897,"e_reads --regions ""CHM13v2.chrY"" --task {}. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpwn2kfxca/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpwn2kfxca/make_examples.tfrecord@24.gz"" --checkpoint ""model.ckpt-364300"" --openvino_model_dir ""/tmp/tmpwn2kfxca"". I0901 22:59:14.275113 140554215814976 call_variants.py:317] From /tmp/tmpwn2kfxca/make_examples.tfrecord-00000-of-00024.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 18]. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_2ucnuw5e/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_2ucnuw5e/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_2ucnuw5e/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_2ucnuw5e/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_2ucnuw5e/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 363, in call_variants. raise ValueError('The number of channels in examples and checkpoint '. ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 6 channels while the examples have 7. **Any additional context:**. DeepVariant 1.4 added an additional default channel. This appears to have broken all previously trained models. Using the convenient ""run_deepvariant"" script with the channels=blank options does not result in make_examples generating input examples with only six features. ***Desired Output:***. An option in run_deepvariant that will allow for the creation of example files with the previously standard six input channels.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/563
https://github.com/google/deepvariant/issues/563:362,integrability,version,version,362,"Optionally drop channel 18; **Describe the issue:**. I am attempting to use DeepVariant 1.4 with a model trained on DeepVariant 1.3. I encounter the error:. ""ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 6 channels while the examples have 7."". **Setup**. - Operating system: Linux Ubuntu 20.04. - DeepVariant version: 1.4. - Installation method: Docker. Just regular bam files being called on the T2T reference fasta. **Steps to reproduce:**. /opt/deepvariant/bin/run_deepvariant \. --ref=hprc-jun1-mc-chm13-minaf.0.1.fasta \. --reads=HSB340-CHM13v2.chrY.sorted.deduped.cram \. --customized_model=model.ckpt-364300 \. --output_vcf=HSB340-CHM13v2.chrY.deepvariant.vcf.gz \. --output_gvcf=HSB340-CHM13v2.chrY.deepvariant.g.vcf.gz \. --model_type WGS \. --make_examples_extra_args phase_reads=true,channels=blank \. --regions CHM13v2.chrY \. --num_shards=24. parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""hprc-jun1-mc-chm13-minaf.0.1.fasta"" --reads ""HSB340-CHM13v2.chrY.sorted.deduped.cram"" -examples ""/tmp/tmpwn2kfxca/make_examples.tfrecord@24.gz"" --channels ""blank"" --gvcf ""/tmp/tmpwn2kfxca/gvcf.tfrecord@24.gz"" --phase_reads --regions ""CHM13v2.chrY"" --task {}. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpwn2kfxca/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpwn2kfxca/make_examples.tfrecord@24.gz"" --checkpoint ""model.ckpt-364300"" --openvino_model_dir ""/tmp/tmpwn2kfxca"". I0901 22:59:14.275113 140554215814976 call_variants.py:317] From /tmp/tmpwn2kfxca/make_examples.tfrecord-00000-of-00024.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 18]. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_2ucnuw5e/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/563
