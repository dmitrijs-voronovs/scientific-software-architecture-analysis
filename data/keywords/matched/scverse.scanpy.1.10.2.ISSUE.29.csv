id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/scverse/scanpy/issues/2496:875,integrability,filter,filtering,875,"`invalid value encountered in divide` when running `pp.normalize_pearson_residuals`; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. Hi, I'm trying to follow [Analytic Pearson residuals](https://www.sc-best-practices.org/preprocessing_visualization/normalization.html). but after getting first 2 nomalizations, I'm stucked with Pearson residuals normalization. It seems like a devided by 0 issue? I don't know. What else I can do to check if the matrix is the problem. already did gene (`min_cells=3`) and cell filtering(`min_genes=200`). ### Minimal code sample (that we can copy&paste without having any data). ```python. analytic_pearson = sc.experimental.pp.normalize_pearson_residuals(adata, inplace=False). ```. ```pytb. /home/sxykdx/miniconda3/envs/scanpy/lib/python3.10/site-packages/scanpy/experimental/pp/_normalization.py:59: RuntimeWarning: invalid value encountered in divide. residuals = diff / np.sqrt(mu + mu**2 / theta). ```. `analytic_pearson[""X""]` . Output:. ```py. array([[-0.08038502, -0.10195383, -0.24513291, ..., 1.47699586,. -0.08709449, -0.16926342],. [-0.08623174, -0.109369 , 3.53739781, ..., -0.54014355,. -0.09342913, -0.18157157],. [-0.07625086, -0.09671059, -0.2325321 , ..., -0.47777291,. 12.02085262, 6.06603257],. ...,. [-0.02799957, -0.03551299, -0.08540438, ..., -0.17560885,. -0.03033674, -0.05896328],. [-0.02840246, -0.03602399, -0.08663319, ..., -0.17813493,. -0.03077326, -0.05981169],. [-0.02914286, -0.03696307, -0.08889143, ..., -0.18277714,. -0.03157547, -0.06137084]]). ```. ```py. adata.layers[""analytic_pearson_residuals""] = analytic_pearson[""X""]. adata.layers[""analytic_pearson_residual",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2496
https://github.com/scverse/scanpy/issues/2496:2077,integrability,Version,Versions,2077,"rocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. Hi, I'm trying to follow [Analytic Pearson residuals](https://www.sc-best-practices.org/preprocessing_visualization/normalization.html). but after getting first 2 nomalizations, I'm stucked with Pearson residuals normalization. It seems like a devided by 0 issue? I don't know. What else I can do to check if the matrix is the problem. already did gene (`min_cells=3`) and cell filtering(`min_genes=200`). ### Minimal code sample (that we can copy&paste without having any data). ```python. analytic_pearson = sc.experimental.pp.normalize_pearson_residuals(adata, inplace=False). ```. ```pytb. /home/sxykdx/miniconda3/envs/scanpy/lib/python3.10/site-packages/scanpy/experimental/pp/_normalization.py:59: RuntimeWarning: invalid value encountered in divide. residuals = diff / np.sqrt(mu + mu**2 / theta). ```. `analytic_pearson[""X""]` . Output:. ```py. array([[-0.08038502, -0.10195383, -0.24513291, ..., 1.47699586,. -0.08709449, -0.16926342],. [-0.08623174, -0.109369 , 3.53739781, ..., -0.54014355,. -0.09342913, -0.18157157],. [-0.07625086, -0.09671059, -0.2325321 , ..., -0.47777291,. 12.02085262, 6.06603257],. ...,. [-0.02799957, -0.03551299, -0.08540438, ..., -0.17560885,. -0.03033674, -0.05896328],. [-0.02840246, -0.03602399, -0.08663319, ..., -0.17813493,. -0.03077326, -0.05981169],. [-0.02914286, -0.03696307, -0.08889143, ..., -0.18277714,. -0.03157547, -0.06137084]]). ```. ```py. adata.layers[""analytic_pearson_residuals""] = analytic_pearson[""X""]. adata.layers[""analytic_pearson_residuals""].sum(1). ```. Output:. `array([nan, nan, nan, ..., nan, nan, nan])`. #### Versions. scanpy==1.9.3 anndata==0.9.1 umap==0.5.3 numpy==1.23.5 scipy==1.10.1 pandas==2.0.1 scikit-learn==1.2.2 statsmodels==0.13.5 python-igraph==0.10.3 pynndescent==0.5.10. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2496
https://github.com/scverse/scanpy/issues/2496:206,modifiability,version,version,206,"`invalid value encountered in divide` when running `pp.normalize_pearson_residuals`; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. Hi, I'm trying to follow [Analytic Pearson residuals](https://www.sc-best-practices.org/preprocessing_visualization/normalization.html). but after getting first 2 nomalizations, I'm stucked with Pearson residuals normalization. It seems like a devided by 0 issue? I don't know. What else I can do to check if the matrix is the problem. already did gene (`min_cells=3`) and cell filtering(`min_genes=200`). ### Minimal code sample (that we can copy&paste without having any data). ```python. analytic_pearson = sc.experimental.pp.normalize_pearson_residuals(adata, inplace=False). ```. ```pytb. /home/sxykdx/miniconda3/envs/scanpy/lib/python3.10/site-packages/scanpy/experimental/pp/_normalization.py:59: RuntimeWarning: invalid value encountered in divide. residuals = diff / np.sqrt(mu + mu**2 / theta). ```. `analytic_pearson[""X""]` . Output:. ```py. array([[-0.08038502, -0.10195383, -0.24513291, ..., 1.47699586,. -0.08709449, -0.16926342],. [-0.08623174, -0.109369 , 3.53739781, ..., -0.54014355,. -0.09342913, -0.18157157],. [-0.07625086, -0.09671059, -0.2325321 , ..., -0.47777291,. 12.02085262, 6.06603257],. ...,. [-0.02799957, -0.03551299, -0.08540438, ..., -0.17560885,. -0.03033674, -0.05896328],. [-0.02840246, -0.03602399, -0.08663319, ..., -0.17813493,. -0.03077326, -0.05981169],. [-0.02914286, -0.03696307, -0.08889143, ..., -0.18277714,. -0.03157547, -0.06137084]]). ```. ```py. adata.layers[""analytic_pearson_residuals""] = analytic_pearson[""X""]. adata.layers[""analytic_pearson_residual",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2496
https://github.com/scverse/scanpy/issues/2496:1147,modifiability,pac,packages,1147,". - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. Hi, I'm trying to follow [Analytic Pearson residuals](https://www.sc-best-practices.org/preprocessing_visualization/normalization.html). but after getting first 2 nomalizations, I'm stucked with Pearson residuals normalization. It seems like a devided by 0 issue? I don't know. What else I can do to check if the matrix is the problem. already did gene (`min_cells=3`) and cell filtering(`min_genes=200`). ### Minimal code sample (that we can copy&paste without having any data). ```python. analytic_pearson = sc.experimental.pp.normalize_pearson_residuals(adata, inplace=False). ```. ```pytb. /home/sxykdx/miniconda3/envs/scanpy/lib/python3.10/site-packages/scanpy/experimental/pp/_normalization.py:59: RuntimeWarning: invalid value encountered in divide. residuals = diff / np.sqrt(mu + mu**2 / theta). ```. `analytic_pearson[""X""]` . Output:. ```py. array([[-0.08038502, -0.10195383, -0.24513291, ..., 1.47699586,. -0.08709449, -0.16926342],. [-0.08623174, -0.109369 , 3.53739781, ..., -0.54014355,. -0.09342913, -0.18157157],. [-0.07625086, -0.09671059, -0.2325321 , ..., -0.47777291,. 12.02085262, 6.06603257],. ...,. [-0.02799957, -0.03551299, -0.08540438, ..., -0.17560885,. -0.03033674, -0.05896328],. [-0.02840246, -0.03602399, -0.08663319, ..., -0.17813493,. -0.03077326, -0.05981169],. [-0.02914286, -0.03696307, -0.08889143, ..., -0.18277714,. -0.03157547, -0.06137084]]). ```. ```py. adata.layers[""analytic_pearson_residuals""] = analytic_pearson[""X""]. adata.layers[""analytic_pearson_residuals""].sum(1). ```. Output:. `array([nan, nan, nan, ..., nan, nan, nan])`. #### Versions. scanpy==1.9.3 anndata==0.9.1 umap==0.5.3 numpy==1.23.5 scipy==1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2496
https://github.com/scverse/scanpy/issues/2496:1899,modifiability,layer,layers,1899,"rocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. Hi, I'm trying to follow [Analytic Pearson residuals](https://www.sc-best-practices.org/preprocessing_visualization/normalization.html). but after getting first 2 nomalizations, I'm stucked with Pearson residuals normalization. It seems like a devided by 0 issue? I don't know. What else I can do to check if the matrix is the problem. already did gene (`min_cells=3`) and cell filtering(`min_genes=200`). ### Minimal code sample (that we can copy&paste without having any data). ```python. analytic_pearson = sc.experimental.pp.normalize_pearson_residuals(adata, inplace=False). ```. ```pytb. /home/sxykdx/miniconda3/envs/scanpy/lib/python3.10/site-packages/scanpy/experimental/pp/_normalization.py:59: RuntimeWarning: invalid value encountered in divide. residuals = diff / np.sqrt(mu + mu**2 / theta). ```. `analytic_pearson[""X""]` . Output:. ```py. array([[-0.08038502, -0.10195383, -0.24513291, ..., 1.47699586,. -0.08709449, -0.16926342],. [-0.08623174, -0.109369 , 3.53739781, ..., -0.54014355,. -0.09342913, -0.18157157],. [-0.07625086, -0.09671059, -0.2325321 , ..., -0.47777291,. 12.02085262, 6.06603257],. ...,. [-0.02799957, -0.03551299, -0.08540438, ..., -0.17560885,. -0.03033674, -0.05896328],. [-0.02840246, -0.03602399, -0.08663319, ..., -0.17813493,. -0.03077326, -0.05981169],. [-0.02914286, -0.03696307, -0.08889143, ..., -0.18277714,. -0.03157547, -0.06137084]]). ```. ```py. adata.layers[""analytic_pearson_residuals""] = analytic_pearson[""X""]. adata.layers[""analytic_pearson_residuals""].sum(1). ```. Output:. `array([nan, nan, nan, ..., nan, nan, nan])`. #### Versions. scanpy==1.9.3 anndata==0.9.1 umap==0.5.3 numpy==1.23.5 scipy==1.10.1 pandas==2.0.1 scikit-learn==1.2.2 statsmodels==0.13.5 python-igraph==0.10.3 pynndescent==0.5.10. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2496
https://github.com/scverse/scanpy/issues/2496:1967,modifiability,layer,layers,1967,"rocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. Hi, I'm trying to follow [Analytic Pearson residuals](https://www.sc-best-practices.org/preprocessing_visualization/normalization.html). but after getting first 2 nomalizations, I'm stucked with Pearson residuals normalization. It seems like a devided by 0 issue? I don't know. What else I can do to check if the matrix is the problem. already did gene (`min_cells=3`) and cell filtering(`min_genes=200`). ### Minimal code sample (that we can copy&paste without having any data). ```python. analytic_pearson = sc.experimental.pp.normalize_pearson_residuals(adata, inplace=False). ```. ```pytb. /home/sxykdx/miniconda3/envs/scanpy/lib/python3.10/site-packages/scanpy/experimental/pp/_normalization.py:59: RuntimeWarning: invalid value encountered in divide. residuals = diff / np.sqrt(mu + mu**2 / theta). ```. `analytic_pearson[""X""]` . Output:. ```py. array([[-0.08038502, -0.10195383, -0.24513291, ..., 1.47699586,. -0.08709449, -0.16926342],. [-0.08623174, -0.109369 , 3.53739781, ..., -0.54014355,. -0.09342913, -0.18157157],. [-0.07625086, -0.09671059, -0.2325321 , ..., -0.47777291,. 12.02085262, 6.06603257],. ...,. [-0.02799957, -0.03551299, -0.08540438, ..., -0.17560885,. -0.03033674, -0.05896328],. [-0.02840246, -0.03602399, -0.08663319, ..., -0.17813493,. -0.03077326, -0.05981169],. [-0.02914286, -0.03696307, -0.08889143, ..., -0.18277714,. -0.03157547, -0.06137084]]). ```. ```py. adata.layers[""analytic_pearson_residuals""] = analytic_pearson[""X""]. adata.layers[""analytic_pearson_residuals""].sum(1). ```. Output:. `array([nan, nan, nan, ..., nan, nan, nan])`. #### Versions. scanpy==1.9.3 anndata==0.9.1 umap==0.5.3 numpy==1.23.5 scipy==1.10.1 pandas==2.0.1 scikit-learn==1.2.2 statsmodels==0.13.5 python-igraph==0.10.3 pynndescent==0.5.10. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2496
https://github.com/scverse/scanpy/issues/2496:2077,modifiability,Version,Versions,2077,"rocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. Hi, I'm trying to follow [Analytic Pearson residuals](https://www.sc-best-practices.org/preprocessing_visualization/normalization.html). but after getting first 2 nomalizations, I'm stucked with Pearson residuals normalization. It seems like a devided by 0 issue? I don't know. What else I can do to check if the matrix is the problem. already did gene (`min_cells=3`) and cell filtering(`min_genes=200`). ### Minimal code sample (that we can copy&paste without having any data). ```python. analytic_pearson = sc.experimental.pp.normalize_pearson_residuals(adata, inplace=False). ```. ```pytb. /home/sxykdx/miniconda3/envs/scanpy/lib/python3.10/site-packages/scanpy/experimental/pp/_normalization.py:59: RuntimeWarning: invalid value encountered in divide. residuals = diff / np.sqrt(mu + mu**2 / theta). ```. `analytic_pearson[""X""]` . Output:. ```py. array([[-0.08038502, -0.10195383, -0.24513291, ..., 1.47699586,. -0.08709449, -0.16926342],. [-0.08623174, -0.109369 , 3.53739781, ..., -0.54014355,. -0.09342913, -0.18157157],. [-0.07625086, -0.09671059, -0.2325321 , ..., -0.47777291,. 12.02085262, 6.06603257],. ...,. [-0.02799957, -0.03551299, -0.08540438, ..., -0.17560885,. -0.03033674, -0.05896328],. [-0.02840246, -0.03602399, -0.08663319, ..., -0.17813493,. -0.03077326, -0.05981169],. [-0.02914286, -0.03696307, -0.08889143, ..., -0.18277714,. -0.03157547, -0.06137084]]). ```. ```py. adata.layers[""analytic_pearson_residuals""] = analytic_pearson[""X""]. adata.layers[""analytic_pearson_residuals""].sum(1). ```. Output:. `array([nan, nan, nan, ..., nan, nan, nan])`. #### Versions. scanpy==1.9.3 anndata==0.9.1 umap==0.5.3 numpy==1.23.5 scipy==1.10.1 pandas==2.0.1 scikit-learn==1.2.2 statsmodels==0.13.5 python-igraph==0.10.3 pynndescent==0.5.10. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2496
https://github.com/scverse/scanpy/issues/2496:571,reliability,pra,practices,571,"`invalid value encountered in divide` when running `pp.normalize_pearson_residuals`; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. Hi, I'm trying to follow [Analytic Pearson residuals](https://www.sc-best-practices.org/preprocessing_visualization/normalization.html). but after getting first 2 nomalizations, I'm stucked with Pearson residuals normalization. It seems like a devided by 0 issue? I don't know. What else I can do to check if the matrix is the problem. already did gene (`min_cells=3`) and cell filtering(`min_genes=200`). ### Minimal code sample (that we can copy&paste without having any data). ```python. analytic_pearson = sc.experimental.pp.normalize_pearson_residuals(adata, inplace=False). ```. ```pytb. /home/sxykdx/miniconda3/envs/scanpy/lib/python3.10/site-packages/scanpy/experimental/pp/_normalization.py:59: RuntimeWarning: invalid value encountered in divide. residuals = diff / np.sqrt(mu + mu**2 / theta). ```. `analytic_pearson[""X""]` . Output:. ```py. array([[-0.08038502, -0.10195383, -0.24513291, ..., 1.47699586,. -0.08709449, -0.16926342],. [-0.08623174, -0.109369 , 3.53739781, ..., -0.54014355,. -0.09342913, -0.18157157],. [-0.07625086, -0.09671059, -0.2325321 , ..., -0.47777291,. 12.02085262, 6.06603257],. ...,. [-0.02799957, -0.03551299, -0.08540438, ..., -0.17560885,. -0.03033674, -0.05896328],. [-0.02840246, -0.03602399, -0.08663319, ..., -0.17813493,. -0.03077326, -0.05981169],. [-0.02914286, -0.03696307, -0.08889143, ..., -0.18277714,. -0.03157547, -0.06137084]]). ```. ```py. adata.layers[""analytic_pearson_residuals""] = analytic_pearson[""X""]. adata.layers[""analytic_pearson_residual",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2496
https://github.com/scverse/scanpy/issues/2496:2281,safety,log,logging,2281,"rocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. Hi, I'm trying to follow [Analytic Pearson residuals](https://www.sc-best-practices.org/preprocessing_visualization/normalization.html). but after getting first 2 nomalizations, I'm stucked with Pearson residuals normalization. It seems like a devided by 0 issue? I don't know. What else I can do to check if the matrix is the problem. already did gene (`min_cells=3`) and cell filtering(`min_genes=200`). ### Minimal code sample (that we can copy&paste without having any data). ```python. analytic_pearson = sc.experimental.pp.normalize_pearson_residuals(adata, inplace=False). ```. ```pytb. /home/sxykdx/miniconda3/envs/scanpy/lib/python3.10/site-packages/scanpy/experimental/pp/_normalization.py:59: RuntimeWarning: invalid value encountered in divide. residuals = diff / np.sqrt(mu + mu**2 / theta). ```. `analytic_pearson[""X""]` . Output:. ```py. array([[-0.08038502, -0.10195383, -0.24513291, ..., 1.47699586,. -0.08709449, -0.16926342],. [-0.08623174, -0.109369 , 3.53739781, ..., -0.54014355,. -0.09342913, -0.18157157],. [-0.07625086, -0.09671059, -0.2325321 , ..., -0.47777291,. 12.02085262, 6.06603257],. ...,. [-0.02799957, -0.03551299, -0.08540438, ..., -0.17560885,. -0.03033674, -0.05896328],. [-0.02840246, -0.03602399, -0.08663319, ..., -0.17813493,. -0.03077326, -0.05981169],. [-0.02914286, -0.03696307, -0.08889143, ..., -0.18277714,. -0.03157547, -0.06137084]]). ```. ```py. adata.layers[""analytic_pearson_residuals""] = analytic_pearson[""X""]. adata.layers[""analytic_pearson_residuals""].sum(1). ```. Output:. `array([nan, nan, nan, ..., nan, nan, nan])`. #### Versions. scanpy==1.9.3 anndata==0.9.1 umap==0.5.3 numpy==1.23.5 scipy==1.10.1 pandas==2.0.1 scikit-learn==1.2.2 statsmodels==0.13.5 python-igraph==0.10.3 pynndescent==0.5.10. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2496
https://github.com/scverse/scanpy/issues/2496:2281,security,log,logging,2281,"rocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. Hi, I'm trying to follow [Analytic Pearson residuals](https://www.sc-best-practices.org/preprocessing_visualization/normalization.html). but after getting first 2 nomalizations, I'm stucked with Pearson residuals normalization. It seems like a devided by 0 issue? I don't know. What else I can do to check if the matrix is the problem. already did gene (`min_cells=3`) and cell filtering(`min_genes=200`). ### Minimal code sample (that we can copy&paste without having any data). ```python. analytic_pearson = sc.experimental.pp.normalize_pearson_residuals(adata, inplace=False). ```. ```pytb. /home/sxykdx/miniconda3/envs/scanpy/lib/python3.10/site-packages/scanpy/experimental/pp/_normalization.py:59: RuntimeWarning: invalid value encountered in divide. residuals = diff / np.sqrt(mu + mu**2 / theta). ```. `analytic_pearson[""X""]` . Output:. ```py. array([[-0.08038502, -0.10195383, -0.24513291, ..., 1.47699586,. -0.08709449, -0.16926342],. [-0.08623174, -0.109369 , 3.53739781, ..., -0.54014355,. -0.09342913, -0.18157157],. [-0.07625086, -0.09671059, -0.2325321 , ..., -0.47777291,. 12.02085262, 6.06603257],. ...,. [-0.02799957, -0.03551299, -0.08540438, ..., -0.17560885,. -0.03033674, -0.05896328],. [-0.02840246, -0.03602399, -0.08663319, ..., -0.17813493,. -0.03077326, -0.05981169],. [-0.02914286, -0.03696307, -0.08889143, ..., -0.18277714,. -0.03157547, -0.06137084]]). ```. ```py. adata.layers[""analytic_pearson_residuals""] = analytic_pearson[""X""]. adata.layers[""analytic_pearson_residuals""].sum(1). ```. Output:. `array([nan, nan, nan, ..., nan, nan, nan])`. #### Versions. scanpy==1.9.3 anndata==0.9.1 umap==0.5.3 numpy==1.23.5 scipy==1.10.1 pandas==2.0.1 scikit-learn==1.2.2 statsmodels==0.13.5 python-igraph==0.10.3 pynndescent==0.5.10. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2496
https://github.com/scverse/scanpy/issues/2496:2281,testability,log,logging,2281,"rocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. Hi, I'm trying to follow [Analytic Pearson residuals](https://www.sc-best-practices.org/preprocessing_visualization/normalization.html). but after getting first 2 nomalizations, I'm stucked with Pearson residuals normalization. It seems like a devided by 0 issue? I don't know. What else I can do to check if the matrix is the problem. already did gene (`min_cells=3`) and cell filtering(`min_genes=200`). ### Minimal code sample (that we can copy&paste without having any data). ```python. analytic_pearson = sc.experimental.pp.normalize_pearson_residuals(adata, inplace=False). ```. ```pytb. /home/sxykdx/miniconda3/envs/scanpy/lib/python3.10/site-packages/scanpy/experimental/pp/_normalization.py:59: RuntimeWarning: invalid value encountered in divide. residuals = diff / np.sqrt(mu + mu**2 / theta). ```. `analytic_pearson[""X""]` . Output:. ```py. array([[-0.08038502, -0.10195383, -0.24513291, ..., 1.47699586,. -0.08709449, -0.16926342],. [-0.08623174, -0.109369 , 3.53739781, ..., -0.54014355,. -0.09342913, -0.18157157],. [-0.07625086, -0.09671059, -0.2325321 , ..., -0.47777291,. 12.02085262, 6.06603257],. ...,. [-0.02799957, -0.03551299, -0.08540438, ..., -0.17560885,. -0.03033674, -0.05896328],. [-0.02840246, -0.03602399, -0.08663319, ..., -0.17813493,. -0.03077326, -0.05981169],. [-0.02914286, -0.03696307, -0.08889143, ..., -0.18277714,. -0.03157547, -0.06137084]]). ```. ```py. adata.layers[""analytic_pearson_residuals""] = analytic_pearson[""X""]. adata.layers[""analytic_pearson_residuals""].sum(1). ```. Output:. `array([nan, nan, nan, ..., nan, nan, nan])`. #### Versions. scanpy==1.9.3 anndata==0.9.1 umap==0.5.3 numpy==1.23.5 scipy==1.10.1 pandas==2.0.1 scikit-learn==1.2.2 statsmodels==0.13.5 python-igraph==0.10.3 pynndescent==0.5.10. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2496
https://github.com/scverse/scanpy/issues/2496:166,usability,confirm,confirmed,166,"`invalid value encountered in divide` when running `pp.normalize_pearson_residuals`; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. Hi, I'm trying to follow [Analytic Pearson residuals](https://www.sc-best-practices.org/preprocessing_visualization/normalization.html). but after getting first 2 nomalizations, I'm stucked with Pearson residuals normalization. It seems like a devided by 0 issue? I don't know. What else I can do to check if the matrix is the problem. already did gene (`min_cells=3`) and cell filtering(`min_genes=200`). ### Minimal code sample (that we can copy&paste without having any data). ```python. analytic_pearson = sc.experimental.pp.normalize_pearson_residuals(adata, inplace=False). ```. ```pytb. /home/sxykdx/miniconda3/envs/scanpy/lib/python3.10/site-packages/scanpy/experimental/pp/_normalization.py:59: RuntimeWarning: invalid value encountered in divide. residuals = diff / np.sqrt(mu + mu**2 / theta). ```. `analytic_pearson[""X""]` . Output:. ```py. array([[-0.08038502, -0.10195383, -0.24513291, ..., 1.47699586,. -0.08709449, -0.16926342],. [-0.08623174, -0.109369 , 3.53739781, ..., -0.54014355,. -0.09342913, -0.18157157],. [-0.07625086, -0.09671059, -0.2325321 , ..., -0.47777291,. 12.02085262, 6.06603257],. ...,. [-0.02799957, -0.03551299, -0.08540438, ..., -0.17560885,. -0.03033674, -0.05896328],. [-0.02840246, -0.03602399, -0.08663319, ..., -0.17813493,. -0.03077326, -0.05981169],. [-0.02914286, -0.03696307, -0.08889143, ..., -0.18277714,. -0.03157547, -0.06137084]]). ```. ```py. adata.layers[""analytic_pearson_residuals""] = analytic_pearson[""X""]. adata.layers[""analytic_pearson_residual",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2496
https://github.com/scverse/scanpy/issues/2496:249,usability,confirm,confirmed,249,"`invalid value encountered in divide` when running `pp.normalize_pearson_residuals`; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. Hi, I'm trying to follow [Analytic Pearson residuals](https://www.sc-best-practices.org/preprocessing_visualization/normalization.html). but after getting first 2 nomalizations, I'm stucked with Pearson residuals normalization. It seems like a devided by 0 issue? I don't know. What else I can do to check if the matrix is the problem. already did gene (`min_cells=3`) and cell filtering(`min_genes=200`). ### Minimal code sample (that we can copy&paste without having any data). ```python. analytic_pearson = sc.experimental.pp.normalize_pearson_residuals(adata, inplace=False). ```. ```pytb. /home/sxykdx/miniconda3/envs/scanpy/lib/python3.10/site-packages/scanpy/experimental/pp/_normalization.py:59: RuntimeWarning: invalid value encountered in divide. residuals = diff / np.sqrt(mu + mu**2 / theta). ```. `analytic_pearson[""X""]` . Output:. ```py. array([[-0.08038502, -0.10195383, -0.24513291, ..., 1.47699586,. -0.08709449, -0.16926342],. [-0.08623174, -0.109369 , 3.53739781, ..., -0.54014355,. -0.09342913, -0.18157157],. [-0.07625086, -0.09671059, -0.2325321 , ..., -0.47777291,. 12.02085262, 6.06603257],. ...,. [-0.02799957, -0.03551299, -0.08540438, ..., -0.17560885,. -0.03033674, -0.05896328],. [-0.02840246, -0.03602399, -0.08663319, ..., -0.17813493,. -0.03077326, -0.05981169],. [-0.02914286, -0.03696307, -0.08889143, ..., -0.18277714,. -0.03157547, -0.06137084]]). ```. ```py. adata.layers[""analytic_pearson_residuals""] = analytic_pearson[""X""]. adata.layers[""analytic_pearson_residual",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2496
https://github.com/scverse/scanpy/issues/2496:340,usability,guid,guide,340,"`invalid value encountered in divide` when running `pp.normalize_pearson_residuals`; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. Hi, I'm trying to follow [Analytic Pearson residuals](https://www.sc-best-practices.org/preprocessing_visualization/normalization.html). but after getting first 2 nomalizations, I'm stucked with Pearson residuals normalization. It seems like a devided by 0 issue? I don't know. What else I can do to check if the matrix is the problem. already did gene (`min_cells=3`) and cell filtering(`min_genes=200`). ### Minimal code sample (that we can copy&paste without having any data). ```python. analytic_pearson = sc.experimental.pp.normalize_pearson_residuals(adata, inplace=False). ```. ```pytb. /home/sxykdx/miniconda3/envs/scanpy/lib/python3.10/site-packages/scanpy/experimental/pp/_normalization.py:59: RuntimeWarning: invalid value encountered in divide. residuals = diff / np.sqrt(mu + mu**2 / theta). ```. `analytic_pearson[""X""]` . Output:. ```py. array([[-0.08038502, -0.10195383, -0.24513291, ..., 1.47699586,. -0.08709449, -0.16926342],. [-0.08623174, -0.109369 , 3.53739781, ..., -0.54014355,. -0.09342913, -0.18157157],. [-0.07625086, -0.09671059, -0.2325321 , ..., -0.47777291,. 12.02085262, 6.06603257],. ...,. [-0.02799957, -0.03551299, -0.08540438, ..., -0.17560885,. -0.03033674, -0.05896328],. [-0.02840246, -0.03602399, -0.08663319, ..., -0.17813493,. -0.03077326, -0.05981169],. [-0.02914286, -0.03696307, -0.08889143, ..., -0.18277714,. -0.03157547, -0.06137084]]). ```. ```py. adata.layers[""analytic_pearson_residuals""] = analytic_pearson[""X""]. adata.layers[""analytic_pearson_residual",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2496
https://github.com/scverse/scanpy/issues/2496:395,usability,minim,minimal-bug-reports,395,"`invalid value encountered in divide` when running `pp.normalize_pearson_residuals`; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. Hi, I'm trying to follow [Analytic Pearson residuals](https://www.sc-best-practices.org/preprocessing_visualization/normalization.html). but after getting first 2 nomalizations, I'm stucked with Pearson residuals normalization. It seems like a devided by 0 issue? I don't know. What else I can do to check if the matrix is the problem. already did gene (`min_cells=3`) and cell filtering(`min_genes=200`). ### Minimal code sample (that we can copy&paste without having any data). ```python. analytic_pearson = sc.experimental.pp.normalize_pearson_residuals(adata, inplace=False). ```. ```pytb. /home/sxykdx/miniconda3/envs/scanpy/lib/python3.10/site-packages/scanpy/experimental/pp/_normalization.py:59: RuntimeWarning: invalid value encountered in divide. residuals = diff / np.sqrt(mu + mu**2 / theta). ```. `analytic_pearson[""X""]` . Output:. ```py. array([[-0.08038502, -0.10195383, -0.24513291, ..., 1.47699586,. -0.08709449, -0.16926342],. [-0.08623174, -0.109369 , 3.53739781, ..., -0.54014355,. -0.09342913, -0.18157157],. [-0.07625086, -0.09671059, -0.2325321 , ..., -0.47777291,. 12.02085262, 6.06603257],. ...,. [-0.02799957, -0.03551299, -0.08540438, ..., -0.17560885,. -0.03033674, -0.05896328],. [-0.02840246, -0.03602399, -0.08663319, ..., -0.17813493,. -0.03077326, -0.05981169],. [-0.02914286, -0.03696307, -0.08889143, ..., -0.18277714,. -0.03157547, -0.06137084]]). ```. ```py. adata.layers[""analytic_pearson_residuals""] = analytic_pearson[""X""]. adata.layers[""analytic_pearson_residual",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2496
https://github.com/scverse/scanpy/issues/2496:907,usability,Minim,Minimal,907,"`invalid value encountered in divide` when running `pp.normalize_pearson_residuals`; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. Hi, I'm trying to follow [Analytic Pearson residuals](https://www.sc-best-practices.org/preprocessing_visualization/normalization.html). but after getting first 2 nomalizations, I'm stucked with Pearson residuals normalization. It seems like a devided by 0 issue? I don't know. What else I can do to check if the matrix is the problem. already did gene (`min_cells=3`) and cell filtering(`min_genes=200`). ### Minimal code sample (that we can copy&paste without having any data). ```python. analytic_pearson = sc.experimental.pp.normalize_pearson_residuals(adata, inplace=False). ```. ```pytb. /home/sxykdx/miniconda3/envs/scanpy/lib/python3.10/site-packages/scanpy/experimental/pp/_normalization.py:59: RuntimeWarning: invalid value encountered in divide. residuals = diff / np.sqrt(mu + mu**2 / theta). ```. `analytic_pearson[""X""]` . Output:. ```py. array([[-0.08038502, -0.10195383, -0.24513291, ..., 1.47699586,. -0.08709449, -0.16926342],. [-0.08623174, -0.109369 , 3.53739781, ..., -0.54014355,. -0.09342913, -0.18157157],. [-0.07625086, -0.09671059, -0.2325321 , ..., -0.47777291,. 12.02085262, 6.06603257],. ...,. [-0.02799957, -0.03551299, -0.08540438, ..., -0.17560885,. -0.03033674, -0.05896328],. [-0.02840246, -0.03602399, -0.08663319, ..., -0.17813493,. -0.03077326, -0.05981169],. [-0.02914286, -0.03696307, -0.08889143, ..., -0.18277714,. -0.03157547, -0.06137084]]). ```. ```py. adata.layers[""analytic_pearson_residuals""] = analytic_pearson[""X""]. adata.layers[""analytic_pearson_residual",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2496
https://github.com/scverse/scanpy/issues/2496:2177,usability,learn,learn,2177,"rocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. Hi, I'm trying to follow [Analytic Pearson residuals](https://www.sc-best-practices.org/preprocessing_visualization/normalization.html). but after getting first 2 nomalizations, I'm stucked with Pearson residuals normalization. It seems like a devided by 0 issue? I don't know. What else I can do to check if the matrix is the problem. already did gene (`min_cells=3`) and cell filtering(`min_genes=200`). ### Minimal code sample (that we can copy&paste without having any data). ```python. analytic_pearson = sc.experimental.pp.normalize_pearson_residuals(adata, inplace=False). ```. ```pytb. /home/sxykdx/miniconda3/envs/scanpy/lib/python3.10/site-packages/scanpy/experimental/pp/_normalization.py:59: RuntimeWarning: invalid value encountered in divide. residuals = diff / np.sqrt(mu + mu**2 / theta). ```. `analytic_pearson[""X""]` . Output:. ```py. array([[-0.08038502, -0.10195383, -0.24513291, ..., 1.47699586,. -0.08709449, -0.16926342],. [-0.08623174, -0.109369 , 3.53739781, ..., -0.54014355,. -0.09342913, -0.18157157],. [-0.07625086, -0.09671059, -0.2325321 , ..., -0.47777291,. 12.02085262, 6.06603257],. ...,. [-0.02799957, -0.03551299, -0.08540438, ..., -0.17560885,. -0.03033674, -0.05896328],. [-0.02840246, -0.03602399, -0.08663319, ..., -0.17813493,. -0.03077326, -0.05981169],. [-0.02914286, -0.03696307, -0.08889143, ..., -0.18277714,. -0.03157547, -0.06137084]]). ```. ```py. adata.layers[""analytic_pearson_residuals""] = analytic_pearson[""X""]. adata.layers[""analytic_pearson_residuals""].sum(1). ```. Output:. `array([nan, nan, nan, ..., nan, nan, nan])`. #### Versions. scanpy==1.9.3 anndata==0.9.1 umap==0.5.3 numpy==1.23.5 scipy==1.10.1 pandas==2.0.1 scikit-learn==1.2.2 statsmodels==0.13.5 python-igraph==0.10.3 pynndescent==0.5.10. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2496
https://github.com/scverse/scanpy/issues/2497:735,availability,error,error,735,"PMBC3k tutorial - issue loading saved object from .h5ad file; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. I am following all the steps from the PBMC3k tutorial (https://scanpy.readthedocs.io/en/latest/tutorials/pbmc3k.html). I ran the wilcoxon method for finding marker genes and saved the object to a .h5ad file, after which I reloaded the object from the file and the log1p dict appears to be empty for some reason. I checked the object before saving it to the file and it had the 'base' key whereas after loading it back it was missing, causing the error below. . ### Minimal code sample (that we can copy&paste without having any data). ```python. adata = sc.read(results_file). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). result = adata.uns['rank_genes_groups']. groups = result['names'].dtype.names. pd.DataFrame(. {group + '_' + key[:1]: result[key][group]. for group in groups for key in ['names', 'pvals']}).head(5). sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. ```pytb. KeyError Traceback (most recent call last). Input In [57], in <cell line: 1>(). ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _Ra",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:183,deployability,version,version,183,"PMBC3k tutorial - issue loading saved object from .h5ad file; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. I am following all the steps from the PBMC3k tutorial (https://scanpy.readthedocs.io/en/latest/tutorials/pbmc3k.html). I ran the wilcoxon method for finding marker genes and saved the object to a .h5ad file, after which I reloaded the object from the file and the log1p dict appears to be empty for some reason. I checked the object before saving it to the file and it had the 'base' key whereas after loading it back it was missing, causing the error below. . ### Minimal code sample (that we can copy&paste without having any data). ```python. adata = sc.read(results_file). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). result = adata.uns['rank_genes_groups']. groups = result['names'].dtype.names. pd.DataFrame(. {group + '_' + key[:1]: result[key][group]. for group in groups for key in ['names', 'pvals']}).head(5). sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. ```pytb. KeyError Traceback (most recent call last). Input In [57], in <cell line: 1>(). ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _Ra",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:1570,deployability,Version,Versions,1570," be empty for some reason. I checked the object before saving it to the file and it had the 'base' key whereas after loading it back it was missing, causing the error below. . ### Minimal code sample (that we can copy&paste without having any data). ```python. adata = sc.read(results_file). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). result = adata.uns['rank_genes_groups']. groups = result['names'].dtype.names. pd.DataFrame(. {group + '_' + key[:1]: result[key][group]. for group in groups for key in ['names', 'pvals']}).head(5). sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. ```pytb. KeyError Traceback (most recent call last). Input In [57], in <cell line: 1>(). ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:2132,deployability,log,logreg,2132,"ank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. ```pytb. KeyError Traceback (most recent call last). Input In [57], in <cell line: 1>(). ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.2.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.0. decorator 5.1.1. defusedxml 0.7.1. entrypoints 0.4. executing 0.8.3. h5py 3.7.0. hypergeom_ufunc NA. igra",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:2146,deployability,log,logg,2146,"ups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. ```pytb. KeyError Traceback (most recent call last). Input In [57], in <cell line: 1>(). ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.2.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.0. decorator 5.1.1. defusedxml 0.7.1. entrypoints 0.4. executing 0.8.3. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.9.11. ip",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:2239,deployability,log,logarithmize,2239,"ta, groups=['0'], n_genes=20). ```. ```pytb. KeyError Traceback (most recent call last). Input In [57], in <cell line: 1>(). ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.2.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.0. decorator 5.1.1. defusedxml 0.7.1. entrypoints 0.4. executing 0.8.3. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.9.11. ipykernel 6.13.0. ipython_genutils 0.2.0. ipywidgets 8.0.2. jedi 0.18.1. joblib 1.2.0. kiwisolver 1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:2347,deployability,Version,Versions,2347," <cell line: 1>(). ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.2.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.0. decorator 5.1.1. defusedxml 0.7.1. entrypoints 0.4. executing 0.8.3. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.9.11. ipykernel 6.13.0. ipython_genutils 0.2.0. ipywidgets 8.0.2. jedi 0.18.1. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.8.10. llvmlite 0.39.1. louvain 0.7.1. matplotlib 3.6.0. matplotlib_inline NA. mpl_toolki",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:2724,deployability,log,log,2724," rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.2.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.0. decorator 5.1.1. defusedxml 0.7.1. entrypoints 0.4. executing 0.8.3. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.9.11. ipykernel 6.13.0. ipython_genutils 0.2.0. ipywidgets 8.0.2. jedi 0.18.1. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.8.10. llvmlite 0.39.1. louvain 0.7.1. matplotlib 3.6.0. matplotlib_inline NA. mpl_toolkits NA. natsort 8.2.0. nbinom_ufunc NA. ncf_ufunc NA. numba 0.56.2. numpy 1.23.0. packaging 21.3. pandas 1.4.3. parso 0.8.3. patsy 0.5.2. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.29. psutil 5.9.0. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.8.0. pydevd_file_utils NA. pydevd_plugins NA. pydevd_t",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:2796,deployability,Version,Versions,2796,", **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.2.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.0. decorator 5.1.1. defusedxml 0.7.1. entrypoints 0.4. executing 0.8.3. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.9.11. ipykernel 6.13.0. ipython_genutils 0.2.0. ipywidgets 8.0.2. jedi 0.18.1. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.8.10. llvmlite 0.39.1. louvain 0.7.1. matplotlib 3.6.0. matplotlib_inline NA. mpl_toolkits NA. natsort 8.2.0. nbinom_ufunc NA. ncf_ufunc NA. numba 0.56.2. numpy 1.23.0. packaging 21.3. pandas 1.4.3. parso 0.8.3. patsy 0.5.2. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.29. psutil 5.9.0. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.8.0. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.11.2. pynndescent 0.5.7. pyparsing 3.0.8. pytz 2022.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:4324,deployability,updat,updated,4324,"_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.2.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.0. decorator 5.1.1. defusedxml 0.7.1. entrypoints 0.4. executing 0.8.3. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.9.11. ipykernel 6.13.0. ipython_genutils 0.2.0. ipywidgets 8.0.2. jedi 0.18.1. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.8.10. llvmlite 0.39.1. louvain 0.7.1. matplotlib 3.6.0. matplotlib_inline NA. mpl_toolkits NA. natsort 8.2.0. nbinom_ufunc NA. ncf_ufunc NA. numba 0.56.2. numpy 1.23.0. packaging 21.3. pandas 1.4.3. parso 0.8.3. patsy 0.5.2. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.29. psutil 5.9.0. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.8.0. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.11.2. pynndescent 0.5.7. pyparsing 3.0.8. pytz 2022.1. ruamel NA. scipy 1.9.1. seaborn 0.12.0. session_info 1.0.0. six 1.16.0. sklearn 1.1.2. stack_data 0.2.0. statsmodels 0.13.2. texttable 1.6.4. threadpoolctl 3.1.0. tornado 6.1. tqdm 4.64.1. traitlets 5.1.1. typing_extensions NA. umap 0.5.3. wcwidth 0.2.5. yaml 5.3.1. zmq 22.3.0. -----. IPython 8.2.0. jupyter_client 7.2.2. jupyter_core 4.9.2. notebook 6.4.10. -----. Python 3.10.2 (v3.10.2:a58ebcc701, Jan 13 2022, 14:50:16) [Clang 13.0.0 (clang-1300.0.29.30)]. macOS-13.3.1-x86_64-i386-64bit. -----. Session information updated at 2023-05-30 21:48. ```. </details>. Any help would be appreciated! Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:24,energy efficiency,load,loading,24,"PMBC3k tutorial - issue loading saved object from .h5ad file; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. I am following all the steps from the PBMC3k tutorial (https://scanpy.readthedocs.io/en/latest/tutorials/pbmc3k.html). I ran the wilcoxon method for finding marker genes and saved the object to a .h5ad file, after which I reloaded the object from the file and the log1p dict appears to be empty for some reason. I checked the object before saving it to the file and it had the 'base' key whereas after loading it back it was missing, causing the error below. . ### Minimal code sample (that we can copy&paste without having any data). ```python. adata = sc.read(results_file). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). result = adata.uns['rank_genes_groups']. groups = result['names'].dtype.names. pd.DataFrame(. {group + '_' + key[:1]: result[key][group]. for group in groups for key in ['names', 'pvals']}).head(5). sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. ```pytb. KeyError Traceback (most recent call last). Input In [57], in <cell line: 1>(). ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _Ra",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:691,energy efficiency,load,loading,691,"PMBC3k tutorial - issue loading saved object from .h5ad file; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. I am following all the steps from the PBMC3k tutorial (https://scanpy.readthedocs.io/en/latest/tutorials/pbmc3k.html). I ran the wilcoxon method for finding marker genes and saved the object to a .h5ad file, after which I reloaded the object from the file and the log1p dict appears to be empty for some reason. I checked the object before saving it to the file and it had the 'base' key whereas after loading it back it was missing, causing the error below. . ### Minimal code sample (that we can copy&paste without having any data). ```python. adata = sc.read(results_file). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). result = adata.uns['rank_genes_groups']. groups = result['names'].dtype.names. pd.DataFrame(. {group + '_' + key[:1]: result[key][group]. for group in groups for key in ['names', 'pvals']}).head(5). sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. ```pytb. KeyError Traceback (most recent call last). Input In [57], in <cell line: 1>(). ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _Ra",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:183,integrability,version,version,183,"PMBC3k tutorial - issue loading saved object from .h5ad file; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. I am following all the steps from the PBMC3k tutorial (https://scanpy.readthedocs.io/en/latest/tutorials/pbmc3k.html). I ran the wilcoxon method for finding marker genes and saved the object to a .h5ad file, after which I reloaded the object from the file and the log1p dict appears to be empty for some reason. I checked the object before saving it to the file and it had the 'base' key whereas after loading it back it was missing, causing the error below. . ### Minimal code sample (that we can copy&paste without having any data). ```python. adata = sc.read(results_file). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). result = adata.uns['rank_genes_groups']. groups = result['names'].dtype.names. pd.DataFrame(. {group + '_' + key[:1]: result[key][group]. for group in groups for key in ['names', 'pvals']}).head(5). sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. ```pytb. KeyError Traceback (most recent call last). Input In [57], in <cell line: 1>(). ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _Ra",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:1570,integrability,Version,Versions,1570," be empty for some reason. I checked the object before saving it to the file and it had the 'base' key whereas after loading it back it was missing, causing the error below. . ### Minimal code sample (that we can copy&paste without having any data). ```python. adata = sc.read(results_file). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). result = adata.uns['rank_genes_groups']. groups = result['names'].dtype.names. pd.DataFrame(. {group + '_' + key[:1]: result[key][group]. for group in groups for key in ['names', 'pvals']}).head(5). sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. ```pytb. KeyError Traceback (most recent call last). Input In [57], in <cell line: 1>(). ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:2347,integrability,Version,Versions,2347," <cell line: 1>(). ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.2.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.0. decorator 5.1.1. defusedxml 0.7.1. entrypoints 0.4. executing 0.8.3. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.9.11. ipykernel 6.13.0. ipython_genutils 0.2.0. ipywidgets 8.0.2. jedi 0.18.1. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.8.10. llvmlite 0.39.1. louvain 0.7.1. matplotlib 3.6.0. matplotlib_inline NA. mpl_toolki",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:2796,integrability,Version,Versions,2796,", **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.2.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.0. decorator 5.1.1. defusedxml 0.7.1. entrypoints 0.4. executing 0.8.3. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.9.11. ipykernel 6.13.0. ipython_genutils 0.2.0. ipywidgets 8.0.2. jedi 0.18.1. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.8.10. llvmlite 0.39.1. louvain 0.7.1. matplotlib 3.6.0. matplotlib_inline NA. mpl_toolkits NA. natsort 8.2.0. nbinom_ufunc NA. ncf_ufunc NA. numba 0.56.2. numpy 1.23.0. packaging 21.3. pandas 1.4.3. parso 0.8.3. patsy 0.5.2. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.29. psutil 5.9.0. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.8.0. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.11.2. pynndescent 0.5.7. pyparsing 3.0.8. pytz 2022.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:183,modifiability,version,version,183,"PMBC3k tutorial - issue loading saved object from .h5ad file; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. I am following all the steps from the PBMC3k tutorial (https://scanpy.readthedocs.io/en/latest/tutorials/pbmc3k.html). I ran the wilcoxon method for finding marker genes and saved the object to a .h5ad file, after which I reloaded the object from the file and the log1p dict appears to be empty for some reason. I checked the object before saving it to the file and it had the 'base' key whereas after loading it back it was missing, causing the error below. . ### Minimal code sample (that we can copy&paste without having any data). ```python. adata = sc.read(results_file). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). result = adata.uns['rank_genes_groups']. groups = result['names'].dtype.names. pd.DataFrame(. {group + '_' + key[:1]: result[key][group]. for group in groups for key in ['names', 'pvals']}).head(5). sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. ```pytb. KeyError Traceback (most recent call last). Input In [57], in <cell line: 1>(). ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _Ra",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:1570,modifiability,Version,Versions,1570," be empty for some reason. I checked the object before saving it to the file and it had the 'base' key whereas after loading it back it was missing, causing the error below. . ### Minimal code sample (that we can copy&paste without having any data). ```python. adata = sc.read(results_file). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). result = adata.uns['rank_genes_groups']. groups = result['names'].dtype.names. pd.DataFrame(. {group + '_' + key[:1]: result[key][group]. for group in groups for key in ['names', 'pvals']}).head(5). sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. ```pytb. KeyError Traceback (most recent call last). Input In [57], in <cell line: 1>(). ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:1604,modifiability,pac,packages,1604,"ed the object before saving it to the file and it had the 'base' key whereas after loading it back it was missing, causing the error below. . ### Minimal code sample (that we can copy&paste without having any data). ```python. adata = sc.read(results_file). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). result = adata.uns['rank_genes_groups']. groups = result['names'].dtype.names. pd.DataFrame(. {group + '_' + key[:1]: result[key][group]. for group in groups for key in ['names', 'pvals']}).head(5). sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. ```pytb. KeyError Traceback (most recent call last). Input In [57], in <cell line: 1>(). ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:1795,modifiability,layer,layer,1795,"without having any data). ```python. adata = sc.read(results_file). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). result = adata.uns['rank_genes_groups']. groups = result['names'].dtype.names. pd.DataFrame(. {group + '_' + key[:1]: result[key][group]. for group in groups for key in ['names', 'pvals']}).head(5). sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. ```pytb. KeyError Traceback (most recent call last). Input In [57], in <cell line: 1>(). ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. #### Ve",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:2058,modifiability,layer,layer,2058,"oup]. for group in groups for key in ['names', 'pvals']}).head(5). sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. ```pytb. KeyError Traceback (most recent call last). Input In [57], in <cell line: 1>(). ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.2.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.0. decorator 5.1.1. defusedxml 0.7",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:2347,modifiability,Version,Versions,2347," <cell line: 1>(). ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.2.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.0. decorator 5.1.1. defusedxml 0.7.1. entrypoints 0.4. executing 0.8.3. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.9.11. ipykernel 6.13.0. ipython_genutils 0.2.0. ipywidgets 8.0.2. jedi 0.18.1. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.8.10. llvmlite 0.39.1. louvain 0.7.1. matplotlib 3.6.0. matplotlib_inline NA. mpl_toolki",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:2381,modifiability,pac,packages,2381,"ank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.2.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.0. decorator 5.1.1. defusedxml 0.7.1. entrypoints 0.4. executing 0.8.3. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.9.11. ipykernel 6.13.0. ipython_genutils 0.2.0. ipywidgets 8.0.2. jedi 0.18.1. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.8.10. llvmlite 0.39.1. louvain 0.7.1. matplotlib 3.6.0. matplotlib_inline NA. mpl_toolkits NA. natsort 8.2.0. nbinom_ufunc",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:2502,modifiability,layer,layer,2502,"ps=['0'], n_genes=20). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.2.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.0. decorator 5.1.1. defusedxml 0.7.1. entrypoints 0.4. executing 0.8.3. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.9.11. ipykernel 6.13.0. ipython_genutils 0.2.0. ipywidgets 8.0.2. jedi 0.18.1. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.8.10. llvmlite 0.39.1. louvain 0.7.1. matplotlib 3.6.0. matplotlib_inline NA. mpl_toolkits NA. natsort 8.2.0. nbinom_ufunc NA. ncf_ufunc NA. numba 0.56.2. numpy 1.23.0. packaging 21.3. pandas 1.4.3. parso 0.8.3. patsy 0.5.2. pexpect 4.8.0. pi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:2796,modifiability,Version,Versions,2796,", **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.2.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.0. decorator 5.1.1. defusedxml 0.7.1. entrypoints 0.4. executing 0.8.3. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.9.11. ipykernel 6.13.0. ipython_genutils 0.2.0. ipywidgets 8.0.2. jedi 0.18.1. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.8.10. llvmlite 0.39.1. louvain 0.7.1. matplotlib 3.6.0. matplotlib_inline NA. mpl_toolkits NA. natsort 8.2.0. nbinom_ufunc NA. ncf_ufunc NA. numba 0.56.2. numpy 1.23.0. packaging 21.3. pandas 1.4.3. parso 0.8.3. patsy 0.5.2. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.29. psutil 5.9.0. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.8.0. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.11.2. pynndescent 0.5.7. pyparsing 3.0.8. pytz 2022.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:3030,modifiability,deco,decorator,3030,"y, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.2.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.0. decorator 5.1.1. defusedxml 0.7.1. entrypoints 0.4. executing 0.8.3. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.9.11. ipykernel 6.13.0. ipython_genutils 0.2.0. ipywidgets 8.0.2. jedi 0.18.1. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.8.10. llvmlite 0.39.1. louvain 0.7.1. matplotlib 3.6.0. matplotlib_inline NA. mpl_toolkits NA. natsort 8.2.0. nbinom_ufunc NA. ncf_ufunc NA. numba 0.56.2. numpy 1.23.0. packaging 21.3. pandas 1.4.3. parso 0.8.3. patsy 0.5.2. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.29. psutil 5.9.0. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.8.0. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.11.2. pynndescent 0.5.7. pyparsing 3.0.8. pytz 2022.1. ruamel NA. scipy 1.9.1. seaborn 0.12.0. session_info 1.0.0. six 1.16.0. sklearn 1.1.2. stack_data 0.2.0. statsmodels 0.13.2. texttable 1.6.4. threadpoolctl 3.1.0. tornado 6.1. tqdm 4.64.1. traitlets 5.1.1. typing_extensions NA. umap",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:3432,modifiability,pac,packaging,3432,"_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.2.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.0. decorator 5.1.1. defusedxml 0.7.1. entrypoints 0.4. executing 0.8.3. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.9.11. ipykernel 6.13.0. ipython_genutils 0.2.0. ipywidgets 8.0.2. jedi 0.18.1. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.8.10. llvmlite 0.39.1. louvain 0.7.1. matplotlib 3.6.0. matplotlib_inline NA. mpl_toolkits NA. natsort 8.2.0. nbinom_ufunc NA. ncf_ufunc NA. numba 0.56.2. numpy 1.23.0. packaging 21.3. pandas 1.4.3. parso 0.8.3. patsy 0.5.2. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.29. psutil 5.9.0. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.8.0. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.11.2. pynndescent 0.5.7. pyparsing 3.0.8. pytz 2022.1. ruamel NA. scipy 1.9.1. seaborn 0.12.0. session_info 1.0.0. six 1.16.0. sklearn 1.1.2. stack_data 0.2.0. statsmodels 0.13.2. texttable 1.6.4. threadpoolctl 3.1.0. tornado 6.1. tqdm 4.64.1. traitlets 5.1.1. typing_extensions NA. umap 0.5.3. wcwidth 0.2.5. yaml 5.3.1. zmq 22.3.0. -----. IPython 8.2.0. jupyter_client 7.2.2. jupyter_core 4.9.2. notebook 6.4.10. -----. Python 3.10.2 (v3.10.2:a58ebcc701, Jan 13 2022, 14:50:16) [Clang 13.0.0 (clang-1300.0.29.30)]. macOS-13.3.1-x86_64-i386-64bit. -----. Session information updated at 2023-05-30 21:48. ```. </details>. Any help would be appreciated! Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:24,performance,load,loading,24,"PMBC3k tutorial - issue loading saved object from .h5ad file; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. I am following all the steps from the PBMC3k tutorial (https://scanpy.readthedocs.io/en/latest/tutorials/pbmc3k.html). I ran the wilcoxon method for finding marker genes and saved the object to a .h5ad file, after which I reloaded the object from the file and the log1p dict appears to be empty for some reason. I checked the object before saving it to the file and it had the 'base' key whereas after loading it back it was missing, causing the error below. . ### Minimal code sample (that we can copy&paste without having any data). ```python. adata = sc.read(results_file). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). result = adata.uns['rank_genes_groups']. groups = result['names'].dtype.names. pd.DataFrame(. {group + '_' + key[:1]: result[key][group]. for group in groups for key in ['names', 'pvals']}).head(5). sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. ```pytb. KeyError Traceback (most recent call last). Input In [57], in <cell line: 1>(). ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _Ra",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:691,performance,load,loading,691,"PMBC3k tutorial - issue loading saved object from .h5ad file; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. I am following all the steps from the PBMC3k tutorial (https://scanpy.readthedocs.io/en/latest/tutorials/pbmc3k.html). I ran the wilcoxon method for finding marker genes and saved the object to a .h5ad file, after which I reloaded the object from the file and the log1p dict appears to be empty for some reason. I checked the object before saving it to the file and it had the 'base' key whereas after loading it back it was missing, causing the error below. . ### Minimal code sample (that we can copy&paste without having any data). ```python. adata = sc.read(results_file). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). result = adata.uns['rank_genes_groups']. groups = result['names'].dtype.names. pd.DataFrame(. {group + '_' + key[:1]: result[key][group]. for group in groups for key in ['names', 'pvals']}).head(5). sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. ```pytb. KeyError Traceback (most recent call last). Input In [57], in <cell line: 1>(). ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _Ra",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:735,performance,error,error,735,"PMBC3k tutorial - issue loading saved object from .h5ad file; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. I am following all the steps from the PBMC3k tutorial (https://scanpy.readthedocs.io/en/latest/tutorials/pbmc3k.html). I ran the wilcoxon method for finding marker genes and saved the object to a .h5ad file, after which I reloaded the object from the file and the log1p dict appears to be empty for some reason. I checked the object before saving it to the file and it had the 'base' key whereas after loading it back it was missing, causing the error below. . ### Minimal code sample (that we can copy&paste without having any data). ```python. adata = sc.read(results_file). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). result = adata.uns['rank_genes_groups']. groups = result['names'].dtype.names. pd.DataFrame(. {group + '_' + key[:1]: result[key][group]. for group in groups for key in ['names', 'pvals']}).head(5). sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. ```pytb. KeyError Traceback (most recent call last). Input In [57], in <cell line: 1>(). ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _Ra",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:735,safety,error,error,735,"PMBC3k tutorial - issue loading saved object from .h5ad file; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. I am following all the steps from the PBMC3k tutorial (https://scanpy.readthedocs.io/en/latest/tutorials/pbmc3k.html). I ran the wilcoxon method for finding marker genes and saved the object to a .h5ad file, after which I reloaded the object from the file and the log1p dict appears to be empty for some reason. I checked the object before saving it to the file and it had the 'base' key whereas after loading it back it was missing, causing the error below. . ### Minimal code sample (that we can copy&paste without having any data). ```python. adata = sc.read(results_file). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). result = adata.uns['rank_genes_groups']. groups = result['names'].dtype.names. pd.DataFrame(. {group + '_' + key[:1]: result[key][group]. for group in groups for key in ['names', 'pvals']}).head(5). sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. ```pytb. KeyError Traceback (most recent call last). Input In [57], in <cell line: 1>(). ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _Ra",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:1334,safety,Input,Input,1334,"orial (https://scanpy.readthedocs.io/en/latest/tutorials/pbmc3k.html). I ran the wilcoxon method for finding marker genes and saved the object to a .h5ad file, after which I reloaded the object from the file and the log1p dict appears to be empty for some reason. I checked the object before saving it to the file and it had the 'base' key whereas after loading it back it was missing, causing the error below. . ### Minimal code sample (that we can copy&paste without having any data). ```python. adata = sc.read(results_file). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). result = adata.uns['rank_genes_groups']. groups = result['names'].dtype.names. pd.DataFrame(. {group + '_' + key[:1]: result[key][group]. for group in groups for key in ['names', 'pvals']}).head(5). sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. ```pytb. KeyError Traceback (most recent call last). Input In [57], in <cell line: 1>(). ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File /Library/Frameworks/Python.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:2132,safety,log,logreg,2132,"ank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. ```pytb. KeyError Traceback (most recent call last). Input In [57], in <cell line: 1>(). ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.2.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.0. decorator 5.1.1. defusedxml 0.7.1. entrypoints 0.4. executing 0.8.3. h5py 3.7.0. hypergeom_ufunc NA. igra",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:2146,safety,log,logg,2146,"ups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. ```pytb. KeyError Traceback (most recent call last). Input In [57], in <cell line: 1>(). ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.2.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.0. decorator 5.1.1. defusedxml 0.7.1. entrypoints 0.4. executing 0.8.3. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.9.11. ip",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:2239,safety,log,logarithmize,2239,"ta, groups=['0'], n_genes=20). ```. ```pytb. KeyError Traceback (most recent call last). Input In [57], in <cell line: 1>(). ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.2.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.0. decorator 5.1.1. defusedxml 0.7.1. entrypoints 0.4. executing 0.8.3. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.9.11. ipykernel 6.13.0. ipython_genutils 0.2.0. ipywidgets 8.0.2. jedi 0.18.1. joblib 1.2.0. kiwisolver 1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:2724,safety,log,log,2724," rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.2.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.0. decorator 5.1.1. defusedxml 0.7.1. entrypoints 0.4. executing 0.8.3. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.9.11. ipykernel 6.13.0. ipython_genutils 0.2.0. ipywidgets 8.0.2. jedi 0.18.1. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.8.10. llvmlite 0.39.1. louvain 0.7.1. matplotlib 3.6.0. matplotlib_inline NA. mpl_toolkits NA. natsort 8.2.0. nbinom_ufunc NA. ncf_ufunc NA. numba 0.56.2. numpy 1.23.0. packaging 21.3. pandas 1.4.3. parso 0.8.3. patsy 0.5.2. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.29. psutil 5.9.0. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.8.0. pydevd_file_utils NA. pydevd_plugins NA. pydevd_t",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:4324,safety,updat,updated,4324,"_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.2.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.0. decorator 5.1.1. defusedxml 0.7.1. entrypoints 0.4. executing 0.8.3. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.9.11. ipykernel 6.13.0. ipython_genutils 0.2.0. ipywidgets 8.0.2. jedi 0.18.1. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.8.10. llvmlite 0.39.1. louvain 0.7.1. matplotlib 3.6.0. matplotlib_inline NA. mpl_toolkits NA. natsort 8.2.0. nbinom_ufunc NA. ncf_ufunc NA. numba 0.56.2. numpy 1.23.0. packaging 21.3. pandas 1.4.3. parso 0.8.3. patsy 0.5.2. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.29. psutil 5.9.0. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.8.0. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.11.2. pynndescent 0.5.7. pyparsing 3.0.8. pytz 2022.1. ruamel NA. scipy 1.9.1. seaborn 0.12.0. session_info 1.0.0. six 1.16.0. sklearn 1.1.2. stack_data 0.2.0. statsmodels 0.13.2. texttable 1.6.4. threadpoolctl 3.1.0. tornado 6.1. tqdm 4.64.1. traitlets 5.1.1. typing_extensions NA. umap 0.5.3. wcwidth 0.2.5. yaml 5.3.1. zmq 22.3.0. -----. IPython 8.2.0. jupyter_client 7.2.2. jupyter_core 4.9.2. notebook 6.4.10. -----. Python 3.10.2 (v3.10.2:a58ebcc701, Jan 13 2022, 14:50:16) [Clang 13.0.0 (clang-1300.0.29.30)]. macOS-13.3.1-x86_64-i386-64bit. -----. Session information updated at 2023-05-30 21:48. ```. </details>. Any help would be appreciated! Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:2132,security,log,logreg,2132,"ank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. ```pytb. KeyError Traceback (most recent call last). Input In [57], in <cell line: 1>(). ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.2.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.0. decorator 5.1.1. defusedxml 0.7.1. entrypoints 0.4. executing 0.8.3. h5py 3.7.0. hypergeom_ufunc NA. igra",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:2146,security,log,logg,2146,"ups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. ```pytb. KeyError Traceback (most recent call last). Input In [57], in <cell line: 1>(). ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.2.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.0. decorator 5.1.1. defusedxml 0.7.1. entrypoints 0.4. executing 0.8.3. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.9.11. ip",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:2239,security,log,logarithmize,2239,"ta, groups=['0'], n_genes=20). ```. ```pytb. KeyError Traceback (most recent call last). Input In [57], in <cell line: 1>(). ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.2.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.0. decorator 5.1.1. defusedxml 0.7.1. entrypoints 0.4. executing 0.8.3. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.9.11. ipykernel 6.13.0. ipython_genutils 0.2.0. ipywidgets 8.0.2. jedi 0.18.1. joblib 1.2.0. kiwisolver 1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:2724,security,log,log,2724," rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.2.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.0. decorator 5.1.1. defusedxml 0.7.1. entrypoints 0.4. executing 0.8.3. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.9.11. ipykernel 6.13.0. ipython_genutils 0.2.0. ipywidgets 8.0.2. jedi 0.18.1. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.8.10. llvmlite 0.39.1. louvain 0.7.1. matplotlib 3.6.0. matplotlib_inline NA. mpl_toolkits NA. natsort 8.2.0. nbinom_ufunc NA. ncf_ufunc NA. numba 0.56.2. numpy 1.23.0. packaging 21.3. pandas 1.4.3. parso 0.8.3. patsy 0.5.2. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.29. psutil 5.9.0. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.8.0. pydevd_file_utils NA. pydevd_plugins NA. pydevd_t",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:4304,security,Session,Session,4304,"_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.2.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.0. decorator 5.1.1. defusedxml 0.7.1. entrypoints 0.4. executing 0.8.3. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.9.11. ipykernel 6.13.0. ipython_genutils 0.2.0. ipywidgets 8.0.2. jedi 0.18.1. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.8.10. llvmlite 0.39.1. louvain 0.7.1. matplotlib 3.6.0. matplotlib_inline NA. mpl_toolkits NA. natsort 8.2.0. nbinom_ufunc NA. ncf_ufunc NA. numba 0.56.2. numpy 1.23.0. packaging 21.3. pandas 1.4.3. parso 0.8.3. patsy 0.5.2. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.29. psutil 5.9.0. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.8.0. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.11.2. pynndescent 0.5.7. pyparsing 3.0.8. pytz 2022.1. ruamel NA. scipy 1.9.1. seaborn 0.12.0. session_info 1.0.0. six 1.16.0. sklearn 1.1.2. stack_data 0.2.0. statsmodels 0.13.2. texttable 1.6.4. threadpoolctl 3.1.0. tornado 6.1. tqdm 4.64.1. traitlets 5.1.1. typing_extensions NA. umap 0.5.3. wcwidth 0.2.5. yaml 5.3.1. zmq 22.3.0. -----. IPython 8.2.0. jupyter_client 7.2.2. jupyter_core 4.9.2. notebook 6.4.10. -----. Python 3.10.2 (v3.10.2:a58ebcc701, Jan 13 2022, 14:50:16) [Clang 13.0.0 (clang-1300.0.29.30)]. macOS-13.3.1-x86_64-i386-64bit. -----. Session information updated at 2023-05-30 21:48. ```. </details>. Any help would be appreciated! Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:4324,security,updat,updated,4324,"_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.2.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.0. decorator 5.1.1. defusedxml 0.7.1. entrypoints 0.4. executing 0.8.3. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.9.11. ipykernel 6.13.0. ipython_genutils 0.2.0. ipywidgets 8.0.2. jedi 0.18.1. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.8.10. llvmlite 0.39.1. louvain 0.7.1. matplotlib 3.6.0. matplotlib_inline NA. mpl_toolkits NA. natsort 8.2.0. nbinom_ufunc NA. ncf_ufunc NA. numba 0.56.2. numpy 1.23.0. packaging 21.3. pandas 1.4.3. parso 0.8.3. patsy 0.5.2. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.29. psutil 5.9.0. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.8.0. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.11.2. pynndescent 0.5.7. pyparsing 3.0.8. pytz 2022.1. ruamel NA. scipy 1.9.1. seaborn 0.12.0. session_info 1.0.0. six 1.16.0. sklearn 1.1.2. stack_data 0.2.0. statsmodels 0.13.2. texttable 1.6.4. threadpoolctl 3.1.0. tornado 6.1. tqdm 4.64.1. traitlets 5.1.1. typing_extensions NA. umap 0.5.3. wcwidth 0.2.5. yaml 5.3.1. zmq 22.3.0. -----. IPython 8.2.0. jupyter_client 7.2.2. jupyter_core 4.9.2. notebook 6.4.10. -----. Python 3.10.2 (v3.10.2:a58ebcc701, Jan 13 2022, 14:50:16) [Clang 13.0.0 (clang-1300.0.29.30)]. macOS-13.3.1-x86_64-i386-64bit. -----. Session information updated at 2023-05-30 21:48. ```. </details>. Any help would be appreciated! Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:1299,testability,Trace,Traceback,1299,"all the steps from the PBMC3k tutorial (https://scanpy.readthedocs.io/en/latest/tutorials/pbmc3k.html). I ran the wilcoxon method for finding marker genes and saved the object to a .h5ad file, after which I reloaded the object from the file and the log1p dict appears to be empty for some reason. I checked the object before saving it to the file and it had the 'base' key whereas after loading it back it was missing, causing the error below. . ### Minimal code sample (that we can copy&paste without having any data). ```python. adata = sc.read(results_file). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). result = adata.uns['rank_genes_groups']. groups = result['names'].dtype.names. pd.DataFrame(. {group + '_' + key[:1]: result[key][group]. for group in groups for key in ['names', 'pvals']}).head(5). sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. ```pytb. KeyError Traceback (most recent call last). Input In [57], in <cell line: 1>(). ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:2132,testability,log,logreg,2132,"ank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. ```pytb. KeyError Traceback (most recent call last). Input In [57], in <cell line: 1>(). ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.2.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.0. decorator 5.1.1. defusedxml 0.7.1. entrypoints 0.4. executing 0.8.3. h5py 3.7.0. hypergeom_ufunc NA. igra",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:2146,testability,log,logg,2146,"ups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. ```pytb. KeyError Traceback (most recent call last). Input In [57], in <cell line: 1>(). ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.2.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.0. decorator 5.1.1. defusedxml 0.7.1. entrypoints 0.4. executing 0.8.3. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.9.11. ip",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:2239,testability,log,logarithmize,2239,"ta, groups=['0'], n_genes=20). ```. ```pytb. KeyError Traceback (most recent call last). Input In [57], in <cell line: 1>(). ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.2.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.0. decorator 5.1.1. defusedxml 0.7.1. entrypoints 0.4. executing 0.8.3. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.9.11. ipykernel 6.13.0. ipython_genutils 0.2.0. ipywidgets 8.0.2. jedi 0.18.1. joblib 1.2.0. kiwisolver 1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:2724,testability,log,log,2724," rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.2.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.0. decorator 5.1.1. defusedxml 0.7.1. entrypoints 0.4. executing 0.8.3. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.9.11. ipykernel 6.13.0. ipython_genutils 0.2.0. ipywidgets 8.0.2. jedi 0.18.1. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.8.10. llvmlite 0.39.1. louvain 0.7.1. matplotlib 3.6.0. matplotlib_inline NA. mpl_toolkits NA. natsort 8.2.0. nbinom_ufunc NA. ncf_ufunc NA. numba 0.56.2. numpy 1.23.0. packaging 21.3. pandas 1.4.3. parso 0.8.3. patsy 0.5.2. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.29. psutil 5.9.0. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.8.0. pydevd_file_utils NA. pydevd_plugins NA. pydevd_t",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:143,usability,confirm,confirmed,143,"PMBC3k tutorial - issue loading saved object from .h5ad file; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. I am following all the steps from the PBMC3k tutorial (https://scanpy.readthedocs.io/en/latest/tutorials/pbmc3k.html). I ran the wilcoxon method for finding marker genes and saved the object to a .h5ad file, after which I reloaded the object from the file and the log1p dict appears to be empty for some reason. I checked the object before saving it to the file and it had the 'base' key whereas after loading it back it was missing, causing the error below. . ### Minimal code sample (that we can copy&paste without having any data). ```python. adata = sc.read(results_file). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). result = adata.uns['rank_genes_groups']. groups = result['names'].dtype.names. pd.DataFrame(. {group + '_' + key[:1]: result[key][group]. for group in groups for key in ['names', 'pvals']}).head(5). sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. ```pytb. KeyError Traceback (most recent call last). Input In [57], in <cell line: 1>(). ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _Ra",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:226,usability,confirm,confirmed,226,"PMBC3k tutorial - issue loading saved object from .h5ad file; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. I am following all the steps from the PBMC3k tutorial (https://scanpy.readthedocs.io/en/latest/tutorials/pbmc3k.html). I ran the wilcoxon method for finding marker genes and saved the object to a .h5ad file, after which I reloaded the object from the file and the log1p dict appears to be empty for some reason. I checked the object before saving it to the file and it had the 'base' key whereas after loading it back it was missing, causing the error below. . ### Minimal code sample (that we can copy&paste without having any data). ```python. adata = sc.read(results_file). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). result = adata.uns['rank_genes_groups']. groups = result['names'].dtype.names. pd.DataFrame(. {group + '_' + key[:1]: result[key][group]. for group in groups for key in ['names', 'pvals']}).head(5). sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. ```pytb. KeyError Traceback (most recent call last). Input In [57], in <cell line: 1>(). ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _Ra",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:735,usability,error,error,735,"PMBC3k tutorial - issue loading saved object from .h5ad file; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. I am following all the steps from the PBMC3k tutorial (https://scanpy.readthedocs.io/en/latest/tutorials/pbmc3k.html). I ran the wilcoxon method for finding marker genes and saved the object to a .h5ad file, after which I reloaded the object from the file and the log1p dict appears to be empty for some reason. I checked the object before saving it to the file and it had the 'base' key whereas after loading it back it was missing, causing the error below. . ### Minimal code sample (that we can copy&paste without having any data). ```python. adata = sc.read(results_file). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). result = adata.uns['rank_genes_groups']. groups = result['names'].dtype.names. pd.DataFrame(. {group + '_' + key[:1]: result[key][group]. for group in groups for key in ['names', 'pvals']}).head(5). sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. ```pytb. KeyError Traceback (most recent call last). Input In [57], in <cell line: 1>(). ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _Ra",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:754,usability,Minim,Minimal,754,"PMBC3k tutorial - issue loading saved object from .h5ad file; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. I am following all the steps from the PBMC3k tutorial (https://scanpy.readthedocs.io/en/latest/tutorials/pbmc3k.html). I ran the wilcoxon method for finding marker genes and saved the object to a .h5ad file, after which I reloaded the object from the file and the log1p dict appears to be empty for some reason. I checked the object before saving it to the file and it had the 'base' key whereas after loading it back it was missing, causing the error below. . ### Minimal code sample (that we can copy&paste without having any data). ```python. adata = sc.read(results_file). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). result = adata.uns['rank_genes_groups']. groups = result['names'].dtype.names. pd.DataFrame(. {group + '_' + key[:1]: result[key][group]. for group in groups for key in ['names', 'pvals']}).head(5). sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. ```pytb. KeyError Traceback (most recent call last). Input In [57], in <cell line: 1>(). ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _Ra",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:1334,usability,Input,Input,1334,"orial (https://scanpy.readthedocs.io/en/latest/tutorials/pbmc3k.html). I ran the wilcoxon method for finding marker genes and saved the object to a .h5ad file, after which I reloaded the object from the file and the log1p dict appears to be empty for some reason. I checked the object before saving it to the file and it had the 'base' key whereas after loading it back it was missing, causing the error below. . ### Minimal code sample (that we can copy&paste without having any data). ```python. adata = sc.read(results_file). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). result = adata.uns['rank_genes_groups']. groups = result['names'].dtype.names. pd.DataFrame(. {group + '_' + key[:1]: result[key][group]. for group in groups for key in ['names', 'pvals']}).head(5). sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. ```pytb. KeyError Traceback (most recent call last). Input In [57], in <cell line: 1>(). ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File /Library/Frameworks/Python.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:1620,usability,tool,tools,1620,"efore saving it to the file and it had the 'base' key whereas after loading it back it was missing, causing the error below. . ### Minimal code sample (that we can copy&paste without having any data). ```python. adata = sc.read(results_file). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). result = adata.uns['rank_genes_groups']. groups = result['names'].dtype.names. pd.DataFrame(. {group + '_' + key[:1]: result[key][group]. for group in groups for key in ['names', 'pvals']}).head(5). sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). ```. ```pytb. KeyError Traceback (most recent call last). Input In [57], in <cell line: 1>(). ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.un",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:2397,usability,tool,tools,2397,"s(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds). 580 adata.uns[key_added] = {}. 581 adata.uns[key_added]['params'] = dict(. 582 groupby=groupby,. 583 reference=reference,. (...). 587 corr_method=corr_method,. 588 ). --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts). 592 if check_nonnegative_integers(test_obj.X) and method != 'logreg':. 593 logg.warning(. 594 ""It seems you use rank_genes_groups on the raw count data. "". 595 ""Please logarithmize your data before calling rank_genes_groups."". 596 ). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.2.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.0. decorator 5.1.1. defusedxml 0.7.1. entrypoints 0.4. executing 0.8.3. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.9.11. ipykernel 6.13.0. ipython_genutils 0.2.0. ipywidgets 8.0.2. jedi 0.18.1. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.8.10. llvmlite 0.39.1. louvain 0.7.1. matplotlib 3.6.0. matplotlib_inline NA. mpl_toolkits NA. natsort 8.2.0. nbinom_ufunc NA. ncf_ufunc ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2497:4374,usability,help,help,4374,"_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts). 82 def __init__(. 83 self,. 84 adata,. (...). 90 comp_pts=False,. 91 ):. ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:. 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])). 95 else:. KeyError: 'base'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.2.0. appnope 0.1.3. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.0. decorator 5.1.1. defusedxml 0.7.1. entrypoints 0.4. executing 0.8.3. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.9.11. ipykernel 6.13.0. ipython_genutils 0.2.0. ipywidgets 8.0.2. jedi 0.18.1. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.8.10. llvmlite 0.39.1. louvain 0.7.1. matplotlib 3.6.0. matplotlib_inline NA. mpl_toolkits NA. natsort 8.2.0. nbinom_ufunc NA. ncf_ufunc NA. numba 0.56.2. numpy 1.23.0. packaging 21.3. pandas 1.4.3. parso 0.8.3. patsy 0.5.2. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.29. psutil 5.9.0. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.8.0. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.11.2. pynndescent 0.5.7. pyparsing 3.0.8. pytz 2022.1. ruamel NA. scipy 1.9.1. seaborn 0.12.0. session_info 1.0.0. six 1.16.0. sklearn 1.1.2. stack_data 0.2.0. statsmodels 0.13.2. texttable 1.6.4. threadpoolctl 3.1.0. tornado 6.1. tqdm 4.64.1. traitlets 5.1.1. typing_extensions NA. umap 0.5.3. wcwidth 0.2.5. yaml 5.3.1. zmq 22.3.0. -----. IPython 8.2.0. jupyter_client 7.2.2. jupyter_core 4.9.2. notebook 6.4.10. -----. Python 3.10.2 (v3.10.2:a58ebcc701, Jan 13 2022, 14:50:16) [Clang 13.0.0 (clang-1300.0.29.30)]. macOS-13.3.1-x86_64-i386-64bit. -----. Session information updated at 2023-05-30 21:48. ```. </details>. Any help would be appreciated! Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497
https://github.com/scverse/scanpy/issues/2499:109,availability,error,error,109,"sc.pl.spatial TypeError: can't multiply sequence by non-int of type 'float'; Hi,. I am getting the following error when trying to plot my spatial data. ```py. with mpl.rc_context({'axes.facecolor': 'black',. 'figure.figsize': [4.5, 5]}):. . sc.pl.spatial(slide, cmap='magma',. color=[""AC-like"", ""Astro"", ""B_T_NK"", ""Inhib.neurons"", ""Micro_Macro"", ""Neurons"", ""OC-like"", ""Oligo"", ""Pericyte""], . ncols=3, size=1.3, . img_key='hires',. # limit color scale at 99.2% quantile of cell abundance. vmin=0, vmax='p99.2' . ). ```. ```pytb. TypeError Traceback (most recent call last). Cell In[11], line 14. 10 # plot in spatial coordinates. 11 with mpl.rc_context({'axes.facecolor': 'black',. 12 'figure.figsize': [4.5, 5]}):. ---> 14 sc.pl.spatial(slide, cmap='magma',. 15 # show first 8 cell types. 16 color=[""AC-like"", ""Astro"", ""B_T_NK"", ""Inhib.neurons"", ""Micro_Macro"", ""Neurons"", ""OC-like"", ""Oligo"", ""Pericyte""], . 17 ncols=3, size=1.3, . 18 img_key=None,. 19 # limit color scale at 99.2% quantile of cell abundance. 20 vmin=0, vmax='p99.2' . 21 ). File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:993, in spatial(adata, basis, img, img_key, library_id, crop_coord, alpha_img, bw, size, scale_factor, spot_size, na_color, show, return_fig, save, **kwargs). 990 cmap_img = None. 991 circle_radius = size * scale_factor * spot_size * 0.5. --> 993 axs = embedding(. 994 adata,. 995 basis=basis,. 996 scale_factor=scale_factor,. 997 size=circle_radius,. 998 na_color=na_color,. 999 show=False,. 1000 save=False,. 1001 **kwargs,. 1002 ). 1003 if not isinstance(axs, list):. 1004 axs = [axs]. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:169, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:255,availability,sli,slide,255,"sc.pl.spatial TypeError: can't multiply sequence by non-int of type 'float'; Hi,. I am getting the following error when trying to plot my spatial data. ```py. with mpl.rc_context({'axes.facecolor': 'black',. 'figure.figsize': [4.5, 5]}):. . sc.pl.spatial(slide, cmap='magma',. color=[""AC-like"", ""Astro"", ""B_T_NK"", ""Inhib.neurons"", ""Micro_Macro"", ""Neurons"", ""OC-like"", ""Oligo"", ""Pericyte""], . ncols=3, size=1.3, . img_key='hires',. # limit color scale at 99.2% quantile of cell abundance. vmin=0, vmax='p99.2' . ). ```. ```pytb. TypeError Traceback (most recent call last). Cell In[11], line 14. 10 # plot in spatial coordinates. 11 with mpl.rc_context({'axes.facecolor': 'black',. 12 'figure.figsize': [4.5, 5]}):. ---> 14 sc.pl.spatial(slide, cmap='magma',. 15 # show first 8 cell types. 16 color=[""AC-like"", ""Astro"", ""B_T_NK"", ""Inhib.neurons"", ""Micro_Macro"", ""Neurons"", ""OC-like"", ""Oligo"", ""Pericyte""], . 17 ncols=3, size=1.3, . 18 img_key=None,. 19 # limit color scale at 99.2% quantile of cell abundance. 20 vmin=0, vmax='p99.2' . 21 ). File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:993, in spatial(adata, basis, img, img_key, library_id, crop_coord, alpha_img, bw, size, scale_factor, spot_size, na_color, show, return_fig, save, **kwargs). 990 cmap_img = None. 991 circle_radius = size * scale_factor * spot_size * 0.5. --> 993 axs = embedding(. 994 adata,. 995 basis=basis,. 996 scale_factor=scale_factor,. 997 size=circle_radius,. 998 na_color=na_color,. 999 show=False,. 1000 save=False,. 1001 **kwargs,. 1002 ). 1003 if not isinstance(axs, list):. 1004 axs = [axs]. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:169, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:737,availability,sli,slide,737,"sc.pl.spatial TypeError: can't multiply sequence by non-int of type 'float'; Hi,. I am getting the following error when trying to plot my spatial data. ```py. with mpl.rc_context({'axes.facecolor': 'black',. 'figure.figsize': [4.5, 5]}):. . sc.pl.spatial(slide, cmap='magma',. color=[""AC-like"", ""Astro"", ""B_T_NK"", ""Inhib.neurons"", ""Micro_Macro"", ""Neurons"", ""OC-like"", ""Oligo"", ""Pericyte""], . ncols=3, size=1.3, . img_key='hires',. # limit color scale at 99.2% quantile of cell abundance. vmin=0, vmax='p99.2' . ). ```. ```pytb. TypeError Traceback (most recent call last). Cell In[11], line 14. 10 # plot in spatial coordinates. 11 with mpl.rc_context({'axes.facecolor': 'black',. 12 'figure.figsize': [4.5, 5]}):. ---> 14 sc.pl.spatial(slide, cmap='magma',. 15 # show first 8 cell types. 16 color=[""AC-like"", ""Astro"", ""B_T_NK"", ""Inhib.neurons"", ""Micro_Macro"", ""Neurons"", ""OC-like"", ""Oligo"", ""Pericyte""], . 17 ncols=3, size=1.3, . 18 img_key=None,. 19 # limit color scale at 99.2% quantile of cell abundance. 20 vmin=0, vmax='p99.2' . 21 ). File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:993, in spatial(adata, basis, img, img_key, library_id, crop_coord, alpha_img, bw, size, scale_factor, spot_size, na_color, show, return_fig, save, **kwargs). 990 cmap_img = None. 991 circle_radius = size * scale_factor * spot_size * 0.5. --> 993 axs = embedding(. 994 adata,. 995 basis=basis,. 996 scale_factor=scale_factor,. 997 size=circle_radius,. 998 na_color=na_color,. 999 show=False,. 1000 save=False,. 1001 **kwargs,. 1002 ). 1003 if not isinstance(axs, list):. 1004 axs = [axs]. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:169, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:445,deployability,scale,scale,445,"sc.pl.spatial TypeError: can't multiply sequence by non-int of type 'float'; Hi,. I am getting the following error when trying to plot my spatial data. ```py. with mpl.rc_context({'axes.facecolor': 'black',. 'figure.figsize': [4.5, 5]}):. . sc.pl.spatial(slide, cmap='magma',. color=[""AC-like"", ""Astro"", ""B_T_NK"", ""Inhib.neurons"", ""Micro_Macro"", ""Neurons"", ""OC-like"", ""Oligo"", ""Pericyte""], . ncols=3, size=1.3, . img_key='hires',. # limit color scale at 99.2% quantile of cell abundance. vmin=0, vmax='p99.2' . ). ```. ```pytb. TypeError Traceback (most recent call last). Cell In[11], line 14. 10 # plot in spatial coordinates. 11 with mpl.rc_context({'axes.facecolor': 'black',. 12 'figure.figsize': [4.5, 5]}):. ---> 14 sc.pl.spatial(slide, cmap='magma',. 15 # show first 8 cell types. 16 color=[""AC-like"", ""Astro"", ""B_T_NK"", ""Inhib.neurons"", ""Micro_Macro"", ""Neurons"", ""OC-like"", ""Oligo"", ""Pericyte""], . 17 ncols=3, size=1.3, . 18 img_key=None,. 19 # limit color scale at 99.2% quantile of cell abundance. 20 vmin=0, vmax='p99.2' . 21 ). File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:993, in spatial(adata, basis, img, img_key, library_id, crop_coord, alpha_img, bw, size, scale_factor, spot_size, na_color, show, return_fig, save, **kwargs). 990 cmap_img = None. 991 circle_radius = size * scale_factor * spot_size * 0.5. --> 993 axs = embedding(. 994 adata,. 995 basis=basis,. 996 scale_factor=scale_factor,. 997 size=circle_radius,. 998 na_color=na_color,. 999 show=False,. 1000 save=False,. 1001 **kwargs,. 1002 ). 1003 if not isinstance(axs, list):. 1004 axs = [axs]. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:169, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:966,deployability,scale,scale,966,"sc.pl.spatial TypeError: can't multiply sequence by non-int of type 'float'; Hi,. I am getting the following error when trying to plot my spatial data. ```py. with mpl.rc_context({'axes.facecolor': 'black',. 'figure.figsize': [4.5, 5]}):. . sc.pl.spatial(slide, cmap='magma',. color=[""AC-like"", ""Astro"", ""B_T_NK"", ""Inhib.neurons"", ""Micro_Macro"", ""Neurons"", ""OC-like"", ""Oligo"", ""Pericyte""], . ncols=3, size=1.3, . img_key='hires',. # limit color scale at 99.2% quantile of cell abundance. vmin=0, vmax='p99.2' . ). ```. ```pytb. TypeError Traceback (most recent call last). Cell In[11], line 14. 10 # plot in spatial coordinates. 11 with mpl.rc_context({'axes.facecolor': 'black',. 12 'figure.figsize': [4.5, 5]}):. ---> 14 sc.pl.spatial(slide, cmap='magma',. 15 # show first 8 cell types. 16 color=[""AC-like"", ""Astro"", ""B_T_NK"", ""Inhib.neurons"", ""Micro_Macro"", ""Neurons"", ""OC-like"", ""Oligo"", ""Pericyte""], . 17 ncols=3, size=1.3, . 18 img_key=None,. 19 # limit color scale at 99.2% quantile of cell abundance. 20 vmin=0, vmax='p99.2' . 21 ). File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:993, in spatial(adata, basis, img, img_key, library_id, crop_coord, alpha_img, bw, size, scale_factor, spot_size, na_color, show, return_fig, save, **kwargs). 990 cmap_img = None. 991 circle_radius = size * scale_factor * spot_size * 0.5. --> 993 axs = embedding(. 994 adata,. 995 basis=basis,. 996 scale_factor=scale_factor,. 997 size=circle_radius,. 998 na_color=na_color,. 999 show=False,. 1000 save=False,. 1001 **kwargs,. 1002 ). 1003 if not isinstance(axs, list):. 1004 axs = [axs]. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:169, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:3074,deployability,scale,scale,3074,"tor,. 997 size=circle_radius,. 998 na_color=na_color,. 999 show=False,. 1000 save=False,. 1001 **kwargs,. 1002 ). 1003 if not isinstance(axs, list):. 1004 axs = [axs]. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:169, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, vcenter, norm, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 165 title = [title] if isinstance(title, str) else list(title). 167 # get the points position and the components list. 168 # (only if components is not None). --> 169 data_points, components_list = _get_data_points(. 170 adata, basis, projection, components, scale_factor. 171 ). 173 # Setup layout. 174 # Most of the code is for the case when multiple plots are required. 175 # 'color' is a list of names that want to be plotted. 176 # Eg. ['Gene1', 'louvain', 'Gene2']. 177 # component_list is a list of components [[0,1], [1,2]]. 178 if (. 179 not isinstance(color, str). 180 and isinstance(color, cabc.Sequence). 181 and len(color) > 1. 182 ) or len(components_list) > 1:. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:1130, in _get_data_points(adata, basis, projection, components, scale_factor). 1127 components_list = []. 1129 if scale_factor is not None: # if basis need scale for img background. -> 1130 data_points[0] = np.multiply(data_points[0], scale_factor). 1132 return data_points, components_list. TypeError: can't multiply sequence by non-int of type 'float'. ```. It looks like the scale.factor for the hires image key is a float: 'tissue_hires_scalef': 0.12813942. Do you know what can be modified so that the data is plottable?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:3296,deployability,scale,scale,3296,"tor,. 997 size=circle_radius,. 998 na_color=na_color,. 999 show=False,. 1000 save=False,. 1001 **kwargs,. 1002 ). 1003 if not isinstance(axs, list):. 1004 axs = [axs]. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:169, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, vcenter, norm, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 165 title = [title] if isinstance(title, str) else list(title). 167 # get the points position and the components list. 168 # (only if components is not None). --> 169 data_points, components_list = _get_data_points(. 170 adata, basis, projection, components, scale_factor. 171 ). 173 # Setup layout. 174 # Most of the code is for the case when multiple plots are required. 175 # 'color' is a list of names that want to be plotted. 176 # Eg. ['Gene1', 'louvain', 'Gene2']. 177 # component_list is a list of components [[0,1], [1,2]]. 178 if (. 179 not isinstance(color, str). 180 and isinstance(color, cabc.Sequence). 181 and len(color) > 1. 182 ) or len(components_list) > 1:. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:1130, in _get_data_points(adata, basis, projection, components, scale_factor). 1127 components_list = []. 1129 if scale_factor is not None: # if basis need scale for img background. -> 1130 data_points[0] = np.multiply(data_points[0], scale_factor). 1132 return data_points, components_list. TypeError: can't multiply sequence by non-int of type 'float'. ```. It looks like the scale.factor for the hires image key is a float: 'tissue_hires_scalef': 0.12813942. Do you know what can be modified so that the data is plottable?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:445,energy efficiency,scale,scale,445,"sc.pl.spatial TypeError: can't multiply sequence by non-int of type 'float'; Hi,. I am getting the following error when trying to plot my spatial data. ```py. with mpl.rc_context({'axes.facecolor': 'black',. 'figure.figsize': [4.5, 5]}):. . sc.pl.spatial(slide, cmap='magma',. color=[""AC-like"", ""Astro"", ""B_T_NK"", ""Inhib.neurons"", ""Micro_Macro"", ""Neurons"", ""OC-like"", ""Oligo"", ""Pericyte""], . ncols=3, size=1.3, . img_key='hires',. # limit color scale at 99.2% quantile of cell abundance. vmin=0, vmax='p99.2' . ). ```. ```pytb. TypeError Traceback (most recent call last). Cell In[11], line 14. 10 # plot in spatial coordinates. 11 with mpl.rc_context({'axes.facecolor': 'black',. 12 'figure.figsize': [4.5, 5]}):. ---> 14 sc.pl.spatial(slide, cmap='magma',. 15 # show first 8 cell types. 16 color=[""AC-like"", ""Astro"", ""B_T_NK"", ""Inhib.neurons"", ""Micro_Macro"", ""Neurons"", ""OC-like"", ""Oligo"", ""Pericyte""], . 17 ncols=3, size=1.3, . 18 img_key=None,. 19 # limit color scale at 99.2% quantile of cell abundance. 20 vmin=0, vmax='p99.2' . 21 ). File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:993, in spatial(adata, basis, img, img_key, library_id, crop_coord, alpha_img, bw, size, scale_factor, spot_size, na_color, show, return_fig, save, **kwargs). 990 cmap_img = None. 991 circle_radius = size * scale_factor * spot_size * 0.5. --> 993 axs = embedding(. 994 adata,. 995 basis=basis,. 996 scale_factor=scale_factor,. 997 size=circle_radius,. 998 na_color=na_color,. 999 show=False,. 1000 save=False,. 1001 **kwargs,. 1002 ). 1003 if not isinstance(axs, list):. 1004 axs = [axs]. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:169, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:966,energy efficiency,scale,scale,966,"sc.pl.spatial TypeError: can't multiply sequence by non-int of type 'float'; Hi,. I am getting the following error when trying to plot my spatial data. ```py. with mpl.rc_context({'axes.facecolor': 'black',. 'figure.figsize': [4.5, 5]}):. . sc.pl.spatial(slide, cmap='magma',. color=[""AC-like"", ""Astro"", ""B_T_NK"", ""Inhib.neurons"", ""Micro_Macro"", ""Neurons"", ""OC-like"", ""Oligo"", ""Pericyte""], . ncols=3, size=1.3, . img_key='hires',. # limit color scale at 99.2% quantile of cell abundance. vmin=0, vmax='p99.2' . ). ```. ```pytb. TypeError Traceback (most recent call last). Cell In[11], line 14. 10 # plot in spatial coordinates. 11 with mpl.rc_context({'axes.facecolor': 'black',. 12 'figure.figsize': [4.5, 5]}):. ---> 14 sc.pl.spatial(slide, cmap='magma',. 15 # show first 8 cell types. 16 color=[""AC-like"", ""Astro"", ""B_T_NK"", ""Inhib.neurons"", ""Micro_Macro"", ""Neurons"", ""OC-like"", ""Oligo"", ""Pericyte""], . 17 ncols=3, size=1.3, . 18 img_key=None,. 19 # limit color scale at 99.2% quantile of cell abundance. 20 vmin=0, vmax='p99.2' . 21 ). File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:993, in spatial(adata, basis, img, img_key, library_id, crop_coord, alpha_img, bw, size, scale_factor, spot_size, na_color, show, return_fig, save, **kwargs). 990 cmap_img = None. 991 circle_radius = size * scale_factor * spot_size * 0.5. --> 993 axs = embedding(. 994 adata,. 995 basis=basis,. 996 scale_factor=scale_factor,. 997 size=circle_radius,. 998 na_color=na_color,. 999 show=False,. 1000 save=False,. 1001 **kwargs,. 1002 ). 1003 if not isinstance(axs, list):. 1004 axs = [axs]. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:169, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:3074,energy efficiency,scale,scale,3074,"tor,. 997 size=circle_radius,. 998 na_color=na_color,. 999 show=False,. 1000 save=False,. 1001 **kwargs,. 1002 ). 1003 if not isinstance(axs, list):. 1004 axs = [axs]. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:169, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, vcenter, norm, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 165 title = [title] if isinstance(title, str) else list(title). 167 # get the points position and the components list. 168 # (only if components is not None). --> 169 data_points, components_list = _get_data_points(. 170 adata, basis, projection, components, scale_factor. 171 ). 173 # Setup layout. 174 # Most of the code is for the case when multiple plots are required. 175 # 'color' is a list of names that want to be plotted. 176 # Eg. ['Gene1', 'louvain', 'Gene2']. 177 # component_list is a list of components [[0,1], [1,2]]. 178 if (. 179 not isinstance(color, str). 180 and isinstance(color, cabc.Sequence). 181 and len(color) > 1. 182 ) or len(components_list) > 1:. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:1130, in _get_data_points(adata, basis, projection, components, scale_factor). 1127 components_list = []. 1129 if scale_factor is not None: # if basis need scale for img background. -> 1130 data_points[0] = np.multiply(data_points[0], scale_factor). 1132 return data_points, components_list. TypeError: can't multiply sequence by non-int of type 'float'. ```. It looks like the scale.factor for the hires image key is a float: 'tissue_hires_scalef': 0.12813942. Do you know what can be modified so that the data is plottable?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:3296,energy efficiency,scale,scale,3296,"tor,. 997 size=circle_radius,. 998 na_color=na_color,. 999 show=False,. 1000 save=False,. 1001 **kwargs,. 1002 ). 1003 if not isinstance(axs, list):. 1004 axs = [axs]. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:169, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, vcenter, norm, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 165 title = [title] if isinstance(title, str) else list(title). 167 # get the points position and the components list. 168 # (only if components is not None). --> 169 data_points, components_list = _get_data_points(. 170 adata, basis, projection, components, scale_factor. 171 ). 173 # Setup layout. 174 # Most of the code is for the case when multiple plots are required. 175 # 'color' is a list of names that want to be plotted. 176 # Eg. ['Gene1', 'louvain', 'Gene2']. 177 # component_list is a list of components [[0,1], [1,2]]. 178 if (. 179 not isinstance(color, str). 180 and isinstance(color, cabc.Sequence). 181 and len(color) > 1. 182 ) or len(components_list) > 1:. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:1130, in _get_data_points(adata, basis, projection, components, scale_factor). 1127 components_list = []. 1129 if scale_factor is not None: # if basis need scale for img background. -> 1130 data_points[0] = np.multiply(data_points[0], scale_factor). 1132 return data_points, components_list. TypeError: can't multiply sequence by non-int of type 'float'. ```. It looks like the scale.factor for the hires image key is a float: 'tissue_hires_scalef': 0.12813942. Do you know what can be modified so that the data is plottable?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:1843,integrability,compon,components,1843,"icro_Macro"", ""Neurons"", ""OC-like"", ""Oligo"", ""Pericyte""], . 17 ncols=3, size=1.3, . 18 img_key=None,. 19 # limit color scale at 99.2% quantile of cell abundance. 20 vmin=0, vmax='p99.2' . 21 ). File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:993, in spatial(adata, basis, img, img_key, library_id, crop_coord, alpha_img, bw, size, scale_factor, spot_size, na_color, show, return_fig, save, **kwargs). 990 cmap_img = None. 991 circle_radius = size * scale_factor * spot_size * 0.5. --> 993 axs = embedding(. 994 adata,. 995 basis=basis,. 996 scale_factor=scale_factor,. 997 size=circle_radius,. 998 na_color=na_color,. 999 show=False,. 1000 save=False,. 1001 **kwargs,. 1002 ). 1003 if not isinstance(axs, list):. 1004 axs = [axs]. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:169, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, vcenter, norm, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 165 title = [title] if isinstance(title, str) else list(title). 167 # get the points position and the components list. 168 # (only if components is not None). --> 169 data_points, components_list = _get_data_points(. 170 adata, basis, projection, components, scale_factor. 171 ). 173 # Setup layout. 174 # Most of the code is for the case when multiple plots are required. 175 # 'color' is a list of names that want to be plotted. 176 # Eg. ['Gene1', 'louvain', 'Gene2']. 177 # component_list is a list of components [[0,1], [1,2]]. 178 if (. 179 not isinstance(color, str). 180 and isinstance(color, cabc.Sequence). 181 and len(color) > 1. 182 ) or len(components_list) > 1:. File ~/.loc",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:2262,integrability,compon,components,2262,"e, **kwargs). 990 cmap_img = None. 991 circle_radius = size * scale_factor * spot_size * 0.5. --> 993 axs = embedding(. 994 adata,. 995 basis=basis,. 996 scale_factor=scale_factor,. 997 size=circle_radius,. 998 na_color=na_color,. 999 show=False,. 1000 save=False,. 1001 **kwargs,. 1002 ). 1003 if not isinstance(axs, list):. 1004 axs = [axs]. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:169, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, vcenter, norm, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 165 title = [title] if isinstance(title, str) else list(title). 167 # get the points position and the components list. 168 # (only if components is not None). --> 169 data_points, components_list = _get_data_points(. 170 adata, basis, projection, components, scale_factor. 171 ). 173 # Setup layout. 174 # Most of the code is for the case when multiple plots are required. 175 # 'color' is a list of names that want to be plotted. 176 # Eg. ['Gene1', 'louvain', 'Gene2']. 177 # component_list is a list of components [[0,1], [1,2]]. 178 if (. 179 not isinstance(color, str). 180 and isinstance(color, cabc.Sequence). 181 and len(color) > 1. 182 ) or len(components_list) > 1:. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:1130, in _get_data_points(adata, basis, projection, components, scale_factor). 1127 components_list = []. 1129 if scale_factor is not None: # if basis need scale for img background. -> 1130 data_points[0] = np.multiply(data_points[0], scale_factor). 1132 return data_points, components_list. TypeError: can't multiply sequence by non-int of type 'fl",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:2294,integrability,compon,components,2294,"e. 991 circle_radius = size * scale_factor * spot_size * 0.5. --> 993 axs = embedding(. 994 adata,. 995 basis=basis,. 996 scale_factor=scale_factor,. 997 size=circle_radius,. 998 na_color=na_color,. 999 show=False,. 1000 save=False,. 1001 **kwargs,. 1002 ). 1003 if not isinstance(axs, list):. 1004 axs = [axs]. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:169, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, vcenter, norm, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 165 title = [title] if isinstance(title, str) else list(title). 167 # get the points position and the components list. 168 # (only if components is not None). --> 169 data_points, components_list = _get_data_points(. 170 adata, basis, projection, components, scale_factor. 171 ). 173 # Setup layout. 174 # Most of the code is for the case when multiple plots are required. 175 # 'color' is a list of names that want to be plotted. 176 # Eg. ['Gene1', 'louvain', 'Gene2']. 177 # component_list is a list of components [[0,1], [1,2]]. 178 if (. 179 not isinstance(color, str). 180 and isinstance(color, cabc.Sequence). 181 and len(color) > 1. 182 ) or len(components_list) > 1:. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:1130, in _get_data_points(adata, basis, projection, components, scale_factor). 1127 components_list = []. 1129 if scale_factor is not None: # if basis need scale for img background. -> 1130 data_points[0] = np.multiply(data_points[0], scale_factor). 1132 return data_points, components_list. TypeError: can't multiply sequence by non-int of type 'float'. ```. It looks like the sca",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:2407,integrability,compon,components,2407,"is,. 996 scale_factor=scale_factor,. 997 size=circle_radius,. 998 na_color=na_color,. 999 show=False,. 1000 save=False,. 1001 **kwargs,. 1002 ). 1003 if not isinstance(axs, list):. 1004 axs = [axs]. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:169, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, vcenter, norm, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 165 title = [title] if isinstance(title, str) else list(title). 167 # get the points position and the components list. 168 # (only if components is not None). --> 169 data_points, components_list = _get_data_points(. 170 adata, basis, projection, components, scale_factor. 171 ). 173 # Setup layout. 174 # Most of the code is for the case when multiple plots are required. 175 # 'color' is a list of names that want to be plotted. 176 # Eg. ['Gene1', 'louvain', 'Gene2']. 177 # component_list is a list of components [[0,1], [1,2]]. 178 if (. 179 not isinstance(color, str). 180 and isinstance(color, cabc.Sequence). 181 and len(color) > 1. 182 ) or len(components_list) > 1:. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:1130, in _get_data_points(adata, basis, projection, components, scale_factor). 1127 components_list = []. 1129 if scale_factor is not None: # if basis need scale for img background. -> 1130 data_points[0] = np.multiply(data_points[0], scale_factor). 1132 return data_points, components_list. TypeError: can't multiply sequence by non-int of type 'float'. ```. It looks like the scale.factor for the hires image key is a float: 'tissue_hires_scalef': 0.12813942. Do you know what can be modified",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:2666,integrability,compon,components,2666,"tor,. 997 size=circle_radius,. 998 na_color=na_color,. 999 show=False,. 1000 save=False,. 1001 **kwargs,. 1002 ). 1003 if not isinstance(axs, list):. 1004 axs = [axs]. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:169, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, vcenter, norm, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 165 title = [title] if isinstance(title, str) else list(title). 167 # get the points position and the components list. 168 # (only if components is not None). --> 169 data_points, components_list = _get_data_points(. 170 adata, basis, projection, components, scale_factor. 171 ). 173 # Setup layout. 174 # Most of the code is for the case when multiple plots are required. 175 # 'color' is a list of names that want to be plotted. 176 # Eg. ['Gene1', 'louvain', 'Gene2']. 177 # component_list is a list of components [[0,1], [1,2]]. 178 if (. 179 not isinstance(color, str). 180 and isinstance(color, cabc.Sequence). 181 and len(color) > 1. 182 ) or len(components_list) > 1:. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:1130, in _get_data_points(adata, basis, projection, components, scale_factor). 1127 components_list = []. 1129 if scale_factor is not None: # if basis need scale for img background. -> 1130 data_points[0] = np.multiply(data_points[0], scale_factor). 1132 return data_points, components_list. TypeError: can't multiply sequence by non-int of type 'float'. ```. It looks like the scale.factor for the hires image key is a float: 'tissue_hires_scalef': 0.12813942. Do you know what can be modified so that the data is plottable?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:2970,integrability,compon,components,2970,"tor,. 997 size=circle_radius,. 998 na_color=na_color,. 999 show=False,. 1000 save=False,. 1001 **kwargs,. 1002 ). 1003 if not isinstance(axs, list):. 1004 axs = [axs]. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:169, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, vcenter, norm, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 165 title = [title] if isinstance(title, str) else list(title). 167 # get the points position and the components list. 168 # (only if components is not None). --> 169 data_points, components_list = _get_data_points(. 170 adata, basis, projection, components, scale_factor. 171 ). 173 # Setup layout. 174 # Most of the code is for the case when multiple plots are required. 175 # 'color' is a list of names that want to be plotted. 176 # Eg. ['Gene1', 'louvain', 'Gene2']. 177 # component_list is a list of components [[0,1], [1,2]]. 178 if (. 179 not isinstance(color, str). 180 and isinstance(color, cabc.Sequence). 181 and len(color) > 1. 182 ) or len(components_list) > 1:. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:1130, in _get_data_points(adata, basis, projection, components, scale_factor). 1127 components_list = []. 1129 if scale_factor is not None: # if basis need scale for img background. -> 1130 data_points[0] = np.multiply(data_points[0], scale_factor). 1132 return data_points, components_list. TypeError: can't multiply sequence by non-int of type 'float'. ```. It looks like the scale.factor for the hires image key is a float: 'tissue_hires_scalef': 0.12813942. Do you know what can be modified so that the data is plottable?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:616,interoperability,coordinat,coordinates,616,"sc.pl.spatial TypeError: can't multiply sequence by non-int of type 'float'; Hi,. I am getting the following error when trying to plot my spatial data. ```py. with mpl.rc_context({'axes.facecolor': 'black',. 'figure.figsize': [4.5, 5]}):. . sc.pl.spatial(slide, cmap='magma',. color=[""AC-like"", ""Astro"", ""B_T_NK"", ""Inhib.neurons"", ""Micro_Macro"", ""Neurons"", ""OC-like"", ""Oligo"", ""Pericyte""], . ncols=3, size=1.3, . img_key='hires',. # limit color scale at 99.2% quantile of cell abundance. vmin=0, vmax='p99.2' . ). ```. ```pytb. TypeError Traceback (most recent call last). Cell In[11], line 14. 10 # plot in spatial coordinates. 11 with mpl.rc_context({'axes.facecolor': 'black',. 12 'figure.figsize': [4.5, 5]}):. ---> 14 sc.pl.spatial(slide, cmap='magma',. 15 # show first 8 cell types. 16 color=[""AC-like"", ""Astro"", ""B_T_NK"", ""Inhib.neurons"", ""Micro_Macro"", ""Neurons"", ""OC-like"", ""Oligo"", ""Pericyte""], . 17 ncols=3, size=1.3, . 18 img_key=None,. 19 # limit color scale at 99.2% quantile of cell abundance. 20 vmin=0, vmax='p99.2' . 21 ). File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:993, in spatial(adata, basis, img, img_key, library_id, crop_coord, alpha_img, bw, size, scale_factor, spot_size, na_color, show, return_fig, save, **kwargs). 990 cmap_img = None. 991 circle_radius = size * scale_factor * spot_size * 0.5. --> 993 axs = embedding(. 994 adata,. 995 basis=basis,. 996 scale_factor=scale_factor,. 997 size=circle_radius,. 998 na_color=na_color,. 999 show=False,. 1000 save=False,. 1001 **kwargs,. 1002 ). 1003 if not isinstance(axs, list):. 1004 axs = [axs]. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:169, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:1843,interoperability,compon,components,1843,"icro_Macro"", ""Neurons"", ""OC-like"", ""Oligo"", ""Pericyte""], . 17 ncols=3, size=1.3, . 18 img_key=None,. 19 # limit color scale at 99.2% quantile of cell abundance. 20 vmin=0, vmax='p99.2' . 21 ). File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:993, in spatial(adata, basis, img, img_key, library_id, crop_coord, alpha_img, bw, size, scale_factor, spot_size, na_color, show, return_fig, save, **kwargs). 990 cmap_img = None. 991 circle_radius = size * scale_factor * spot_size * 0.5. --> 993 axs = embedding(. 994 adata,. 995 basis=basis,. 996 scale_factor=scale_factor,. 997 size=circle_radius,. 998 na_color=na_color,. 999 show=False,. 1000 save=False,. 1001 **kwargs,. 1002 ). 1003 if not isinstance(axs, list):. 1004 axs = [axs]. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:169, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, vcenter, norm, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 165 title = [title] if isinstance(title, str) else list(title). 167 # get the points position and the components list. 168 # (only if components is not None). --> 169 data_points, components_list = _get_data_points(. 170 adata, basis, projection, components, scale_factor. 171 ). 173 # Setup layout. 174 # Most of the code is for the case when multiple plots are required. 175 # 'color' is a list of names that want to be plotted. 176 # Eg. ['Gene1', 'louvain', 'Gene2']. 177 # component_list is a list of components [[0,1], [1,2]]. 178 if (. 179 not isinstance(color, str). 180 and isinstance(color, cabc.Sequence). 181 and len(color) > 1. 182 ) or len(components_list) > 1:. File ~/.loc",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:2262,interoperability,compon,components,2262,"e, **kwargs). 990 cmap_img = None. 991 circle_radius = size * scale_factor * spot_size * 0.5. --> 993 axs = embedding(. 994 adata,. 995 basis=basis,. 996 scale_factor=scale_factor,. 997 size=circle_radius,. 998 na_color=na_color,. 999 show=False,. 1000 save=False,. 1001 **kwargs,. 1002 ). 1003 if not isinstance(axs, list):. 1004 axs = [axs]. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:169, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, vcenter, norm, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 165 title = [title] if isinstance(title, str) else list(title). 167 # get the points position and the components list. 168 # (only if components is not None). --> 169 data_points, components_list = _get_data_points(. 170 adata, basis, projection, components, scale_factor. 171 ). 173 # Setup layout. 174 # Most of the code is for the case when multiple plots are required. 175 # 'color' is a list of names that want to be plotted. 176 # Eg. ['Gene1', 'louvain', 'Gene2']. 177 # component_list is a list of components [[0,1], [1,2]]. 178 if (. 179 not isinstance(color, str). 180 and isinstance(color, cabc.Sequence). 181 and len(color) > 1. 182 ) or len(components_list) > 1:. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:1130, in _get_data_points(adata, basis, projection, components, scale_factor). 1127 components_list = []. 1129 if scale_factor is not None: # if basis need scale for img background. -> 1130 data_points[0] = np.multiply(data_points[0], scale_factor). 1132 return data_points, components_list. TypeError: can't multiply sequence by non-int of type 'fl",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:2294,interoperability,compon,components,2294,"e. 991 circle_radius = size * scale_factor * spot_size * 0.5. --> 993 axs = embedding(. 994 adata,. 995 basis=basis,. 996 scale_factor=scale_factor,. 997 size=circle_radius,. 998 na_color=na_color,. 999 show=False,. 1000 save=False,. 1001 **kwargs,. 1002 ). 1003 if not isinstance(axs, list):. 1004 axs = [axs]. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:169, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, vcenter, norm, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 165 title = [title] if isinstance(title, str) else list(title). 167 # get the points position and the components list. 168 # (only if components is not None). --> 169 data_points, components_list = _get_data_points(. 170 adata, basis, projection, components, scale_factor. 171 ). 173 # Setup layout. 174 # Most of the code is for the case when multiple plots are required. 175 # 'color' is a list of names that want to be plotted. 176 # Eg. ['Gene1', 'louvain', 'Gene2']. 177 # component_list is a list of components [[0,1], [1,2]]. 178 if (. 179 not isinstance(color, str). 180 and isinstance(color, cabc.Sequence). 181 and len(color) > 1. 182 ) or len(components_list) > 1:. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:1130, in _get_data_points(adata, basis, projection, components, scale_factor). 1127 components_list = []. 1129 if scale_factor is not None: # if basis need scale for img background. -> 1130 data_points[0] = np.multiply(data_points[0], scale_factor). 1132 return data_points, components_list. TypeError: can't multiply sequence by non-int of type 'float'. ```. It looks like the sca",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:2407,interoperability,compon,components,2407,"is,. 996 scale_factor=scale_factor,. 997 size=circle_radius,. 998 na_color=na_color,. 999 show=False,. 1000 save=False,. 1001 **kwargs,. 1002 ). 1003 if not isinstance(axs, list):. 1004 axs = [axs]. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:169, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, vcenter, norm, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 165 title = [title] if isinstance(title, str) else list(title). 167 # get the points position and the components list. 168 # (only if components is not None). --> 169 data_points, components_list = _get_data_points(. 170 adata, basis, projection, components, scale_factor. 171 ). 173 # Setup layout. 174 # Most of the code is for the case when multiple plots are required. 175 # 'color' is a list of names that want to be plotted. 176 # Eg. ['Gene1', 'louvain', 'Gene2']. 177 # component_list is a list of components [[0,1], [1,2]]. 178 if (. 179 not isinstance(color, str). 180 and isinstance(color, cabc.Sequence). 181 and len(color) > 1. 182 ) or len(components_list) > 1:. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:1130, in _get_data_points(adata, basis, projection, components, scale_factor). 1127 components_list = []. 1129 if scale_factor is not None: # if basis need scale for img background. -> 1130 data_points[0] = np.multiply(data_points[0], scale_factor). 1132 return data_points, components_list. TypeError: can't multiply sequence by non-int of type 'float'. ```. It looks like the scale.factor for the hires image key is a float: 'tissue_hires_scalef': 0.12813942. Do you know what can be modified",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:2666,interoperability,compon,components,2666,"tor,. 997 size=circle_radius,. 998 na_color=na_color,. 999 show=False,. 1000 save=False,. 1001 **kwargs,. 1002 ). 1003 if not isinstance(axs, list):. 1004 axs = [axs]. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:169, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, vcenter, norm, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 165 title = [title] if isinstance(title, str) else list(title). 167 # get the points position and the components list. 168 # (only if components is not None). --> 169 data_points, components_list = _get_data_points(. 170 adata, basis, projection, components, scale_factor. 171 ). 173 # Setup layout. 174 # Most of the code is for the case when multiple plots are required. 175 # 'color' is a list of names that want to be plotted. 176 # Eg. ['Gene1', 'louvain', 'Gene2']. 177 # component_list is a list of components [[0,1], [1,2]]. 178 if (. 179 not isinstance(color, str). 180 and isinstance(color, cabc.Sequence). 181 and len(color) > 1. 182 ) or len(components_list) > 1:. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:1130, in _get_data_points(adata, basis, projection, components, scale_factor). 1127 components_list = []. 1129 if scale_factor is not None: # if basis need scale for img background. -> 1130 data_points[0] = np.multiply(data_points[0], scale_factor). 1132 return data_points, components_list. TypeError: can't multiply sequence by non-int of type 'float'. ```. It looks like the scale.factor for the hires image key is a float: 'tissue_hires_scalef': 0.12813942. Do you know what can be modified so that the data is plottable?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:2970,interoperability,compon,components,2970,"tor,. 997 size=circle_radius,. 998 na_color=na_color,. 999 show=False,. 1000 save=False,. 1001 **kwargs,. 1002 ). 1003 if not isinstance(axs, list):. 1004 axs = [axs]. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:169, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, vcenter, norm, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 165 title = [title] if isinstance(title, str) else list(title). 167 # get the points position and the components list. 168 # (only if components is not None). --> 169 data_points, components_list = _get_data_points(. 170 adata, basis, projection, components, scale_factor. 171 ). 173 # Setup layout. 174 # Most of the code is for the case when multiple plots are required. 175 # 'color' is a list of names that want to be plotted. 176 # Eg. ['Gene1', 'louvain', 'Gene2']. 177 # component_list is a list of components [[0,1], [1,2]]. 178 if (. 179 not isinstance(color, str). 180 and isinstance(color, cabc.Sequence). 181 and len(color) > 1. 182 ) or len(components_list) > 1:. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:1130, in _get_data_points(adata, basis, projection, components, scale_factor). 1127 components_list = []. 1129 if scale_factor is not None: # if basis need scale for img background. -> 1130 data_points[0] = np.multiply(data_points[0], scale_factor). 1132 return data_points, components_list. TypeError: can't multiply sequence by non-int of type 'float'. ```. It looks like the scale.factor for the hires image key is a float: 'tissue_hires_scalef': 0.12813942. Do you know what can be modified so that the data is plottable?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:445,modifiability,scal,scale,445,"sc.pl.spatial TypeError: can't multiply sequence by non-int of type 'float'; Hi,. I am getting the following error when trying to plot my spatial data. ```py. with mpl.rc_context({'axes.facecolor': 'black',. 'figure.figsize': [4.5, 5]}):. . sc.pl.spatial(slide, cmap='magma',. color=[""AC-like"", ""Astro"", ""B_T_NK"", ""Inhib.neurons"", ""Micro_Macro"", ""Neurons"", ""OC-like"", ""Oligo"", ""Pericyte""], . ncols=3, size=1.3, . img_key='hires',. # limit color scale at 99.2% quantile of cell abundance. vmin=0, vmax='p99.2' . ). ```. ```pytb. TypeError Traceback (most recent call last). Cell In[11], line 14. 10 # plot in spatial coordinates. 11 with mpl.rc_context({'axes.facecolor': 'black',. 12 'figure.figsize': [4.5, 5]}):. ---> 14 sc.pl.spatial(slide, cmap='magma',. 15 # show first 8 cell types. 16 color=[""AC-like"", ""Astro"", ""B_T_NK"", ""Inhib.neurons"", ""Micro_Macro"", ""Neurons"", ""OC-like"", ""Oligo"", ""Pericyte""], . 17 ncols=3, size=1.3, . 18 img_key=None,. 19 # limit color scale at 99.2% quantile of cell abundance. 20 vmin=0, vmax='p99.2' . 21 ). File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:993, in spatial(adata, basis, img, img_key, library_id, crop_coord, alpha_img, bw, size, scale_factor, spot_size, na_color, show, return_fig, save, **kwargs). 990 cmap_img = None. 991 circle_radius = size * scale_factor * spot_size * 0.5. --> 993 axs = embedding(. 994 adata,. 995 basis=basis,. 996 scale_factor=scale_factor,. 997 size=circle_radius,. 998 na_color=na_color,. 999 show=False,. 1000 save=False,. 1001 **kwargs,. 1002 ). 1003 if not isinstance(axs, list):. 1004 axs = [axs]. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:169, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:966,modifiability,scal,scale,966,"sc.pl.spatial TypeError: can't multiply sequence by non-int of type 'float'; Hi,. I am getting the following error when trying to plot my spatial data. ```py. with mpl.rc_context({'axes.facecolor': 'black',. 'figure.figsize': [4.5, 5]}):. . sc.pl.spatial(slide, cmap='magma',. color=[""AC-like"", ""Astro"", ""B_T_NK"", ""Inhib.neurons"", ""Micro_Macro"", ""Neurons"", ""OC-like"", ""Oligo"", ""Pericyte""], . ncols=3, size=1.3, . img_key='hires',. # limit color scale at 99.2% quantile of cell abundance. vmin=0, vmax='p99.2' . ). ```. ```pytb. TypeError Traceback (most recent call last). Cell In[11], line 14. 10 # plot in spatial coordinates. 11 with mpl.rc_context({'axes.facecolor': 'black',. 12 'figure.figsize': [4.5, 5]}):. ---> 14 sc.pl.spatial(slide, cmap='magma',. 15 # show first 8 cell types. 16 color=[""AC-like"", ""Astro"", ""B_T_NK"", ""Inhib.neurons"", ""Micro_Macro"", ""Neurons"", ""OC-like"", ""Oligo"", ""Pericyte""], . 17 ncols=3, size=1.3, . 18 img_key=None,. 19 # limit color scale at 99.2% quantile of cell abundance. 20 vmin=0, vmax='p99.2' . 21 ). File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:993, in spatial(adata, basis, img, img_key, library_id, crop_coord, alpha_img, bw, size, scale_factor, spot_size, na_color, show, return_fig, save, **kwargs). 990 cmap_img = None. 991 circle_radius = size * scale_factor * spot_size * 0.5. --> 993 axs = embedding(. 994 adata,. 995 basis=basis,. 996 scale_factor=scale_factor,. 997 size=circle_radius,. 998 na_color=na_color,. 999 show=False,. 1000 save=False,. 1001 **kwargs,. 1002 ). 1003 if not isinstance(axs, list):. 1004 axs = [axs]. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:169, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:1074,modifiability,pac,packages,1074,"i,. I am getting the following error when trying to plot my spatial data. ```py. with mpl.rc_context({'axes.facecolor': 'black',. 'figure.figsize': [4.5, 5]}):. . sc.pl.spatial(slide, cmap='magma',. color=[""AC-like"", ""Astro"", ""B_T_NK"", ""Inhib.neurons"", ""Micro_Macro"", ""Neurons"", ""OC-like"", ""Oligo"", ""Pericyte""], . ncols=3, size=1.3, . img_key='hires',. # limit color scale at 99.2% quantile of cell abundance. vmin=0, vmax='p99.2' . ). ```. ```pytb. TypeError Traceback (most recent call last). Cell In[11], line 14. 10 # plot in spatial coordinates. 11 with mpl.rc_context({'axes.facecolor': 'black',. 12 'figure.figsize': [4.5, 5]}):. ---> 14 sc.pl.spatial(slide, cmap='magma',. 15 # show first 8 cell types. 16 color=[""AC-like"", ""Astro"", ""B_T_NK"", ""Inhib.neurons"", ""Micro_Macro"", ""Neurons"", ""OC-like"", ""Oligo"", ""Pericyte""], . 17 ncols=3, size=1.3, . 18 img_key=None,. 19 # limit color scale at 99.2% quantile of cell abundance. 20 vmin=0, vmax='p99.2' . 21 ). File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:993, in spatial(adata, basis, img, img_key, library_id, crop_coord, alpha_img, bw, size, scale_factor, spot_size, na_color, show, return_fig, save, **kwargs). 990 cmap_img = None. 991 circle_radius = size * scale_factor * spot_size * 0.5. --> 993 axs = embedding(. 994 adata,. 995 basis=basis,. 996 scale_factor=scale_factor,. 997 size=circle_radius,. 998 na_color=na_color,. 999 show=False,. 1000 save=False,. 1001 **kwargs,. 1002 ). 1003 if not isinstance(axs, list):. 1004 axs = [axs]. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:169, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, vcenter, norm, add_outline, outline_width, ou",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:1644,modifiability,pac,packages,1644,"ext({'axes.facecolor': 'black',. 12 'figure.figsize': [4.5, 5]}):. ---> 14 sc.pl.spatial(slide, cmap='magma',. 15 # show first 8 cell types. 16 color=[""AC-like"", ""Astro"", ""B_T_NK"", ""Inhib.neurons"", ""Micro_Macro"", ""Neurons"", ""OC-like"", ""Oligo"", ""Pericyte""], . 17 ncols=3, size=1.3, . 18 img_key=None,. 19 # limit color scale at 99.2% quantile of cell abundance. 20 vmin=0, vmax='p99.2' . 21 ). File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:993, in spatial(adata, basis, img, img_key, library_id, crop_coord, alpha_img, bw, size, scale_factor, spot_size, na_color, show, return_fig, save, **kwargs). 990 cmap_img = None. 991 circle_radius = size * scale_factor * spot_size * 0.5. --> 993 axs = embedding(. 994 adata,. 995 basis=basis,. 996 scale_factor=scale_factor,. 997 size=circle_radius,. 998 na_color=na_color,. 999 show=False,. 1000 save=False,. 1001 **kwargs,. 1002 ). 1003 if not isinstance(axs, list):. 1004 axs = [axs]. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:169, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, vcenter, norm, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 165 title = [title] if isinstance(title, str) else list(title). 167 # get the points position and the components list. 168 # (only if components is not None). --> 169 data_points, components_list = _get_data_points(. 170 adata, basis, projection, components, scale_factor. 171 ). 173 # Setup layout. 174 # Most of the code is for the case when multiple plots are required. 175 # 'color' is a list of names that want to be plotted. 176 # Eg. ['Gene1', 'louvain', 'Gene2']. 177 # component_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:1843,modifiability,compon,components,1843,"icro_Macro"", ""Neurons"", ""OC-like"", ""Oligo"", ""Pericyte""], . 17 ncols=3, size=1.3, . 18 img_key=None,. 19 # limit color scale at 99.2% quantile of cell abundance. 20 vmin=0, vmax='p99.2' . 21 ). File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:993, in spatial(adata, basis, img, img_key, library_id, crop_coord, alpha_img, bw, size, scale_factor, spot_size, na_color, show, return_fig, save, **kwargs). 990 cmap_img = None. 991 circle_radius = size * scale_factor * spot_size * 0.5. --> 993 axs = embedding(. 994 adata,. 995 basis=basis,. 996 scale_factor=scale_factor,. 997 size=circle_radius,. 998 na_color=na_color,. 999 show=False,. 1000 save=False,. 1001 **kwargs,. 1002 ). 1003 if not isinstance(axs, list):. 1004 axs = [axs]. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:169, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, vcenter, norm, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 165 title = [title] if isinstance(title, str) else list(title). 167 # get the points position and the components list. 168 # (only if components is not None). --> 169 data_points, components_list = _get_data_points(. 170 adata, basis, projection, components, scale_factor. 171 ). 173 # Setup layout. 174 # Most of the code is for the case when multiple plots are required. 175 # 'color' is a list of names that want to be plotted. 176 # Eg. ['Gene1', 'louvain', 'Gene2']. 177 # component_list is a list of components [[0,1], [1,2]]. 178 if (. 179 not isinstance(color, str). 180 and isinstance(color, cabc.Sequence). 181 and len(color) > 1. 182 ) or len(components_list) > 1:. File ~/.loc",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:1855,modifiability,layer,layer,1855,""", ""Neurons"", ""OC-like"", ""Oligo"", ""Pericyte""], . 17 ncols=3, size=1.3, . 18 img_key=None,. 19 # limit color scale at 99.2% quantile of cell abundance. 20 vmin=0, vmax='p99.2' . 21 ). File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:993, in spatial(adata, basis, img, img_key, library_id, crop_coord, alpha_img, bw, size, scale_factor, spot_size, na_color, show, return_fig, save, **kwargs). 990 cmap_img = None. 991 circle_radius = size * scale_factor * spot_size * 0.5. --> 993 axs = embedding(. 994 adata,. 995 basis=basis,. 996 scale_factor=scale_factor,. 997 size=circle_radius,. 998 na_color=na_color,. 999 show=False,. 1000 save=False,. 1001 **kwargs,. 1002 ). 1003 if not isinstance(axs, list):. 1004 axs = [axs]. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:169, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, vcenter, norm, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 165 title = [title] if isinstance(title, str) else list(title). 167 # get the points position and the components list. 168 # (only if components is not None). --> 169 data_points, components_list = _get_data_points(. 170 adata, basis, projection, components, scale_factor. 171 ). 173 # Setup layout. 174 # Most of the code is for the case when multiple plots are required. 175 # 'color' is a list of names that want to be plotted. 176 # Eg. ['Gene1', 'louvain', 'Gene2']. 177 # component_list is a list of components [[0,1], [1,2]]. 178 if (. 179 not isinstance(color, str). 180 and isinstance(color, cabc.Sequence). 181 and len(color) > 1. 182 ) or len(components_list) > 1:. File ~/.local/lib/pyt",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:2262,modifiability,compon,components,2262,"e, **kwargs). 990 cmap_img = None. 991 circle_radius = size * scale_factor * spot_size * 0.5. --> 993 axs = embedding(. 994 adata,. 995 basis=basis,. 996 scale_factor=scale_factor,. 997 size=circle_radius,. 998 na_color=na_color,. 999 show=False,. 1000 save=False,. 1001 **kwargs,. 1002 ). 1003 if not isinstance(axs, list):. 1004 axs = [axs]. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:169, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, vcenter, norm, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 165 title = [title] if isinstance(title, str) else list(title). 167 # get the points position and the components list. 168 # (only if components is not None). --> 169 data_points, components_list = _get_data_points(. 170 adata, basis, projection, components, scale_factor. 171 ). 173 # Setup layout. 174 # Most of the code is for the case when multiple plots are required. 175 # 'color' is a list of names that want to be plotted. 176 # Eg. ['Gene1', 'louvain', 'Gene2']. 177 # component_list is a list of components [[0,1], [1,2]]. 178 if (. 179 not isinstance(color, str). 180 and isinstance(color, cabc.Sequence). 181 and len(color) > 1. 182 ) or len(components_list) > 1:. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:1130, in _get_data_points(adata, basis, projection, components, scale_factor). 1127 components_list = []. 1129 if scale_factor is not None: # if basis need scale for img background. -> 1130 data_points[0] = np.multiply(data_points[0], scale_factor). 1132 return data_points, components_list. TypeError: can't multiply sequence by non-int of type 'fl",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:2294,modifiability,compon,components,2294,"e. 991 circle_radius = size * scale_factor * spot_size * 0.5. --> 993 axs = embedding(. 994 adata,. 995 basis=basis,. 996 scale_factor=scale_factor,. 997 size=circle_radius,. 998 na_color=na_color,. 999 show=False,. 1000 save=False,. 1001 **kwargs,. 1002 ). 1003 if not isinstance(axs, list):. 1004 axs = [axs]. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:169, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, vcenter, norm, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 165 title = [title] if isinstance(title, str) else list(title). 167 # get the points position and the components list. 168 # (only if components is not None). --> 169 data_points, components_list = _get_data_points(. 170 adata, basis, projection, components, scale_factor. 171 ). 173 # Setup layout. 174 # Most of the code is for the case when multiple plots are required. 175 # 'color' is a list of names that want to be plotted. 176 # Eg. ['Gene1', 'louvain', 'Gene2']. 177 # component_list is a list of components [[0,1], [1,2]]. 178 if (. 179 not isinstance(color, str). 180 and isinstance(color, cabc.Sequence). 181 and len(color) > 1. 182 ) or len(components_list) > 1:. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:1130, in _get_data_points(adata, basis, projection, components, scale_factor). 1127 components_list = []. 1129 if scale_factor is not None: # if basis need scale for img background. -> 1130 data_points[0] = np.multiply(data_points[0], scale_factor). 1132 return data_points, components_list. TypeError: can't multiply sequence by non-int of type 'float'. ```. It looks like the sca",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:2407,modifiability,compon,components,2407,"is,. 996 scale_factor=scale_factor,. 997 size=circle_radius,. 998 na_color=na_color,. 999 show=False,. 1000 save=False,. 1001 **kwargs,. 1002 ). 1003 if not isinstance(axs, list):. 1004 axs = [axs]. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:169, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, vcenter, norm, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 165 title = [title] if isinstance(title, str) else list(title). 167 # get the points position and the components list. 168 # (only if components is not None). --> 169 data_points, components_list = _get_data_points(. 170 adata, basis, projection, components, scale_factor. 171 ). 173 # Setup layout. 174 # Most of the code is for the case when multiple plots are required. 175 # 'color' is a list of names that want to be plotted. 176 # Eg. ['Gene1', 'louvain', 'Gene2']. 177 # component_list is a list of components [[0,1], [1,2]]. 178 if (. 179 not isinstance(color, str). 180 and isinstance(color, cabc.Sequence). 181 and len(color) > 1. 182 ) or len(components_list) > 1:. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:1130, in _get_data_points(adata, basis, projection, components, scale_factor). 1127 components_list = []. 1129 if scale_factor is not None: # if basis need scale for img background. -> 1130 data_points[0] = np.multiply(data_points[0], scale_factor). 1132 return data_points, components_list. TypeError: can't multiply sequence by non-int of type 'float'. ```. It looks like the scale.factor for the hires image key is a float: 'tissue_hires_scalef': 0.12813942. Do you know what can be modified",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:2666,modifiability,compon,components,2666,"tor,. 997 size=circle_radius,. 998 na_color=na_color,. 999 show=False,. 1000 save=False,. 1001 **kwargs,. 1002 ). 1003 if not isinstance(axs, list):. 1004 axs = [axs]. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:169, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, vcenter, norm, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 165 title = [title] if isinstance(title, str) else list(title). 167 # get the points position and the components list. 168 # (only if components is not None). --> 169 data_points, components_list = _get_data_points(. 170 adata, basis, projection, components, scale_factor. 171 ). 173 # Setup layout. 174 # Most of the code is for the case when multiple plots are required. 175 # 'color' is a list of names that want to be plotted. 176 # Eg. ['Gene1', 'louvain', 'Gene2']. 177 # component_list is a list of components [[0,1], [1,2]]. 178 if (. 179 not isinstance(color, str). 180 and isinstance(color, cabc.Sequence). 181 and len(color) > 1. 182 ) or len(components_list) > 1:. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:1130, in _get_data_points(adata, basis, projection, components, scale_factor). 1127 components_list = []. 1129 if scale_factor is not None: # if basis need scale for img background. -> 1130 data_points[0] = np.multiply(data_points[0], scale_factor). 1132 return data_points, components_list. TypeError: can't multiply sequence by non-int of type 'float'. ```. It looks like the scale.factor for the hires image key is a float: 'tissue_hires_scalef': 0.12813942. Do you know what can be modified so that the data is plottable?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:2870,modifiability,pac,packages,2870,"tor,. 997 size=circle_radius,. 998 na_color=na_color,. 999 show=False,. 1000 save=False,. 1001 **kwargs,. 1002 ). 1003 if not isinstance(axs, list):. 1004 axs = [axs]. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:169, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, vcenter, norm, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 165 title = [title] if isinstance(title, str) else list(title). 167 # get the points position and the components list. 168 # (only if components is not None). --> 169 data_points, components_list = _get_data_points(. 170 adata, basis, projection, components, scale_factor. 171 ). 173 # Setup layout. 174 # Most of the code is for the case when multiple plots are required. 175 # 'color' is a list of names that want to be plotted. 176 # Eg. ['Gene1', 'louvain', 'Gene2']. 177 # component_list is a list of components [[0,1], [1,2]]. 178 if (. 179 not isinstance(color, str). 180 and isinstance(color, cabc.Sequence). 181 and len(color) > 1. 182 ) or len(components_list) > 1:. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:1130, in _get_data_points(adata, basis, projection, components, scale_factor). 1127 components_list = []. 1129 if scale_factor is not None: # if basis need scale for img background. -> 1130 data_points[0] = np.multiply(data_points[0], scale_factor). 1132 return data_points, components_list. TypeError: can't multiply sequence by non-int of type 'float'. ```. It looks like the scale.factor for the hires image key is a float: 'tissue_hires_scalef': 0.12813942. Do you know what can be modified so that the data is plottable?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:2970,modifiability,compon,components,2970,"tor,. 997 size=circle_radius,. 998 na_color=na_color,. 999 show=False,. 1000 save=False,. 1001 **kwargs,. 1002 ). 1003 if not isinstance(axs, list):. 1004 axs = [axs]. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:169, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, vcenter, norm, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 165 title = [title] if isinstance(title, str) else list(title). 167 # get the points position and the components list. 168 # (only if components is not None). --> 169 data_points, components_list = _get_data_points(. 170 adata, basis, projection, components, scale_factor. 171 ). 173 # Setup layout. 174 # Most of the code is for the case when multiple plots are required. 175 # 'color' is a list of names that want to be plotted. 176 # Eg. ['Gene1', 'louvain', 'Gene2']. 177 # component_list is a list of components [[0,1], [1,2]]. 178 if (. 179 not isinstance(color, str). 180 and isinstance(color, cabc.Sequence). 181 and len(color) > 1. 182 ) or len(components_list) > 1:. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:1130, in _get_data_points(adata, basis, projection, components, scale_factor). 1127 components_list = []. 1129 if scale_factor is not None: # if basis need scale for img background. -> 1130 data_points[0] = np.multiply(data_points[0], scale_factor). 1132 return data_points, components_list. TypeError: can't multiply sequence by non-int of type 'float'. ```. It looks like the scale.factor for the hires image key is a float: 'tissue_hires_scalef': 0.12813942. Do you know what can be modified so that the data is plottable?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:3074,modifiability,scal,scale,3074,"tor,. 997 size=circle_radius,. 998 na_color=na_color,. 999 show=False,. 1000 save=False,. 1001 **kwargs,. 1002 ). 1003 if not isinstance(axs, list):. 1004 axs = [axs]. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:169, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, vcenter, norm, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 165 title = [title] if isinstance(title, str) else list(title). 167 # get the points position and the components list. 168 # (only if components is not None). --> 169 data_points, components_list = _get_data_points(. 170 adata, basis, projection, components, scale_factor. 171 ). 173 # Setup layout. 174 # Most of the code is for the case when multiple plots are required. 175 # 'color' is a list of names that want to be plotted. 176 # Eg. ['Gene1', 'louvain', 'Gene2']. 177 # component_list is a list of components [[0,1], [1,2]]. 178 if (. 179 not isinstance(color, str). 180 and isinstance(color, cabc.Sequence). 181 and len(color) > 1. 182 ) or len(components_list) > 1:. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:1130, in _get_data_points(adata, basis, projection, components, scale_factor). 1127 components_list = []. 1129 if scale_factor is not None: # if basis need scale for img background. -> 1130 data_points[0] = np.multiply(data_points[0], scale_factor). 1132 return data_points, components_list. TypeError: can't multiply sequence by non-int of type 'float'. ```. It looks like the scale.factor for the hires image key is a float: 'tissue_hires_scalef': 0.12813942. Do you know what can be modified so that the data is plottable?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:3296,modifiability,scal,scale,3296,"tor,. 997 size=circle_radius,. 998 na_color=na_color,. 999 show=False,. 1000 save=False,. 1001 **kwargs,. 1002 ). 1003 if not isinstance(axs, list):. 1004 axs = [axs]. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:169, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, vcenter, norm, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 165 title = [title] if isinstance(title, str) else list(title). 167 # get the points position and the components list. 168 # (only if components is not None). --> 169 data_points, components_list = _get_data_points(. 170 adata, basis, projection, components, scale_factor. 171 ). 173 # Setup layout. 174 # Most of the code is for the case when multiple plots are required. 175 # 'color' is a list of names that want to be plotted. 176 # Eg. ['Gene1', 'louvain', 'Gene2']. 177 # component_list is a list of components [[0,1], [1,2]]. 178 if (. 179 not isinstance(color, str). 180 and isinstance(color, cabc.Sequence). 181 and len(color) > 1. 182 ) or len(components_list) > 1:. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:1130, in _get_data_points(adata, basis, projection, components, scale_factor). 1127 components_list = []. 1129 if scale_factor is not None: # if basis need scale for img background. -> 1130 data_points[0] = np.multiply(data_points[0], scale_factor). 1132 return data_points, components_list. TypeError: can't multiply sequence by non-int of type 'float'. ```. It looks like the scale.factor for the hires image key is a float: 'tissue_hires_scalef': 0.12813942. Do you know what can be modified so that the data is plottable?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:109,performance,error,error,109,"sc.pl.spatial TypeError: can't multiply sequence by non-int of type 'float'; Hi,. I am getting the following error when trying to plot my spatial data. ```py. with mpl.rc_context({'axes.facecolor': 'black',. 'figure.figsize': [4.5, 5]}):. . sc.pl.spatial(slide, cmap='magma',. color=[""AC-like"", ""Astro"", ""B_T_NK"", ""Inhib.neurons"", ""Micro_Macro"", ""Neurons"", ""OC-like"", ""Oligo"", ""Pericyte""], . ncols=3, size=1.3, . img_key='hires',. # limit color scale at 99.2% quantile of cell abundance. vmin=0, vmax='p99.2' . ). ```. ```pytb. TypeError Traceback (most recent call last). Cell In[11], line 14. 10 # plot in spatial coordinates. 11 with mpl.rc_context({'axes.facecolor': 'black',. 12 'figure.figsize': [4.5, 5]}):. ---> 14 sc.pl.spatial(slide, cmap='magma',. 15 # show first 8 cell types. 16 color=[""AC-like"", ""Astro"", ""B_T_NK"", ""Inhib.neurons"", ""Micro_Macro"", ""Neurons"", ""OC-like"", ""Oligo"", ""Pericyte""], . 17 ncols=3, size=1.3, . 18 img_key=None,. 19 # limit color scale at 99.2% quantile of cell abundance. 20 vmin=0, vmax='p99.2' . 21 ). File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:993, in spatial(adata, basis, img, img_key, library_id, crop_coord, alpha_img, bw, size, scale_factor, spot_size, na_color, show, return_fig, save, **kwargs). 990 cmap_img = None. 991 circle_radius = size * scale_factor * spot_size * 0.5. --> 993 axs = embedding(. 994 adata,. 995 basis=basis,. 996 scale_factor=scale_factor,. 997 size=circle_radius,. 998 na_color=na_color,. 999 show=False,. 1000 save=False,. 1001 **kwargs,. 1002 ). 1003 if not isinstance(axs, list):. 1004 axs = [axs]. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:169, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:445,performance,scale,scale,445,"sc.pl.spatial TypeError: can't multiply sequence by non-int of type 'float'; Hi,. I am getting the following error when trying to plot my spatial data. ```py. with mpl.rc_context({'axes.facecolor': 'black',. 'figure.figsize': [4.5, 5]}):. . sc.pl.spatial(slide, cmap='magma',. color=[""AC-like"", ""Astro"", ""B_T_NK"", ""Inhib.neurons"", ""Micro_Macro"", ""Neurons"", ""OC-like"", ""Oligo"", ""Pericyte""], . ncols=3, size=1.3, . img_key='hires',. # limit color scale at 99.2% quantile of cell abundance. vmin=0, vmax='p99.2' . ). ```. ```pytb. TypeError Traceback (most recent call last). Cell In[11], line 14. 10 # plot in spatial coordinates. 11 with mpl.rc_context({'axes.facecolor': 'black',. 12 'figure.figsize': [4.5, 5]}):. ---> 14 sc.pl.spatial(slide, cmap='magma',. 15 # show first 8 cell types. 16 color=[""AC-like"", ""Astro"", ""B_T_NK"", ""Inhib.neurons"", ""Micro_Macro"", ""Neurons"", ""OC-like"", ""Oligo"", ""Pericyte""], . 17 ncols=3, size=1.3, . 18 img_key=None,. 19 # limit color scale at 99.2% quantile of cell abundance. 20 vmin=0, vmax='p99.2' . 21 ). File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:993, in spatial(adata, basis, img, img_key, library_id, crop_coord, alpha_img, bw, size, scale_factor, spot_size, na_color, show, return_fig, save, **kwargs). 990 cmap_img = None. 991 circle_radius = size * scale_factor * spot_size * 0.5. --> 993 axs = embedding(. 994 adata,. 995 basis=basis,. 996 scale_factor=scale_factor,. 997 size=circle_radius,. 998 na_color=na_color,. 999 show=False,. 1000 save=False,. 1001 **kwargs,. 1002 ). 1003 if not isinstance(axs, list):. 1004 axs = [axs]. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:169, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:966,performance,scale,scale,966,"sc.pl.spatial TypeError: can't multiply sequence by non-int of type 'float'; Hi,. I am getting the following error when trying to plot my spatial data. ```py. with mpl.rc_context({'axes.facecolor': 'black',. 'figure.figsize': [4.5, 5]}):. . sc.pl.spatial(slide, cmap='magma',. color=[""AC-like"", ""Astro"", ""B_T_NK"", ""Inhib.neurons"", ""Micro_Macro"", ""Neurons"", ""OC-like"", ""Oligo"", ""Pericyte""], . ncols=3, size=1.3, . img_key='hires',. # limit color scale at 99.2% quantile of cell abundance. vmin=0, vmax='p99.2' . ). ```. ```pytb. TypeError Traceback (most recent call last). Cell In[11], line 14. 10 # plot in spatial coordinates. 11 with mpl.rc_context({'axes.facecolor': 'black',. 12 'figure.figsize': [4.5, 5]}):. ---> 14 sc.pl.spatial(slide, cmap='magma',. 15 # show first 8 cell types. 16 color=[""AC-like"", ""Astro"", ""B_T_NK"", ""Inhib.neurons"", ""Micro_Macro"", ""Neurons"", ""OC-like"", ""Oligo"", ""Pericyte""], . 17 ncols=3, size=1.3, . 18 img_key=None,. 19 # limit color scale at 99.2% quantile of cell abundance. 20 vmin=0, vmax='p99.2' . 21 ). File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:993, in spatial(adata, basis, img, img_key, library_id, crop_coord, alpha_img, bw, size, scale_factor, spot_size, na_color, show, return_fig, save, **kwargs). 990 cmap_img = None. 991 circle_radius = size * scale_factor * spot_size * 0.5. --> 993 axs = embedding(. 994 adata,. 995 basis=basis,. 996 scale_factor=scale_factor,. 997 size=circle_radius,. 998 na_color=na_color,. 999 show=False,. 1000 save=False,. 1001 **kwargs,. 1002 ). 1003 if not isinstance(axs, list):. 1004 axs = [axs]. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:169, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:3074,performance,scale,scale,3074,"tor,. 997 size=circle_radius,. 998 na_color=na_color,. 999 show=False,. 1000 save=False,. 1001 **kwargs,. 1002 ). 1003 if not isinstance(axs, list):. 1004 axs = [axs]. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:169, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, vcenter, norm, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 165 title = [title] if isinstance(title, str) else list(title). 167 # get the points position and the components list. 168 # (only if components is not None). --> 169 data_points, components_list = _get_data_points(. 170 adata, basis, projection, components, scale_factor. 171 ). 173 # Setup layout. 174 # Most of the code is for the case when multiple plots are required. 175 # 'color' is a list of names that want to be plotted. 176 # Eg. ['Gene1', 'louvain', 'Gene2']. 177 # component_list is a list of components [[0,1], [1,2]]. 178 if (. 179 not isinstance(color, str). 180 and isinstance(color, cabc.Sequence). 181 and len(color) > 1. 182 ) or len(components_list) > 1:. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:1130, in _get_data_points(adata, basis, projection, components, scale_factor). 1127 components_list = []. 1129 if scale_factor is not None: # if basis need scale for img background. -> 1130 data_points[0] = np.multiply(data_points[0], scale_factor). 1132 return data_points, components_list. TypeError: can't multiply sequence by non-int of type 'float'. ```. It looks like the scale.factor for the hires image key is a float: 'tissue_hires_scalef': 0.12813942. Do you know what can be modified so that the data is plottable?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:3296,performance,scale,scale,3296,"tor,. 997 size=circle_radius,. 998 na_color=na_color,. 999 show=False,. 1000 save=False,. 1001 **kwargs,. 1002 ). 1003 if not isinstance(axs, list):. 1004 axs = [axs]. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:169, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, vcenter, norm, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 165 title = [title] if isinstance(title, str) else list(title). 167 # get the points position and the components list. 168 # (only if components is not None). --> 169 data_points, components_list = _get_data_points(. 170 adata, basis, projection, components, scale_factor. 171 ). 173 # Setup layout. 174 # Most of the code is for the case when multiple plots are required. 175 # 'color' is a list of names that want to be plotted. 176 # Eg. ['Gene1', 'louvain', 'Gene2']. 177 # component_list is a list of components [[0,1], [1,2]]. 178 if (. 179 not isinstance(color, str). 180 and isinstance(color, cabc.Sequence). 181 and len(color) > 1. 182 ) or len(components_list) > 1:. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:1130, in _get_data_points(adata, basis, projection, components, scale_factor). 1127 components_list = []. 1129 if scale_factor is not None: # if basis need scale for img background. -> 1130 data_points[0] = np.multiply(data_points[0], scale_factor). 1132 return data_points, components_list. TypeError: can't multiply sequence by non-int of type 'float'. ```. It looks like the scale.factor for the hires image key is a float: 'tissue_hires_scalef': 0.12813942. Do you know what can be modified so that the data is plottable?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:255,reliability,sli,slide,255,"sc.pl.spatial TypeError: can't multiply sequence by non-int of type 'float'; Hi,. I am getting the following error when trying to plot my spatial data. ```py. with mpl.rc_context({'axes.facecolor': 'black',. 'figure.figsize': [4.5, 5]}):. . sc.pl.spatial(slide, cmap='magma',. color=[""AC-like"", ""Astro"", ""B_T_NK"", ""Inhib.neurons"", ""Micro_Macro"", ""Neurons"", ""OC-like"", ""Oligo"", ""Pericyte""], . ncols=3, size=1.3, . img_key='hires',. # limit color scale at 99.2% quantile of cell abundance. vmin=0, vmax='p99.2' . ). ```. ```pytb. TypeError Traceback (most recent call last). Cell In[11], line 14. 10 # plot in spatial coordinates. 11 with mpl.rc_context({'axes.facecolor': 'black',. 12 'figure.figsize': [4.5, 5]}):. ---> 14 sc.pl.spatial(slide, cmap='magma',. 15 # show first 8 cell types. 16 color=[""AC-like"", ""Astro"", ""B_T_NK"", ""Inhib.neurons"", ""Micro_Macro"", ""Neurons"", ""OC-like"", ""Oligo"", ""Pericyte""], . 17 ncols=3, size=1.3, . 18 img_key=None,. 19 # limit color scale at 99.2% quantile of cell abundance. 20 vmin=0, vmax='p99.2' . 21 ). File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:993, in spatial(adata, basis, img, img_key, library_id, crop_coord, alpha_img, bw, size, scale_factor, spot_size, na_color, show, return_fig, save, **kwargs). 990 cmap_img = None. 991 circle_radius = size * scale_factor * spot_size * 0.5. --> 993 axs = embedding(. 994 adata,. 995 basis=basis,. 996 scale_factor=scale_factor,. 997 size=circle_radius,. 998 na_color=na_color,. 999 show=False,. 1000 save=False,. 1001 **kwargs,. 1002 ). 1003 if not isinstance(axs, list):. 1004 axs = [axs]. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:169, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:737,reliability,sli,slide,737,"sc.pl.spatial TypeError: can't multiply sequence by non-int of type 'float'; Hi,. I am getting the following error when trying to plot my spatial data. ```py. with mpl.rc_context({'axes.facecolor': 'black',. 'figure.figsize': [4.5, 5]}):. . sc.pl.spatial(slide, cmap='magma',. color=[""AC-like"", ""Astro"", ""B_T_NK"", ""Inhib.neurons"", ""Micro_Macro"", ""Neurons"", ""OC-like"", ""Oligo"", ""Pericyte""], . ncols=3, size=1.3, . img_key='hires',. # limit color scale at 99.2% quantile of cell abundance. vmin=0, vmax='p99.2' . ). ```. ```pytb. TypeError Traceback (most recent call last). Cell In[11], line 14. 10 # plot in spatial coordinates. 11 with mpl.rc_context({'axes.facecolor': 'black',. 12 'figure.figsize': [4.5, 5]}):. ---> 14 sc.pl.spatial(slide, cmap='magma',. 15 # show first 8 cell types. 16 color=[""AC-like"", ""Astro"", ""B_T_NK"", ""Inhib.neurons"", ""Micro_Macro"", ""Neurons"", ""OC-like"", ""Oligo"", ""Pericyte""], . 17 ncols=3, size=1.3, . 18 img_key=None,. 19 # limit color scale at 99.2% quantile of cell abundance. 20 vmin=0, vmax='p99.2' . 21 ). File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:993, in spatial(adata, basis, img, img_key, library_id, crop_coord, alpha_img, bw, size, scale_factor, spot_size, na_color, show, return_fig, save, **kwargs). 990 cmap_img = None. 991 circle_radius = size * scale_factor * spot_size * 0.5. --> 993 axs = embedding(. 994 adata,. 995 basis=basis,. 996 scale_factor=scale_factor,. 997 size=circle_radius,. 998 na_color=na_color,. 999 show=False,. 1000 save=False,. 1001 **kwargs,. 1002 ). 1003 if not isinstance(axs, list):. 1004 axs = [axs]. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:169, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:109,safety,error,error,109,"sc.pl.spatial TypeError: can't multiply sequence by non-int of type 'float'; Hi,. I am getting the following error when trying to plot my spatial data. ```py. with mpl.rc_context({'axes.facecolor': 'black',. 'figure.figsize': [4.5, 5]}):. . sc.pl.spatial(slide, cmap='magma',. color=[""AC-like"", ""Astro"", ""B_T_NK"", ""Inhib.neurons"", ""Micro_Macro"", ""Neurons"", ""OC-like"", ""Oligo"", ""Pericyte""], . ncols=3, size=1.3, . img_key='hires',. # limit color scale at 99.2% quantile of cell abundance. vmin=0, vmax='p99.2' . ). ```. ```pytb. TypeError Traceback (most recent call last). Cell In[11], line 14. 10 # plot in spatial coordinates. 11 with mpl.rc_context({'axes.facecolor': 'black',. 12 'figure.figsize': [4.5, 5]}):. ---> 14 sc.pl.spatial(slide, cmap='magma',. 15 # show first 8 cell types. 16 color=[""AC-like"", ""Astro"", ""B_T_NK"", ""Inhib.neurons"", ""Micro_Macro"", ""Neurons"", ""OC-like"", ""Oligo"", ""Pericyte""], . 17 ncols=3, size=1.3, . 18 img_key=None,. 19 # limit color scale at 99.2% quantile of cell abundance. 20 vmin=0, vmax='p99.2' . 21 ). File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:993, in spatial(adata, basis, img, img_key, library_id, crop_coord, alpha_img, bw, size, scale_factor, spot_size, na_color, show, return_fig, save, **kwargs). 990 cmap_img = None. 991 circle_radius = size * scale_factor * spot_size * 0.5. --> 993 axs = embedding(. 994 adata,. 995 basis=basis,. 996 scale_factor=scale_factor,. 997 size=circle_radius,. 998 na_color=na_color,. 999 show=False,. 1000 save=False,. 1001 **kwargs,. 1002 ). 1003 if not isinstance(axs, list):. 1004 axs = [axs]. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:169, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:3404,security,modif,modified,3404,"tor,. 997 size=circle_radius,. 998 na_color=na_color,. 999 show=False,. 1000 save=False,. 1001 **kwargs,. 1002 ). 1003 if not isinstance(axs, list):. 1004 axs = [axs]. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:169, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, vcenter, norm, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 165 title = [title] if isinstance(title, str) else list(title). 167 # get the points position and the components list. 168 # (only if components is not None). --> 169 data_points, components_list = _get_data_points(. 170 adata, basis, projection, components, scale_factor. 171 ). 173 # Setup layout. 174 # Most of the code is for the case when multiple plots are required. 175 # 'color' is a list of names that want to be plotted. 176 # Eg. ['Gene1', 'louvain', 'Gene2']. 177 # component_list is a list of components [[0,1], [1,2]]. 178 if (. 179 not isinstance(color, str). 180 and isinstance(color, cabc.Sequence). 181 and len(color) > 1. 182 ) or len(components_list) > 1:. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:1130, in _get_data_points(adata, basis, projection, components, scale_factor). 1127 components_list = []. 1129 if scale_factor is not None: # if basis need scale for img background. -> 1130 data_points[0] = np.multiply(data_points[0], scale_factor). 1132 return data_points, components_list. TypeError: can't multiply sequence by non-int of type 'float'. ```. It looks like the scale.factor for the hires image key is a float: 'tissue_hires_scalef': 0.12813942. Do you know what can be modified so that the data is plottable?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:538,testability,Trace,Traceback,538,"sc.pl.spatial TypeError: can't multiply sequence by non-int of type 'float'; Hi,. I am getting the following error when trying to plot my spatial data. ```py. with mpl.rc_context({'axes.facecolor': 'black',. 'figure.figsize': [4.5, 5]}):. . sc.pl.spatial(slide, cmap='magma',. color=[""AC-like"", ""Astro"", ""B_T_NK"", ""Inhib.neurons"", ""Micro_Macro"", ""Neurons"", ""OC-like"", ""Oligo"", ""Pericyte""], . ncols=3, size=1.3, . img_key='hires',. # limit color scale at 99.2% quantile of cell abundance. vmin=0, vmax='p99.2' . ). ```. ```pytb. TypeError Traceback (most recent call last). Cell In[11], line 14. 10 # plot in spatial coordinates. 11 with mpl.rc_context({'axes.facecolor': 'black',. 12 'figure.figsize': [4.5, 5]}):. ---> 14 sc.pl.spatial(slide, cmap='magma',. 15 # show first 8 cell types. 16 color=[""AC-like"", ""Astro"", ""B_T_NK"", ""Inhib.neurons"", ""Micro_Macro"", ""Neurons"", ""OC-like"", ""Oligo"", ""Pericyte""], . 17 ncols=3, size=1.3, . 18 img_key=None,. 19 # limit color scale at 99.2% quantile of cell abundance. 20 vmin=0, vmax='p99.2' . 21 ). File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:993, in spatial(adata, basis, img, img_key, library_id, crop_coord, alpha_img, bw, size, scale_factor, spot_size, na_color, show, return_fig, save, **kwargs). 990 cmap_img = None. 991 circle_radius = size * scale_factor * spot_size * 0.5. --> 993 axs = embedding(. 994 adata,. 995 basis=basis,. 996 scale_factor=scale_factor,. 997 size=circle_radius,. 998 na_color=na_color,. 999 show=False,. 1000 save=False,. 1001 **kwargs,. 1002 ). 1003 if not isinstance(axs, list):. 1004 axs = [axs]. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:169, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2499:109,usability,error,error,109,"sc.pl.spatial TypeError: can't multiply sequence by non-int of type 'float'; Hi,. I am getting the following error when trying to plot my spatial data. ```py. with mpl.rc_context({'axes.facecolor': 'black',. 'figure.figsize': [4.5, 5]}):. . sc.pl.spatial(slide, cmap='magma',. color=[""AC-like"", ""Astro"", ""B_T_NK"", ""Inhib.neurons"", ""Micro_Macro"", ""Neurons"", ""OC-like"", ""Oligo"", ""Pericyte""], . ncols=3, size=1.3, . img_key='hires',. # limit color scale at 99.2% quantile of cell abundance. vmin=0, vmax='p99.2' . ). ```. ```pytb. TypeError Traceback (most recent call last). Cell In[11], line 14. 10 # plot in spatial coordinates. 11 with mpl.rc_context({'axes.facecolor': 'black',. 12 'figure.figsize': [4.5, 5]}):. ---> 14 sc.pl.spatial(slide, cmap='magma',. 15 # show first 8 cell types. 16 color=[""AC-like"", ""Astro"", ""B_T_NK"", ""Inhib.neurons"", ""Micro_Macro"", ""Neurons"", ""OC-like"", ""Oligo"", ""Pericyte""], . 17 ncols=3, size=1.3, . 18 img_key=None,. 19 # limit color scale at 99.2% quantile of cell abundance. 20 vmin=0, vmax='p99.2' . 21 ). File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:993, in spatial(adata, basis, img, img_key, library_id, crop_coord, alpha_img, bw, size, scale_factor, spot_size, na_color, show, return_fig, save, **kwargs). 990 cmap_img = None. 991 circle_radius = size * scale_factor * spot_size * 0.5. --> 993 axs = embedding(. 994 adata,. 995 basis=basis,. 996 scale_factor=scale_factor,. 997 size=circle_radius,. 998 na_color=na_color,. 999 show=False,. 1000 save=False,. 1001 **kwargs,. 1002 ). 1003 if not isinstance(axs, list):. 1004 axs = [axs]. File ~/.local/lib/python3.9/site-packages/scanpy/plotting/_tools/scatterplots.py:169, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499
https://github.com/scverse/scanpy/issues/2500:190,deployability,version,version,190,"`normalize_per_cell` raises `numpy.core._exceptions.UFuncTypeError`; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import numpy as np. import anndata as ad. import scanpy as sc. adata = ad.AnnData(np.random.poisson(0.2, (100, 200))). sc.pp.normalize_per_cell(adata, 1e6). ```. ```pytb. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/scottgigante/envs/immunaISR/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py"", line 527, in normalize_per_cell. normalize_per_cell(adata.X, counts_per_cell_after, counts_per_cell). File ""/Users/scottgigante/envs/immunaISR/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py"", line 562, in normalize_per_cell. counts_per_cell /= counts_per_cell_after. numpy.core._exceptions.UFuncTypeError: Cannot cast ufunc 'divide' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. cffi 1.15.1. cloudpickle 2.2.1. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. defusedxml 0.7.1. dill 0.3.6. google NA. h5py 3.8.0. importlib_resources NA. joblib 1.3.0.dev0. kiwisolver 1.4.4. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. numba 0.57.0. numpy 1.24.3. packaging 23.1. pandas 2.0.2. psutil 5.9.5. pyarrow 12.0.0. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. six 1.16.0. sklearn 1.2.2. sympy 1.11.1. threadpoolctl 3.1.0. torch 2.0.0. tqdm 4.65.0. typing_extensions NA. yaml 6.0. zipp NA. -----. Python 3.8.10 (v3.8.10:3d8993a744, May 3 2021, 08:55:58) [Clang 6.0 (clang-600.0.57)]. macOS-10.16-x86_64-i386-64bit. -----. Session information updated at 2023-06-05 12:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:616,deployability,modul,module,616,"`normalize_per_cell` raises `numpy.core._exceptions.UFuncTypeError`; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import numpy as np. import anndata as ad. import scanpy as sc. adata = ad.AnnData(np.random.poisson(0.2, (100, 200))). sc.pp.normalize_per_cell(adata, 1e6). ```. ```pytb. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/scottgigante/envs/immunaISR/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py"", line 527, in normalize_per_cell. normalize_per_cell(adata.X, counts_per_cell_after, counts_per_cell). File ""/Users/scottgigante/envs/immunaISR/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py"", line 562, in normalize_per_cell. counts_per_cell /= counts_per_cell_after. numpy.core._exceptions.UFuncTypeError: Cannot cast ufunc 'divide' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. cffi 1.15.1. cloudpickle 2.2.1. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. defusedxml 0.7.1. dill 0.3.6. google NA. h5py 3.8.0. importlib_resources NA. joblib 1.3.0.dev0. kiwisolver 1.4.4. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. numba 0.57.0. numpy 1.24.3. packaging 23.1. pandas 2.0.2. psutil 5.9.5. pyarrow 12.0.0. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. six 1.16.0. sklearn 1.2.2. sympy 1.11.1. threadpoolctl 3.1.0. torch 2.0.0. tqdm 4.65.0. typing_extensions NA. yaml 6.0. zipp NA. -----. Python 3.8.10 (v3.8.10:3d8993a744, May 3 2021, 08:55:58) [Clang 6.0 (clang-600.0.57)]. macOS-10.16-x86_64-i386-64bit. -----. Session information updated at 2023-06-05 12:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:1162,deployability,Version,Versions,1162,"`normalize_per_cell` raises `numpy.core._exceptions.UFuncTypeError`; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import numpy as np. import anndata as ad. import scanpy as sc. adata = ad.AnnData(np.random.poisson(0.2, (100, 200))). sc.pp.normalize_per_cell(adata, 1e6). ```. ```pytb. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/scottgigante/envs/immunaISR/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py"", line 527, in normalize_per_cell. normalize_per_cell(adata.X, counts_per_cell_after, counts_per_cell). File ""/Users/scottgigante/envs/immunaISR/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py"", line 562, in normalize_per_cell. counts_per_cell /= counts_per_cell_after. numpy.core._exceptions.UFuncTypeError: Cannot cast ufunc 'divide' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. cffi 1.15.1. cloudpickle 2.2.1. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. defusedxml 0.7.1. dill 0.3.6. google NA. h5py 3.8.0. importlib_resources NA. joblib 1.3.0.dev0. kiwisolver 1.4.4. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. numba 0.57.0. numpy 1.24.3. packaging 23.1. pandas 2.0.2. psutil 5.9.5. pyarrow 12.0.0. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. six 1.16.0. sklearn 1.2.2. sympy 1.11.1. threadpoolctl 3.1.0. torch 2.0.0. tqdm 4.65.0. typing_extensions NA. yaml 6.0. zipp NA. -----. Python 3.8.10 (v3.8.10:3d8993a744, May 3 2021, 08:55:58) [Clang 6.0 (clang-600.0.57)]. macOS-10.16-x86_64-i386-64bit. -----. Session information updated at 2023-06-05 12:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:1952,deployability,updat,updated,1952,"`normalize_per_cell` raises `numpy.core._exceptions.UFuncTypeError`; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import numpy as np. import anndata as ad. import scanpy as sc. adata = ad.AnnData(np.random.poisson(0.2, (100, 200))). sc.pp.normalize_per_cell(adata, 1e6). ```. ```pytb. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/scottgigante/envs/immunaISR/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py"", line 527, in normalize_per_cell. normalize_per_cell(adata.X, counts_per_cell_after, counts_per_cell). File ""/Users/scottgigante/envs/immunaISR/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py"", line 562, in normalize_per_cell. counts_per_cell /= counts_per_cell_after. numpy.core._exceptions.UFuncTypeError: Cannot cast ufunc 'divide' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. cffi 1.15.1. cloudpickle 2.2.1. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. defusedxml 0.7.1. dill 0.3.6. google NA. h5py 3.8.0. importlib_resources NA. joblib 1.3.0.dev0. kiwisolver 1.4.4. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. numba 0.57.0. numpy 1.24.3. packaging 23.1. pandas 2.0.2. psutil 5.9.5. pyarrow 12.0.0. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. six 1.16.0. sklearn 1.2.2. sympy 1.11.1. threadpoolctl 3.1.0. torch 2.0.0. tqdm 4.65.0. typing_extensions NA. yaml 6.0. zipp NA. -----. Python 3.8.10 (v3.8.10:3d8993a744, May 3 2021, 08:55:58) [Clang 6.0 (clang-600.0.57)]. macOS-10.16-x86_64-i386-64bit. -----. Session information updated at 2023-06-05 12:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:35,energy efficiency,core,core,35,"`normalize_per_cell` raises `numpy.core._exceptions.UFuncTypeError`; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import numpy as np. import anndata as ad. import scanpy as sc. adata = ad.AnnData(np.random.poisson(0.2, (100, 200))). sc.pp.normalize_per_cell(adata, 1e6). ```. ```pytb. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/scottgigante/envs/immunaISR/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py"", line 527, in normalize_per_cell. normalize_per_cell(adata.X, counts_per_cell_after, counts_per_cell). File ""/Users/scottgigante/envs/immunaISR/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py"", line 562, in normalize_per_cell. counts_per_cell /= counts_per_cell_after. numpy.core._exceptions.UFuncTypeError: Cannot cast ufunc 'divide' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. cffi 1.15.1. cloudpickle 2.2.1. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. defusedxml 0.7.1. dill 0.3.6. google NA. h5py 3.8.0. importlib_resources NA. joblib 1.3.0.dev0. kiwisolver 1.4.4. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. numba 0.57.0. numpy 1.24.3. packaging 23.1. pandas 2.0.2. psutil 5.9.5. pyarrow 12.0.0. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. six 1.16.0. sklearn 1.2.2. sympy 1.11.1. threadpoolctl 3.1.0. torch 2.0.0. tqdm 4.65.0. typing_extensions NA. yaml 6.0. zipp NA. -----. Python 3.8.10 (v3.8.10:3d8993a744, May 3 2021, 08:55:58) [Clang 6.0 (clang-600.0.57)]. macOS-10.16-x86_64-i386-64bit. -----. Session information updated at 2023-06-05 12:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:1014,energy efficiency,core,core,1014,"`normalize_per_cell` raises `numpy.core._exceptions.UFuncTypeError`; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import numpy as np. import anndata as ad. import scanpy as sc. adata = ad.AnnData(np.random.poisson(0.2, (100, 200))). sc.pp.normalize_per_cell(adata, 1e6). ```. ```pytb. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/scottgigante/envs/immunaISR/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py"", line 527, in normalize_per_cell. normalize_per_cell(adata.X, counts_per_cell_after, counts_per_cell). File ""/Users/scottgigante/envs/immunaISR/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py"", line 562, in normalize_per_cell. counts_per_cell /= counts_per_cell_after. numpy.core._exceptions.UFuncTypeError: Cannot cast ufunc 'divide' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. cffi 1.15.1. cloudpickle 2.2.1. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. defusedxml 0.7.1. dill 0.3.6. google NA. h5py 3.8.0. importlib_resources NA. joblib 1.3.0.dev0. kiwisolver 1.4.4. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. numba 0.57.0. numpy 1.24.3. packaging 23.1. pandas 2.0.2. psutil 5.9.5. pyarrow 12.0.0. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. six 1.16.0. sklearn 1.2.2. sympy 1.11.1. threadpoolctl 3.1.0. torch 2.0.0. tqdm 4.65.0. typing_extensions NA. yaml 6.0. zipp NA. -----. Python 3.8.10 (v3.8.10:3d8993a744, May 3 2021, 08:55:58) [Clang 6.0 (clang-600.0.57)]. macOS-10.16-x86_64-i386-64bit. -----. Session information updated at 2023-06-05 12:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:1255,energy efficiency,cloud,cloudpickle,1255,"`normalize_per_cell` raises `numpy.core._exceptions.UFuncTypeError`; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import numpy as np. import anndata as ad. import scanpy as sc. adata = ad.AnnData(np.random.poisson(0.2, (100, 200))). sc.pp.normalize_per_cell(adata, 1e6). ```. ```pytb. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/scottgigante/envs/immunaISR/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py"", line 527, in normalize_per_cell. normalize_per_cell(adata.X, counts_per_cell_after, counts_per_cell). File ""/Users/scottgigante/envs/immunaISR/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py"", line 562, in normalize_per_cell. counts_per_cell /= counts_per_cell_after. numpy.core._exceptions.UFuncTypeError: Cannot cast ufunc 'divide' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. cffi 1.15.1. cloudpickle 2.2.1. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. defusedxml 0.7.1. dill 0.3.6. google NA. h5py 3.8.0. importlib_resources NA. joblib 1.3.0.dev0. kiwisolver 1.4.4. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. numba 0.57.0. numpy 1.24.3. packaging 23.1. pandas 2.0.2. psutil 5.9.5. pyarrow 12.0.0. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. six 1.16.0. sklearn 1.2.2. sympy 1.11.1. threadpoolctl 3.1.0. torch 2.0.0. tqdm 4.65.0. typing_extensions NA. yaml 6.0. zipp NA. -----. Python 3.8.10 (v3.8.10:3d8993a744, May 3 2021, 08:55:58) [Clang 6.0 (clang-600.0.57)]. macOS-10.16-x86_64-i386-64bit. -----. Session information updated at 2023-06-05 12:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:190,integrability,version,version,190,"`normalize_per_cell` raises `numpy.core._exceptions.UFuncTypeError`; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import numpy as np. import anndata as ad. import scanpy as sc. adata = ad.AnnData(np.random.poisson(0.2, (100, 200))). sc.pp.normalize_per_cell(adata, 1e6). ```. ```pytb. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/scottgigante/envs/immunaISR/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py"", line 527, in normalize_per_cell. normalize_per_cell(adata.X, counts_per_cell_after, counts_per_cell). File ""/Users/scottgigante/envs/immunaISR/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py"", line 562, in normalize_per_cell. counts_per_cell /= counts_per_cell_after. numpy.core._exceptions.UFuncTypeError: Cannot cast ufunc 'divide' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. cffi 1.15.1. cloudpickle 2.2.1. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. defusedxml 0.7.1. dill 0.3.6. google NA. h5py 3.8.0. importlib_resources NA. joblib 1.3.0.dev0. kiwisolver 1.4.4. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. numba 0.57.0. numpy 1.24.3. packaging 23.1. pandas 2.0.2. psutil 5.9.5. pyarrow 12.0.0. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. six 1.16.0. sklearn 1.2.2. sympy 1.11.1. threadpoolctl 3.1.0. torch 2.0.0. tqdm 4.65.0. typing_extensions NA. yaml 6.0. zipp NA. -----. Python 3.8.10 (v3.8.10:3d8993a744, May 3 2021, 08:55:58) [Clang 6.0 (clang-600.0.57)]. macOS-10.16-x86_64-i386-64bit. -----. Session information updated at 2023-06-05 12:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:1162,integrability,Version,Versions,1162,"`normalize_per_cell` raises `numpy.core._exceptions.UFuncTypeError`; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import numpy as np. import anndata as ad. import scanpy as sc. adata = ad.AnnData(np.random.poisson(0.2, (100, 200))). sc.pp.normalize_per_cell(adata, 1e6). ```. ```pytb. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/scottgigante/envs/immunaISR/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py"", line 527, in normalize_per_cell. normalize_per_cell(adata.X, counts_per_cell_after, counts_per_cell). File ""/Users/scottgigante/envs/immunaISR/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py"", line 562, in normalize_per_cell. counts_per_cell /= counts_per_cell_after. numpy.core._exceptions.UFuncTypeError: Cannot cast ufunc 'divide' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. cffi 1.15.1. cloudpickle 2.2.1. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. defusedxml 0.7.1. dill 0.3.6. google NA. h5py 3.8.0. importlib_resources NA. joblib 1.3.0.dev0. kiwisolver 1.4.4. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. numba 0.57.0. numpy 1.24.3. packaging 23.1. pandas 2.0.2. psutil 5.9.5. pyarrow 12.0.0. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. six 1.16.0. sklearn 1.2.2. sympy 1.11.1. threadpoolctl 3.1.0. torch 2.0.0. tqdm 4.65.0. typing_extensions NA. yaml 6.0. zipp NA. -----. Python 3.8.10 (v3.8.10:3d8993a744, May 3 2021, 08:55:58) [Clang 6.0 (clang-600.0.57)]. macOS-10.16-x86_64-i386-64bit. -----. Session information updated at 2023-06-05 12:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:190,modifiability,version,version,190,"`normalize_per_cell` raises `numpy.core._exceptions.UFuncTypeError`; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import numpy as np. import anndata as ad. import scanpy as sc. adata = ad.AnnData(np.random.poisson(0.2, (100, 200))). sc.pp.normalize_per_cell(adata, 1e6). ```. ```pytb. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/scottgigante/envs/immunaISR/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py"", line 527, in normalize_per_cell. normalize_per_cell(adata.X, counts_per_cell_after, counts_per_cell). File ""/Users/scottgigante/envs/immunaISR/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py"", line 562, in normalize_per_cell. counts_per_cell /= counts_per_cell_after. numpy.core._exceptions.UFuncTypeError: Cannot cast ufunc 'divide' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. cffi 1.15.1. cloudpickle 2.2.1. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. defusedxml 0.7.1. dill 0.3.6. google NA. h5py 3.8.0. importlib_resources NA. joblib 1.3.0.dev0. kiwisolver 1.4.4. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. numba 0.57.0. numpy 1.24.3. packaging 23.1. pandas 2.0.2. psutil 5.9.5. pyarrow 12.0.0. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. six 1.16.0. sklearn 1.2.2. sympy 1.11.1. threadpoolctl 3.1.0. torch 2.0.0. tqdm 4.65.0. typing_extensions NA. yaml 6.0. zipp NA. -----. Python 3.8.10 (v3.8.10:3d8993a744, May 3 2021, 08:55:58) [Clang 6.0 (clang-600.0.57)]. macOS-10.16-x86_64-i386-64bit. -----. Session information updated at 2023-06-05 12:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:616,modifiability,modul,module,616,"`normalize_per_cell` raises `numpy.core._exceptions.UFuncTypeError`; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import numpy as np. import anndata as ad. import scanpy as sc. adata = ad.AnnData(np.random.poisson(0.2, (100, 200))). sc.pp.normalize_per_cell(adata, 1e6). ```. ```pytb. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/scottgigante/envs/immunaISR/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py"", line 527, in normalize_per_cell. normalize_per_cell(adata.X, counts_per_cell_after, counts_per_cell). File ""/Users/scottgigante/envs/immunaISR/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py"", line 562, in normalize_per_cell. counts_per_cell /= counts_per_cell_after. numpy.core._exceptions.UFuncTypeError: Cannot cast ufunc 'divide' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. cffi 1.15.1. cloudpickle 2.2.1. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. defusedxml 0.7.1. dill 0.3.6. google NA. h5py 3.8.0. importlib_resources NA. joblib 1.3.0.dev0. kiwisolver 1.4.4. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. numba 0.57.0. numpy 1.24.3. packaging 23.1. pandas 2.0.2. psutil 5.9.5. pyarrow 12.0.0. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. six 1.16.0. sklearn 1.2.2. sympy 1.11.1. threadpoolctl 3.1.0. torch 2.0.0. tqdm 4.65.0. typing_extensions NA. yaml 6.0. zipp NA. -----. Python 3.8.10 (v3.8.10:3d8993a744, May 3 2021, 08:55:58) [Clang 6.0 (clang-600.0.57)]. macOS-10.16-x86_64-i386-64bit. -----. Session information updated at 2023-06-05 12:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:685,modifiability,pac,packages,685,"`normalize_per_cell` raises `numpy.core._exceptions.UFuncTypeError`; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import numpy as np. import anndata as ad. import scanpy as sc. adata = ad.AnnData(np.random.poisson(0.2, (100, 200))). sc.pp.normalize_per_cell(adata, 1e6). ```. ```pytb. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/scottgigante/envs/immunaISR/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py"", line 527, in normalize_per_cell. normalize_per_cell(adata.X, counts_per_cell_after, counts_per_cell). File ""/Users/scottgigante/envs/immunaISR/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py"", line 562, in normalize_per_cell. counts_per_cell /= counts_per_cell_after. numpy.core._exceptions.UFuncTypeError: Cannot cast ufunc 'divide' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. cffi 1.15.1. cloudpickle 2.2.1. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. defusedxml 0.7.1. dill 0.3.6. google NA. h5py 3.8.0. importlib_resources NA. joblib 1.3.0.dev0. kiwisolver 1.4.4. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. numba 0.57.0. numpy 1.24.3. packaging 23.1. pandas 2.0.2. psutil 5.9.5. pyarrow 12.0.0. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. six 1.16.0. sklearn 1.2.2. sympy 1.11.1. threadpoolctl 3.1.0. torch 2.0.0. tqdm 4.65.0. typing_extensions NA. yaml 6.0. zipp NA. -----. Python 3.8.10 (v3.8.10:3d8993a744, May 3 2021, 08:55:58) [Clang 6.0 (clang-600.0.57)]. macOS-10.16-x86_64-i386-64bit. -----. Session information updated at 2023-06-05 12:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:890,modifiability,pac,packages,890,"`normalize_per_cell` raises `numpy.core._exceptions.UFuncTypeError`; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import numpy as np. import anndata as ad. import scanpy as sc. adata = ad.AnnData(np.random.poisson(0.2, (100, 200))). sc.pp.normalize_per_cell(adata, 1e6). ```. ```pytb. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/scottgigante/envs/immunaISR/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py"", line 527, in normalize_per_cell. normalize_per_cell(adata.X, counts_per_cell_after, counts_per_cell). File ""/Users/scottgigante/envs/immunaISR/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py"", line 562, in normalize_per_cell. counts_per_cell /= counts_per_cell_after. numpy.core._exceptions.UFuncTypeError: Cannot cast ufunc 'divide' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. cffi 1.15.1. cloudpickle 2.2.1. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. defusedxml 0.7.1. dill 0.3.6. google NA. h5py 3.8.0. importlib_resources NA. joblib 1.3.0.dev0. kiwisolver 1.4.4. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. numba 0.57.0. numpy 1.24.3. packaging 23.1. pandas 2.0.2. psutil 5.9.5. pyarrow 12.0.0. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. six 1.16.0. sklearn 1.2.2. sympy 1.11.1. threadpoolctl 3.1.0. torch 2.0.0. tqdm 4.65.0. typing_extensions NA. yaml 6.0. zipp NA. -----. Python 3.8.10 (v3.8.10:3d8993a744, May 3 2021, 08:55:58) [Clang 6.0 (clang-600.0.57)]. macOS-10.16-x86_64-i386-64bit. -----. Session information updated at 2023-06-05 12:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:1162,modifiability,Version,Versions,1162,"`normalize_per_cell` raises `numpy.core._exceptions.UFuncTypeError`; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import numpy as np. import anndata as ad. import scanpy as sc. adata = ad.AnnData(np.random.poisson(0.2, (100, 200))). sc.pp.normalize_per_cell(adata, 1e6). ```. ```pytb. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/scottgigante/envs/immunaISR/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py"", line 527, in normalize_per_cell. normalize_per_cell(adata.X, counts_per_cell_after, counts_per_cell). File ""/Users/scottgigante/envs/immunaISR/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py"", line 562, in normalize_per_cell. counts_per_cell /= counts_per_cell_after. numpy.core._exceptions.UFuncTypeError: Cannot cast ufunc 'divide' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. cffi 1.15.1. cloudpickle 2.2.1. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. defusedxml 0.7.1. dill 0.3.6. google NA. h5py 3.8.0. importlib_resources NA. joblib 1.3.0.dev0. kiwisolver 1.4.4. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. numba 0.57.0. numpy 1.24.3. packaging 23.1. pandas 2.0.2. psutil 5.9.5. pyarrow 12.0.0. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. six 1.16.0. sklearn 1.2.2. sympy 1.11.1. threadpoolctl 3.1.0. torch 2.0.0. tqdm 4.65.0. typing_extensions NA. yaml 6.0. zipp NA. -----. Python 3.8.10 (v3.8.10:3d8993a744, May 3 2021, 08:55:58) [Clang 6.0 (clang-600.0.57)]. macOS-10.16-x86_64-i386-64bit. -----. Session information updated at 2023-06-05 12:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:1547,modifiability,pac,packaging,1547,"`normalize_per_cell` raises `numpy.core._exceptions.UFuncTypeError`; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import numpy as np. import anndata as ad. import scanpy as sc. adata = ad.AnnData(np.random.poisson(0.2, (100, 200))). sc.pp.normalize_per_cell(adata, 1e6). ```. ```pytb. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/scottgigante/envs/immunaISR/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py"", line 527, in normalize_per_cell. normalize_per_cell(adata.X, counts_per_cell_after, counts_per_cell). File ""/Users/scottgigante/envs/immunaISR/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py"", line 562, in normalize_per_cell. counts_per_cell /= counts_per_cell_after. numpy.core._exceptions.UFuncTypeError: Cannot cast ufunc 'divide' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. cffi 1.15.1. cloudpickle 2.2.1. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. defusedxml 0.7.1. dill 0.3.6. google NA. h5py 3.8.0. importlib_resources NA. joblib 1.3.0.dev0. kiwisolver 1.4.4. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. numba 0.57.0. numpy 1.24.3. packaging 23.1. pandas 2.0.2. psutil 5.9.5. pyarrow 12.0.0. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. six 1.16.0. sklearn 1.2.2. sympy 1.11.1. threadpoolctl 3.1.0. torch 2.0.0. tqdm 4.65.0. typing_extensions NA. yaml 6.0. zipp NA. -----. Python 3.8.10 (v3.8.10:3d8993a744, May 3 2021, 08:55:58) [Clang 6.0 (clang-600.0.57)]. macOS-10.16-x86_64-i386-64bit. -----. Session information updated at 2023-06-05 12:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:473,reliability,poisson,poisson,473,"`normalize_per_cell` raises `numpy.core._exceptions.UFuncTypeError`; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import numpy as np. import anndata as ad. import scanpy as sc. adata = ad.AnnData(np.random.poisson(0.2, (100, 200))). sc.pp.normalize_per_cell(adata, 1e6). ```. ```pytb. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/scottgigante/envs/immunaISR/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py"", line 527, in normalize_per_cell. normalize_per_cell(adata.X, counts_per_cell_after, counts_per_cell). File ""/Users/scottgigante/envs/immunaISR/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py"", line 562, in normalize_per_cell. counts_per_cell /= counts_per_cell_after. numpy.core._exceptions.UFuncTypeError: Cannot cast ufunc 'divide' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. cffi 1.15.1. cloudpickle 2.2.1. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. defusedxml 0.7.1. dill 0.3.6. google NA. h5py 3.8.0. importlib_resources NA. joblib 1.3.0.dev0. kiwisolver 1.4.4. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. numba 0.57.0. numpy 1.24.3. packaging 23.1. pandas 2.0.2. psutil 5.9.5. pyarrow 12.0.0. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. six 1.16.0. sklearn 1.2.2. sympy 1.11.1. threadpoolctl 3.1.0. torch 2.0.0. tqdm 4.65.0. typing_extensions NA. yaml 6.0. zipp NA. -----. Python 3.8.10 (v3.8.10:3d8993a744, May 3 2021, 08:55:58) [Clang 6.0 (clang-600.0.57)]. macOS-10.16-x86_64-i386-64bit. -----. Session information updated at 2023-06-05 12:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:616,safety,modul,module,616,"`normalize_per_cell` raises `numpy.core._exceptions.UFuncTypeError`; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import numpy as np. import anndata as ad. import scanpy as sc. adata = ad.AnnData(np.random.poisson(0.2, (100, 200))). sc.pp.normalize_per_cell(adata, 1e6). ```. ```pytb. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/scottgigante/envs/immunaISR/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py"", line 527, in normalize_per_cell. normalize_per_cell(adata.X, counts_per_cell_after, counts_per_cell). File ""/Users/scottgigante/envs/immunaISR/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py"", line 562, in normalize_per_cell. counts_per_cell /= counts_per_cell_after. numpy.core._exceptions.UFuncTypeError: Cannot cast ufunc 'divide' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. cffi 1.15.1. cloudpickle 2.2.1. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. defusedxml 0.7.1. dill 0.3.6. google NA. h5py 3.8.0. importlib_resources NA. joblib 1.3.0.dev0. kiwisolver 1.4.4. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. numba 0.57.0. numpy 1.24.3. packaging 23.1. pandas 2.0.2. psutil 5.9.5. pyarrow 12.0.0. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. six 1.16.0. sklearn 1.2.2. sympy 1.11.1. threadpoolctl 3.1.0. torch 2.0.0. tqdm 4.65.0. typing_extensions NA. yaml 6.0. zipp NA. -----. Python 3.8.10 (v3.8.10:3d8993a744, May 3 2021, 08:55:58) [Clang 6.0 (clang-600.0.57)]. macOS-10.16-x86_64-i386-64bit. -----. Session information updated at 2023-06-05 12:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:1952,safety,updat,updated,1952,"`normalize_per_cell` raises `numpy.core._exceptions.UFuncTypeError`; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import numpy as np. import anndata as ad. import scanpy as sc. adata = ad.AnnData(np.random.poisson(0.2, (100, 200))). sc.pp.normalize_per_cell(adata, 1e6). ```. ```pytb. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/scottgigante/envs/immunaISR/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py"", line 527, in normalize_per_cell. normalize_per_cell(adata.X, counts_per_cell_after, counts_per_cell). File ""/Users/scottgigante/envs/immunaISR/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py"", line 562, in normalize_per_cell. counts_per_cell /= counts_per_cell_after. numpy.core._exceptions.UFuncTypeError: Cannot cast ufunc 'divide' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. cffi 1.15.1. cloudpickle 2.2.1. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. defusedxml 0.7.1. dill 0.3.6. google NA. h5py 3.8.0. importlib_resources NA. joblib 1.3.0.dev0. kiwisolver 1.4.4. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. numba 0.57.0. numpy 1.24.3. packaging 23.1. pandas 2.0.2. psutil 5.9.5. pyarrow 12.0.0. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. six 1.16.0. sklearn 1.2.2. sympy 1.11.1. threadpoolctl 3.1.0. torch 2.0.0. tqdm 4.65.0. typing_extensions NA. yaml 6.0. zipp NA. -----. Python 3.8.10 (v3.8.10:3d8993a744, May 3 2021, 08:55:58) [Clang 6.0 (clang-600.0.57)]. macOS-10.16-x86_64-i386-64bit. -----. Session information updated at 2023-06-05 12:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:1932,security,Session,Session,1932,"`normalize_per_cell` raises `numpy.core._exceptions.UFuncTypeError`; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import numpy as np. import anndata as ad. import scanpy as sc. adata = ad.AnnData(np.random.poisson(0.2, (100, 200))). sc.pp.normalize_per_cell(adata, 1e6). ```. ```pytb. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/scottgigante/envs/immunaISR/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py"", line 527, in normalize_per_cell. normalize_per_cell(adata.X, counts_per_cell_after, counts_per_cell). File ""/Users/scottgigante/envs/immunaISR/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py"", line 562, in normalize_per_cell. counts_per_cell /= counts_per_cell_after. numpy.core._exceptions.UFuncTypeError: Cannot cast ufunc 'divide' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. cffi 1.15.1. cloudpickle 2.2.1. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. defusedxml 0.7.1. dill 0.3.6. google NA. h5py 3.8.0. importlib_resources NA. joblib 1.3.0.dev0. kiwisolver 1.4.4. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. numba 0.57.0. numpy 1.24.3. packaging 23.1. pandas 2.0.2. psutil 5.9.5. pyarrow 12.0.0. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. six 1.16.0. sklearn 1.2.2. sympy 1.11.1. threadpoolctl 3.1.0. torch 2.0.0. tqdm 4.65.0. typing_extensions NA. yaml 6.0. zipp NA. -----. Python 3.8.10 (v3.8.10:3d8993a744, May 3 2021, 08:55:58) [Clang 6.0 (clang-600.0.57)]. macOS-10.16-x86_64-i386-64bit. -----. Session information updated at 2023-06-05 12:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:1952,security,updat,updated,1952,"`normalize_per_cell` raises `numpy.core._exceptions.UFuncTypeError`; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import numpy as np. import anndata as ad. import scanpy as sc. adata = ad.AnnData(np.random.poisson(0.2, (100, 200))). sc.pp.normalize_per_cell(adata, 1e6). ```. ```pytb. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/scottgigante/envs/immunaISR/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py"", line 527, in normalize_per_cell. normalize_per_cell(adata.X, counts_per_cell_after, counts_per_cell). File ""/Users/scottgigante/envs/immunaISR/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py"", line 562, in normalize_per_cell. counts_per_cell /= counts_per_cell_after. numpy.core._exceptions.UFuncTypeError: Cannot cast ufunc 'divide' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. cffi 1.15.1. cloudpickle 2.2.1. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. defusedxml 0.7.1. dill 0.3.6. google NA. h5py 3.8.0. importlib_resources NA. joblib 1.3.0.dev0. kiwisolver 1.4.4. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. numba 0.57.0. numpy 1.24.3. packaging 23.1. pandas 2.0.2. psutil 5.9.5. pyarrow 12.0.0. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. six 1.16.0. sklearn 1.2.2. sympy 1.11.1. threadpoolctl 3.1.0. torch 2.0.0. tqdm 4.65.0. typing_extensions NA. yaml 6.0. zipp NA. -----. Python 3.8.10 (v3.8.10:3d8993a744, May 3 2021, 08:55:58) [Clang 6.0 (clang-600.0.57)]. macOS-10.16-x86_64-i386-64bit. -----. Session information updated at 2023-06-05 12:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:552,testability,Trace,Traceback,552,"`normalize_per_cell` raises `numpy.core._exceptions.UFuncTypeError`; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import numpy as np. import anndata as ad. import scanpy as sc. adata = ad.AnnData(np.random.poisson(0.2, (100, 200))). sc.pp.normalize_per_cell(adata, 1e6). ```. ```pytb. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/scottgigante/envs/immunaISR/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py"", line 527, in normalize_per_cell. normalize_per_cell(adata.X, counts_per_cell_after, counts_per_cell). File ""/Users/scottgigante/envs/immunaISR/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py"", line 562, in normalize_per_cell. counts_per_cell /= counts_per_cell_after. numpy.core._exceptions.UFuncTypeError: Cannot cast ufunc 'divide' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. cffi 1.15.1. cloudpickle 2.2.1. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. defusedxml 0.7.1. dill 0.3.6. google NA. h5py 3.8.0. importlib_resources NA. joblib 1.3.0.dev0. kiwisolver 1.4.4. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. numba 0.57.0. numpy 1.24.3. packaging 23.1. pandas 2.0.2. psutil 5.9.5. pyarrow 12.0.0. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. six 1.16.0. sklearn 1.2.2. sympy 1.11.1. threadpoolctl 3.1.0. torch 2.0.0. tqdm 4.65.0. typing_extensions NA. yaml 6.0. zipp NA. -----. Python 3.8.10 (v3.8.10:3d8993a744, May 3 2021, 08:55:58) [Clang 6.0 (clang-600.0.57)]. macOS-10.16-x86_64-i386-64bit. -----. Session information updated at 2023-06-05 12:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:150,usability,confirm,confirmed,150,"`normalize_per_cell` raises `numpy.core._exceptions.UFuncTypeError`; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import numpy as np. import anndata as ad. import scanpy as sc. adata = ad.AnnData(np.random.poisson(0.2, (100, 200))). sc.pp.normalize_per_cell(adata, 1e6). ```. ```pytb. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/scottgigante/envs/immunaISR/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py"", line 527, in normalize_per_cell. normalize_per_cell(adata.X, counts_per_cell_after, counts_per_cell). File ""/Users/scottgigante/envs/immunaISR/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py"", line 562, in normalize_per_cell. counts_per_cell /= counts_per_cell_after. numpy.core._exceptions.UFuncTypeError: Cannot cast ufunc 'divide' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. cffi 1.15.1. cloudpickle 2.2.1. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. defusedxml 0.7.1. dill 0.3.6. google NA. h5py 3.8.0. importlib_resources NA. joblib 1.3.0.dev0. kiwisolver 1.4.4. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. numba 0.57.0. numpy 1.24.3. packaging 23.1. pandas 2.0.2. psutil 5.9.5. pyarrow 12.0.0. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. six 1.16.0. sklearn 1.2.2. sympy 1.11.1. threadpoolctl 3.1.0. torch 2.0.0. tqdm 4.65.0. typing_extensions NA. yaml 6.0. zipp NA. -----. Python 3.8.10 (v3.8.10:3d8993a744, May 3 2021, 08:55:58) [Clang 6.0 (clang-600.0.57)]. macOS-10.16-x86_64-i386-64bit. -----. Session information updated at 2023-06-05 12:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:233,usability,confirm,confirmed,233,"`normalize_per_cell` raises `numpy.core._exceptions.UFuncTypeError`; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import numpy as np. import anndata as ad. import scanpy as sc. adata = ad.AnnData(np.random.poisson(0.2, (100, 200))). sc.pp.normalize_per_cell(adata, 1e6). ```. ```pytb. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/scottgigante/envs/immunaISR/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py"", line 527, in normalize_per_cell. normalize_per_cell(adata.X, counts_per_cell_after, counts_per_cell). File ""/Users/scottgigante/envs/immunaISR/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py"", line 562, in normalize_per_cell. counts_per_cell /= counts_per_cell_after. numpy.core._exceptions.UFuncTypeError: Cannot cast ufunc 'divide' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. cffi 1.15.1. cloudpickle 2.2.1. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. defusedxml 0.7.1. dill 0.3.6. google NA. h5py 3.8.0. importlib_resources NA. joblib 1.3.0.dev0. kiwisolver 1.4.4. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. numba 0.57.0. numpy 1.24.3. packaging 23.1. pandas 2.0.2. psutil 5.9.5. pyarrow 12.0.0. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. six 1.16.0. sklearn 1.2.2. sympy 1.11.1. threadpoolctl 3.1.0. torch 2.0.0. tqdm 4.65.0. typing_extensions NA. yaml 6.0. zipp NA. -----. Python 3.8.10 (v3.8.10:3d8993a744, May 3 2021, 08:55:58) [Clang 6.0 (clang-600.0.57)]. macOS-10.16-x86_64-i386-64bit. -----. Session information updated at 2023-06-05 12:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:300,usability,Minim,Minimal,300,"`normalize_per_cell` raises `numpy.core._exceptions.UFuncTypeError`; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import numpy as np. import anndata as ad. import scanpy as sc. adata = ad.AnnData(np.random.poisson(0.2, (100, 200))). sc.pp.normalize_per_cell(adata, 1e6). ```. ```pytb. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/scottgigante/envs/immunaISR/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py"", line 527, in normalize_per_cell. normalize_per_cell(adata.X, counts_per_cell_after, counts_per_cell). File ""/Users/scottgigante/envs/immunaISR/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py"", line 562, in normalize_per_cell. counts_per_cell /= counts_per_cell_after. numpy.core._exceptions.UFuncTypeError: Cannot cast ufunc 'divide' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. cffi 1.15.1. cloudpickle 2.2.1. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. defusedxml 0.7.1. dill 0.3.6. google NA. h5py 3.8.0. importlib_resources NA. joblib 1.3.0.dev0. kiwisolver 1.4.4. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. numba 0.57.0. numpy 1.24.3. packaging 23.1. pandas 2.0.2. psutil 5.9.5. pyarrow 12.0.0. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. six 1.16.0. sklearn 1.2.2. sympy 1.11.1. threadpoolctl 3.1.0. torch 2.0.0. tqdm 4.65.0. typing_extensions NA. yaml 6.0. zipp NA. -----. Python 3.8.10 (v3.8.10:3d8993a744, May 3 2021, 08:55:58) [Clang 6.0 (clang-600.0.57)]. macOS-10.16-x86_64-i386-64bit. -----. Session information updated at 2023-06-05 12:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:632,usability,User,Users,632,"`normalize_per_cell` raises `numpy.core._exceptions.UFuncTypeError`; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import numpy as np. import anndata as ad. import scanpy as sc. adata = ad.AnnData(np.random.poisson(0.2, (100, 200))). sc.pp.normalize_per_cell(adata, 1e6). ```. ```pytb. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/scottgigante/envs/immunaISR/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py"", line 527, in normalize_per_cell. normalize_per_cell(adata.X, counts_per_cell_after, counts_per_cell). File ""/Users/scottgigante/envs/immunaISR/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py"", line 562, in normalize_per_cell. counts_per_cell /= counts_per_cell_after. numpy.core._exceptions.UFuncTypeError: Cannot cast ufunc 'divide' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. cffi 1.15.1. cloudpickle 2.2.1. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. defusedxml 0.7.1. dill 0.3.6. google NA. h5py 3.8.0. importlib_resources NA. joblib 1.3.0.dev0. kiwisolver 1.4.4. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. numba 0.57.0. numpy 1.24.3. packaging 23.1. pandas 2.0.2. psutil 5.9.5. pyarrow 12.0.0. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. six 1.16.0. sklearn 1.2.2. sympy 1.11.1. threadpoolctl 3.1.0. torch 2.0.0. tqdm 4.65.0. typing_extensions NA. yaml 6.0. zipp NA. -----. Python 3.8.10 (v3.8.10:3d8993a744, May 3 2021, 08:55:58) [Clang 6.0 (clang-600.0.57)]. macOS-10.16-x86_64-i386-64bit. -----. Session information updated at 2023-06-05 12:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2500:837,usability,User,Users,837,"`normalize_per_cell` raises `numpy.core._exceptions.UFuncTypeError`; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import numpy as np. import anndata as ad. import scanpy as sc. adata = ad.AnnData(np.random.poisson(0.2, (100, 200))). sc.pp.normalize_per_cell(adata, 1e6). ```. ```pytb. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/scottgigante/envs/immunaISR/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py"", line 527, in normalize_per_cell. normalize_per_cell(adata.X, counts_per_cell_after, counts_per_cell). File ""/Users/scottgigante/envs/immunaISR/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py"", line 562, in normalize_per_cell. counts_per_cell /= counts_per_cell_after. numpy.core._exceptions.UFuncTypeError: Cannot cast ufunc 'divide' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'. ```. #### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. cffi 1.15.1. cloudpickle 2.2.1. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. defusedxml 0.7.1. dill 0.3.6. google NA. h5py 3.8.0. importlib_resources NA. joblib 1.3.0.dev0. kiwisolver 1.4.4. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.3.0. natsort 8.3.1. numba 0.57.0. numpy 1.24.3. packaging 23.1. pandas 2.0.2. psutil 5.9.5. pyarrow 12.0.0. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. six 1.16.0. sklearn 1.2.2. sympy 1.11.1. threadpoolctl 3.1.0. torch 2.0.0. tqdm 4.65.0. typing_extensions NA. yaml 6.0. zipp NA. -----. Python 3.8.10 (v3.8.10:3d8993a744, May 3 2021, 08:55:58) [Clang 6.0 (clang-600.0.57)]. macOS-10.16-x86_64-i386-64bit. -----. Session information updated at 2023-06-05 12:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500
https://github.com/scverse/scanpy/issues/2501:267,availability,restor,restore,267,"> [Bug] `rank_genes_groups` after `normalize_pearson_residuals`; > . Hi, @jlause . There's a issue when using `normalize_pearson_residuals`, it seems that we can't calculated the `log2foldchange` in `rank_genes_groups` will be failed. That's because `np.expm1` can't restore the `adata.X` after `normalize_pearson_residuals`. Could you solve this issue that completed the downstream currently? <img width=""770"" alt=""image"" src=""https://github.com/scverse/scanpy/assets/46667721/a8e64ab1-360d-43a2-a07b-a766049bcbcd"">.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2501
https://github.com/scverse/scanpy/issues/2501:372,availability,down,downstream,372,"> [Bug] `rank_genes_groups` after `normalize_pearson_residuals`; > . Hi, @jlause . There's a issue when using `normalize_pearson_residuals`, it seems that we can't calculated the `log2foldchange` in `rank_genes_groups` will be failed. That's because `np.expm1` can't restore the `adata.X` after `normalize_pearson_residuals`. Could you solve this issue that completed the downstream currently? <img width=""770"" alt=""image"" src=""https://github.com/scverse/scanpy/assets/46667721/a8e64ab1-360d-43a2-a07b-a766049bcbcd"">.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2501
https://github.com/scverse/scanpy/issues/2501:227,deployability,fail,failed,227,"> [Bug] `rank_genes_groups` after `normalize_pearson_residuals`; > . Hi, @jlause . There's a issue when using `normalize_pearson_residuals`, it seems that we can't calculated the `log2foldchange` in `rank_genes_groups` will be failed. That's because `np.expm1` can't restore the `adata.X` after `normalize_pearson_residuals`. Could you solve this issue that completed the downstream currently? <img width=""770"" alt=""image"" src=""https://github.com/scverse/scanpy/assets/46667721/a8e64ab1-360d-43a2-a07b-a766049bcbcd"">.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2501
https://github.com/scverse/scanpy/issues/2501:383,energy efficiency,current,currently,383,"> [Bug] `rank_genes_groups` after `normalize_pearson_residuals`; > . Hi, @jlause . There's a issue when using `normalize_pearson_residuals`, it seems that we can't calculated the `log2foldchange` in `rank_genes_groups` will be failed. That's because `np.expm1` can't restore the `adata.X` after `normalize_pearson_residuals`. Could you solve this issue that completed the downstream currently? <img width=""770"" alt=""image"" src=""https://github.com/scverse/scanpy/assets/46667721/a8e64ab1-360d-43a2-a07b-a766049bcbcd"">.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2501
https://github.com/scverse/scanpy/issues/2501:227,reliability,fail,failed,227,"> [Bug] `rank_genes_groups` after `normalize_pearson_residuals`; > . Hi, @jlause . There's a issue when using `normalize_pearson_residuals`, it seems that we can't calculated the `log2foldchange` in `rank_genes_groups` will be failed. That's because `np.expm1` can't restore the `adata.X` after `normalize_pearson_residuals`. Could you solve this issue that completed the downstream currently? <img width=""770"" alt=""image"" src=""https://github.com/scverse/scanpy/assets/46667721/a8e64ab1-360d-43a2-a07b-a766049bcbcd"">.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2501
https://github.com/scverse/scanpy/issues/2501:267,reliability,restor,restore,267,"> [Bug] `rank_genes_groups` after `normalize_pearson_residuals`; > . Hi, @jlause . There's a issue when using `normalize_pearson_residuals`, it seems that we can't calculated the `log2foldchange` in `rank_genes_groups` will be failed. That's because `np.expm1` can't restore the `adata.X` after `normalize_pearson_residuals`. Could you solve this issue that completed the downstream currently? <img width=""770"" alt=""image"" src=""https://github.com/scverse/scanpy/assets/46667721/a8e64ab1-360d-43a2-a07b-a766049bcbcd"">.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2501
https://github.com/scverse/scanpy/issues/2501:358,safety,compl,completed,358,"> [Bug] `rank_genes_groups` after `normalize_pearson_residuals`; > . Hi, @jlause . There's a issue when using `normalize_pearson_residuals`, it seems that we can't calculated the `log2foldchange` in `rank_genes_groups` will be failed. That's because `np.expm1` can't restore the `adata.X` after `normalize_pearson_residuals`. Could you solve this issue that completed the downstream currently? <img width=""770"" alt=""image"" src=""https://github.com/scverse/scanpy/assets/46667721/a8e64ab1-360d-43a2-a07b-a766049bcbcd"">.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2501
https://github.com/scverse/scanpy/issues/2501:358,security,compl,completed,358,"> [Bug] `rank_genes_groups` after `normalize_pearson_residuals`; > . Hi, @jlause . There's a issue when using `normalize_pearson_residuals`, it seems that we can't calculated the `log2foldchange` in `rank_genes_groups` will be failed. That's because `np.expm1` can't restore the `adata.X` after `normalize_pearson_residuals`. Could you solve this issue that completed the downstream currently? <img width=""770"" alt=""image"" src=""https://github.com/scverse/scanpy/assets/46667721/a8e64ab1-360d-43a2-a07b-a766049bcbcd"">.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2501
https://github.com/scverse/scanpy/pull/2502:0,deployability,updat,update,0,update tutorials; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2502
https://github.com/scverse/scanpy/pull/2502:0,safety,updat,update,0,update tutorials; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2502
https://github.com/scverse/scanpy/pull/2502:237,safety,review,review,237,update tutorials; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2502
https://github.com/scverse/scanpy/pull/2502:0,security,updat,update,0,update tutorials; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2502
https://github.com/scverse/scanpy/pull/2502:237,testability,review,review,237,update tutorials; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2502
https://github.com/scverse/scanpy/pull/2502:88,usability,guid,guidelines,88,update tutorials; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2502
https://github.com/scverse/scanpy/pull/2502:119,usability,guid,guide,119,update tutorials; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2502
https://github.com/scverse/scanpy/pull/2502:215,usability,workflow,workflow,215,update tutorials; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2502
https://github.com/scverse/scanpy/pull/2503:102,reliability,doe,doesn,102,"Convert issue report templates to forms; That makes it easier for people to fill it out. Since GitHub doesnt provide previews, I added them here: https://github.com/flying-sheep/playground/issues/new/choose",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2503
https://github.com/scverse/scanpy/issues/2505:170,modifiability,deco,decorator,170,Tell people theyre using deprecated functions; #2500 probably happened because the user didnt see that the function was deprecated. We should maybe use a `@deprecated` decorator to both add the deprecation notice to the docs and also throw a `FutureWarning`.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2505
https://github.com/scverse/scanpy/issues/2505:84,usability,user,user,84,Tell people theyre using deprecated functions; #2500 probably happened because the user didnt see that the function was deprecated. We should maybe use a `@deprecated` decorator to both add the deprecation notice to the docs and also throw a `FutureWarning`.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2505
https://github.com/scverse/scanpy/issues/2506:434,availability,error,errors,434,"`compute_neighbors_umap` does not use `scanpy._settings.ScanpyConfig.n_jobs`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When trying to compute neighbors with default settings, I'm getting errors in slurm jobs, as the code tries to allocate all available cores on a node. I tried to control the number of cores available by setting `scanpy._settings.ScanpyConfig.n_jobs`, however I found that this setting does not get used in the offending function `compute_neighbors_umap`, where the following gets called. https://github.com/scverse/scanpy/blob/9cab8cfa4033d3f47a36c7bb816b2c9fae5cfdc6/scanpy/neighbors/__init__.py#L314-L322. The `nearest_neighbors` function from the `umap` provides the parameter `n_jobs`, which is set to -1 (i.e. all cores) by default. https://github.com/lmcinnes/umap/blob/e0cff20b392192f2b62fcc1843c9cb9ca2c9ee27/umap/umap_.py#L255-L266. However, as a user I would expect the number of cores to be defined by the ScanpyConfig. I assume this could also cause the kernel dying unintentionally in other work setups when users aren't aware of the number of cores the function is requesting. ### Minimal code sample. ```python. import scanpy as sc. sc._settings.ScanpyConfig(n_jobs=1). ad = sc.read('quite_large_h5ad_file'). sc.pp.pca(ad). sc.pp.neighbors(ad). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. RuntimeError Traceback (most recent call last). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:212, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 211 try:. --> 212 self._repopulate_pool(). 213 except Exception:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:303, in Pool.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:490,availability,avail,available,490,"`compute_neighbors_umap` does not use `scanpy._settings.ScanpyConfig.n_jobs`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When trying to compute neighbors with default settings, I'm getting errors in slurm jobs, as the code tries to allocate all available cores on a node. I tried to control the number of cores available by setting `scanpy._settings.ScanpyConfig.n_jobs`, however I found that this setting does not get used in the offending function `compute_neighbors_umap`, where the following gets called. https://github.com/scverse/scanpy/blob/9cab8cfa4033d3f47a36c7bb816b2c9fae5cfdc6/scanpy/neighbors/__init__.py#L314-L322. The `nearest_neighbors` function from the `umap` provides the parameter `n_jobs`, which is set to -1 (i.e. all cores) by default. https://github.com/lmcinnes/umap/blob/e0cff20b392192f2b62fcc1843c9cb9ca2c9ee27/umap/umap_.py#L255-L266. However, as a user I would expect the number of cores to be defined by the ScanpyConfig. I assume this could also cause the kernel dying unintentionally in other work setups when users aren't aware of the number of cores the function is requesting. ### Minimal code sample. ```python. import scanpy as sc. sc._settings.ScanpyConfig(n_jobs=1). ad = sc.read('quite_large_h5ad_file'). sc.pp.pca(ad). sc.pp.neighbors(ad). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. RuntimeError Traceback (most recent call last). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:212, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 211 try:. --> 212 self._repopulate_pool(). 213 except Exception:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:303, in Pool.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:556,availability,avail,available,556,"`compute_neighbors_umap` does not use `scanpy._settings.ScanpyConfig.n_jobs`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When trying to compute neighbors with default settings, I'm getting errors in slurm jobs, as the code tries to allocate all available cores on a node. I tried to control the number of cores available by setting `scanpy._settings.ScanpyConfig.n_jobs`, however I found that this setting does not get used in the offending function `compute_neighbors_umap`, where the following gets called. https://github.com/scverse/scanpy/blob/9cab8cfa4033d3f47a36c7bb816b2c9fae5cfdc6/scanpy/neighbors/__init__.py#L314-L322. The `nearest_neighbors` function from the `umap` provides the parameter `n_jobs`, which is set to -1 (i.e. all cores) by default. https://github.com/lmcinnes/umap/blob/e0cff20b392192f2b62fcc1843c9cb9ca2c9ee27/umap/umap_.py#L255-L266. However, as a user I would expect the number of cores to be defined by the ScanpyConfig. I assume this could also cause the kernel dying unintentionally in other work setups when users aren't aware of the number of cores the function is requesting. ### Minimal code sample. ```python. import scanpy as sc. sc._settings.ScanpyConfig(n_jobs=1). ad = sc.read('quite_large_h5ad_file'). sc.pp.pca(ad). sc.pp.neighbors(ad). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. RuntimeError Traceback (most recent call last). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:212, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 211 try:. --> 212 self._repopulate_pool(). 213 except Exception:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:303, in Pool.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:1535,availability,Error,Error,1535,"e number of cores available by setting `scanpy._settings.ScanpyConfig.n_jobs`, however I found that this setting does not get used in the offending function `compute_neighbors_umap`, where the following gets called. https://github.com/scverse/scanpy/blob/9cab8cfa4033d3f47a36c7bb816b2c9fae5cfdc6/scanpy/neighbors/__init__.py#L314-L322. The `nearest_neighbors` function from the `umap` provides the parameter `n_jobs`, which is set to -1 (i.e. all cores) by default. https://github.com/lmcinnes/umap/blob/e0cff20b392192f2b62fcc1843c9cb9ca2c9ee27/umap/umap_.py#L255-L266. However, as a user I would expect the number of cores to be defined by the ScanpyConfig. I assume this could also cause the kernel dying unintentionally in other work setups when users aren't aware of the number of cores the function is requesting. ### Minimal code sample. ```python. import scanpy as sc. sc._settings.ScanpyConfig(n_jobs=1). ad = sc.read('quite_large_h5ad_file'). sc.pp.pca(ad). sc.pp.neighbors(ad). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. RuntimeError Traceback (most recent call last). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:212, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 211 try:. --> 212 self._repopulate_pool(). 213 except Exception:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:303, in Pool._repopulate_pool(self). 302 def _repopulate_pool(self):. --> 303 return self._repopulate_pool_static(self._ctx, self.Process,. 304 self._processes,. 305 self._pool, self._inqueue,. 306 self._outqueue, self._initializer,. 307 self._initargs,. 308 self._maxtasksperchild,. 309 self._wrap_exception). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:326, in Pool._repopulate_pool_static(ctx, Process, processes, pool, inqueue, outqueue, initializer, initargs, maxtasksperchild, wrap_exception). 325 w.daemon = True. ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:246,deployability,version,version,246,"`compute_neighbors_umap` does not use `scanpy._settings.ScanpyConfig.n_jobs`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When trying to compute neighbors with default settings, I'm getting errors in slurm jobs, as the code tries to allocate all available cores on a node. I tried to control the number of cores available by setting `scanpy._settings.ScanpyConfig.n_jobs`, however I found that this setting does not get used in the offending function `compute_neighbors_umap`, where the following gets called. https://github.com/scverse/scanpy/blob/9cab8cfa4033d3f47a36c7bb816b2c9fae5cfdc6/scanpy/neighbors/__init__.py#L314-L322. The `nearest_neighbors` function from the `umap` provides the parameter `n_jobs`, which is set to -1 (i.e. all cores) by default. https://github.com/lmcinnes/umap/blob/e0cff20b392192f2b62fcc1843c9cb9ca2c9ee27/umap/umap_.py#L255-L266. However, as a user I would expect the number of cores to be defined by the ScanpyConfig. I assume this could also cause the kernel dying unintentionally in other work setups when users aren't aware of the number of cores the function is requesting. ### Minimal code sample. ```python. import scanpy as sc. sc._settings.ScanpyConfig(n_jobs=1). ad = sc.read('quite_large_h5ad_file'). sc.pp.pca(ad). sc.pp.neighbors(ad). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. RuntimeError Traceback (most recent call last). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:212, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 211 try:. --> 212 self._repopulate_pool(). 213 except Exception:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:303, in Pool.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:5953,deployability,Build,Building,5953,"_memory, use_pynndescent, n_jobs, verbose). 325 n_trees = min(64, 5 + int(round((X.shape[0]) ** 0.5 / 20.0))). 326 n_iters = max(5, int(round(np.log2(X.shape[0])))). --> 328 knn_search_index = NNDescent(. 329 X,. 330 n_neighbors=n_neighbors,. 331 metric=metric,. 332 metric_kwds=metric_kwds,. 333 random_state=random_state,. 334 n_trees=n_trees,. 335 n_iters=n_iters,. 336 max_candidates=60,. 337 low_memory=low_memory,. 338 n_jobs=n_jobs,. 339 verbose=verbose,. 340 compressed=False,. 341 ). 342 knn_indices, knn_dists = knn_search_index.neighbor_graph. 344 if verbose:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/pynndescent/pynndescent_.py:794, in NNDescent.__init__(self, data, metric, metric_kwds, n_neighbors, n_trees, leaf_size, pruning_degree_multiplier, diversify_prob, n_search_trees, tree_init, init_graph, init_dist, random_state, low_memory, max_candidates, n_iters, delta, n_jobs, compressed, parallel_batch_queries, verbose). 792 if verbose:. 793 print(ts(), ""Building RP forest with"", str(n_trees), ""trees""). --> 794 self._rp_forest = make_forest(. 795 data,. 796 n_neighbors,. 797 n_trees,. 798 leaf_size,. 799 self.rng_state,. 800 current_random_state,. 801 self.n_jobs,. 802 self._angular_trees,. 803 ). 804 leaf_array = rptree_leaf_array(self._rp_forest). 805 else:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/pynndescent/rp_trees.py:1056, in make_forest(data, n_neighbors, n_trees, leaf_size, rng_state, random_state, n_jobs, angular). 1044 result = joblib.Parallel(n_jobs=n_jobs, require=""sharedmem"")(. 1045 joblib.delayed(make_sparse_tree)(. 1046 data.indices,. (...). 1053 for i in range(n_trees). 1054 ). 1055 else:. -> 1056 result = joblib.Parallel(n_jobs=n_jobs, require=""sharedmem"")(. 1057 joblib.delayed(make_dense_tree)(data, rng_states[i], leaf_size, angular). 1058 for i in range(n_trees). 1059 ). 1060 except (RuntimeError, RecursionError, SystemError):. 1061 warn(. 1062 ""Random Projection forest initialisation failed due t",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:6945,deployability,fail,failed,6945,"(), ""Building RP forest with"", str(n_trees), ""trees""). --> 794 self._rp_forest = make_forest(. 795 data,. 796 n_neighbors,. 797 n_trees,. 798 leaf_size,. 799 self.rng_state,. 800 current_random_state,. 801 self.n_jobs,. 802 self._angular_trees,. 803 ). 804 leaf_array = rptree_leaf_array(self._rp_forest). 805 else:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/pynndescent/rp_trees.py:1056, in make_forest(data, n_neighbors, n_trees, leaf_size, rng_state, random_state, n_jobs, angular). 1044 result = joblib.Parallel(n_jobs=n_jobs, require=""sharedmem"")(. 1045 joblib.delayed(make_sparse_tree)(. 1046 data.indices,. (...). 1053 for i in range(n_trees). 1054 ). 1055 else:. -> 1056 result = joblib.Parallel(n_jobs=n_jobs, require=""sharedmem"")(. 1057 joblib.delayed(make_dense_tree)(data, rng_states[i], leaf_size, angular). 1058 for i in range(n_trees). 1059 ). 1060 except (RuntimeError, RecursionError, SystemError):. 1061 warn(. 1062 ""Random Projection forest initialisation failed due to recursion"". 1063 ""limit being reached. Something is a little strange with your "". 1064 ""graph_data, and this may take longer than normal to compute."". 1065 ). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:1085, in Parallel.__call__(self, iterable). 1076 try:. 1077 # Only set self._iterating to True if at least a batch. 1078 # was dispatched. In particular this covers the edge. (...). 1082 # was very quick and its callback already dispatched all the. 1083 # remaining jobs. 1084 self._iterating = False. -> 1085 if self.dispatch_one_batch(iterator):. 1086 self._iterating = self._original_iterator is not None. 1088 while self.dispatch_one_batch(iterator):. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:901, in Parallel.dispatch_one_batch(self, iterator). 899 return False. 900 else:. --> 901 self._dispatch(tasks). 902 return True. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/paralle",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:9727,deployability,Version,Versions,9727,"s/scanpy/lib/python3.9/site-packages/joblib/_parallel_backends.py:407, in ThreadingBackend._get_pool(self). 401 """"""Lazily initialize the thread pool. 402 . 403 The actual pool of worker threads is only initialized at the first. 404 call to apply_async. 405 """""". 406 if self._pool is None:. --> 407 self._pool = ThreadPool(self._n_jobs). 408 return self._pool. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:927, in ThreadPool.__init__(self, processes, initializer, initargs). 926 def __init__(self, processes=None, initializer=None, initargs=()):. --> 927 Pool.__init__(self, processes, initializer, initargs). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:216, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 214 for p in self._pool:. 215 if p.exitcode is None:. --> 216 p.terminate(). 217 for p in self._pool:. 218 p.join(). AttributeError: 'DummyProcess' object has no attribute 'terminate'. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asttokens NA. backcall 0.2.0. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.8.0. igraph 0.10.4. ipykernel 6.23.1. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.6.3. mpl_toolkits NA. natsort 8.3.1. numba 0.56.4. numpy 1.23.5. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. sklearn 1.2.2. stack_data 0.6.2. texttable 1.6.7. threadpoolctl 3.1.0. tornado 6.3. traitlets 5.9.0. typing_extensions NA. wcwidth 0.2.6. zipp N",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:477,energy efficiency,alloc,allocate,477,"`compute_neighbors_umap` does not use `scanpy._settings.ScanpyConfig.n_jobs`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When trying to compute neighbors with default settings, I'm getting errors in slurm jobs, as the code tries to allocate all available cores on a node. I tried to control the number of cores available by setting `scanpy._settings.ScanpyConfig.n_jobs`, however I found that this setting does not get used in the offending function `compute_neighbors_umap`, where the following gets called. https://github.com/scverse/scanpy/blob/9cab8cfa4033d3f47a36c7bb816b2c9fae5cfdc6/scanpy/neighbors/__init__.py#L314-L322. The `nearest_neighbors` function from the `umap` provides the parameter `n_jobs`, which is set to -1 (i.e. all cores) by default. https://github.com/lmcinnes/umap/blob/e0cff20b392192f2b62fcc1843c9cb9ca2c9ee27/umap/umap_.py#L255-L266. However, as a user I would expect the number of cores to be defined by the ScanpyConfig. I assume this could also cause the kernel dying unintentionally in other work setups when users aren't aware of the number of cores the function is requesting. ### Minimal code sample. ```python. import scanpy as sc. sc._settings.ScanpyConfig(n_jobs=1). ad = sc.read('quite_large_h5ad_file'). sc.pp.pca(ad). sc.pp.neighbors(ad). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. RuntimeError Traceback (most recent call last). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:212, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 211 try:. --> 212 self._repopulate_pool(). 213 except Exception:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:303, in Pool.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:500,energy efficiency,core,cores,500,"`compute_neighbors_umap` does not use `scanpy._settings.ScanpyConfig.n_jobs`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When trying to compute neighbors with default settings, I'm getting errors in slurm jobs, as the code tries to allocate all available cores on a node. I tried to control the number of cores available by setting `scanpy._settings.ScanpyConfig.n_jobs`, however I found that this setting does not get used in the offending function `compute_neighbors_umap`, where the following gets called. https://github.com/scverse/scanpy/blob/9cab8cfa4033d3f47a36c7bb816b2c9fae5cfdc6/scanpy/neighbors/__init__.py#L314-L322. The `nearest_neighbors` function from the `umap` provides the parameter `n_jobs`, which is set to -1 (i.e. all cores) by default. https://github.com/lmcinnes/umap/blob/e0cff20b392192f2b62fcc1843c9cb9ca2c9ee27/umap/umap_.py#L255-L266. However, as a user I would expect the number of cores to be defined by the ScanpyConfig. I assume this could also cause the kernel dying unintentionally in other work setups when users aren't aware of the number of cores the function is requesting. ### Minimal code sample. ```python. import scanpy as sc. sc._settings.ScanpyConfig(n_jobs=1). ad = sc.read('quite_large_h5ad_file'). sc.pp.pca(ad). sc.pp.neighbors(ad). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. RuntimeError Traceback (most recent call last). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:212, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 211 try:. --> 212 self._repopulate_pool(). 213 except Exception:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:303, in Pool.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:550,energy efficiency,core,cores,550,"`compute_neighbors_umap` does not use `scanpy._settings.ScanpyConfig.n_jobs`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When trying to compute neighbors with default settings, I'm getting errors in slurm jobs, as the code tries to allocate all available cores on a node. I tried to control the number of cores available by setting `scanpy._settings.ScanpyConfig.n_jobs`, however I found that this setting does not get used in the offending function `compute_neighbors_umap`, where the following gets called. https://github.com/scverse/scanpy/blob/9cab8cfa4033d3f47a36c7bb816b2c9fae5cfdc6/scanpy/neighbors/__init__.py#L314-L322. The `nearest_neighbors` function from the `umap` provides the parameter `n_jobs`, which is set to -1 (i.e. all cores) by default. https://github.com/lmcinnes/umap/blob/e0cff20b392192f2b62fcc1843c9cb9ca2c9ee27/umap/umap_.py#L255-L266. However, as a user I would expect the number of cores to be defined by the ScanpyConfig. I assume this could also cause the kernel dying unintentionally in other work setups when users aren't aware of the number of cores the function is requesting. ### Minimal code sample. ```python. import scanpy as sc. sc._settings.ScanpyConfig(n_jobs=1). ad = sc.read('quite_large_h5ad_file'). sc.pp.pca(ad). sc.pp.neighbors(ad). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. RuntimeError Traceback (most recent call last). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:212, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 211 try:. --> 212 self._repopulate_pool(). 213 except Exception:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:303, in Pool.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:985,energy efficiency,core,cores,985,"`compute_neighbors_umap` does not use `scanpy._settings.ScanpyConfig.n_jobs`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When trying to compute neighbors with default settings, I'm getting errors in slurm jobs, as the code tries to allocate all available cores on a node. I tried to control the number of cores available by setting `scanpy._settings.ScanpyConfig.n_jobs`, however I found that this setting does not get used in the offending function `compute_neighbors_umap`, where the following gets called. https://github.com/scverse/scanpy/blob/9cab8cfa4033d3f47a36c7bb816b2c9fae5cfdc6/scanpy/neighbors/__init__.py#L314-L322. The `nearest_neighbors` function from the `umap` provides the parameter `n_jobs`, which is set to -1 (i.e. all cores) by default. https://github.com/lmcinnes/umap/blob/e0cff20b392192f2b62fcc1843c9cb9ca2c9ee27/umap/umap_.py#L255-L266. However, as a user I would expect the number of cores to be defined by the ScanpyConfig. I assume this could also cause the kernel dying unintentionally in other work setups when users aren't aware of the number of cores the function is requesting. ### Minimal code sample. ```python. import scanpy as sc. sc._settings.ScanpyConfig(n_jobs=1). ad = sc.read('quite_large_h5ad_file'). sc.pp.pca(ad). sc.pp.neighbors(ad). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. RuntimeError Traceback (most recent call last). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:212, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 211 try:. --> 212 self._repopulate_pool(). 213 except Exception:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:303, in Pool.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:1156,energy efficiency,core,cores,1156,"ue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When trying to compute neighbors with default settings, I'm getting errors in slurm jobs, as the code tries to allocate all available cores on a node. I tried to control the number of cores available by setting `scanpy._settings.ScanpyConfig.n_jobs`, however I found that this setting does not get used in the offending function `compute_neighbors_umap`, where the following gets called. https://github.com/scverse/scanpy/blob/9cab8cfa4033d3f47a36c7bb816b2c9fae5cfdc6/scanpy/neighbors/__init__.py#L314-L322. The `nearest_neighbors` function from the `umap` provides the parameter `n_jobs`, which is set to -1 (i.e. all cores) by default. https://github.com/lmcinnes/umap/blob/e0cff20b392192f2b62fcc1843c9cb9ca2c9ee27/umap/umap_.py#L255-L266. However, as a user I would expect the number of cores to be defined by the ScanpyConfig. I assume this could also cause the kernel dying unintentionally in other work setups when users aren't aware of the number of cores the function is requesting. ### Minimal code sample. ```python. import scanpy as sc. sc._settings.ScanpyConfig(n_jobs=1). ad = sc.read('quite_large_h5ad_file'). sc.pp.pca(ad). sc.pp.neighbors(ad). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. RuntimeError Traceback (most recent call last). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:212, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 211 try:. --> 212 self._repopulate_pool(). 213 except Exception:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:303, in Pool._repopulate_pool(self). 302 def _repopulate_pool(self):. --> 303 return self._repopulate_pool_static(self._ctx, self.Process,. 304 self._processes,. 305 self._",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:1323,energy efficiency,core,cores,1323,"er branch of scanpy. ### What happened? When trying to compute neighbors with default settings, I'm getting errors in slurm jobs, as the code tries to allocate all available cores on a node. I tried to control the number of cores available by setting `scanpy._settings.ScanpyConfig.n_jobs`, however I found that this setting does not get used in the offending function `compute_neighbors_umap`, where the following gets called. https://github.com/scverse/scanpy/blob/9cab8cfa4033d3f47a36c7bb816b2c9fae5cfdc6/scanpy/neighbors/__init__.py#L314-L322. The `nearest_neighbors` function from the `umap` provides the parameter `n_jobs`, which is set to -1 (i.e. all cores) by default. https://github.com/lmcinnes/umap/blob/e0cff20b392192f2b62fcc1843c9cb9ca2c9ee27/umap/umap_.py#L255-L266. However, as a user I would expect the number of cores to be defined by the ScanpyConfig. I assume this could also cause the kernel dying unintentionally in other work setups when users aren't aware of the number of cores the function is requesting. ### Minimal code sample. ```python. import scanpy as sc. sc._settings.ScanpyConfig(n_jobs=1). ad = sc.read('quite_large_h5ad_file'). sc.pp.pca(ad). sc.pp.neighbors(ad). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. RuntimeError Traceback (most recent call last). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:212, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 211 try:. --> 212 self._repopulate_pool(). 213 except Exception:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:303, in Pool._repopulate_pool(self). 302 def _repopulate_pool(self):. --> 303 return self._repopulate_pool_static(self._ctx, self.Process,. 304 self._processes,. 305 self._pool, self._inqueue,. 306 self._outqueue, self._initializer,. 307 self._initargs,. 308 self._maxtasksperchild,. 309 self._wrap_exception). File ~/tools/mambaforge/envs",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:8584,energy efficiency,Schedul,Schedule,8584,"iterator is not None. 1088 while self.dispatch_one_batch(iterator):. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:901, in Parallel.dispatch_one_batch(self, iterator). 899 return False. 900 else:. --> 901 self._dispatch(tasks). 902 return True. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:819, in Parallel._dispatch(self, batch). 817 with self._lock:. 818 job_idx = len(self._jobs). --> 819 job = self._backend.apply_async(batch, callback=cb). 820 # A job can complete so quickly than its callback is. 821 # called before we get here, causing self._jobs to. 822 # grow. To ensure correct results ordering, .insert is. 823 # used (rather than .append) in the following line. 824 self._jobs.insert(job_idx, job). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/_parallel_backends.py:252, in PoolManagerMixin.apply_async(self, func, callback). 250 def apply_async(self, func, callback=None):. 251 """"""Schedule a func to be run"""""". --> 252 return self._get_pool().apply_async(. 253 SafeFunction(func), callback=callback). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/_parallel_backends.py:407, in ThreadingBackend._get_pool(self). 401 """"""Lazily initialize the thread pool. 402 . 403 The actual pool of worker threads is only initialized at the first. 404 call to apply_async. 405 """""". 406 if self._pool is None:. --> 407 self._pool = ThreadPool(self._n_jobs). 408 return self._pool. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:927, in ThreadPool.__init__(self, processes, initializer, initargs). 926 def __init__(self, processes=None, initializer=None, initargs=()):. --> 927 Pool.__init__(self, processes, initializer, initargs). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:216, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 214 for p in self._pool:. 215 if p.exitcode is None:. -",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:246,integrability,version,version,246,"`compute_neighbors_umap` does not use `scanpy._settings.ScanpyConfig.n_jobs`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When trying to compute neighbors with default settings, I'm getting errors in slurm jobs, as the code tries to allocate all available cores on a node. I tried to control the number of cores available by setting `scanpy._settings.ScanpyConfig.n_jobs`, however I found that this setting does not get used in the offending function `compute_neighbors_umap`, where the following gets called. https://github.com/scverse/scanpy/blob/9cab8cfa4033d3f47a36c7bb816b2c9fae5cfdc6/scanpy/neighbors/__init__.py#L314-L322. The `nearest_neighbors` function from the `umap` provides the parameter `n_jobs`, which is set to -1 (i.e. all cores) by default. https://github.com/lmcinnes/umap/blob/e0cff20b392192f2b62fcc1843c9cb9ca2c9ee27/umap/umap_.py#L255-L266. However, as a user I would expect the number of cores to be defined by the ScanpyConfig. I assume this could also cause the kernel dying unintentionally in other work setups when users aren't aware of the number of cores the function is requesting. ### Minimal code sample. ```python. import scanpy as sc. sc._settings.ScanpyConfig(n_jobs=1). ad = sc.read('quite_large_h5ad_file'). sc.pp.pca(ad). sc.pp.neighbors(ad). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. RuntimeError Traceback (most recent call last). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:212, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 211 try:. --> 212 self._repopulate_pool(). 213 except Exception:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:303, in Pool.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:7310,integrability,batch,batch,7310,"9/site-packages/pynndescent/rp_trees.py:1056, in make_forest(data, n_neighbors, n_trees, leaf_size, rng_state, random_state, n_jobs, angular). 1044 result = joblib.Parallel(n_jobs=n_jobs, require=""sharedmem"")(. 1045 joblib.delayed(make_sparse_tree)(. 1046 data.indices,. (...). 1053 for i in range(n_trees). 1054 ). 1055 else:. -> 1056 result = joblib.Parallel(n_jobs=n_jobs, require=""sharedmem"")(. 1057 joblib.delayed(make_dense_tree)(data, rng_states[i], leaf_size, angular). 1058 for i in range(n_trees). 1059 ). 1060 except (RuntimeError, RecursionError, SystemError):. 1061 warn(. 1062 ""Random Projection forest initialisation failed due to recursion"". 1063 ""limit being reached. Something is a little strange with your "". 1064 ""graph_data, and this may take longer than normal to compute."". 1065 ). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:1085, in Parallel.__call__(self, iterable). 1076 try:. 1077 # Only set self._iterating to True if at least a batch. 1078 # was dispatched. In particular this covers the edge. (...). 1082 # was very quick and its callback already dispatched all the. 1083 # remaining jobs. 1084 self._iterating = False. -> 1085 if self.dispatch_one_batch(iterator):. 1086 self._iterating = self._original_iterator is not None. 1088 while self.dispatch_one_batch(iterator):. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:901, in Parallel.dispatch_one_batch(self, iterator). 899 return False. 900 else:. --> 901 self._dispatch(tasks). 902 return True. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:819, in Parallel._dispatch(self, batch). 817 with self._lock:. 818 job_idx = len(self._jobs). --> 819 job = self._backend.apply_async(batch, callback=cb). 820 # A job can complete so quickly than its callback is. 821 # called before we get here, causing self._jobs to. 822 # grow. To ensure correct results ordering, .insert is. 823 # used (rather than .append",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:7986,integrability,batch,batch,7986,"reached. Something is a little strange with your "". 1064 ""graph_data, and this may take longer than normal to compute."". 1065 ). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:1085, in Parallel.__call__(self, iterable). 1076 try:. 1077 # Only set self._iterating to True if at least a batch. 1078 # was dispatched. In particular this covers the edge. (...). 1082 # was very quick and its callback already dispatched all the. 1083 # remaining jobs. 1084 self._iterating = False. -> 1085 if self.dispatch_one_batch(iterator):. 1086 self._iterating = self._original_iterator is not None. 1088 while self.dispatch_one_batch(iterator):. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:901, in Parallel.dispatch_one_batch(self, iterator). 899 return False. 900 else:. --> 901 self._dispatch(tasks). 902 return True. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:819, in Parallel._dispatch(self, batch). 817 with self._lock:. 818 job_idx = len(self._jobs). --> 819 job = self._backend.apply_async(batch, callback=cb). 820 # A job can complete so quickly than its callback is. 821 # called before we get here, causing self._jobs to. 822 # grow. To ensure correct results ordering, .insert is. 823 # used (rather than .append) in the following line. 824 self._jobs.insert(job_idx, job). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/_parallel_backends.py:252, in PoolManagerMixin.apply_async(self, func, callback). 250 def apply_async(self, func, callback=None):. 251 """"""Schedule a func to be run"""""". --> 252 return self._get_pool().apply_async(. 253 SafeFunction(func), callback=callback). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/_parallel_backends.py:407, in ThreadingBackend._get_pool(self). 401 """"""Lazily initialize the thread pool. 402 . 403 The actual pool of worker threads is only initialized at the first. 404 call to apply_async. 405 """,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:8087,integrability,batch,batch,8087,"ormal to compute."". 1065 ). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:1085, in Parallel.__call__(self, iterable). 1076 try:. 1077 # Only set self._iterating to True if at least a batch. 1078 # was dispatched. In particular this covers the edge. (...). 1082 # was very quick and its callback already dispatched all the. 1083 # remaining jobs. 1084 self._iterating = False. -> 1085 if self.dispatch_one_batch(iterator):. 1086 self._iterating = self._original_iterator is not None. 1088 while self.dispatch_one_batch(iterator):. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:901, in Parallel.dispatch_one_batch(self, iterator). 899 return False. 900 else:. --> 901 self._dispatch(tasks). 902 return True. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:819, in Parallel._dispatch(self, batch). 817 with self._lock:. 818 job_idx = len(self._jobs). --> 819 job = self._backend.apply_async(batch, callback=cb). 820 # A job can complete so quickly than its callback is. 821 # called before we get here, causing self._jobs to. 822 # grow. To ensure correct results ordering, .insert is. 823 # used (rather than .append) in the following line. 824 self._jobs.insert(job_idx, job). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/_parallel_backends.py:252, in PoolManagerMixin.apply_async(self, func, callback). 250 def apply_async(self, func, callback=None):. 251 """"""Schedule a func to be run"""""". --> 252 return self._get_pool().apply_async(. 253 SafeFunction(func), callback=callback). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/_parallel_backends.py:407, in ThreadingBackend._get_pool(self). 401 """"""Lazily initialize the thread pool. 402 . 403 The actual pool of worker threads is only initialized at the first. 404 call to apply_async. 405 """""". 406 if self._pool is None:. --> 407 self._pool = ThreadPool(self._n_jobs). 408 return self._pool.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:9727,integrability,Version,Versions,9727,"s/scanpy/lib/python3.9/site-packages/joblib/_parallel_backends.py:407, in ThreadingBackend._get_pool(self). 401 """"""Lazily initialize the thread pool. 402 . 403 The actual pool of worker threads is only initialized at the first. 404 call to apply_async. 405 """""". 406 if self._pool is None:. --> 407 self._pool = ThreadPool(self._n_jobs). 408 return self._pool. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:927, in ThreadPool.__init__(self, processes, initializer, initargs). 926 def __init__(self, processes=None, initializer=None, initargs=()):. --> 927 Pool.__init__(self, processes, initializer, initargs). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:216, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 214 for p in self._pool:. 215 if p.exitcode is None:. --> 216 p.terminate(). 217 for p in self._pool:. 218 p.join(). AttributeError: 'DummyProcess' object has no attribute 'terminate'. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asttokens NA. backcall 0.2.0. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.8.0. igraph 0.10.4. ipykernel 6.23.1. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.6.3. mpl_toolkits NA. natsort 8.3.1. numba 0.56.4. numpy 1.23.5. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. sklearn 1.2.2. stack_data 0.6.2. texttable 1.6.7. threadpoolctl 3.1.0. tornado 6.3. traitlets 5.9.0. typing_extensions NA. wcwidth 0.2.6. zipp N",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:6510,interoperability,share,sharedmem,6510," if verbose:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/pynndescent/pynndescent_.py:794, in NNDescent.__init__(self, data, metric, metric_kwds, n_neighbors, n_trees, leaf_size, pruning_degree_multiplier, diversify_prob, n_search_trees, tree_init, init_graph, init_dist, random_state, low_memory, max_candidates, n_iters, delta, n_jobs, compressed, parallel_batch_queries, verbose). 792 if verbose:. 793 print(ts(), ""Building RP forest with"", str(n_trees), ""trees""). --> 794 self._rp_forest = make_forest(. 795 data,. 796 n_neighbors,. 797 n_trees,. 798 leaf_size,. 799 self.rng_state,. 800 current_random_state,. 801 self.n_jobs,. 802 self._angular_trees,. 803 ). 804 leaf_array = rptree_leaf_array(self._rp_forest). 805 else:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/pynndescent/rp_trees.py:1056, in make_forest(data, n_neighbors, n_trees, leaf_size, rng_state, random_state, n_jobs, angular). 1044 result = joblib.Parallel(n_jobs=n_jobs, require=""sharedmem"")(. 1045 joblib.delayed(make_sparse_tree)(. 1046 data.indices,. (...). 1053 for i in range(n_trees). 1054 ). 1055 else:. -> 1056 result = joblib.Parallel(n_jobs=n_jobs, require=""sharedmem"")(. 1057 joblib.delayed(make_dense_tree)(data, rng_states[i], leaf_size, angular). 1058 for i in range(n_trees). 1059 ). 1060 except (RuntimeError, RecursionError, SystemError):. 1061 warn(. 1062 ""Random Projection forest initialisation failed due to recursion"". 1063 ""limit being reached. Something is a little strange with your "". 1064 ""graph_data, and this may take longer than normal to compute."". 1065 ). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:1085, in Parallel.__call__(self, iterable). 1076 try:. 1077 # Only set self._iterating to True if at least a batch. 1078 # was dispatched. In particular this covers the edge. (...). 1082 # was very quick and its callback already dispatched all the. 1083 # remaining jobs. 1084 self._iterating = False. -> 1085 if s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:6698,interoperability,share,sharedmem,6698,"leaf_size, pruning_degree_multiplier, diversify_prob, n_search_trees, tree_init, init_graph, init_dist, random_state, low_memory, max_candidates, n_iters, delta, n_jobs, compressed, parallel_batch_queries, verbose). 792 if verbose:. 793 print(ts(), ""Building RP forest with"", str(n_trees), ""trees""). --> 794 self._rp_forest = make_forest(. 795 data,. 796 n_neighbors,. 797 n_trees,. 798 leaf_size,. 799 self.rng_state,. 800 current_random_state,. 801 self.n_jobs,. 802 self._angular_trees,. 803 ). 804 leaf_array = rptree_leaf_array(self._rp_forest). 805 else:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/pynndescent/rp_trees.py:1056, in make_forest(data, n_neighbors, n_trees, leaf_size, rng_state, random_state, n_jobs, angular). 1044 result = joblib.Parallel(n_jobs=n_jobs, require=""sharedmem"")(. 1045 joblib.delayed(make_sparse_tree)(. 1046 data.indices,. (...). 1053 for i in range(n_trees). 1054 ). 1055 else:. -> 1056 result = joblib.Parallel(n_jobs=n_jobs, require=""sharedmem"")(. 1057 joblib.delayed(make_dense_tree)(data, rng_states[i], leaf_size, angular). 1058 for i in range(n_trees). 1059 ). 1060 except (RuntimeError, RecursionError, SystemError):. 1061 warn(. 1062 ""Random Projection forest initialisation failed due to recursion"". 1063 ""limit being reached. Something is a little strange with your "". 1064 ""graph_data, and this may take longer than normal to compute."". 1065 ). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:1085, in Parallel.__call__(self, iterable). 1076 try:. 1077 # Only set self._iterating to True if at least a batch. 1078 # was dispatched. In particular this covers the edge. (...). 1082 # was very quick and its callback already dispatched all the. 1083 # remaining jobs. 1084 self._iterating = False. -> 1085 if self.dispatch_one_batch(iterator):. 1086 self._iterating = self._original_iterator is not None. 1088 while self.dispatch_one_batch(iterator):. File ~/tools/mambaforge/envs/scanpy/lib/python",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:10274,interoperability,platform,platformdirs,10274,". 406 if self._pool is None:. --> 407 self._pool = ThreadPool(self._n_jobs). 408 return self._pool. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:927, in ThreadPool.__init__(self, processes, initializer, initargs). 926 def __init__(self, processes=None, initializer=None, initargs=()):. --> 927 Pool.__init__(self, processes, initializer, initargs). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:216, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 214 for p in self._pool:. 215 if p.exitcode is None:. --> 216 p.terminate(). 217 for p in self._pool:. 218 p.join(). AttributeError: 'DummyProcess' object has no attribute 'terminate'. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asttokens NA. backcall 0.2.0. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.8.0. igraph 0.10.4. ipykernel 6.23.1. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.6.3. mpl_toolkits NA. natsort 8.3.1. numba 0.56.4. numpy 1.23.5. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. sklearn 1.2.2. stack_data 0.6.2. texttable 1.6.7. threadpoolctl 3.1.0. tornado 6.3. traitlets 5.9.0. typing_extensions NA. wcwidth 0.2.6. zipp NA. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.9.16 | packaged by conda-forge | (main, Feb 1 2023, 21:39:03) [GCC 11.3.0]. Linux-4.12.14-122.80-default-x86_64-with-glibc2.22. -----. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:246,modifiability,version,version,246,"`compute_neighbors_umap` does not use `scanpy._settings.ScanpyConfig.n_jobs`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When trying to compute neighbors with default settings, I'm getting errors in slurm jobs, as the code tries to allocate all available cores on a node. I tried to control the number of cores available by setting `scanpy._settings.ScanpyConfig.n_jobs`, however I found that this setting does not get used in the offending function `compute_neighbors_umap`, where the following gets called. https://github.com/scverse/scanpy/blob/9cab8cfa4033d3f47a36c7bb816b2c9fae5cfdc6/scanpy/neighbors/__init__.py#L314-L322. The `nearest_neighbors` function from the `umap` provides the parameter `n_jobs`, which is set to -1 (i.e. all cores) by default. https://github.com/lmcinnes/umap/blob/e0cff20b392192f2b62fcc1843c9cb9ca2c9ee27/umap/umap_.py#L255-L266. However, as a user I would expect the number of cores to be defined by the ScanpyConfig. I assume this could also cause the kernel dying unintentionally in other work setups when users aren't aware of the number of cores the function is requesting. ### Minimal code sample. ```python. import scanpy as sc. sc._settings.ScanpyConfig(n_jobs=1). ad = sc.read('quite_large_h5ad_file'). sc.pp.pca(ad). sc.pp.neighbors(ad). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. RuntimeError Traceback (most recent call last). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:212, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 211 try:. --> 212 self._repopulate_pool(). 213 except Exception:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:303, in Pool.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:936,modifiability,paramet,parameter,936,"`compute_neighbors_umap` does not use `scanpy._settings.ScanpyConfig.n_jobs`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When trying to compute neighbors with default settings, I'm getting errors in slurm jobs, as the code tries to allocate all available cores on a node. I tried to control the number of cores available by setting `scanpy._settings.ScanpyConfig.n_jobs`, however I found that this setting does not get used in the offending function `compute_neighbors_umap`, where the following gets called. https://github.com/scverse/scanpy/blob/9cab8cfa4033d3f47a36c7bb816b2c9fae5cfdc6/scanpy/neighbors/__init__.py#L314-L322. The `nearest_neighbors` function from the `umap` provides the parameter `n_jobs`, which is set to -1 (i.e. all cores) by default. https://github.com/lmcinnes/umap/blob/e0cff20b392192f2b62fcc1843c9cb9ca2c9ee27/umap/umap_.py#L255-L266. However, as a user I would expect the number of cores to be defined by the ScanpyConfig. I assume this could also cause the kernel dying unintentionally in other work setups when users aren't aware of the number of cores the function is requesting. ### Minimal code sample. ```python. import scanpy as sc. sc._settings.ScanpyConfig(n_jobs=1). ad = sc.read('quite_large_h5ad_file'). sc.pp.pca(ad). sc.pp.neighbors(ad). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. RuntimeError Traceback (most recent call last). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:212, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 211 try:. --> 212 self._repopulate_pool(). 213 except Exception:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:303, in Pool.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:3206,modifiability,pac,packages,3206,"tializer,. 307 self._initargs,. 308 self._maxtasksperchild,. 309 self._wrap_exception). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:326, in Pool._repopulate_pool_static(ctx, Process, processes, pool, inqueue, outqueue, initializer, initargs, maxtasksperchild, wrap_exception). 325 w.daemon = True. --> 326 w.start(). 327 pool.append(w). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/dummy/__init__.py:51, in DummyProcess.start(self). 50 self._parent._children[self] = None. ---> 51 threading.Thread.start(self). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/threading.py:899, in Thread.start(self). 898 try:. --> 899 _start_new_thread(self._bootstrap, ()). 900 except Exception:. RuntimeError: can't start new thread. During handling of the above exception, another exception occurred:. AttributeError Traceback (most recent call last). Cell In[13], line 1. ----> 1 sc.pp.neighbors(ad). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py:139, in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 137 adata._init_as_actual(adata.copy()). 138 neighbors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. 142 n_pcs=n_pcs,. 143 use_rep=use_rep,. 144 method=method,. 145 metric=metric,. 146 metric_kwds=metric_kwds,. 147 random_state=random_state,. 148 ). 150 if key_added is None:. 151 key_added = 'neighbors'. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py:794, in Neighbors.compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 792 X = pairwise_distances(X, metric=metric, **metric_kwds). 793 metric = 'precomputed'. --> 794 knn_indices, knn_distances, forest = compute_neighbors_umap(. 795 X, n_neighbors, random_state, metric=metric, metric_kwds=metric_kwds. 796 ). 797 # very c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:3781,modifiability,pac,packages,3781,"mambaforge/envs/scanpy/lib/python3.9/threading.py:899, in Thread.start(self). 898 try:. --> 899 _start_new_thread(self._bootstrap, ()). 900 except Exception:. RuntimeError: can't start new thread. During handling of the above exception, another exception occurred:. AttributeError Traceback (most recent call last). Cell In[13], line 1. ----> 1 sc.pp.neighbors(ad). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py:139, in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 137 adata._init_as_actual(adata.copy()). 138 neighbors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. 142 n_pcs=n_pcs,. 143 use_rep=use_rep,. 144 method=method,. 145 metric=metric,. 146 metric_kwds=metric_kwds,. 147 random_state=random_state,. 148 ). 150 if key_added is None:. 151 key_added = 'neighbors'. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py:794, in Neighbors.compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 792 X = pairwise_distances(X, metric=metric, **metric_kwds). 793 metric = 'precomputed'. --> 794 knn_indices, knn_distances, forest = compute_neighbors_umap(. 795 X, n_neighbors, random_state, metric=metric, metric_kwds=metric_kwds. 796 ). 797 # very cautious here. 798 try:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py:305, in compute_neighbors_umap(X, n_neighbors, random_state, metric, metric_kwds, angular, verbose). 301 from umap.umap_ import nearest_neighbors. 303 random_state = check_random_state(random_state). --> 305 knn_indices, knn_dists, forest = nearest_neighbors(. 306 X,. 307 n_neighbors,. 308 random_state=random_state,. 309 metric=metric,. 310 metric_kwds=metric_kwds,. 311 angular=angular,. 312 verbose=verbose,. 313 ). 315 return knn_indices, knn_dists, for",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:4289,modifiability,pac,packages,4289,"rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 137 adata._init_as_actual(adata.copy()). 138 neighbors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. 142 n_pcs=n_pcs,. 143 use_rep=use_rep,. 144 method=method,. 145 metric=metric,. 146 metric_kwds=metric_kwds,. 147 random_state=random_state,. 148 ). 150 if key_added is None:. 151 key_added = 'neighbors'. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py:794, in Neighbors.compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 792 X = pairwise_distances(X, metric=metric, **metric_kwds). 793 metric = 'precomputed'. --> 794 knn_indices, knn_distances, forest = compute_neighbors_umap(. 795 X, n_neighbors, random_state, metric=metric, metric_kwds=metric_kwds. 796 ). 797 # very cautious here. 798 try:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py:305, in compute_neighbors_umap(X, n_neighbors, random_state, metric, metric_kwds, angular, verbose). 301 from umap.umap_ import nearest_neighbors. 303 random_state = check_random_state(random_state). --> 305 knn_indices, knn_dists, forest = nearest_neighbors(. 306 X,. 307 n_neighbors,. 308 random_state=random_state,. 309 metric=metric,. 310 metric_kwds=metric_kwds,. 311 angular=angular,. 312 verbose=verbose,. 313 ). 315 return knn_indices, knn_dists, forest. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/umap/umap_.py:328, in nearest_neighbors(X, n_neighbors, metric, metric_kwds, angular, random_state, low_memory, use_pynndescent, n_jobs, verbose). 325 n_trees = min(64, 5 + int(round((X.shape[0]) ** 0.5 / 20.0))). 326 n_iters = max(5, int(round(np.log2(X.shape[0])))). --> 328 knn_search_index = NNDescent(. 329 X,. 330 n_neighbors=n_neighbors,. 331 metric=metric,. 332 metric_kwds=metric_kwds,. 333 random_state=random_state,. 334 n_trees",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:4845,modifiability,pac,packages,4845,"hbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 792 X = pairwise_distances(X, metric=metric, **metric_kwds). 793 metric = 'precomputed'. --> 794 knn_indices, knn_distances, forest = compute_neighbors_umap(. 795 X, n_neighbors, random_state, metric=metric, metric_kwds=metric_kwds. 796 ). 797 # very cautious here. 798 try:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py:305, in compute_neighbors_umap(X, n_neighbors, random_state, metric, metric_kwds, angular, verbose). 301 from umap.umap_ import nearest_neighbors. 303 random_state = check_random_state(random_state). --> 305 knn_indices, knn_dists, forest = nearest_neighbors(. 306 X,. 307 n_neighbors,. 308 random_state=random_state,. 309 metric=metric,. 310 metric_kwds=metric_kwds,. 311 angular=angular,. 312 verbose=verbose,. 313 ). 315 return knn_indices, knn_dists, forest. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/umap/umap_.py:328, in nearest_neighbors(X, n_neighbors, metric, metric_kwds, angular, random_state, low_memory, use_pynndescent, n_jobs, verbose). 325 n_trees = min(64, 5 + int(round((X.shape[0]) ** 0.5 / 20.0))). 326 n_iters = max(5, int(round(np.log2(X.shape[0])))). --> 328 knn_search_index = NNDescent(. 329 X,. 330 n_neighbors=n_neighbors,. 331 metric=metric,. 332 metric_kwds=metric_kwds,. 333 random_state=random_state,. 334 n_trees=n_trees,. 335 n_iters=n_iters,. 336 max_candidates=60,. 337 low_memory=low_memory,. 338 n_jobs=n_jobs,. 339 verbose=verbose,. 340 compressed=False,. 341 ). 342 knn_indices, knn_dists = knn_search_index.neighbor_graph. 344 if verbose:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/pynndescent/pynndescent_.py:794, in NNDescent.__init__(self, data, metric, metric_kwds, n_neighbors, n_trees, leaf_size, pruning_degree_multiplier, diversify_prob, n_search_trees, tree_init, init_graph, init_dist, random_state, low_memory, max_candidates, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:5584,modifiability,pac,packages,5584,"306 X,. 307 n_neighbors,. 308 random_state=random_state,. 309 metric=metric,. 310 metric_kwds=metric_kwds,. 311 angular=angular,. 312 verbose=verbose,. 313 ). 315 return knn_indices, knn_dists, forest. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/umap/umap_.py:328, in nearest_neighbors(X, n_neighbors, metric, metric_kwds, angular, random_state, low_memory, use_pynndescent, n_jobs, verbose). 325 n_trees = min(64, 5 + int(round((X.shape[0]) ** 0.5 / 20.0))). 326 n_iters = max(5, int(round(np.log2(X.shape[0])))). --> 328 knn_search_index = NNDescent(. 329 X,. 330 n_neighbors=n_neighbors,. 331 metric=metric,. 332 metric_kwds=metric_kwds,. 333 random_state=random_state,. 334 n_trees=n_trees,. 335 n_iters=n_iters,. 336 max_candidates=60,. 337 low_memory=low_memory,. 338 n_jobs=n_jobs,. 339 verbose=verbose,. 340 compressed=False,. 341 ). 342 knn_indices, knn_dists = knn_search_index.neighbor_graph. 344 if verbose:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/pynndescent/pynndescent_.py:794, in NNDescent.__init__(self, data, metric, metric_kwds, n_neighbors, n_trees, leaf_size, pruning_degree_multiplier, diversify_prob, n_search_trees, tree_init, init_graph, init_dist, random_state, low_memory, max_candidates, n_iters, delta, n_jobs, compressed, parallel_batch_queries, verbose). 792 if verbose:. 793 print(ts(), ""Building RP forest with"", str(n_trees), ""trees""). --> 794 self._rp_forest = make_forest(. 795 data,. 796 n_neighbors,. 797 n_trees,. 798 leaf_size,. 799 self.rng_state,. 800 current_random_state,. 801 self.n_jobs,. 802 self._angular_trees,. 803 ). 804 leaf_array = rptree_leaf_array(self._rp_forest). 805 else:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/pynndescent/rp_trees.py:1056, in make_forest(data, n_neighbors, n_trees, leaf_size, rng_state, random_state, n_jobs, angular). 1044 result = joblib.Parallel(n_jobs=n_jobs, require=""sharedmem"")(. 1045 joblib.delayed(make_sparse_tree)(. 1046 data.indices,. (...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:6320,modifiability,pac,packages,6320,". 336 max_candidates=60,. 337 low_memory=low_memory,. 338 n_jobs=n_jobs,. 339 verbose=verbose,. 340 compressed=False,. 341 ). 342 knn_indices, knn_dists = knn_search_index.neighbor_graph. 344 if verbose:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/pynndescent/pynndescent_.py:794, in NNDescent.__init__(self, data, metric, metric_kwds, n_neighbors, n_trees, leaf_size, pruning_degree_multiplier, diversify_prob, n_search_trees, tree_init, init_graph, init_dist, random_state, low_memory, max_candidates, n_iters, delta, n_jobs, compressed, parallel_batch_queries, verbose). 792 if verbose:. 793 print(ts(), ""Building RP forest with"", str(n_trees), ""trees""). --> 794 self._rp_forest = make_forest(. 795 data,. 796 n_neighbors,. 797 n_trees,. 798 leaf_size,. 799 self.rng_state,. 800 current_random_state,. 801 self.n_jobs,. 802 self._angular_trees,. 803 ). 804 leaf_array = rptree_leaf_array(self._rp_forest). 805 else:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/pynndescent/rp_trees.py:1056, in make_forest(data, n_neighbors, n_trees, leaf_size, rng_state, random_state, n_jobs, angular). 1044 result = joblib.Parallel(n_jobs=n_jobs, require=""sharedmem"")(. 1045 joblib.delayed(make_sparse_tree)(. 1046 data.indices,. (...). 1053 for i in range(n_trees). 1054 ). 1055 else:. -> 1056 result = joblib.Parallel(n_jobs=n_jobs, require=""sharedmem"")(. 1057 joblib.delayed(make_dense_tree)(data, rng_states[i], leaf_size, angular). 1058 for i in range(n_trees). 1059 ). 1060 except (RuntimeError, RecursionError, SystemError):. 1061 warn(. 1062 ""Random Projection forest initialisation failed due to recursion"". 1063 ""limit being reached. Something is a little strange with your "". 1064 ""graph_data, and this may take longer than normal to compute."". 1065 ). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:1085, in Parallel.__call__(self, iterable). 1076 try:. 1077 # Only set self._iterating to True if at least a batch. 1078 # ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:7173,modifiability,pac,packages,7173,"_angular_trees,. 803 ). 804 leaf_array = rptree_leaf_array(self._rp_forest). 805 else:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/pynndescent/rp_trees.py:1056, in make_forest(data, n_neighbors, n_trees, leaf_size, rng_state, random_state, n_jobs, angular). 1044 result = joblib.Parallel(n_jobs=n_jobs, require=""sharedmem"")(. 1045 joblib.delayed(make_sparse_tree)(. 1046 data.indices,. (...). 1053 for i in range(n_trees). 1054 ). 1055 else:. -> 1056 result = joblib.Parallel(n_jobs=n_jobs, require=""sharedmem"")(. 1057 joblib.delayed(make_dense_tree)(data, rng_states[i], leaf_size, angular). 1058 for i in range(n_trees). 1059 ). 1060 except (RuntimeError, RecursionError, SystemError):. 1061 warn(. 1062 ""Random Projection forest initialisation failed due to recursion"". 1063 ""limit being reached. Something is a little strange with your "". 1064 ""graph_data, and this may take longer than normal to compute."". 1065 ). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:1085, in Parallel.__call__(self, iterable). 1076 try:. 1077 # Only set self._iterating to True if at least a batch. 1078 # was dispatched. In particular this covers the edge. (...). 1082 # was very quick and its callback already dispatched all the. 1083 # remaining jobs. 1084 self._iterating = False. -> 1085 if self.dispatch_one_batch(iterator):. 1086 self._iterating = self._original_iterator is not None. 1088 while self.dispatch_one_batch(iterator):. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:901, in Parallel.dispatch_one_batch(self, iterator). 899 return False. 900 else:. --> 901 self._dispatch(tasks). 902 return True. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:819, in Parallel._dispatch(self, batch). 817 with self._lock:. 818 job_idx = len(self._jobs). --> 819 job = self._backend.apply_async(batch, callback=cb). 820 # A job can complete so quickly than its callback is. 821 # calle",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:7712,modifiability,pac,packages,7712," joblib.delayed(make_dense_tree)(data, rng_states[i], leaf_size, angular). 1058 for i in range(n_trees). 1059 ). 1060 except (RuntimeError, RecursionError, SystemError):. 1061 warn(. 1062 ""Random Projection forest initialisation failed due to recursion"". 1063 ""limit being reached. Something is a little strange with your "". 1064 ""graph_data, and this may take longer than normal to compute."". 1065 ). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:1085, in Parallel.__call__(self, iterable). 1076 try:. 1077 # Only set self._iterating to True if at least a batch. 1078 # was dispatched. In particular this covers the edge. (...). 1082 # was very quick and its callback already dispatched all the. 1083 # remaining jobs. 1084 self._iterating = False. -> 1085 if self.dispatch_one_batch(iterator):. 1086 self._iterating = self._original_iterator is not None. 1088 while self.dispatch_one_batch(iterator):. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:901, in Parallel.dispatch_one_batch(self, iterator). 899 return False. 900 else:. --> 901 self._dispatch(tasks). 902 return True. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:819, in Parallel._dispatch(self, batch). 817 with self._lock:. 818 job_idx = len(self._jobs). --> 819 job = self._backend.apply_async(batch, callback=cb). 820 # A job can complete so quickly than its callback is. 821 # called before we get here, causing self._jobs to. 822 # grow. To ensure correct results ordering, .insert is. 823 # used (rather than .append) in the following line. 824 self._jobs.insert(job_idx, job). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/_parallel_backends.py:252, in PoolManagerMixin.apply_async(self, func, callback). 250 def apply_async(self, func, callback=None):. 251 """"""Schedule a func to be run"""""". --> 252 return self._get_pool().apply_async(. 253 SafeFunction(func), callback=callback). File ~/tools",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:7925,modifiability,pac,packages,7925," initialisation failed due to recursion"". 1063 ""limit being reached. Something is a little strange with your "". 1064 ""graph_data, and this may take longer than normal to compute."". 1065 ). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:1085, in Parallel.__call__(self, iterable). 1076 try:. 1077 # Only set self._iterating to True if at least a batch. 1078 # was dispatched. In particular this covers the edge. (...). 1082 # was very quick and its callback already dispatched all the. 1083 # remaining jobs. 1084 self._iterating = False. -> 1085 if self.dispatch_one_batch(iterator):. 1086 self._iterating = self._original_iterator is not None. 1088 while self.dispatch_one_batch(iterator):. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:901, in Parallel.dispatch_one_batch(self, iterator). 899 return False. 900 else:. --> 901 self._dispatch(tasks). 902 return True. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:819, in Parallel._dispatch(self, batch). 817 with self._lock:. 818 job_idx = len(self._jobs). --> 819 job = self._backend.apply_async(batch, callback=cb). 820 # A job can complete so quickly than its callback is. 821 # called before we get here, causing self._jobs to. 822 # grow. To ensure correct results ordering, .insert is. 823 # used (rather than .append) in the following line. 824 self._jobs.insert(job_idx, job). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/_parallel_backends.py:252, in PoolManagerMixin.apply_async(self, func, callback). 250 def apply_async(self, func, callback=None):. 251 """"""Schedule a func to be run"""""". --> 252 return self._get_pool().apply_async(. 253 SafeFunction(func), callback=callback). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/_parallel_backends.py:407, in ThreadingBackend._get_pool(self). 401 """"""Lazily initialize the thread pool. 402 . 403 The actual pool of worker threads is o",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:8430,modifiability,pac,packages,8430,"atched all the. 1083 # remaining jobs. 1084 self._iterating = False. -> 1085 if self.dispatch_one_batch(iterator):. 1086 self._iterating = self._original_iterator is not None. 1088 while self.dispatch_one_batch(iterator):. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:901, in Parallel.dispatch_one_batch(self, iterator). 899 return False. 900 else:. --> 901 self._dispatch(tasks). 902 return True. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:819, in Parallel._dispatch(self, batch). 817 with self._lock:. 818 job_idx = len(self._jobs). --> 819 job = self._backend.apply_async(batch, callback=cb). 820 # A job can complete so quickly than its callback is. 821 # called before we get here, causing self._jobs to. 822 # grow. To ensure correct results ordering, .insert is. 823 # used (rather than .append) in the following line. 824 self._jobs.insert(job_idx, job). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/_parallel_backends.py:252, in PoolManagerMixin.apply_async(self, func, callback). 250 def apply_async(self, func, callback=None):. 251 """"""Schedule a func to be run"""""". --> 252 return self._get_pool().apply_async(. 253 SafeFunction(func), callback=callback). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/_parallel_backends.py:407, in ThreadingBackend._get_pool(self). 401 """"""Lazily initialize the thread pool. 402 . 403 The actual pool of worker threads is only initialized at the first. 404 call to apply_async. 405 """""". 406 if self._pool is None:. --> 407 self._pool = ThreadPool(self._n_jobs). 408 return self._pool. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:927, in ThreadPool.__init__(self, processes, initializer, initargs). 926 def __init__(self, processes=None, initializer=None, initargs=()):. --> 927 Pool.__init__(self, processes, initializer, initargs). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:8759,modifiability,pac,packages,8759,"ch_one_batch(self, iterator). 899 return False. 900 else:. --> 901 self._dispatch(tasks). 902 return True. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:819, in Parallel._dispatch(self, batch). 817 with self._lock:. 818 job_idx = len(self._jobs). --> 819 job = self._backend.apply_async(batch, callback=cb). 820 # A job can complete so quickly than its callback is. 821 # called before we get here, causing self._jobs to. 822 # grow. To ensure correct results ordering, .insert is. 823 # used (rather than .append) in the following line. 824 self._jobs.insert(job_idx, job). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/_parallel_backends.py:252, in PoolManagerMixin.apply_async(self, func, callback). 250 def apply_async(self, func, callback=None):. 251 """"""Schedule a func to be run"""""". --> 252 return self._get_pool().apply_async(. 253 SafeFunction(func), callback=callback). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/_parallel_backends.py:407, in ThreadingBackend._get_pool(self). 401 """"""Lazily initialize the thread pool. 402 . 403 The actual pool of worker threads is only initialized at the first. 404 call to apply_async. 405 """""". 406 if self._pool is None:. --> 407 self._pool = ThreadPool(self._n_jobs). 408 return self._pool. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:927, in ThreadPool.__init__(self, processes, initializer, initargs). 926 def __init__(self, processes=None, initializer=None, initargs=()):. --> 927 Pool.__init__(self, processes, initializer, initargs). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:216, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 214 for p in self._pool:. 215 if p.exitcode is None:. --> 216 p.terminate(). 217 for p in self._pool:. 218 p.join(). AttributeError: 'DummyProcess' object has no attribute 'terminate'. ```. ### Versions. <details>. ```. -----. ann",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:9727,modifiability,Version,Versions,9727,"s/scanpy/lib/python3.9/site-packages/joblib/_parallel_backends.py:407, in ThreadingBackend._get_pool(self). 401 """"""Lazily initialize the thread pool. 402 . 403 The actual pool of worker threads is only initialized at the first. 404 call to apply_async. 405 """""". 406 if self._pool is None:. --> 407 self._pool = ThreadPool(self._n_jobs). 408 return self._pool. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:927, in ThreadPool.__init__(self, processes, initializer, initargs). 926 def __init__(self, processes=None, initializer=None, initargs=()):. --> 927 Pool.__init__(self, processes, initializer, initargs). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:216, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 214 for p in self._pool:. 215 if p.exitcode is None:. --> 216 p.terminate(). 217 for p in self._pool:. 218 p.join(). AttributeError: 'DummyProcess' object has no attribute 'terminate'. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asttokens NA. backcall 0.2.0. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.8.0. igraph 0.10.4. ipykernel 6.23.1. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.6.3. mpl_toolkits NA. natsort 8.3.1. numba 0.56.4. numpy 1.23.5. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. sklearn 1.2.2. stack_data 0.6.2. texttable 1.6.7. threadpoolctl 3.1.0. tornado 6.3. traitlets 5.9.0. typing_extensions NA. wcwidth 0.2.6. zipp N",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:9943,modifiability,deco,decorator,9943,"the first. 404 call to apply_async. 405 """""". 406 if self._pool is None:. --> 407 self._pool = ThreadPool(self._n_jobs). 408 return self._pool. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:927, in ThreadPool.__init__(self, processes, initializer, initargs). 926 def __init__(self, processes=None, initializer=None, initargs=()):. --> 927 Pool.__init__(self, processes, initializer, initargs). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:216, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 214 for p in self._pool:. 215 if p.exitcode is None:. --> 216 p.terminate(). 217 for p in self._pool:. 218 p.join(). AttributeError: 'DummyProcess' object has no attribute 'terminate'. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asttokens NA. backcall 0.2.0. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.8.0. igraph 0.10.4. ipykernel 6.23.1. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.6.3. mpl_toolkits NA. natsort 8.3.1. numba 0.56.4. numpy 1.23.5. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. sklearn 1.2.2. stack_data 0.6.2. texttable 1.6.7. threadpoolctl 3.1.0. tornado 6.3. traitlets 5.9.0. typing_extensions NA. wcwidth 0.2.6. zipp NA. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.9.16 | packaged by conda-forge | (main, Feb 1 2023, 21:39:03) [GCC 11.3.0]. Linux-4.12.14-122.80-default-x86",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:10179,modifiability,pac,packaging,10179,". 406 if self._pool is None:. --> 407 self._pool = ThreadPool(self._n_jobs). 408 return self._pool. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:927, in ThreadPool.__init__(self, processes, initializer, initargs). 926 def __init__(self, processes=None, initializer=None, initargs=()):. --> 927 Pool.__init__(self, processes, initializer, initargs). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:216, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 214 for p in self._pool:. 215 if p.exitcode is None:. --> 216 p.terminate(). 217 for p in self._pool:. 218 p.join(). AttributeError: 'DummyProcess' object has no attribute 'terminate'. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asttokens NA. backcall 0.2.0. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.8.0. igraph 0.10.4. ipykernel 6.23.1. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.6.3. mpl_toolkits NA. natsort 8.3.1. numba 0.56.4. numpy 1.23.5. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. sklearn 1.2.2. stack_data 0.6.2. texttable 1.6.7. threadpoolctl 3.1.0. tornado 6.3. traitlets 5.9.0. typing_extensions NA. wcwidth 0.2.6. zipp NA. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.9.16 | packaged by conda-forge | (main, Feb 1 2023, 21:39:03) [GCC 11.3.0]. Linux-4.12.14-122.80-default-x86_64-with-glibc2.22. -----. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:10847,modifiability,pac,packaged,10847,". 406 if self._pool is None:. --> 407 self._pool = ThreadPool(self._n_jobs). 408 return self._pool. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:927, in ThreadPool.__init__(self, processes, initializer, initargs). 926 def __init__(self, processes=None, initializer=None, initargs=()):. --> 927 Pool.__init__(self, processes, initializer, initargs). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:216, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 214 for p in self._pool:. 215 if p.exitcode is None:. --> 216 p.terminate(). 217 for p in self._pool:. 218 p.join(). AttributeError: 'DummyProcess' object has no attribute 'terminate'. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asttokens NA. backcall 0.2.0. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.8.0. igraph 0.10.4. ipykernel 6.23.1. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.6.3. mpl_toolkits NA. natsort 8.3.1. numba 0.56.4. numpy 1.23.5. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.0.9. pytz 2023.3. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. sklearn 1.2.2. stack_data 0.6.2. texttable 1.6.7. threadpoolctl 3.1.0. tornado 6.3. traitlets 5.9.0. typing_extensions NA. wcwidth 0.2.6. zipp NA. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.1. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.9.16 | packaged by conda-forge | (main, Feb 1 2023, 21:39:03) [GCC 11.3.0]. Linux-4.12.14-122.80-default-x86_64-with-glibc2.22. -----. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:434,performance,error,errors,434,"`compute_neighbors_umap` does not use `scanpy._settings.ScanpyConfig.n_jobs`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When trying to compute neighbors with default settings, I'm getting errors in slurm jobs, as the code tries to allocate all available cores on a node. I tried to control the number of cores available by setting `scanpy._settings.ScanpyConfig.n_jobs`, however I found that this setting does not get used in the offending function `compute_neighbors_umap`, where the following gets called. https://github.com/scverse/scanpy/blob/9cab8cfa4033d3f47a36c7bb816b2c9fae5cfdc6/scanpy/neighbors/__init__.py#L314-L322. The `nearest_neighbors` function from the `umap` provides the parameter `n_jobs`, which is set to -1 (i.e. all cores) by default. https://github.com/lmcinnes/umap/blob/e0cff20b392192f2b62fcc1843c9cb9ca2c9ee27/umap/umap_.py#L255-L266. However, as a user I would expect the number of cores to be defined by the ScanpyConfig. I assume this could also cause the kernel dying unintentionally in other work setups when users aren't aware of the number of cores the function is requesting. ### Minimal code sample. ```python. import scanpy as sc. sc._settings.ScanpyConfig(n_jobs=1). ad = sc.read('quite_large_h5ad_file'). sc.pp.pca(ad). sc.pp.neighbors(ad). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. RuntimeError Traceback (most recent call last). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:212, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 211 try:. --> 212 self._repopulate_pool(). 213 except Exception:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:303, in Pool.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:1535,performance,Error,Error,1535,"e number of cores available by setting `scanpy._settings.ScanpyConfig.n_jobs`, however I found that this setting does not get used in the offending function `compute_neighbors_umap`, where the following gets called. https://github.com/scverse/scanpy/blob/9cab8cfa4033d3f47a36c7bb816b2c9fae5cfdc6/scanpy/neighbors/__init__.py#L314-L322. The `nearest_neighbors` function from the `umap` provides the parameter `n_jobs`, which is set to -1 (i.e. all cores) by default. https://github.com/lmcinnes/umap/blob/e0cff20b392192f2b62fcc1843c9cb9ca2c9ee27/umap/umap_.py#L255-L266. However, as a user I would expect the number of cores to be defined by the ScanpyConfig. I assume this could also cause the kernel dying unintentionally in other work setups when users aren't aware of the number of cores the function is requesting. ### Minimal code sample. ```python. import scanpy as sc. sc._settings.ScanpyConfig(n_jobs=1). ad = sc.read('quite_large_h5ad_file'). sc.pp.pca(ad). sc.pp.neighbors(ad). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. RuntimeError Traceback (most recent call last). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:212, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 211 try:. --> 212 self._repopulate_pool(). 213 except Exception:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:303, in Pool._repopulate_pool(self). 302 def _repopulate_pool(self):. --> 303 return self._repopulate_pool_static(self._ctx, self.Process,. 304 self._processes,. 305 self._pool, self._inqueue,. 306 self._outqueue, self._initializer,. 307 self._initargs,. 308 self._maxtasksperchild,. 309 self._wrap_exception). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:326, in Pool._repopulate_pool_static(ctx, Process, processes, pool, inqueue, outqueue, initializer, initargs, maxtasksperchild, wrap_exception). 325 w.daemon = True. ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:6477,performance,Parallel,Parallel,6477,"n_search_index.neighbor_graph. 344 if verbose:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/pynndescent/pynndescent_.py:794, in NNDescent.__init__(self, data, metric, metric_kwds, n_neighbors, n_trees, leaf_size, pruning_degree_multiplier, diversify_prob, n_search_trees, tree_init, init_graph, init_dist, random_state, low_memory, max_candidates, n_iters, delta, n_jobs, compressed, parallel_batch_queries, verbose). 792 if verbose:. 793 print(ts(), ""Building RP forest with"", str(n_trees), ""trees""). --> 794 self._rp_forest = make_forest(. 795 data,. 796 n_neighbors,. 797 n_trees,. 798 leaf_size,. 799 self.rng_state,. 800 current_random_state,. 801 self.n_jobs,. 802 self._angular_trees,. 803 ). 804 leaf_array = rptree_leaf_array(self._rp_forest). 805 else:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/pynndescent/rp_trees.py:1056, in make_forest(data, n_neighbors, n_trees, leaf_size, rng_state, random_state, n_jobs, angular). 1044 result = joblib.Parallel(n_jobs=n_jobs, require=""sharedmem"")(. 1045 joblib.delayed(make_sparse_tree)(. 1046 data.indices,. (...). 1053 for i in range(n_trees). 1054 ). 1055 else:. -> 1056 result = joblib.Parallel(n_jobs=n_jobs, require=""sharedmem"")(. 1057 joblib.delayed(make_dense_tree)(data, rng_states[i], leaf_size, angular). 1058 for i in range(n_trees). 1059 ). 1060 except (RuntimeError, RecursionError, SystemError):. 1061 warn(. 1062 ""Random Projection forest initialisation failed due to recursion"". 1063 ""limit being reached. Something is a little strange with your "". 1064 ""graph_data, and this may take longer than normal to compute."". 1065 ). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:1085, in Parallel.__call__(self, iterable). 1076 try:. 1077 # Only set self._iterating to True if at least a batch. 1078 # was dispatched. In particular this covers the edge. (...). 1082 # was very quick and its callback already dispatched all the. 1083 # remaining jobs. 1084 sel",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:6665,performance,Parallel,Parallel,6665,"etric_kwds, n_neighbors, n_trees, leaf_size, pruning_degree_multiplier, diversify_prob, n_search_trees, tree_init, init_graph, init_dist, random_state, low_memory, max_candidates, n_iters, delta, n_jobs, compressed, parallel_batch_queries, verbose). 792 if verbose:. 793 print(ts(), ""Building RP forest with"", str(n_trees), ""trees""). --> 794 self._rp_forest = make_forest(. 795 data,. 796 n_neighbors,. 797 n_trees,. 798 leaf_size,. 799 self.rng_state,. 800 current_random_state,. 801 self.n_jobs,. 802 self._angular_trees,. 803 ). 804 leaf_array = rptree_leaf_array(self._rp_forest). 805 else:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/pynndescent/rp_trees.py:1056, in make_forest(data, n_neighbors, n_trees, leaf_size, rng_state, random_state, n_jobs, angular). 1044 result = joblib.Parallel(n_jobs=n_jobs, require=""sharedmem"")(. 1045 joblib.delayed(make_sparse_tree)(. 1046 data.indices,. (...). 1053 for i in range(n_trees). 1054 ). 1055 else:. -> 1056 result = joblib.Parallel(n_jobs=n_jobs, require=""sharedmem"")(. 1057 joblib.delayed(make_dense_tree)(data, rng_states[i], leaf_size, angular). 1058 for i in range(n_trees). 1059 ). 1060 except (RuntimeError, RecursionError, SystemError):. 1061 warn(. 1062 ""Random Projection forest initialisation failed due to recursion"". 1063 ""limit being reached. Something is a little strange with your "". 1064 ""graph_data, and this may take longer than normal to compute."". 1065 ). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:1085, in Parallel.__call__(self, iterable). 1076 try:. 1077 # Only set self._iterating to True if at least a batch. 1078 # was dispatched. In particular this covers the edge. (...). 1082 # was very quick and its callback already dispatched all the. 1083 # remaining jobs. 1084 self._iterating = False. -> 1085 if self.dispatch_one_batch(iterator):. 1086 self._iterating = self._original_iterator is not None. 1088 while self.dispatch_one_batch(iterator):. File ~/tools",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:7189,performance,parallel,parallel,7189," 803 ). 804 leaf_array = rptree_leaf_array(self._rp_forest). 805 else:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/pynndescent/rp_trees.py:1056, in make_forest(data, n_neighbors, n_trees, leaf_size, rng_state, random_state, n_jobs, angular). 1044 result = joblib.Parallel(n_jobs=n_jobs, require=""sharedmem"")(. 1045 joblib.delayed(make_sparse_tree)(. 1046 data.indices,. (...). 1053 for i in range(n_trees). 1054 ). 1055 else:. -> 1056 result = joblib.Parallel(n_jobs=n_jobs, require=""sharedmem"")(. 1057 joblib.delayed(make_dense_tree)(data, rng_states[i], leaf_size, angular). 1058 for i in range(n_trees). 1059 ). 1060 except (RuntimeError, RecursionError, SystemError):. 1061 warn(. 1062 ""Random Projection forest initialisation failed due to recursion"". 1063 ""limit being reached. Something is a little strange with your "". 1064 ""graph_data, and this may take longer than normal to compute."". 1065 ). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:1085, in Parallel.__call__(self, iterable). 1076 try:. 1077 # Only set self._iterating to True if at least a batch. 1078 # was dispatched. In particular this covers the edge. (...). 1082 # was very quick and its callback already dispatched all the. 1083 # remaining jobs. 1084 self._iterating = False. -> 1085 if self.dispatch_one_batch(iterator):. 1086 self._iterating = self._original_iterator is not None. 1088 while self.dispatch_one_batch(iterator):. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:901, in Parallel.dispatch_one_batch(self, iterator). 899 return False. 900 else:. --> 901 self._dispatch(tasks). 902 return True. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:819, in Parallel._dispatch(self, batch). 817 with self._lock:. 818 job_idx = len(self._jobs). --> 819 job = self._backend.apply_async(batch, callback=cb). 820 # A job can complete so quickly than its callback is. 821 # called before we get ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:7210,performance,Parallel,Parallel,7210,"y = rptree_leaf_array(self._rp_forest). 805 else:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/pynndescent/rp_trees.py:1056, in make_forest(data, n_neighbors, n_trees, leaf_size, rng_state, random_state, n_jobs, angular). 1044 result = joblib.Parallel(n_jobs=n_jobs, require=""sharedmem"")(. 1045 joblib.delayed(make_sparse_tree)(. 1046 data.indices,. (...). 1053 for i in range(n_trees). 1054 ). 1055 else:. -> 1056 result = joblib.Parallel(n_jobs=n_jobs, require=""sharedmem"")(. 1057 joblib.delayed(make_dense_tree)(data, rng_states[i], leaf_size, angular). 1058 for i in range(n_trees). 1059 ). 1060 except (RuntimeError, RecursionError, SystemError):. 1061 warn(. 1062 ""Random Projection forest initialisation failed due to recursion"". 1063 ""limit being reached. Something is a little strange with your "". 1064 ""graph_data, and this may take longer than normal to compute."". 1065 ). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:1085, in Parallel.__call__(self, iterable). 1076 try:. 1077 # Only set self._iterating to True if at least a batch. 1078 # was dispatched. In particular this covers the edge. (...). 1082 # was very quick and its callback already dispatched all the. 1083 # remaining jobs. 1084 self._iterating = False. -> 1085 if self.dispatch_one_batch(iterator):. 1086 self._iterating = self._original_iterator is not None. 1088 while self.dispatch_one_batch(iterator):. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:901, in Parallel.dispatch_one_batch(self, iterator). 899 return False. 900 else:. --> 901 self._dispatch(tasks). 902 return True. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:819, in Parallel._dispatch(self, batch). 817 with self._lock:. 818 job_idx = len(self._jobs). --> 819 job = self._backend.apply_async(batch, callback=cb). 820 # A job can complete so quickly than its callback is. 821 # called before we get here, causing self._j",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:7310,performance,batch,batch,7310,"9/site-packages/pynndescent/rp_trees.py:1056, in make_forest(data, n_neighbors, n_trees, leaf_size, rng_state, random_state, n_jobs, angular). 1044 result = joblib.Parallel(n_jobs=n_jobs, require=""sharedmem"")(. 1045 joblib.delayed(make_sparse_tree)(. 1046 data.indices,. (...). 1053 for i in range(n_trees). 1054 ). 1055 else:. -> 1056 result = joblib.Parallel(n_jobs=n_jobs, require=""sharedmem"")(. 1057 joblib.delayed(make_dense_tree)(data, rng_states[i], leaf_size, angular). 1058 for i in range(n_trees). 1059 ). 1060 except (RuntimeError, RecursionError, SystemError):. 1061 warn(. 1062 ""Random Projection forest initialisation failed due to recursion"". 1063 ""limit being reached. Something is a little strange with your "". 1064 ""graph_data, and this may take longer than normal to compute."". 1065 ). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:1085, in Parallel.__call__(self, iterable). 1076 try:. 1077 # Only set self._iterating to True if at least a batch. 1078 # was dispatched. In particular this covers the edge. (...). 1082 # was very quick and its callback already dispatched all the. 1083 # remaining jobs. 1084 self._iterating = False. -> 1085 if self.dispatch_one_batch(iterator):. 1086 self._iterating = self._original_iterator is not None. 1088 while self.dispatch_one_batch(iterator):. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:901, in Parallel.dispatch_one_batch(self, iterator). 899 return False. 900 else:. --> 901 self._dispatch(tasks). 902 return True. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:819, in Parallel._dispatch(self, batch). 817 with self._lock:. 818 job_idx = len(self._jobs). --> 819 job = self._backend.apply_async(batch, callback=cb). 820 # A job can complete so quickly than its callback is. 821 # called before we get here, causing self._jobs to. 822 # grow. To ensure correct results ordering, .insert is. 823 # used (rather than .append",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:7728,performance,parallel,parallel,7728,"make_dense_tree)(data, rng_states[i], leaf_size, angular). 1058 for i in range(n_trees). 1059 ). 1060 except (RuntimeError, RecursionError, SystemError):. 1061 warn(. 1062 ""Random Projection forest initialisation failed due to recursion"". 1063 ""limit being reached. Something is a little strange with your "". 1064 ""graph_data, and this may take longer than normal to compute."". 1065 ). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:1085, in Parallel.__call__(self, iterable). 1076 try:. 1077 # Only set self._iterating to True if at least a batch. 1078 # was dispatched. In particular this covers the edge. (...). 1082 # was very quick and its callback already dispatched all the. 1083 # remaining jobs. 1084 self._iterating = False. -> 1085 if self.dispatch_one_batch(iterator):. 1086 self._iterating = self._original_iterator is not None. 1088 while self.dispatch_one_batch(iterator):. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:901, in Parallel.dispatch_one_batch(self, iterator). 899 return False. 900 else:. --> 901 self._dispatch(tasks). 902 return True. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:819, in Parallel._dispatch(self, batch). 817 with self._lock:. 818 job_idx = len(self._jobs). --> 819 job = self._backend.apply_async(batch, callback=cb). 820 # A job can complete so quickly than its callback is. 821 # called before we get here, causing self._jobs to. 822 # grow. To ensure correct results ordering, .insert is. 823 # used (rather than .append) in the following line. 824 self._jobs.insert(job_idx, job). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/_parallel_backends.py:252, in PoolManagerMixin.apply_async(self, func, callback). 250 def apply_async(self, func, callback=None):. 251 """"""Schedule a func to be run"""""". --> 252 return self._get_pool().apply_async(. 253 SafeFunction(func), callback=callback). File ~/tools/mambaforge/envs",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:7748,performance,Parallel,Parallel,7748,"a, rng_states[i], leaf_size, angular). 1058 for i in range(n_trees). 1059 ). 1060 except (RuntimeError, RecursionError, SystemError):. 1061 warn(. 1062 ""Random Projection forest initialisation failed due to recursion"". 1063 ""limit being reached. Something is a little strange with your "". 1064 ""graph_data, and this may take longer than normal to compute."". 1065 ). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:1085, in Parallel.__call__(self, iterable). 1076 try:. 1077 # Only set self._iterating to True if at least a batch. 1078 # was dispatched. In particular this covers the edge. (...). 1082 # was very quick and its callback already dispatched all the. 1083 # remaining jobs. 1084 self._iterating = False. -> 1085 if self.dispatch_one_batch(iterator):. 1086 self._iterating = self._original_iterator is not None. 1088 while self.dispatch_one_batch(iterator):. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:901, in Parallel.dispatch_one_batch(self, iterator). 899 return False. 900 else:. --> 901 self._dispatch(tasks). 902 return True. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:819, in Parallel._dispatch(self, batch). 817 with self._lock:. 818 job_idx = len(self._jobs). --> 819 job = self._backend.apply_async(batch, callback=cb). 820 # A job can complete so quickly than its callback is. 821 # called before we get here, causing self._jobs to. 822 # grow. To ensure correct results ordering, .insert is. 823 # used (rather than .append) in the following line. 824 self._jobs.insert(job_idx, job). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/_parallel_backends.py:252, in PoolManagerMixin.apply_async(self, func, callback). 250 def apply_async(self, func, callback=None):. 251 """"""Schedule a func to be run"""""". --> 252 return self._get_pool().apply_async(. 253 SafeFunction(func), callback=callback). File ~/tools/mambaforge/envs/scanpy/lib/python3.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:7941,performance,parallel,parallel,7941,"failed due to recursion"". 1063 ""limit being reached. Something is a little strange with your "". 1064 ""graph_data, and this may take longer than normal to compute."". 1065 ). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:1085, in Parallel.__call__(self, iterable). 1076 try:. 1077 # Only set self._iterating to True if at least a batch. 1078 # was dispatched. In particular this covers the edge. (...). 1082 # was very quick and its callback already dispatched all the. 1083 # remaining jobs. 1084 self._iterating = False. -> 1085 if self.dispatch_one_batch(iterator):. 1086 self._iterating = self._original_iterator is not None. 1088 while self.dispatch_one_batch(iterator):. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:901, in Parallel.dispatch_one_batch(self, iterator). 899 return False. 900 else:. --> 901 self._dispatch(tasks). 902 return True. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:819, in Parallel._dispatch(self, batch). 817 with self._lock:. 818 job_idx = len(self._jobs). --> 819 job = self._backend.apply_async(batch, callback=cb). 820 # A job can complete so quickly than its callback is. 821 # called before we get here, causing self._jobs to. 822 # grow. To ensure correct results ordering, .insert is. 823 # used (rather than .append) in the following line. 824 self._jobs.insert(job_idx, job). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/_parallel_backends.py:252, in PoolManagerMixin.apply_async(self, func, callback). 250 def apply_async(self, func, callback=None):. 251 """"""Schedule a func to be run"""""". --> 252 return self._get_pool().apply_async(. 253 SafeFunction(func), callback=callback). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/_parallel_backends.py:407, in ThreadingBackend._get_pool(self). 401 """"""Lazily initialize the thread pool. 402 . 403 The actual pool of worker threads is only initialized ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:7961,performance,Parallel,Parallel,7961,"ion"". 1063 ""limit being reached. Something is a little strange with your "". 1064 ""graph_data, and this may take longer than normal to compute."". 1065 ). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:1085, in Parallel.__call__(self, iterable). 1076 try:. 1077 # Only set self._iterating to True if at least a batch. 1078 # was dispatched. In particular this covers the edge. (...). 1082 # was very quick and its callback already dispatched all the. 1083 # remaining jobs. 1084 self._iterating = False. -> 1085 if self.dispatch_one_batch(iterator):. 1086 self._iterating = self._original_iterator is not None. 1088 while self.dispatch_one_batch(iterator):. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:901, in Parallel.dispatch_one_batch(self, iterator). 899 return False. 900 else:. --> 901 self._dispatch(tasks). 902 return True. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:819, in Parallel._dispatch(self, batch). 817 with self._lock:. 818 job_idx = len(self._jobs). --> 819 job = self._backend.apply_async(batch, callback=cb). 820 # A job can complete so quickly than its callback is. 821 # called before we get here, causing self._jobs to. 822 # grow. To ensure correct results ordering, .insert is. 823 # used (rather than .append) in the following line. 824 self._jobs.insert(job_idx, job). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/_parallel_backends.py:252, in PoolManagerMixin.apply_async(self, func, callback). 250 def apply_async(self, func, callback=None):. 251 """"""Schedule a func to be run"""""". --> 252 return self._get_pool().apply_async(. 253 SafeFunction(func), callback=callback). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/_parallel_backends.py:407, in ThreadingBackend._get_pool(self). 401 """"""Lazily initialize the thread pool. 402 . 403 The actual pool of worker threads is only initialized at the first. 404 ca",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:7986,performance,batch,batch,7986,"reached. Something is a little strange with your "". 1064 ""graph_data, and this may take longer than normal to compute."". 1065 ). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:1085, in Parallel.__call__(self, iterable). 1076 try:. 1077 # Only set self._iterating to True if at least a batch. 1078 # was dispatched. In particular this covers the edge. (...). 1082 # was very quick and its callback already dispatched all the. 1083 # remaining jobs. 1084 self._iterating = False. -> 1085 if self.dispatch_one_batch(iterator):. 1086 self._iterating = self._original_iterator is not None. 1088 while self.dispatch_one_batch(iterator):. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:901, in Parallel.dispatch_one_batch(self, iterator). 899 return False. 900 else:. --> 901 self._dispatch(tasks). 902 return True. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:819, in Parallel._dispatch(self, batch). 817 with self._lock:. 818 job_idx = len(self._jobs). --> 819 job = self._backend.apply_async(batch, callback=cb). 820 # A job can complete so quickly than its callback is. 821 # called before we get here, causing self._jobs to. 822 # grow. To ensure correct results ordering, .insert is. 823 # used (rather than .append) in the following line. 824 self._jobs.insert(job_idx, job). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/_parallel_backends.py:252, in PoolManagerMixin.apply_async(self, func, callback). 250 def apply_async(self, func, callback=None):. 251 """"""Schedule a func to be run"""""". --> 252 return self._get_pool().apply_async(. 253 SafeFunction(func), callback=callback). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/_parallel_backends.py:407, in ThreadingBackend._get_pool(self). 401 """"""Lazily initialize the thread pool. 402 . 403 The actual pool of worker threads is only initialized at the first. 404 call to apply_async. 405 """,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:8087,performance,batch,batch,8087,"ormal to compute."". 1065 ). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:1085, in Parallel.__call__(self, iterable). 1076 try:. 1077 # Only set self._iterating to True if at least a batch. 1078 # was dispatched. In particular this covers the edge. (...). 1082 # was very quick and its callback already dispatched all the. 1083 # remaining jobs. 1084 self._iterating = False. -> 1085 if self.dispatch_one_batch(iterator):. 1086 self._iterating = self._original_iterator is not None. 1088 while self.dispatch_one_batch(iterator):. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:901, in Parallel.dispatch_one_batch(self, iterator). 899 return False. 900 else:. --> 901 self._dispatch(tasks). 902 return True. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:819, in Parallel._dispatch(self, batch). 817 with self._lock:. 818 job_idx = len(self._jobs). --> 819 job = self._backend.apply_async(batch, callback=cb). 820 # A job can complete so quickly than its callback is. 821 # called before we get here, causing self._jobs to. 822 # grow. To ensure correct results ordering, .insert is. 823 # used (rather than .append) in the following line. 824 self._jobs.insert(job_idx, job). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/_parallel_backends.py:252, in PoolManagerMixin.apply_async(self, func, callback). 250 def apply_async(self, func, callback=None):. 251 """"""Schedule a func to be run"""""". --> 252 return self._get_pool().apply_async(. 253 SafeFunction(func), callback=callback). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/_parallel_backends.py:407, in ThreadingBackend._get_pool(self). 401 """"""Lazily initialize the thread pool. 402 . 403 The actual pool of worker threads is only initialized at the first. 404 call to apply_async. 405 """""". 406 if self._pool is None:. --> 407 self._pool = ThreadPool(self._n_jobs). 408 return self._pool.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:8584,performance,Schedul,Schedule,8584,"iterator is not None. 1088 while self.dispatch_one_batch(iterator):. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:901, in Parallel.dispatch_one_batch(self, iterator). 899 return False. 900 else:. --> 901 self._dispatch(tasks). 902 return True. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:819, in Parallel._dispatch(self, batch). 817 with self._lock:. 818 job_idx = len(self._jobs). --> 819 job = self._backend.apply_async(batch, callback=cb). 820 # A job can complete so quickly than its callback is. 821 # called before we get here, causing self._jobs to. 822 # grow. To ensure correct results ordering, .insert is. 823 # used (rather than .append) in the following line. 824 self._jobs.insert(job_idx, job). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/_parallel_backends.py:252, in PoolManagerMixin.apply_async(self, func, callback). 250 def apply_async(self, func, callback=None):. 251 """"""Schedule a func to be run"""""". --> 252 return self._get_pool().apply_async(. 253 SafeFunction(func), callback=callback). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/_parallel_backends.py:407, in ThreadingBackend._get_pool(self). 401 """"""Lazily initialize the thread pool. 402 . 403 The actual pool of worker threads is only initialized at the first. 404 call to apply_async. 405 """""". 406 if self._pool is None:. --> 407 self._pool = ThreadPool(self._n_jobs). 408 return self._pool. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:927, in ThreadPool.__init__(self, processes, initializer, initargs). 926 def __init__(self, processes=None, initializer=None, initargs=()):. --> 927 Pool.__init__(self, processes, initializer, initargs). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:216, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 214 for p in self._pool:. 215 if p.exitcode is None:. -",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:25,reliability,doe,does,25,"`compute_neighbors_umap` does not use `scanpy._settings.ScanpyConfig.n_jobs`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When trying to compute neighbors with default settings, I'm getting errors in slurm jobs, as the code tries to allocate all available cores on a node. I tried to control the number of cores available by setting `scanpy._settings.ScanpyConfig.n_jobs`, however I found that this setting does not get used in the offending function `compute_neighbors_umap`, where the following gets called. https://github.com/scverse/scanpy/blob/9cab8cfa4033d3f47a36c7bb816b2c9fae5cfdc6/scanpy/neighbors/__init__.py#L314-L322. The `nearest_neighbors` function from the `umap` provides the parameter `n_jobs`, which is set to -1 (i.e. all cores) by default. https://github.com/lmcinnes/umap/blob/e0cff20b392192f2b62fcc1843c9cb9ca2c9ee27/umap/umap_.py#L255-L266. However, as a user I would expect the number of cores to be defined by the ScanpyConfig. I assume this could also cause the kernel dying unintentionally in other work setups when users aren't aware of the number of cores the function is requesting. ### Minimal code sample. ```python. import scanpy as sc. sc._settings.ScanpyConfig(n_jobs=1). ad = sc.read('quite_large_h5ad_file'). sc.pp.pca(ad). sc.pp.neighbors(ad). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. RuntimeError Traceback (most recent call last). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:212, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 211 try:. --> 212 self._repopulate_pool(). 213 except Exception:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:303, in Pool.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:490,reliability,availab,available,490,"`compute_neighbors_umap` does not use `scanpy._settings.ScanpyConfig.n_jobs`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When trying to compute neighbors with default settings, I'm getting errors in slurm jobs, as the code tries to allocate all available cores on a node. I tried to control the number of cores available by setting `scanpy._settings.ScanpyConfig.n_jobs`, however I found that this setting does not get used in the offending function `compute_neighbors_umap`, where the following gets called. https://github.com/scverse/scanpy/blob/9cab8cfa4033d3f47a36c7bb816b2c9fae5cfdc6/scanpy/neighbors/__init__.py#L314-L322. The `nearest_neighbors` function from the `umap` provides the parameter `n_jobs`, which is set to -1 (i.e. all cores) by default. https://github.com/lmcinnes/umap/blob/e0cff20b392192f2b62fcc1843c9cb9ca2c9ee27/umap/umap_.py#L255-L266. However, as a user I would expect the number of cores to be defined by the ScanpyConfig. I assume this could also cause the kernel dying unintentionally in other work setups when users aren't aware of the number of cores the function is requesting. ### Minimal code sample. ```python. import scanpy as sc. sc._settings.ScanpyConfig(n_jobs=1). ad = sc.read('quite_large_h5ad_file'). sc.pp.pca(ad). sc.pp.neighbors(ad). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. RuntimeError Traceback (most recent call last). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:212, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 211 try:. --> 212 self._repopulate_pool(). 213 except Exception:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:303, in Pool.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:556,reliability,availab,available,556,"`compute_neighbors_umap` does not use `scanpy._settings.ScanpyConfig.n_jobs`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When trying to compute neighbors with default settings, I'm getting errors in slurm jobs, as the code tries to allocate all available cores on a node. I tried to control the number of cores available by setting `scanpy._settings.ScanpyConfig.n_jobs`, however I found that this setting does not get used in the offending function `compute_neighbors_umap`, where the following gets called. https://github.com/scverse/scanpy/blob/9cab8cfa4033d3f47a36c7bb816b2c9fae5cfdc6/scanpy/neighbors/__init__.py#L314-L322. The `nearest_neighbors` function from the `umap` provides the parameter `n_jobs`, which is set to -1 (i.e. all cores) by default. https://github.com/lmcinnes/umap/blob/e0cff20b392192f2b62fcc1843c9cb9ca2c9ee27/umap/umap_.py#L255-L266. However, as a user I would expect the number of cores to be defined by the ScanpyConfig. I assume this could also cause the kernel dying unintentionally in other work setups when users aren't aware of the number of cores the function is requesting. ### Minimal code sample. ```python. import scanpy as sc. sc._settings.ScanpyConfig(n_jobs=1). ad = sc.read('quite_large_h5ad_file'). sc.pp.pca(ad). sc.pp.neighbors(ad). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. RuntimeError Traceback (most recent call last). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:212, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 211 try:. --> 212 self._repopulate_pool(). 213 except Exception:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:303, in Pool.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:651,reliability,doe,does,651,"`compute_neighbors_umap` does not use `scanpy._settings.ScanpyConfig.n_jobs`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When trying to compute neighbors with default settings, I'm getting errors in slurm jobs, as the code tries to allocate all available cores on a node. I tried to control the number of cores available by setting `scanpy._settings.ScanpyConfig.n_jobs`, however I found that this setting does not get used in the offending function `compute_neighbors_umap`, where the following gets called. https://github.com/scverse/scanpy/blob/9cab8cfa4033d3f47a36c7bb816b2c9fae5cfdc6/scanpy/neighbors/__init__.py#L314-L322. The `nearest_neighbors` function from the `umap` provides the parameter `n_jobs`, which is set to -1 (i.e. all cores) by default. https://github.com/lmcinnes/umap/blob/e0cff20b392192f2b62fcc1843c9cb9ca2c9ee27/umap/umap_.py#L255-L266. However, as a user I would expect the number of cores to be defined by the ScanpyConfig. I assume this could also cause the kernel dying unintentionally in other work setups when users aren't aware of the number of cores the function is requesting. ### Minimal code sample. ```python. import scanpy as sc. sc._settings.ScanpyConfig(n_jobs=1). ad = sc.read('quite_large_h5ad_file'). sc.pp.pca(ad). sc.pp.neighbors(ad). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. RuntimeError Traceback (most recent call last). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:212, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 211 try:. --> 212 self._repopulate_pool(). 213 except Exception:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:303, in Pool.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:6945,reliability,fail,failed,6945,"(), ""Building RP forest with"", str(n_trees), ""trees""). --> 794 self._rp_forest = make_forest(. 795 data,. 796 n_neighbors,. 797 n_trees,. 798 leaf_size,. 799 self.rng_state,. 800 current_random_state,. 801 self.n_jobs,. 802 self._angular_trees,. 803 ). 804 leaf_array = rptree_leaf_array(self._rp_forest). 805 else:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/pynndescent/rp_trees.py:1056, in make_forest(data, n_neighbors, n_trees, leaf_size, rng_state, random_state, n_jobs, angular). 1044 result = joblib.Parallel(n_jobs=n_jobs, require=""sharedmem"")(. 1045 joblib.delayed(make_sparse_tree)(. 1046 data.indices,. (...). 1053 for i in range(n_trees). 1054 ). 1055 else:. -> 1056 result = joblib.Parallel(n_jobs=n_jobs, require=""sharedmem"")(. 1057 joblib.delayed(make_dense_tree)(data, rng_states[i], leaf_size, angular). 1058 for i in range(n_trees). 1059 ). 1060 except (RuntimeError, RecursionError, SystemError):. 1061 warn(. 1062 ""Random Projection forest initialisation failed due to recursion"". 1063 ""limit being reached. Something is a little strange with your "". 1064 ""graph_data, and this may take longer than normal to compute."". 1065 ). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:1085, in Parallel.__call__(self, iterable). 1076 try:. 1077 # Only set self._iterating to True if at least a batch. 1078 # was dispatched. In particular this covers the edge. (...). 1082 # was very quick and its callback already dispatched all the. 1083 # remaining jobs. 1084 self._iterating = False. -> 1085 if self.dispatch_one_batch(iterator):. 1086 self._iterating = self._original_iterator is not None. 1088 while self.dispatch_one_batch(iterator):. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:901, in Parallel.dispatch_one_batch(self, iterator). 899 return False. 900 else:. --> 901 self._dispatch(tasks). 902 return True. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/paralle",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:434,safety,error,errors,434,"`compute_neighbors_umap` does not use `scanpy._settings.ScanpyConfig.n_jobs`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When trying to compute neighbors with default settings, I'm getting errors in slurm jobs, as the code tries to allocate all available cores on a node. I tried to control the number of cores available by setting `scanpy._settings.ScanpyConfig.n_jobs`, however I found that this setting does not get used in the offending function `compute_neighbors_umap`, where the following gets called. https://github.com/scverse/scanpy/blob/9cab8cfa4033d3f47a36c7bb816b2c9fae5cfdc6/scanpy/neighbors/__init__.py#L314-L322. The `nearest_neighbors` function from the `umap` provides the parameter `n_jobs`, which is set to -1 (i.e. all cores) by default. https://github.com/lmcinnes/umap/blob/e0cff20b392192f2b62fcc1843c9cb9ca2c9ee27/umap/umap_.py#L255-L266. However, as a user I would expect the number of cores to be defined by the ScanpyConfig. I assume this could also cause the kernel dying unintentionally in other work setups when users aren't aware of the number of cores the function is requesting. ### Minimal code sample. ```python. import scanpy as sc. sc._settings.ScanpyConfig(n_jobs=1). ad = sc.read('quite_large_h5ad_file'). sc.pp.pca(ad). sc.pp.neighbors(ad). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. RuntimeError Traceback (most recent call last). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:212, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 211 try:. --> 212 self._repopulate_pool(). 213 except Exception:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:303, in Pool.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:490,safety,avail,available,490,"`compute_neighbors_umap` does not use `scanpy._settings.ScanpyConfig.n_jobs`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When trying to compute neighbors with default settings, I'm getting errors in slurm jobs, as the code tries to allocate all available cores on a node. I tried to control the number of cores available by setting `scanpy._settings.ScanpyConfig.n_jobs`, however I found that this setting does not get used in the offending function `compute_neighbors_umap`, where the following gets called. https://github.com/scverse/scanpy/blob/9cab8cfa4033d3f47a36c7bb816b2c9fae5cfdc6/scanpy/neighbors/__init__.py#L314-L322. The `nearest_neighbors` function from the `umap` provides the parameter `n_jobs`, which is set to -1 (i.e. all cores) by default. https://github.com/lmcinnes/umap/blob/e0cff20b392192f2b62fcc1843c9cb9ca2c9ee27/umap/umap_.py#L255-L266. However, as a user I would expect the number of cores to be defined by the ScanpyConfig. I assume this could also cause the kernel dying unintentionally in other work setups when users aren't aware of the number of cores the function is requesting. ### Minimal code sample. ```python. import scanpy as sc. sc._settings.ScanpyConfig(n_jobs=1). ad = sc.read('quite_large_h5ad_file'). sc.pp.pca(ad). sc.pp.neighbors(ad). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. RuntimeError Traceback (most recent call last). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:212, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 211 try:. --> 212 self._repopulate_pool(). 213 except Exception:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:303, in Pool.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:556,safety,avail,available,556,"`compute_neighbors_umap` does not use `scanpy._settings.ScanpyConfig.n_jobs`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When trying to compute neighbors with default settings, I'm getting errors in slurm jobs, as the code tries to allocate all available cores on a node. I tried to control the number of cores available by setting `scanpy._settings.ScanpyConfig.n_jobs`, however I found that this setting does not get used in the offending function `compute_neighbors_umap`, where the following gets called. https://github.com/scverse/scanpy/blob/9cab8cfa4033d3f47a36c7bb816b2c9fae5cfdc6/scanpy/neighbors/__init__.py#L314-L322. The `nearest_neighbors` function from the `umap` provides the parameter `n_jobs`, which is set to -1 (i.e. all cores) by default. https://github.com/lmcinnes/umap/blob/e0cff20b392192f2b62fcc1843c9cb9ca2c9ee27/umap/umap_.py#L255-L266. However, as a user I would expect the number of cores to be defined by the ScanpyConfig. I assume this could also cause the kernel dying unintentionally in other work setups when users aren't aware of the number of cores the function is requesting. ### Minimal code sample. ```python. import scanpy as sc. sc._settings.ScanpyConfig(n_jobs=1). ad = sc.read('quite_large_h5ad_file'). sc.pp.pca(ad). sc.pp.neighbors(ad). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. RuntimeError Traceback (most recent call last). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:212, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 211 try:. --> 212 self._repopulate_pool(). 213 except Exception:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:303, in Pool.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:1535,safety,Error,Error,1535,"e number of cores available by setting `scanpy._settings.ScanpyConfig.n_jobs`, however I found that this setting does not get used in the offending function `compute_neighbors_umap`, where the following gets called. https://github.com/scverse/scanpy/blob/9cab8cfa4033d3f47a36c7bb816b2c9fae5cfdc6/scanpy/neighbors/__init__.py#L314-L322. The `nearest_neighbors` function from the `umap` provides the parameter `n_jobs`, which is set to -1 (i.e. all cores) by default. https://github.com/lmcinnes/umap/blob/e0cff20b392192f2b62fcc1843c9cb9ca2c9ee27/umap/umap_.py#L255-L266. However, as a user I would expect the number of cores to be defined by the ScanpyConfig. I assume this could also cause the kernel dying unintentionally in other work setups when users aren't aware of the number of cores the function is requesting. ### Minimal code sample. ```python. import scanpy as sc. sc._settings.ScanpyConfig(n_jobs=1). ad = sc.read('quite_large_h5ad_file'). sc.pp.pca(ad). sc.pp.neighbors(ad). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. RuntimeError Traceback (most recent call last). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:212, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 211 try:. --> 212 self._repopulate_pool(). 213 except Exception:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:303, in Pool._repopulate_pool(self). 302 def _repopulate_pool(self):. --> 303 return self._repopulate_pool_static(self._ctx, self.Process,. 304 self._processes,. 305 self._pool, self._inqueue,. 306 self._outqueue, self._initializer,. 307 self._initargs,. 308 self._maxtasksperchild,. 309 self._wrap_exception). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:326, in Pool._repopulate_pool_static(ctx, Process, processes, pool, inqueue, outqueue, initializer, initargs, maxtasksperchild, wrap_exception). 325 w.daemon = True. ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:1894,safety,except,except,1894," function from the `umap` provides the parameter `n_jobs`, which is set to -1 (i.e. all cores) by default. https://github.com/lmcinnes/umap/blob/e0cff20b392192f2b62fcc1843c9cb9ca2c9ee27/umap/umap_.py#L255-L266. However, as a user I would expect the number of cores to be defined by the ScanpyConfig. I assume this could also cause the kernel dying unintentionally in other work setups when users aren't aware of the number of cores the function is requesting. ### Minimal code sample. ```python. import scanpy as sc. sc._settings.ScanpyConfig(n_jobs=1). ad = sc.read('quite_large_h5ad_file'). sc.pp.pca(ad). sc.pp.neighbors(ad). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. RuntimeError Traceback (most recent call last). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:212, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 211 try:. --> 212 self._repopulate_pool(). 213 except Exception:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:303, in Pool._repopulate_pool(self). 302 def _repopulate_pool(self):. --> 303 return self._repopulate_pool_static(self._ctx, self.Process,. 304 self._processes,. 305 self._pool, self._inqueue,. 306 self._outqueue, self._initializer,. 307 self._initargs,. 308 self._maxtasksperchild,. 309 self._wrap_exception). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:326, in Pool._repopulate_pool_static(ctx, Process, processes, pool, inqueue, outqueue, initializer, initargs, maxtasksperchild, wrap_exception). 325 w.daemon = True. --> 326 w.start(). 327 pool.append(w). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/dummy/__init__.py:51, in DummyProcess.start(self). 50 self._parent._children[self] = None. ---> 51 threading.Thread.start(self). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/threading.py:899, in Thread.start(self). 898 try:. --> 899 _start_new_threa",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:1901,safety,Except,Exception,1901," from the `umap` provides the parameter `n_jobs`, which is set to -1 (i.e. all cores) by default. https://github.com/lmcinnes/umap/blob/e0cff20b392192f2b62fcc1843c9cb9ca2c9ee27/umap/umap_.py#L255-L266. However, as a user I would expect the number of cores to be defined by the ScanpyConfig. I assume this could also cause the kernel dying unintentionally in other work setups when users aren't aware of the number of cores the function is requesting. ### Minimal code sample. ```python. import scanpy as sc. sc._settings.ScanpyConfig(n_jobs=1). ad = sc.read('quite_large_h5ad_file'). sc.pp.pca(ad). sc.pp.neighbors(ad). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. RuntimeError Traceback (most recent call last). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:212, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 211 try:. --> 212 self._repopulate_pool(). 213 except Exception:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:303, in Pool._repopulate_pool(self). 302 def _repopulate_pool(self):. --> 303 return self._repopulate_pool_static(self._ctx, self.Process,. 304 self._processes,. 305 self._pool, self._inqueue,. 306 self._outqueue, self._initializer,. 307 self._initargs,. 308 self._maxtasksperchild,. 309 self._wrap_exception). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:326, in Pool._repopulate_pool_static(ctx, Process, processes, pool, inqueue, outqueue, initializer, initargs, maxtasksperchild, wrap_exception). 325 w.daemon = True. --> 326 w.start(). 327 pool.append(w). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/dummy/__init__.py:51, in DummyProcess.start(self). 50 self._parent._children[self] = None. ---> 51 threading.Thread.start(self). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/threading.py:899, in Thread.start(self). 898 try:. --> 899 _start_new_thread(self._b",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:2925,safety,except,except,2925,"mbaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:303, in Pool._repopulate_pool(self). 302 def _repopulate_pool(self):. --> 303 return self._repopulate_pool_static(self._ctx, self.Process,. 304 self._processes,. 305 self._pool, self._inqueue,. 306 self._outqueue, self._initializer,. 307 self._initargs,. 308 self._maxtasksperchild,. 309 self._wrap_exception). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:326, in Pool._repopulate_pool_static(ctx, Process, processes, pool, inqueue, outqueue, initializer, initargs, maxtasksperchild, wrap_exception). 325 w.daemon = True. --> 326 w.start(). 327 pool.append(w). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/dummy/__init__.py:51, in DummyProcess.start(self). 50 self._parent._children[self] = None. ---> 51 threading.Thread.start(self). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/threading.py:899, in Thread.start(self). 898 try:. --> 899 _start_new_thread(self._bootstrap, ()). 900 except Exception:. RuntimeError: can't start new thread. During handling of the above exception, another exception occurred:. AttributeError Traceback (most recent call last). Cell In[13], line 1. ----> 1 sc.pp.neighbors(ad). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py:139, in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 137 adata._init_as_actual(adata.copy()). 138 neighbors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. 142 n_pcs=n_pcs,. 143 use_rep=use_rep,. 144 method=method,. 145 metric=metric,. 146 metric_kwds=metric_kwds,. 147 random_state=random_state,. 148 ). 150 if key_added is None:. 151 key_added = 'neighbors'. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py:794, in Neighbors.compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_i",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:2932,safety,Except,Exception,2932,"envs/scanpy/lib/python3.9/multiprocessing/pool.py:303, in Pool._repopulate_pool(self). 302 def _repopulate_pool(self):. --> 303 return self._repopulate_pool_static(self._ctx, self.Process,. 304 self._processes,. 305 self._pool, self._inqueue,. 306 self._outqueue, self._initializer,. 307 self._initargs,. 308 self._maxtasksperchild,. 309 self._wrap_exception). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:326, in Pool._repopulate_pool_static(ctx, Process, processes, pool, inqueue, outqueue, initializer, initargs, maxtasksperchild, wrap_exception). 325 w.daemon = True. --> 326 w.start(). 327 pool.append(w). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/dummy/__init__.py:51, in DummyProcess.start(self). 50 self._parent._children[self] = None. ---> 51 threading.Thread.start(self). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/threading.py:899, in Thread.start(self). 898 try:. --> 899 _start_new_thread(self._bootstrap, ()). 900 except Exception:. RuntimeError: can't start new thread. During handling of the above exception, another exception occurred:. AttributeError Traceback (most recent call last). Cell In[13], line 1. ----> 1 sc.pp.neighbors(ad). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py:139, in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 137 adata._init_as_actual(adata.copy()). 138 neighbors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. 142 n_pcs=n_pcs,. 143 use_rep=use_rep,. 144 method=method,. 145 metric=metric,. 146 metric_kwds=metric_kwds,. 147 random_state=random_state,. 148 ). 150 if key_added is None:. 151 key_added = 'neighbors'. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py:794, in Neighbors.compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, m",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:3011,safety,except,exception,3011,"(self). 302 def _repopulate_pool(self):. --> 303 return self._repopulate_pool_static(self._ctx, self.Process,. 304 self._processes,. 305 self._pool, self._inqueue,. 306 self._outqueue, self._initializer,. 307 self._initargs,. 308 self._maxtasksperchild,. 309 self._wrap_exception). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:326, in Pool._repopulate_pool_static(ctx, Process, processes, pool, inqueue, outqueue, initializer, initargs, maxtasksperchild, wrap_exception). 325 w.daemon = True. --> 326 w.start(). 327 pool.append(w). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/dummy/__init__.py:51, in DummyProcess.start(self). 50 self._parent._children[self] = None. ---> 51 threading.Thread.start(self). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/threading.py:899, in Thread.start(self). 898 try:. --> 899 _start_new_thread(self._bootstrap, ()). 900 except Exception:. RuntimeError: can't start new thread. During handling of the above exception, another exception occurred:. AttributeError Traceback (most recent call last). Cell In[13], line 1. ----> 1 sc.pp.neighbors(ad). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py:139, in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 137 adata._init_as_actual(adata.copy()). 138 neighbors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. 142 n_pcs=n_pcs,. 143 use_rep=use_rep,. 144 method=method,. 145 metric=metric,. 146 metric_kwds=metric_kwds,. 147 random_state=random_state,. 148 ). 150 if key_added is None:. 151 key_added = 'neighbors'. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py:794, in Neighbors.compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 792 X = pairwise_distances(X, metric=metric, **metric_kwds",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:3030,safety,except,exception,3030,"populate_pool(self):. --> 303 return self._repopulate_pool_static(self._ctx, self.Process,. 304 self._processes,. 305 self._pool, self._inqueue,. 306 self._outqueue, self._initializer,. 307 self._initargs,. 308 self._maxtasksperchild,. 309 self._wrap_exception). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:326, in Pool._repopulate_pool_static(ctx, Process, processes, pool, inqueue, outqueue, initializer, initargs, maxtasksperchild, wrap_exception). 325 w.daemon = True. --> 326 w.start(). 327 pool.append(w). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/dummy/__init__.py:51, in DummyProcess.start(self). 50 self._parent._children[self] = None. ---> 51 threading.Thread.start(self). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/threading.py:899, in Thread.start(self). 898 try:. --> 899 _start_new_thread(self._bootstrap, ()). 900 except Exception:. RuntimeError: can't start new thread. During handling of the above exception, another exception occurred:. AttributeError Traceback (most recent call last). Cell In[13], line 1. ----> 1 sc.pp.neighbors(ad). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py:139, in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 137 adata._init_as_actual(adata.copy()). 138 neighbors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. 142 n_pcs=n_pcs,. 143 use_rep=use_rep,. 144 method=method,. 145 metric=metric,. 146 metric_kwds=metric_kwds,. 147 random_state=random_state,. 148 ). 150 if key_added is None:. 151 key_added = 'neighbors'. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py:794, in Neighbors.compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 792 X = pairwise_distances(X, metric=metric, **metric_kwds). 793 metric = 'pr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:6834,safety,except,except,6834,"candidates, n_iters, delta, n_jobs, compressed, parallel_batch_queries, verbose). 792 if verbose:. 793 print(ts(), ""Building RP forest with"", str(n_trees), ""trees""). --> 794 self._rp_forest = make_forest(. 795 data,. 796 n_neighbors,. 797 n_trees,. 798 leaf_size,. 799 self.rng_state,. 800 current_random_state,. 801 self.n_jobs,. 802 self._angular_trees,. 803 ). 804 leaf_array = rptree_leaf_array(self._rp_forest). 805 else:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/pynndescent/rp_trees.py:1056, in make_forest(data, n_neighbors, n_trees, leaf_size, rng_state, random_state, n_jobs, angular). 1044 result = joblib.Parallel(n_jobs=n_jobs, require=""sharedmem"")(. 1045 joblib.delayed(make_sparse_tree)(. 1046 data.indices,. (...). 1053 for i in range(n_trees). 1054 ). 1055 else:. -> 1056 result = joblib.Parallel(n_jobs=n_jobs, require=""sharedmem"")(. 1057 joblib.delayed(make_dense_tree)(data, rng_states[i], leaf_size, angular). 1058 for i in range(n_trees). 1059 ). 1060 except (RuntimeError, RecursionError, SystemError):. 1061 warn(. 1062 ""Random Projection forest initialisation failed due to recursion"". 1063 ""limit being reached. Something is a little strange with your "". 1064 ""graph_data, and this may take longer than normal to compute."". 1065 ). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:1085, in Parallel.__call__(self, iterable). 1076 try:. 1077 # Only set self._iterating to True if at least a batch. 1078 # was dispatched. In particular this covers the edge. (...). 1082 # was very quick and its callback already dispatched all the. 1083 # remaining jobs. 1084 self._iterating = False. -> 1085 if self.dispatch_one_batch(iterator):. 1086 self._iterating = self._original_iterator is not None. 1088 while self.dispatch_one_batch(iterator):. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:901, in Parallel.dispatch_one_batch(self, iterator). 899 return False. 900 else:. --> 901 self._d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:8124,safety,compl,complete,8124,"ls/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:1085, in Parallel.__call__(self, iterable). 1076 try:. 1077 # Only set self._iterating to True if at least a batch. 1078 # was dispatched. In particular this covers the edge. (...). 1082 # was very quick and its callback already dispatched all the. 1083 # remaining jobs. 1084 self._iterating = False. -> 1085 if self.dispatch_one_batch(iterator):. 1086 self._iterating = self._original_iterator is not None. 1088 while self.dispatch_one_batch(iterator):. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:901, in Parallel.dispatch_one_batch(self, iterator). 899 return False. 900 else:. --> 901 self._dispatch(tasks). 902 return True. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:819, in Parallel._dispatch(self, batch). 817 with self._lock:. 818 job_idx = len(self._jobs). --> 819 job = self._backend.apply_async(batch, callback=cb). 820 # A job can complete so quickly than its callback is. 821 # called before we get here, causing self._jobs to. 822 # grow. To ensure correct results ordering, .insert is. 823 # used (rather than .append) in the following line. 824 self._jobs.insert(job_idx, job). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/_parallel_backends.py:252, in PoolManagerMixin.apply_async(self, func, callback). 250 def apply_async(self, func, callback=None):. 251 """"""Schedule a func to be run"""""". --> 252 return self._get_pool().apply_async(. 253 SafeFunction(func), callback=callback). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/_parallel_backends.py:407, in ThreadingBackend._get_pool(self). 401 """"""Lazily initialize the thread pool. 402 . 403 The actual pool of worker threads is only initialized at the first. 404 call to apply_async. 405 """""". 406 if self._pool is None:. --> 407 self._pool = ThreadPool(self._n_jobs). 408 return self._pool. File ~/tools/mambaforge/envs/scanpy/l",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:8664,safety,Safe,SafeFunction,8664,"mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:901, in Parallel.dispatch_one_batch(self, iterator). 899 return False. 900 else:. --> 901 self._dispatch(tasks). 902 return True. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:819, in Parallel._dispatch(self, batch). 817 with self._lock:. 818 job_idx = len(self._jobs). --> 819 job = self._backend.apply_async(batch, callback=cb). 820 # A job can complete so quickly than its callback is. 821 # called before we get here, causing self._jobs to. 822 # grow. To ensure correct results ordering, .insert is. 823 # used (rather than .append) in the following line. 824 self._jobs.insert(job_idx, job). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/_parallel_backends.py:252, in PoolManagerMixin.apply_async(self, func, callback). 250 def apply_async(self, func, callback=None):. 251 """"""Schedule a func to be run"""""". --> 252 return self._get_pool().apply_async(. 253 SafeFunction(func), callback=callback). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/_parallel_backends.py:407, in ThreadingBackend._get_pool(self). 401 """"""Lazily initialize the thread pool. 402 . 403 The actual pool of worker threads is only initialized at the first. 404 call to apply_async. 405 """""". 406 if self._pool is None:. --> 407 self._pool = ThreadPool(self._n_jobs). 408 return self._pool. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:927, in ThreadPool.__init__(self, processes, initializer, initargs). 926 def __init__(self, processes=None, initializer=None, initargs=()):. --> 927 Pool.__init__(self, processes, initializer, initargs). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:216, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 214 for p in self._pool:. 215 if p.exitcode is None:. --> 216 p.terminate(). 217 for p in self._pool:. 218 p.join(). AttributeError: 'Dum",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:490,security,availab,available,490,"`compute_neighbors_umap` does not use `scanpy._settings.ScanpyConfig.n_jobs`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When trying to compute neighbors with default settings, I'm getting errors in slurm jobs, as the code tries to allocate all available cores on a node. I tried to control the number of cores available by setting `scanpy._settings.ScanpyConfig.n_jobs`, however I found that this setting does not get used in the offending function `compute_neighbors_umap`, where the following gets called. https://github.com/scverse/scanpy/blob/9cab8cfa4033d3f47a36c7bb816b2c9fae5cfdc6/scanpy/neighbors/__init__.py#L314-L322. The `nearest_neighbors` function from the `umap` provides the parameter `n_jobs`, which is set to -1 (i.e. all cores) by default. https://github.com/lmcinnes/umap/blob/e0cff20b392192f2b62fcc1843c9cb9ca2c9ee27/umap/umap_.py#L255-L266. However, as a user I would expect the number of cores to be defined by the ScanpyConfig. I assume this could also cause the kernel dying unintentionally in other work setups when users aren't aware of the number of cores the function is requesting. ### Minimal code sample. ```python. import scanpy as sc. sc._settings.ScanpyConfig(n_jobs=1). ad = sc.read('quite_large_h5ad_file'). sc.pp.pca(ad). sc.pp.neighbors(ad). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. RuntimeError Traceback (most recent call last). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:212, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 211 try:. --> 212 self._repopulate_pool(). 213 except Exception:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:303, in Pool.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:528,security,control,control,528,"`compute_neighbors_umap` does not use `scanpy._settings.ScanpyConfig.n_jobs`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When trying to compute neighbors with default settings, I'm getting errors in slurm jobs, as the code tries to allocate all available cores on a node. I tried to control the number of cores available by setting `scanpy._settings.ScanpyConfig.n_jobs`, however I found that this setting does not get used in the offending function `compute_neighbors_umap`, where the following gets called. https://github.com/scverse/scanpy/blob/9cab8cfa4033d3f47a36c7bb816b2c9fae5cfdc6/scanpy/neighbors/__init__.py#L314-L322. The `nearest_neighbors` function from the `umap` provides the parameter `n_jobs`, which is set to -1 (i.e. all cores) by default. https://github.com/lmcinnes/umap/blob/e0cff20b392192f2b62fcc1843c9cb9ca2c9ee27/umap/umap_.py#L255-L266. However, as a user I would expect the number of cores to be defined by the ScanpyConfig. I assume this could also cause the kernel dying unintentionally in other work setups when users aren't aware of the number of cores the function is requesting. ### Minimal code sample. ```python. import scanpy as sc. sc._settings.ScanpyConfig(n_jobs=1). ad = sc.read('quite_large_h5ad_file'). sc.pp.pca(ad). sc.pp.neighbors(ad). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. RuntimeError Traceback (most recent call last). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:212, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 211 try:. --> 212 self._repopulate_pool(). 213 except Exception:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:303, in Pool.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:556,security,availab,available,556,"`compute_neighbors_umap` does not use `scanpy._settings.ScanpyConfig.n_jobs`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When trying to compute neighbors with default settings, I'm getting errors in slurm jobs, as the code tries to allocate all available cores on a node. I tried to control the number of cores available by setting `scanpy._settings.ScanpyConfig.n_jobs`, however I found that this setting does not get used in the offending function `compute_neighbors_umap`, where the following gets called. https://github.com/scverse/scanpy/blob/9cab8cfa4033d3f47a36c7bb816b2c9fae5cfdc6/scanpy/neighbors/__init__.py#L314-L322. The `nearest_neighbors` function from the `umap` provides the parameter `n_jobs`, which is set to -1 (i.e. all cores) by default. https://github.com/lmcinnes/umap/blob/e0cff20b392192f2b62fcc1843c9cb9ca2c9ee27/umap/umap_.py#L255-L266. However, as a user I would expect the number of cores to be defined by the ScanpyConfig. I assume this could also cause the kernel dying unintentionally in other work setups when users aren't aware of the number of cores the function is requesting. ### Minimal code sample. ```python. import scanpy as sc. sc._settings.ScanpyConfig(n_jobs=1). ad = sc.read('quite_large_h5ad_file'). sc.pp.pca(ad). sc.pp.neighbors(ad). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. RuntimeError Traceback (most recent call last). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:212, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 211 try:. --> 212 self._repopulate_pool(). 213 except Exception:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:303, in Pool.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:8124,security,compl,complete,8124,"ls/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:1085, in Parallel.__call__(self, iterable). 1076 try:. 1077 # Only set self._iterating to True if at least a batch. 1078 # was dispatched. In particular this covers the edge. (...). 1082 # was very quick and its callback already dispatched all the. 1083 # remaining jobs. 1084 self._iterating = False. -> 1085 if self.dispatch_one_batch(iterator):. 1086 self._iterating = self._original_iterator is not None. 1088 while self.dispatch_one_batch(iterator):. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:901, in Parallel.dispatch_one_batch(self, iterator). 899 return False. 900 else:. --> 901 self._dispatch(tasks). 902 return True. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:819, in Parallel._dispatch(self, batch). 817 with self._lock:. 818 job_idx = len(self._jobs). --> 819 job = self._backend.apply_async(batch, callback=cb). 820 # A job can complete so quickly than its callback is. 821 # called before we get here, causing self._jobs to. 822 # grow. To ensure correct results ordering, .insert is. 823 # used (rather than .append) in the following line. 824 self._jobs.insert(job_idx, job). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/_parallel_backends.py:252, in PoolManagerMixin.apply_async(self, func, callback). 250 def apply_async(self, func, callback=None):. 251 """"""Schedule a func to be run"""""". --> 252 return self._get_pool().apply_async(. 253 SafeFunction(func), callback=callback). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/_parallel_backends.py:407, in ThreadingBackend._get_pool(self). 401 """"""Lazily initialize the thread pool. 402 . 403 The actual pool of worker threads is only initialized at the first. 404 call to apply_async. 405 """""". 406 if self._pool is None:. --> 407 self._pool = ThreadPool(self._n_jobs). 408 return self._pool. File ~/tools/mambaforge/envs/scanpy/l",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:528,testability,control,control,528,"`compute_neighbors_umap` does not use `scanpy._settings.ScanpyConfig.n_jobs`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When trying to compute neighbors with default settings, I'm getting errors in slurm jobs, as the code tries to allocate all available cores on a node. I tried to control the number of cores available by setting `scanpy._settings.ScanpyConfig.n_jobs`, however I found that this setting does not get used in the offending function `compute_neighbors_umap`, where the following gets called. https://github.com/scverse/scanpy/blob/9cab8cfa4033d3f47a36c7bb816b2c9fae5cfdc6/scanpy/neighbors/__init__.py#L314-L322. The `nearest_neighbors` function from the `umap` provides the parameter `n_jobs`, which is set to -1 (i.e. all cores) by default. https://github.com/lmcinnes/umap/blob/e0cff20b392192f2b62fcc1843c9cb9ca2c9ee27/umap/umap_.py#L255-L266. However, as a user I would expect the number of cores to be defined by the ScanpyConfig. I assume this could also cause the kernel dying unintentionally in other work setups when users aren't aware of the number of cores the function is requesting. ### Minimal code sample. ```python. import scanpy as sc. sc._settings.ScanpyConfig(n_jobs=1). ad = sc.read('quite_large_h5ad_file'). sc.pp.pca(ad). sc.pp.neighbors(ad). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. RuntimeError Traceback (most recent call last). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:212, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 211 try:. --> 212 self._repopulate_pool(). 213 except Exception:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:303, in Pool.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:1648,testability,Trace,Traceback,1648,"es not get used in the offending function `compute_neighbors_umap`, where the following gets called. https://github.com/scverse/scanpy/blob/9cab8cfa4033d3f47a36c7bb816b2c9fae5cfdc6/scanpy/neighbors/__init__.py#L314-L322. The `nearest_neighbors` function from the `umap` provides the parameter `n_jobs`, which is set to -1 (i.e. all cores) by default. https://github.com/lmcinnes/umap/blob/e0cff20b392192f2b62fcc1843c9cb9ca2c9ee27/umap/umap_.py#L255-L266. However, as a user I would expect the number of cores to be defined by the ScanpyConfig. I assume this could also cause the kernel dying unintentionally in other work setups when users aren't aware of the number of cores the function is requesting. ### Minimal code sample. ```python. import scanpy as sc. sc._settings.ScanpyConfig(n_jobs=1). ad = sc.read('quite_large_h5ad_file'). sc.pp.pca(ad). sc.pp.neighbors(ad). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. RuntimeError Traceback (most recent call last). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:212, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 211 try:. --> 212 self._repopulate_pool(). 213 except Exception:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:303, in Pool._repopulate_pool(self). 302 def _repopulate_pool(self):. --> 303 return self._repopulate_pool_static(self._ctx, self.Process,. 304 self._processes,. 305 self._pool, self._inqueue,. 306 self._outqueue, self._initializer,. 307 self._initargs,. 308 self._maxtasksperchild,. 309 self._wrap_exception). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:326, in Pool._repopulate_pool_static(ctx, Process, processes, pool, inqueue, outqueue, initializer, initargs, maxtasksperchild, wrap_exception). 325 w.daemon = True. --> 326 w.start(). 327 pool.append(w). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/dummy/__in",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:1837,testability,context,context,1837,"neighbors/__init__.py#L314-L322. The `nearest_neighbors` function from the `umap` provides the parameter `n_jobs`, which is set to -1 (i.e. all cores) by default. https://github.com/lmcinnes/umap/blob/e0cff20b392192f2b62fcc1843c9cb9ca2c9ee27/umap/umap_.py#L255-L266. However, as a user I would expect the number of cores to be defined by the ScanpyConfig. I assume this could also cause the kernel dying unintentionally in other work setups when users aren't aware of the number of cores the function is requesting. ### Minimal code sample. ```python. import scanpy as sc. sc._settings.ScanpyConfig(n_jobs=1). ad = sc.read('quite_large_h5ad_file'). sc.pp.pca(ad). sc.pp.neighbors(ad). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. RuntimeError Traceback (most recent call last). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:212, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 211 try:. --> 212 self._repopulate_pool(). 213 except Exception:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:303, in Pool._repopulate_pool(self). 302 def _repopulate_pool(self):. --> 303 return self._repopulate_pool_static(self._ctx, self.Process,. 304 self._processes,. 305 self._pool, self._inqueue,. 306 self._outqueue, self._initializer,. 307 self._initargs,. 308 self._maxtasksperchild,. 309 self._wrap_exception). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:326, in Pool._repopulate_pool_static(ctx, Process, processes, pool, inqueue, outqueue, initializer, initargs, maxtasksperchild, wrap_exception). 325 w.daemon = True. --> 326 w.start(). 327 pool.append(w). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/dummy/__init__.py:51, in DummyProcess.start(self). 50 self._parent._children[self] = None. ---> 51 threading.Thread.start(self). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/threading.py:899, i",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:3066,testability,Trace,Traceback,3066," self._repopulate_pool_static(self._ctx, self.Process,. 304 self._processes,. 305 self._pool, self._inqueue,. 306 self._outqueue, self._initializer,. 307 self._initargs,. 308 self._maxtasksperchild,. 309 self._wrap_exception). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:326, in Pool._repopulate_pool_static(ctx, Process, processes, pool, inqueue, outqueue, initializer, initargs, maxtasksperchild, wrap_exception). 325 w.daemon = True. --> 326 w.start(). 327 pool.append(w). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/dummy/__init__.py:51, in DummyProcess.start(self). 50 self._parent._children[self] = None. ---> 51 threading.Thread.start(self). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/threading.py:899, in Thread.start(self). 898 try:. --> 899 _start_new_thread(self._bootstrap, ()). 900 except Exception:. RuntimeError: can't start new thread. During handling of the above exception, another exception occurred:. AttributeError Traceback (most recent call last). Cell In[13], line 1. ----> 1 sc.pp.neighbors(ad). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py:139, in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 137 adata._init_as_actual(adata.copy()). 138 neighbors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. 142 n_pcs=n_pcs,. 143 use_rep=use_rep,. 144 method=method,. 145 metric=metric,. 146 metric_kwds=metric_kwds,. 147 random_state=random_state,. 148 ). 150 if key_added is None:. 151 key_added = 'neighbors'. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py:794, in Neighbors.compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 792 X = pairwise_distances(X, metric=metric, **metric_kwds). 793 metric = 'precomputed'. --> 794 knn_indices, knn",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:9523,testability,context,context,9523," 250 def apply_async(self, func, callback=None):. 251 """"""Schedule a func to be run"""""". --> 252 return self._get_pool().apply_async(. 253 SafeFunction(func), callback=callback). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/_parallel_backends.py:407, in ThreadingBackend._get_pool(self). 401 """"""Lazily initialize the thread pool. 402 . 403 The actual pool of worker threads is only initialized at the first. 404 call to apply_async. 405 """""". 406 if self._pool is None:. --> 407 self._pool = ThreadPool(self._n_jobs). 408 return self._pool. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:927, in ThreadPool.__init__(self, processes, initializer, initargs). 926 def __init__(self, processes=None, initializer=None, initargs=()):. --> 927 Pool.__init__(self, processes, initializer, initargs). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:216, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 214 for p in self._pool:. 215 if p.exitcode is None:. --> 216 p.terminate(). 217 for p in self._pool:. 218 p.join(). AttributeError: 'DummyProcess' object has no attribute 'terminate'. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asttokens NA. backcall 0.2.0. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.8.0. igraph 0.10.4. ipykernel 6.23.1. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.6.3. mpl_toolkits NA. natsort 8.3.1. numba 0.56.4. numpy 1.23.5. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.0.9. pytz 2023.3. scipy",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:206,usability,confirm,confirmed,206,"`compute_neighbors_umap` does not use `scanpy._settings.ScanpyConfig.n_jobs`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When trying to compute neighbors with default settings, I'm getting errors in slurm jobs, as the code tries to allocate all available cores on a node. I tried to control the number of cores available by setting `scanpy._settings.ScanpyConfig.n_jobs`, however I found that this setting does not get used in the offending function `compute_neighbors_umap`, where the following gets called. https://github.com/scverse/scanpy/blob/9cab8cfa4033d3f47a36c7bb816b2c9fae5cfdc6/scanpy/neighbors/__init__.py#L314-L322. The `nearest_neighbors` function from the `umap` provides the parameter `n_jobs`, which is set to -1 (i.e. all cores) by default. https://github.com/lmcinnes/umap/blob/e0cff20b392192f2b62fcc1843c9cb9ca2c9ee27/umap/umap_.py#L255-L266. However, as a user I would expect the number of cores to be defined by the ScanpyConfig. I assume this could also cause the kernel dying unintentionally in other work setups when users aren't aware of the number of cores the function is requesting. ### Minimal code sample. ```python. import scanpy as sc. sc._settings.ScanpyConfig(n_jobs=1). ad = sc.read('quite_large_h5ad_file'). sc.pp.pca(ad). sc.pp.neighbors(ad). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. RuntimeError Traceback (most recent call last). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:212, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 211 try:. --> 212 self._repopulate_pool(). 213 except Exception:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:303, in Pool.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:289,usability,confirm,confirmed,289,"`compute_neighbors_umap` does not use `scanpy._settings.ScanpyConfig.n_jobs`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When trying to compute neighbors with default settings, I'm getting errors in slurm jobs, as the code tries to allocate all available cores on a node. I tried to control the number of cores available by setting `scanpy._settings.ScanpyConfig.n_jobs`, however I found that this setting does not get used in the offending function `compute_neighbors_umap`, where the following gets called. https://github.com/scverse/scanpy/blob/9cab8cfa4033d3f47a36c7bb816b2c9fae5cfdc6/scanpy/neighbors/__init__.py#L314-L322. The `nearest_neighbors` function from the `umap` provides the parameter `n_jobs`, which is set to -1 (i.e. all cores) by default. https://github.com/lmcinnes/umap/blob/e0cff20b392192f2b62fcc1843c9cb9ca2c9ee27/umap/umap_.py#L255-L266. However, as a user I would expect the number of cores to be defined by the ScanpyConfig. I assume this could also cause the kernel dying unintentionally in other work setups when users aren't aware of the number of cores the function is requesting. ### Minimal code sample. ```python. import scanpy as sc. sc._settings.ScanpyConfig(n_jobs=1). ad = sc.read('quite_large_h5ad_file'). sc.pp.pca(ad). sc.pp.neighbors(ad). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. RuntimeError Traceback (most recent call last). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:212, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 211 try:. --> 212 self._repopulate_pool(). 213 except Exception:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:303, in Pool.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:434,usability,error,errors,434,"`compute_neighbors_umap` does not use `scanpy._settings.ScanpyConfig.n_jobs`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When trying to compute neighbors with default settings, I'm getting errors in slurm jobs, as the code tries to allocate all available cores on a node. I tried to control the number of cores available by setting `scanpy._settings.ScanpyConfig.n_jobs`, however I found that this setting does not get used in the offending function `compute_neighbors_umap`, where the following gets called. https://github.com/scverse/scanpy/blob/9cab8cfa4033d3f47a36c7bb816b2c9fae5cfdc6/scanpy/neighbors/__init__.py#L314-L322. The `nearest_neighbors` function from the `umap` provides the parameter `n_jobs`, which is set to -1 (i.e. all cores) by default. https://github.com/lmcinnes/umap/blob/e0cff20b392192f2b62fcc1843c9cb9ca2c9ee27/umap/umap_.py#L255-L266. However, as a user I would expect the number of cores to be defined by the ScanpyConfig. I assume this could also cause the kernel dying unintentionally in other work setups when users aren't aware of the number of cores the function is requesting. ### Minimal code sample. ```python. import scanpy as sc. sc._settings.ScanpyConfig(n_jobs=1). ad = sc.read('quite_large_h5ad_file'). sc.pp.pca(ad). sc.pp.neighbors(ad). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. RuntimeError Traceback (most recent call last). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:212, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 211 try:. --> 212 self._repopulate_pool(). 213 except Exception:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:303, in Pool.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:1122,usability,user,user,1122," - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When trying to compute neighbors with default settings, I'm getting errors in slurm jobs, as the code tries to allocate all available cores on a node. I tried to control the number of cores available by setting `scanpy._settings.ScanpyConfig.n_jobs`, however I found that this setting does not get used in the offending function `compute_neighbors_umap`, where the following gets called. https://github.com/scverse/scanpy/blob/9cab8cfa4033d3f47a36c7bb816b2c9fae5cfdc6/scanpy/neighbors/__init__.py#L314-L322. The `nearest_neighbors` function from the `umap` provides the parameter `n_jobs`, which is set to -1 (i.e. all cores) by default. https://github.com/lmcinnes/umap/blob/e0cff20b392192f2b62fcc1843c9cb9ca2c9ee27/umap/umap_.py#L255-L266. However, as a user I would expect the number of cores to be defined by the ScanpyConfig. I assume this could also cause the kernel dying unintentionally in other work setups when users aren't aware of the number of cores the function is requesting. ### Minimal code sample. ```python. import scanpy as sc. sc._settings.ScanpyConfig(n_jobs=1). ad = sc.read('quite_large_h5ad_file'). sc.pp.pca(ad). sc.pp.neighbors(ad). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. RuntimeError Traceback (most recent call last). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:212, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 211 try:. --> 212 self._repopulate_pool(). 213 except Exception:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:303, in Pool._repopulate_pool(self). 302 def _repopulate_pool(self):. --> 303 return self._repopulate_pool_static(self._ctx, self.Process",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:1287,usability,user,users,1287,"onfirmed this bug exists on the master branch of scanpy. ### What happened? When trying to compute neighbors with default settings, I'm getting errors in slurm jobs, as the code tries to allocate all available cores on a node. I tried to control the number of cores available by setting `scanpy._settings.ScanpyConfig.n_jobs`, however I found that this setting does not get used in the offending function `compute_neighbors_umap`, where the following gets called. https://github.com/scverse/scanpy/blob/9cab8cfa4033d3f47a36c7bb816b2c9fae5cfdc6/scanpy/neighbors/__init__.py#L314-L322. The `nearest_neighbors` function from the `umap` provides the parameter `n_jobs`, which is set to -1 (i.e. all cores) by default. https://github.com/lmcinnes/umap/blob/e0cff20b392192f2b62fcc1843c9cb9ca2c9ee27/umap/umap_.py#L255-L266. However, as a user I would expect the number of cores to be defined by the ScanpyConfig. I assume this could also cause the kernel dying unintentionally in other work setups when users aren't aware of the number of cores the function is requesting. ### Minimal code sample. ```python. import scanpy as sc. sc._settings.ScanpyConfig(n_jobs=1). ad = sc.read('quite_large_h5ad_file'). sc.pp.pca(ad). sc.pp.neighbors(ad). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. RuntimeError Traceback (most recent call last). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:212, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 211 try:. --> 212 self._repopulate_pool(). 213 except Exception:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:303, in Pool._repopulate_pool(self). 302 def _repopulate_pool(self):. --> 303 return self._repopulate_pool_static(self._ctx, self.Process,. 304 self._processes,. 305 self._pool, self._inqueue,. 306 self._outqueue, self._initializer,. 307 self._initargs,. 308 self._maxtasksperchild,. 309 self._wrap_exce",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:1361,usability,Minim,Minimal,1361," When trying to compute neighbors with default settings, I'm getting errors in slurm jobs, as the code tries to allocate all available cores on a node. I tried to control the number of cores available by setting `scanpy._settings.ScanpyConfig.n_jobs`, however I found that this setting does not get used in the offending function `compute_neighbors_umap`, where the following gets called. https://github.com/scverse/scanpy/blob/9cab8cfa4033d3f47a36c7bb816b2c9fae5cfdc6/scanpy/neighbors/__init__.py#L314-L322. The `nearest_neighbors` function from the `umap` provides the parameter `n_jobs`, which is set to -1 (i.e. all cores) by default. https://github.com/lmcinnes/umap/blob/e0cff20b392192f2b62fcc1843c9cb9ca2c9ee27/umap/umap_.py#L255-L266. However, as a user I would expect the number of cores to be defined by the ScanpyConfig. I assume this could also cause the kernel dying unintentionally in other work setups when users aren't aware of the number of cores the function is requesting. ### Minimal code sample. ```python. import scanpy as sc. sc._settings.ScanpyConfig(n_jobs=1). ad = sc.read('quite_large_h5ad_file'). sc.pp.pca(ad). sc.pp.neighbors(ad). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. RuntimeError Traceback (most recent call last). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:212, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 211 try:. --> 212 self._repopulate_pool(). 213 except Exception:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:303, in Pool._repopulate_pool(self). 302 def _repopulate_pool(self):. --> 303 return self._repopulate_pool_static(self._ctx, self.Process,. 304 self._processes,. 305 self._pool, self._inqueue,. 306 self._outqueue, self._initializer,. 307 self._initargs,. 308 self._maxtasksperchild,. 309 self._wrap_exception). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/p",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:1535,usability,Error,Error,1535,"e number of cores available by setting `scanpy._settings.ScanpyConfig.n_jobs`, however I found that this setting does not get used in the offending function `compute_neighbors_umap`, where the following gets called. https://github.com/scverse/scanpy/blob/9cab8cfa4033d3f47a36c7bb816b2c9fae5cfdc6/scanpy/neighbors/__init__.py#L314-L322. The `nearest_neighbors` function from the `umap` provides the parameter `n_jobs`, which is set to -1 (i.e. all cores) by default. https://github.com/lmcinnes/umap/blob/e0cff20b392192f2b62fcc1843c9cb9ca2c9ee27/umap/umap_.py#L255-L266. However, as a user I would expect the number of cores to be defined by the ScanpyConfig. I assume this could also cause the kernel dying unintentionally in other work setups when users aren't aware of the number of cores the function is requesting. ### Minimal code sample. ```python. import scanpy as sc. sc._settings.ScanpyConfig(n_jobs=1). ad = sc.read('quite_large_h5ad_file'). sc.pp.pca(ad). sc.pp.neighbors(ad). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. RuntimeError Traceback (most recent call last). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:212, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 211 try:. --> 212 self._repopulate_pool(). 213 except Exception:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:303, in Pool._repopulate_pool(self). 302 def _repopulate_pool(self):. --> 303 return self._repopulate_pool_static(self._ctx, self.Process,. 304 self._processes,. 305 self._pool, self._inqueue,. 306 self._outqueue, self._initializer,. 307 self._initargs,. 308 self._maxtasksperchild,. 309 self._wrap_exception). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:326, in Pool._repopulate_pool_static(ctx, Process, processes, pool, inqueue, outqueue, initializer, initargs, maxtasksperchild, wrap_exception). 325 w.daemon = True. ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:1690,usability,tool,tools,1690,"n `compute_neighbors_umap`, where the following gets called. https://github.com/scverse/scanpy/blob/9cab8cfa4033d3f47a36c7bb816b2c9fae5cfdc6/scanpy/neighbors/__init__.py#L314-L322. The `nearest_neighbors` function from the `umap` provides the parameter `n_jobs`, which is set to -1 (i.e. all cores) by default. https://github.com/lmcinnes/umap/blob/e0cff20b392192f2b62fcc1843c9cb9ca2c9ee27/umap/umap_.py#L255-L266. However, as a user I would expect the number of cores to be defined by the ScanpyConfig. I assume this could also cause the kernel dying unintentionally in other work setups when users aren't aware of the number of cores the function is requesting. ### Minimal code sample. ```python. import scanpy as sc. sc._settings.ScanpyConfig(n_jobs=1). ad = sc.read('quite_large_h5ad_file'). sc.pp.pca(ad). sc.pp.neighbors(ad). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. RuntimeError Traceback (most recent call last). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:212, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 211 try:. --> 212 self._repopulate_pool(). 213 except Exception:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:303, in Pool._repopulate_pool(self). 302 def _repopulate_pool(self):. --> 303 return self._repopulate_pool_static(self._ctx, self.Process,. 304 self._processes,. 305 self._pool, self._inqueue,. 306 self._outqueue, self._initializer,. 307 self._initargs,. 308 self._maxtasksperchild,. 309 self._wrap_exception). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:326, in Pool._repopulate_pool_static(ctx, Process, processes, pool, inqueue, outqueue, initializer, initargs, maxtasksperchild, wrap_exception). 325 w.daemon = True. --> 326 w.start(). 327 pool.append(w). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/dummy/__init__.py:51, in DummyProcess.start(self).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:1920,usability,tool,tools,1920,"provides the parameter `n_jobs`, which is set to -1 (i.e. all cores) by default. https://github.com/lmcinnes/umap/blob/e0cff20b392192f2b62fcc1843c9cb9ca2c9ee27/umap/umap_.py#L255-L266. However, as a user I would expect the number of cores to be defined by the ScanpyConfig. I assume this could also cause the kernel dying unintentionally in other work setups when users aren't aware of the number of cores the function is requesting. ### Minimal code sample. ```python. import scanpy as sc. sc._settings.ScanpyConfig(n_jobs=1). ad = sc.read('quite_large_h5ad_file'). sc.pp.pca(ad). sc.pp.neighbors(ad). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. RuntimeError Traceback (most recent call last). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:212, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 211 try:. --> 212 self._repopulate_pool(). 213 except Exception:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:303, in Pool._repopulate_pool(self). 302 def _repopulate_pool(self):. --> 303 return self._repopulate_pool_static(self._ctx, self.Process,. 304 self._processes,. 305 self._pool, self._inqueue,. 306 self._outqueue, self._initializer,. 307 self._initargs,. 308 self._maxtasksperchild,. 309 self._wrap_exception). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:326, in Pool._repopulate_pool_static(ctx, Process, processes, pool, inqueue, outqueue, initializer, initargs, maxtasksperchild, wrap_exception). 325 w.daemon = True. --> 326 w.start(). 327 pool.append(w). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/dummy/__init__.py:51, in DummyProcess.start(self). 50 self._parent._children[self] = None. ---> 51 threading.Thread.start(self). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/threading.py:899, in Thread.start(self). 898 try:. --> 899 _start_new_thread(self._bootstrap, ()). 90",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:2305,usability,tool,tools,2305," the number of cores the function is requesting. ### Minimal code sample. ```python. import scanpy as sc. sc._settings.ScanpyConfig(n_jobs=1). ad = sc.read('quite_large_h5ad_file'). sc.pp.pca(ad). sc.pp.neighbors(ad). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. RuntimeError Traceback (most recent call last). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:212, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 211 try:. --> 212 self._repopulate_pool(). 213 except Exception:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:303, in Pool._repopulate_pool(self). 302 def _repopulate_pool(self):. --> 303 return self._repopulate_pool_static(self._ctx, self.Process,. 304 self._processes,. 305 self._pool, self._inqueue,. 306 self._outqueue, self._initializer,. 307 self._initargs,. 308 self._maxtasksperchild,. 309 self._wrap_exception). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:326, in Pool._repopulate_pool_static(ctx, Process, processes, pool, inqueue, outqueue, initializer, initargs, maxtasksperchild, wrap_exception). 325 w.daemon = True. --> 326 w.start(). 327 pool.append(w). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/dummy/__init__.py:51, in DummyProcess.start(self). 50 self._parent._children[self] = None. ---> 51 threading.Thread.start(self). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/threading.py:899, in Thread.start(self). 898 try:. --> 899 _start_new_thread(self._bootstrap, ()). 900 except Exception:. RuntimeError: can't start new thread. During handling of the above exception, another exception occurred:. AttributeError Traceback (most recent call last). Cell In[13], line 1. ----> 1 sc.pp.neighbors(ad). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py:139, in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, rando",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:2584,usability,tool,tools,2584,"----------------------------------------------. RuntimeError Traceback (most recent call last). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:212, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 211 try:. --> 212 self._repopulate_pool(). 213 except Exception:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:303, in Pool._repopulate_pool(self). 302 def _repopulate_pool(self):. --> 303 return self._repopulate_pool_static(self._ctx, self.Process,. 304 self._processes,. 305 self._pool, self._inqueue,. 306 self._outqueue, self._initializer,. 307 self._initargs,. 308 self._maxtasksperchild,. 309 self._wrap_exception). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:326, in Pool._repopulate_pool_static(ctx, Process, processes, pool, inqueue, outqueue, initializer, initargs, maxtasksperchild, wrap_exception). 325 w.daemon = True. --> 326 w.start(). 327 pool.append(w). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/dummy/__init__.py:51, in DummyProcess.start(self). 50 self._parent._children[self] = None. ---> 51 threading.Thread.start(self). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/threading.py:899, in Thread.start(self). 898 try:. --> 899 _start_new_thread(self._bootstrap, ()). 900 except Exception:. RuntimeError: can't start new thread. During handling of the above exception, another exception occurred:. AttributeError Traceback (most recent call last). Cell In[13], line 1. ----> 1 sc.pp.neighbors(ad). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py:139, in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 137 adata._init_as_actual(adata.copy()). 138 neighbors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. 142 n_pcs=n_pcs,. 143 use_rep=use_rep,. 144 method=method,. 145 me",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:2779,usability,tool,tools,2779,"f, processes, initializer, initargs, maxtasksperchild, context). 211 try:. --> 212 self._repopulate_pool(). 213 except Exception:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:303, in Pool._repopulate_pool(self). 302 def _repopulate_pool(self):. --> 303 return self._repopulate_pool_static(self._ctx, self.Process,. 304 self._processes,. 305 self._pool, self._inqueue,. 306 self._outqueue, self._initializer,. 307 self._initargs,. 308 self._maxtasksperchild,. 309 self._wrap_exception). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:326, in Pool._repopulate_pool_static(ctx, Process, processes, pool, inqueue, outqueue, initializer, initargs, maxtasksperchild, wrap_exception). 325 w.daemon = True. --> 326 w.start(). 327 pool.append(w). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/dummy/__init__.py:51, in DummyProcess.start(self). 50 self._parent._children[self] = None. ---> 51 threading.Thread.start(self). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/threading.py:899, in Thread.start(self). 898 try:. --> 899 _start_new_thread(self._bootstrap, ()). 900 except Exception:. RuntimeError: can't start new thread. During handling of the above exception, another exception occurred:. AttributeError Traceback (most recent call last). Cell In[13], line 1. ----> 1 sc.pp.neighbors(ad). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py:139, in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 137 adata._init_as_actual(adata.copy()). 138 neighbors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. 142 n_pcs=n_pcs,. 143 use_rep=use_rep,. 144 method=method,. 145 metric=metric,. 146 metric_kwds=metric_kwds,. 147 random_state=random_state,. 148 ). 150 if key_added is None:. 151 key_added = 'neighbors'. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-p",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:3158,usability,tool,tools,3158,"ol, self._inqueue,. 306 self._outqueue, self._initializer,. 307 self._initargs,. 308 self._maxtasksperchild,. 309 self._wrap_exception). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:326, in Pool._repopulate_pool_static(ctx, Process, processes, pool, inqueue, outqueue, initializer, initargs, maxtasksperchild, wrap_exception). 325 w.daemon = True. --> 326 w.start(). 327 pool.append(w). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/dummy/__init__.py:51, in DummyProcess.start(self). 50 self._parent._children[self] = None. ---> 51 threading.Thread.start(self). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/threading.py:899, in Thread.start(self). 898 try:. --> 899 _start_new_thread(self._bootstrap, ()). 900 except Exception:. RuntimeError: can't start new thread. During handling of the above exception, another exception occurred:. AttributeError Traceback (most recent call last). Cell In[13], line 1. ----> 1 sc.pp.neighbors(ad). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py:139, in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 137 adata._init_as_actual(adata.copy()). 138 neighbors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. 142 n_pcs=n_pcs,. 143 use_rep=use_rep,. 144 method=method,. 145 metric=metric,. 146 metric_kwds=metric_kwds,. 147 random_state=random_state,. 148 ). 150 if key_added is None:. 151 key_added = 'neighbors'. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py:794, in Neighbors.compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 792 X = pairwise_distances(X, metric=metric, **metric_kwds). 793 metric = 'precomputed'. --> 794 knn_indices, knn_distances, forest = compute_neighbors_umap(. 795 X, n_neighbors, random_state, metric=met",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:3733,usability,tool,tools,3733,"-> 51 threading.Thread.start(self). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/threading.py:899, in Thread.start(self). 898 try:. --> 899 _start_new_thread(self._bootstrap, ()). 900 except Exception:. RuntimeError: can't start new thread. During handling of the above exception, another exception occurred:. AttributeError Traceback (most recent call last). Cell In[13], line 1. ----> 1 sc.pp.neighbors(ad). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py:139, in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 137 adata._init_as_actual(adata.copy()). 138 neighbors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. 142 n_pcs=n_pcs,. 143 use_rep=use_rep,. 144 method=method,. 145 metric=metric,. 146 metric_kwds=metric_kwds,. 147 random_state=random_state,. 148 ). 150 if key_added is None:. 151 key_added = 'neighbors'. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py:794, in Neighbors.compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 792 X = pairwise_distances(X, metric=metric, **metric_kwds). 793 metric = 'precomputed'. --> 794 knn_indices, knn_distances, forest = compute_neighbors_umap(. 795 X, n_neighbors, random_state, metric=metric, metric_kwds=metric_kwds. 796 ). 797 # very cautious here. 798 try:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py:305, in compute_neighbors_umap(X, n_neighbors, random_state, metric, metric_kwds, angular, verbose). 301 from umap.umap_ import nearest_neighbors. 303 random_state = check_random_state(random_state). --> 305 knn_indices, knn_dists, forest = nearest_neighbors(. 306 X,. 307 n_neighbors,. 308 random_state=random_state,. 309 metric=metric,. 310 metric_kwds=metric_kwds,. 311 angular=angular,. 312 verbose=verbos",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:4241,usability,tool,tools,4241,"139, in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 137 adata._init_as_actual(adata.copy()). 138 neighbors = Neighbors(adata). --> 139 neighbors.compute_neighbors(. 140 n_neighbors=n_neighbors,. 141 knn=knn,. 142 n_pcs=n_pcs,. 143 use_rep=use_rep,. 144 method=method,. 145 metric=metric,. 146 metric_kwds=metric_kwds,. 147 random_state=random_state,. 148 ). 150 if key_added is None:. 151 key_added = 'neighbors'. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py:794, in Neighbors.compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 792 X = pairwise_distances(X, metric=metric, **metric_kwds). 793 metric = 'precomputed'. --> 794 knn_indices, knn_distances, forest = compute_neighbors_umap(. 795 X, n_neighbors, random_state, metric=metric, metric_kwds=metric_kwds. 796 ). 797 # very cautious here. 798 try:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py:305, in compute_neighbors_umap(X, n_neighbors, random_state, metric, metric_kwds, angular, verbose). 301 from umap.umap_ import nearest_neighbors. 303 random_state = check_random_state(random_state). --> 305 knn_indices, knn_dists, forest = nearest_neighbors(. 306 X,. 307 n_neighbors,. 308 random_state=random_state,. 309 metric=metric,. 310 metric_kwds=metric_kwds,. 311 angular=angular,. 312 verbose=verbose,. 313 ). 315 return knn_indices, knn_dists, forest. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/umap/umap_.py:328, in nearest_neighbors(X, n_neighbors, metric, metric_kwds, angular, random_state, low_memory, use_pynndescent, n_jobs, verbose). 325 n_trees = min(64, 5 + int(round((X.shape[0]) ** 0.5 / 20.0))). 326 n_iters = max(5, int(round(np.log2(X.shape[0])))). --> 328 knn_search_index = NNDescent(. 329 X,. 330 n_neighbors=n_neighbors,. 331 metric=metric,. 332 metric_kwds=metric_k",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:4797,usability,tool,tools,4797,"ghbors/__init__.py:794, in Neighbors.compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 792 X = pairwise_distances(X, metric=metric, **metric_kwds). 793 metric = 'precomputed'. --> 794 knn_indices, knn_distances, forest = compute_neighbors_umap(. 795 X, n_neighbors, random_state, metric=metric, metric_kwds=metric_kwds. 796 ). 797 # very cautious here. 798 try:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py:305, in compute_neighbors_umap(X, n_neighbors, random_state, metric, metric_kwds, angular, verbose). 301 from umap.umap_ import nearest_neighbors. 303 random_state = check_random_state(random_state). --> 305 knn_indices, knn_dists, forest = nearest_neighbors(. 306 X,. 307 n_neighbors,. 308 random_state=random_state,. 309 metric=metric,. 310 metric_kwds=metric_kwds,. 311 angular=angular,. 312 verbose=verbose,. 313 ). 315 return knn_indices, knn_dists, forest. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/umap/umap_.py:328, in nearest_neighbors(X, n_neighbors, metric, metric_kwds, angular, random_state, low_memory, use_pynndescent, n_jobs, verbose). 325 n_trees = min(64, 5 + int(round((X.shape[0]) ** 0.5 / 20.0))). 326 n_iters = max(5, int(round(np.log2(X.shape[0])))). --> 328 knn_search_index = NNDescent(. 329 X,. 330 n_neighbors=n_neighbors,. 331 metric=metric,. 332 metric_kwds=metric_kwds,. 333 random_state=random_state,. 334 n_trees=n_trees,. 335 n_iters=n_iters,. 336 max_candidates=60,. 337 low_memory=low_memory,. 338 n_jobs=n_jobs,. 339 verbose=verbose,. 340 compressed=False,. 341 ). 342 knn_indices, knn_dists = knn_search_index.neighbor_graph. 344 if verbose:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/pynndescent/pynndescent_.py:794, in NNDescent.__init__(self, data, metric, metric_kwds, n_neighbors, n_trees, leaf_size, pruning_degree_multiplier, diversify_prob, n_search_trees, tree_init, init_graph, init",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:5536,usability,tool,tools,5536,"indices, knn_dists, forest = nearest_neighbors(. 306 X,. 307 n_neighbors,. 308 random_state=random_state,. 309 metric=metric,. 310 metric_kwds=metric_kwds,. 311 angular=angular,. 312 verbose=verbose,. 313 ). 315 return knn_indices, knn_dists, forest. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/umap/umap_.py:328, in nearest_neighbors(X, n_neighbors, metric, metric_kwds, angular, random_state, low_memory, use_pynndescent, n_jobs, verbose). 325 n_trees = min(64, 5 + int(round((X.shape[0]) ** 0.5 / 20.0))). 326 n_iters = max(5, int(round(np.log2(X.shape[0])))). --> 328 knn_search_index = NNDescent(. 329 X,. 330 n_neighbors=n_neighbors,. 331 metric=metric,. 332 metric_kwds=metric_kwds,. 333 random_state=random_state,. 334 n_trees=n_trees,. 335 n_iters=n_iters,. 336 max_candidates=60,. 337 low_memory=low_memory,. 338 n_jobs=n_jobs,. 339 verbose=verbose,. 340 compressed=False,. 341 ). 342 knn_indices, knn_dists = knn_search_index.neighbor_graph. 344 if verbose:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/pynndescent/pynndescent_.py:794, in NNDescent.__init__(self, data, metric, metric_kwds, n_neighbors, n_trees, leaf_size, pruning_degree_multiplier, diversify_prob, n_search_trees, tree_init, init_graph, init_dist, random_state, low_memory, max_candidates, n_iters, delta, n_jobs, compressed, parallel_batch_queries, verbose). 792 if verbose:. 793 print(ts(), ""Building RP forest with"", str(n_trees), ""trees""). --> 794 self._rp_forest = make_forest(. 795 data,. 796 n_neighbors,. 797 n_trees,. 798 leaf_size,. 799 self.rng_state,. 800 current_random_state,. 801 self.n_jobs,. 802 self._angular_trees,. 803 ). 804 leaf_array = rptree_leaf_array(self._rp_forest). 805 else:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/pynndescent/rp_trees.py:1056, in make_forest(data, n_neighbors, n_trees, leaf_size, rng_state, random_state, n_jobs, angular). 1044 result = joblib.Parallel(n_jobs=n_jobs, require=""sharedmem"")(. 1045 joblib.del",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:6272,usability,tool,tools,6272,"tate,. 334 n_trees=n_trees,. 335 n_iters=n_iters,. 336 max_candidates=60,. 337 low_memory=low_memory,. 338 n_jobs=n_jobs,. 339 verbose=verbose,. 340 compressed=False,. 341 ). 342 knn_indices, knn_dists = knn_search_index.neighbor_graph. 344 if verbose:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/pynndescent/pynndescent_.py:794, in NNDescent.__init__(self, data, metric, metric_kwds, n_neighbors, n_trees, leaf_size, pruning_degree_multiplier, diversify_prob, n_search_trees, tree_init, init_graph, init_dist, random_state, low_memory, max_candidates, n_iters, delta, n_jobs, compressed, parallel_batch_queries, verbose). 792 if verbose:. 793 print(ts(), ""Building RP forest with"", str(n_trees), ""trees""). --> 794 self._rp_forest = make_forest(. 795 data,. 796 n_neighbors,. 797 n_trees,. 798 leaf_size,. 799 self.rng_state,. 800 current_random_state,. 801 self.n_jobs,. 802 self._angular_trees,. 803 ). 804 leaf_array = rptree_leaf_array(self._rp_forest). 805 else:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/pynndescent/rp_trees.py:1056, in make_forest(data, n_neighbors, n_trees, leaf_size, rng_state, random_state, n_jobs, angular). 1044 result = joblib.Parallel(n_jobs=n_jobs, require=""sharedmem"")(. 1045 joblib.delayed(make_sparse_tree)(. 1046 data.indices,. (...). 1053 for i in range(n_trees). 1054 ). 1055 else:. -> 1056 result = joblib.Parallel(n_jobs=n_jobs, require=""sharedmem"")(. 1057 joblib.delayed(make_dense_tree)(data, rng_states[i], leaf_size, angular). 1058 for i in range(n_trees). 1059 ). 1060 except (RuntimeError, RecursionError, SystemError):. 1061 warn(. 1062 ""Random Projection forest initialisation failed due to recursion"". 1063 ""limit being reached. Something is a little strange with your "". 1064 ""graph_data, and this may take longer than normal to compute."". 1065 ). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:1085, in Parallel.__call__(self, iterable). 1076 try:. 1077 # Only set sel",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:7125,usability,tool,tools,7125,"urrent_random_state,. 801 self.n_jobs,. 802 self._angular_trees,. 803 ). 804 leaf_array = rptree_leaf_array(self._rp_forest). 805 else:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/pynndescent/rp_trees.py:1056, in make_forest(data, n_neighbors, n_trees, leaf_size, rng_state, random_state, n_jobs, angular). 1044 result = joblib.Parallel(n_jobs=n_jobs, require=""sharedmem"")(. 1045 joblib.delayed(make_sparse_tree)(. 1046 data.indices,. (...). 1053 for i in range(n_trees). 1054 ). 1055 else:. -> 1056 result = joblib.Parallel(n_jobs=n_jobs, require=""sharedmem"")(. 1057 joblib.delayed(make_dense_tree)(data, rng_states[i], leaf_size, angular). 1058 for i in range(n_trees). 1059 ). 1060 except (RuntimeError, RecursionError, SystemError):. 1061 warn(. 1062 ""Random Projection forest initialisation failed due to recursion"". 1063 ""limit being reached. Something is a little strange with your "". 1064 ""graph_data, and this may take longer than normal to compute."". 1065 ). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:1085, in Parallel.__call__(self, iterable). 1076 try:. 1077 # Only set self._iterating to True if at least a batch. 1078 # was dispatched. In particular this covers the edge. (...). 1082 # was very quick and its callback already dispatched all the. 1083 # remaining jobs. 1084 self._iterating = False. -> 1085 if self.dispatch_one_batch(iterator):. 1086 self._iterating = self._original_iterator is not None. 1088 while self.dispatch_one_batch(iterator):. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:901, in Parallel.dispatch_one_batch(self, iterator). 899 return False. 900 else:. --> 901 self._dispatch(tasks). 902 return True. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:819, in Parallel._dispatch(self, batch). 817 with self._lock:. 818 job_idx = len(self._jobs). --> 819 job = self._backend.apply_async(batch, callback=cb). 820 # A job can comp",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:7664,usability,tool,tools,7664,"rallel(n_jobs=n_jobs, require=""sharedmem"")(. 1057 joblib.delayed(make_dense_tree)(data, rng_states[i], leaf_size, angular). 1058 for i in range(n_trees). 1059 ). 1060 except (RuntimeError, RecursionError, SystemError):. 1061 warn(. 1062 ""Random Projection forest initialisation failed due to recursion"". 1063 ""limit being reached. Something is a little strange with your "". 1064 ""graph_data, and this may take longer than normal to compute."". 1065 ). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:1085, in Parallel.__call__(self, iterable). 1076 try:. 1077 # Only set self._iterating to True if at least a batch. 1078 # was dispatched. In particular this covers the edge. (...). 1082 # was very quick and its callback already dispatched all the. 1083 # remaining jobs. 1084 self._iterating = False. -> 1085 if self.dispatch_one_batch(iterator):. 1086 self._iterating = self._original_iterator is not None. 1088 while self.dispatch_one_batch(iterator):. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:901, in Parallel.dispatch_one_batch(self, iterator). 899 return False. 900 else:. --> 901 self._dispatch(tasks). 902 return True. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:819, in Parallel._dispatch(self, batch). 817 with self._lock:. 818 job_idx = len(self._jobs). --> 819 job = self._backend.apply_async(batch, callback=cb). 820 # A job can complete so quickly than its callback is. 821 # called before we get here, causing self._jobs to. 822 # grow. To ensure correct results ordering, .insert is. 823 # used (rather than .append) in the following line. 824 self._jobs.insert(job_idx, job). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/_parallel_backends.py:252, in PoolManagerMixin.apply_async(self, func, callback). 250 def apply_async(self, func, callback=None):. 251 """"""Schedule a func to be run"""""". --> 252 return self._get_pool().apply_async(. 253 Saf",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:7877,usability,tool,tools,7877,"ror):. 1061 warn(. 1062 ""Random Projection forest initialisation failed due to recursion"". 1063 ""limit being reached. Something is a little strange with your "". 1064 ""graph_data, and this may take longer than normal to compute."". 1065 ). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:1085, in Parallel.__call__(self, iterable). 1076 try:. 1077 # Only set self._iterating to True if at least a batch. 1078 # was dispatched. In particular this covers the edge. (...). 1082 # was very quick and its callback already dispatched all the. 1083 # remaining jobs. 1084 self._iterating = False. -> 1085 if self.dispatch_one_batch(iterator):. 1086 self._iterating = self._original_iterator is not None. 1088 while self.dispatch_one_batch(iterator):. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:901, in Parallel.dispatch_one_batch(self, iterator). 899 return False. 900 else:. --> 901 self._dispatch(tasks). 902 return True. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:819, in Parallel._dispatch(self, batch). 817 with self._lock:. 818 job_idx = len(self._jobs). --> 819 job = self._backend.apply_async(batch, callback=cb). 820 # A job can complete so quickly than its callback is. 821 # called before we get here, causing self._jobs to. 822 # grow. To ensure correct results ordering, .insert is. 823 # used (rather than .append) in the following line. 824 self._jobs.insert(job_idx, job). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/_parallel_backends.py:252, in PoolManagerMixin.apply_async(self, func, callback). 250 def apply_async(self, func, callback=None):. 251 """"""Schedule a func to be run"""""". --> 252 return self._get_pool().apply_async(. 253 SafeFunction(func), callback=callback). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/_parallel_backends.py:407, in ThreadingBackend._get_pool(self). 401 """"""Lazily initialize the thread pool.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:8382,usability,tool,tools,8382,"82 # was very quick and its callback already dispatched all the. 1083 # remaining jobs. 1084 self._iterating = False. -> 1085 if self.dispatch_one_batch(iterator):. 1086 self._iterating = self._original_iterator is not None. 1088 while self.dispatch_one_batch(iterator):. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:901, in Parallel.dispatch_one_batch(self, iterator). 899 return False. 900 else:. --> 901 self._dispatch(tasks). 902 return True. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:819, in Parallel._dispatch(self, batch). 817 with self._lock:. 818 job_idx = len(self._jobs). --> 819 job = self._backend.apply_async(batch, callback=cb). 820 # A job can complete so quickly than its callback is. 821 # called before we get here, causing self._jobs to. 822 # grow. To ensure correct results ordering, .insert is. 823 # used (rather than .append) in the following line. 824 self._jobs.insert(job_idx, job). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/_parallel_backends.py:252, in PoolManagerMixin.apply_async(self, func, callback). 250 def apply_async(self, func, callback=None):. 251 """"""Schedule a func to be run"""""". --> 252 return self._get_pool().apply_async(. 253 SafeFunction(func), callback=callback). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/_parallel_backends.py:407, in ThreadingBackend._get_pool(self). 401 """"""Lazily initialize the thread pool. 402 . 403 The actual pool of worker threads is only initialized at the first. 404 call to apply_async. 405 """""". 406 if self._pool is None:. --> 407 self._pool = ThreadPool(self._n_jobs). 408 return self._pool. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:927, in ThreadPool.__init__(self, processes, initializer, initargs). 926 def __init__(self, processes=None, initializer=None, initargs=()):. --> 927 Pool.__init__(self, processes, initializer, initargs). File ~/tools/mam",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:8711,usability,tool,tools,8711,"ckages/joblib/parallel.py:901, in Parallel.dispatch_one_batch(self, iterator). 899 return False. 900 else:. --> 901 self._dispatch(tasks). 902 return True. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:819, in Parallel._dispatch(self, batch). 817 with self._lock:. 818 job_idx = len(self._jobs). --> 819 job = self._backend.apply_async(batch, callback=cb). 820 # A job can complete so quickly than its callback is. 821 # called before we get here, causing self._jobs to. 822 # grow. To ensure correct results ordering, .insert is. 823 # used (rather than .append) in the following line. 824 self._jobs.insert(job_idx, job). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/_parallel_backends.py:252, in PoolManagerMixin.apply_async(self, func, callback). 250 def apply_async(self, func, callback=None):. 251 """"""Schedule a func to be run"""""". --> 252 return self._get_pool().apply_async(. 253 SafeFunction(func), callback=callback). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/_parallel_backends.py:407, in ThreadingBackend._get_pool(self). 401 """"""Lazily initialize the thread pool. 402 . 403 The actual pool of worker threads is only initialized at the first. 404 call to apply_async. 405 """""". 406 if self._pool is None:. --> 407 self._pool = ThreadPool(self._n_jobs). 408 return self._pool. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:927, in ThreadPool.__init__(self, processes, initializer, initargs). 926 def __init__(self, processes=None, initializer=None, initargs=()):. --> 927 Pool.__init__(self, processes, initializer, initargs). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:216, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 214 for p in self._pool:. 215 if p.exitcode is None:. --> 216 p.terminate(). 217 for p in self._pool:. 218 p.join(). AttributeError: 'DummyProcess' object has no attribute 'terminat",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:9098,usability,tool,tools,9098,"k=cb). 820 # A job can complete so quickly than its callback is. 821 # called before we get here, causing self._jobs to. 822 # grow. To ensure correct results ordering, .insert is. 823 # used (rather than .append) in the following line. 824 self._jobs.insert(job_idx, job). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/_parallel_backends.py:252, in PoolManagerMixin.apply_async(self, func, callback). 250 def apply_async(self, func, callback=None):. 251 """"""Schedule a func to be run"""""". --> 252 return self._get_pool().apply_async(. 253 SafeFunction(func), callback=callback). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/_parallel_backends.py:407, in ThreadingBackend._get_pool(self). 401 """"""Lazily initialize the thread pool. 402 . 403 The actual pool of worker threads is only initialized at the first. 404 call to apply_async. 405 """""". 406 if self._pool is None:. --> 407 self._pool = ThreadPool(self._n_jobs). 408 return self._pool. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:927, in ThreadPool.__init__(self, processes, initializer, initargs). 926 def __init__(self, processes=None, initializer=None, initargs=()):. --> 927 Pool.__init__(self, processes, initializer, initargs). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:216, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 214 for p in self._pool:. 215 if p.exitcode is None:. --> 216 p.terminate(). 217 for p in self._pool:. 218 p.join(). AttributeError: 'DummyProcess' object has no attribute 'terminate'. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asttokens NA. backcall 0.2.0. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.8.0. igraph 0.10.4. ipykernel 6.23.1. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/issues/2506:9376,usability,tool,tools,9376," ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/_parallel_backends.py:252, in PoolManagerMixin.apply_async(self, func, callback). 250 def apply_async(self, func, callback=None):. 251 """"""Schedule a func to be run"""""". --> 252 return self._get_pool().apply_async(. 253 SafeFunction(func), callback=callback). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/_parallel_backends.py:407, in ThreadingBackend._get_pool(self). 401 """"""Lazily initialize the thread pool. 402 . 403 The actual pool of worker threads is only initialized at the first. 404 call to apply_async. 405 """""". 406 if self._pool is None:. --> 407 self._pool = ThreadPool(self._n_jobs). 408 return self._pool. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:927, in ThreadPool.__init__(self, processes, initializer, initargs). 926 def __init__(self, processes=None, initializer=None, initargs=()):. --> 927 Pool.__init__(self, processes, initializer, initargs). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:216, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context). 214 for p in self._pool:. 215 if p.exitcode is None:. --> 216 p.terminate(). 217 for p in self._pool:. 218 p.join(). AttributeError: 'DummyProcess' object has no attribute 'terminate'. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asttokens NA. backcall 0.2.0. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.8.0. igraph 0.10.4. ipykernel 6.23.1. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.6.3. mpl_toolkits NA. natsort 8.3.1. numba 0.56.4. numpy 1.23.5. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pydev_ipython",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506
https://github.com/scverse/scanpy/pull/2507:315,safety,review,review,315,Passing ScanpyConfig.settings.n_jobs to the `umap.nearest_neighbors` function; addresses #2506. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2507
https://github.com/scverse/scanpy/pull/2507:315,testability,review,review,315,Passing ScanpyConfig.settings.n_jobs to the `umap.nearest_neighbors` function; addresses #2506. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2507
https://github.com/scverse/scanpy/pull/2507:166,usability,guid,guidelines,166,Passing ScanpyConfig.settings.n_jobs to the `umap.nearest_neighbors` function; addresses #2506. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2507
https://github.com/scverse/scanpy/pull/2507:197,usability,guid,guide,197,Passing ScanpyConfig.settings.n_jobs to the `umap.nearest_neighbors` function; addresses #2506. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2507
https://github.com/scverse/scanpy/pull/2507:293,usability,workflow,workflow,293,Passing ScanpyConfig.settings.n_jobs to the `umap.nearest_neighbors` function; addresses #2506. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2507
https://github.com/scverse/scanpy/pull/2508:22,interoperability,compatib,compatibility,22,"Advertise Python 3.11 compatibility in package metadata; Upon Numba compatibility with python3.11, scanpy is now compatible with python3.11. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2508
https://github.com/scverse/scanpy/pull/2508:68,interoperability,compatib,compatibility,68,"Advertise Python 3.11 compatibility in package metadata; Upon Numba compatibility with python3.11, scanpy is now compatible with python3.11. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2508
https://github.com/scverse/scanpy/pull/2508:113,interoperability,compatib,compatible,113,"Advertise Python 3.11 compatibility in package metadata; Upon Numba compatibility with python3.11, scanpy is now compatible with python3.11. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2508
https://github.com/scverse/scanpy/pull/2508:39,modifiability,pac,package,39,"Advertise Python 3.11 compatibility in package metadata; Upon Numba compatibility with python3.11, scanpy is now compatible with python3.11. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2508
https://github.com/scverse/scanpy/pull/2508:360,safety,review,review,360,"Advertise Python 3.11 compatibility in package metadata; Upon Numba compatibility with python3.11, scanpy is now compatible with python3.11. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2508
https://github.com/scverse/scanpy/pull/2508:360,testability,review,review,360,"Advertise Python 3.11 compatibility in package metadata; Upon Numba compatibility with python3.11, scanpy is now compatible with python3.11. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2508
https://github.com/scverse/scanpy/pull/2508:211,usability,guid,guidelines,211,"Advertise Python 3.11 compatibility in package metadata; Upon Numba compatibility with python3.11, scanpy is now compatible with python3.11. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2508
https://github.com/scverse/scanpy/pull/2508:242,usability,guid,guide,242,"Advertise Python 3.11 compatibility in package metadata; Upon Numba compatibility with python3.11, scanpy is now compatible with python3.11. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2508
https://github.com/scverse/scanpy/pull/2508:338,usability,workflow,workflow,338,"Advertise Python 3.11 compatibility in package metadata; Upon Numba compatibility with python3.11, scanpy is now compatible with python3.11. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2508
https://github.com/scverse/scanpy/pull/2509:92,availability,cluster,clustered,92,"More inline example plots in docs; Added examples for stacked violin plots, tracksplots and clustered heatmaps. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2509
https://github.com/scverse/scanpy/pull/2509:54,deployability,stack,stacked,54,"More inline example plots in docs; Added examples for stacked violin plots, tracksplots and clustered heatmaps. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2509
https://github.com/scverse/scanpy/pull/2509:92,deployability,cluster,clustered,92,"More inline example plots in docs; Added examples for stacked violin plots, tracksplots and clustered heatmaps. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2509
https://github.com/scverse/scanpy/pull/2509:102,energy efficiency,heat,heatmaps,102,"More inline example plots in docs; Added examples for stacked violin plots, tracksplots and clustered heatmaps. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2509
https://github.com/scverse/scanpy/pull/2509:331,safety,review,review,331,"More inline example plots in docs; Added examples for stacked violin plots, tracksplots and clustered heatmaps. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2509
https://github.com/scverse/scanpy/pull/2509:331,testability,review,review,331,"More inline example plots in docs; Added examples for stacked violin plots, tracksplots and clustered heatmaps. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2509
https://github.com/scverse/scanpy/pull/2509:182,usability,guid,guidelines,182,"More inline example plots in docs; Added examples for stacked violin plots, tracksplots and clustered heatmaps. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2509
https://github.com/scverse/scanpy/pull/2509:213,usability,guid,guide,213,"More inline example plots in docs; Added examples for stacked violin plots, tracksplots and clustered heatmaps. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2509
https://github.com/scverse/scanpy/pull/2509:309,usability,workflow,workflow,309,"More inline example plots in docs; Added examples for stacked violin plots, tracksplots and clustered heatmaps. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2509
https://github.com/scverse/scanpy/issues/2510:191,deployability,version,version,191,Bug label looks wrong; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I saw that the label is actually called Bug  and not bug,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2510
https://github.com/scverse/scanpy/issues/2510:191,integrability,version,version,191,Bug label looks wrong; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I saw that the label is actually called Bug  and not bug,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2510
https://github.com/scverse/scanpy/issues/2510:191,modifiability,version,version,191,Bug label looks wrong; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I saw that the label is actually called Bug  and not bug,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2510
https://github.com/scverse/scanpy/issues/2510:151,usability,confirm,confirmed,151,Bug label looks wrong; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I saw that the label is actually called Bug  and not bug,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2510
https://github.com/scverse/scanpy/issues/2510:234,usability,confirm,confirmed,234,Bug label looks wrong; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I saw that the label is actually called Bug  and not bug,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2510
https://github.com/scverse/scanpy/pull/2512:256,availability,avail,available,256,"Integration of Marsilea to create Heatmap from AnnData; This PR is for #2444 . The project ""heatgraphy"" mentioned in the issue is renamed to ""Marsilea"", https://github.com/Marsilea-viz/marsilea. The previous link to the image mentioned in the issue is not available anymore. Please refer to [this link](https://marsilea.readthedocs.io/en/latest/auto_examples/plot_pbmc3k.html). I added a minimum example in the `external` part. Here is a code example of using the plotting function. Remember to install Marsilea before proceeding:. ```shell. pip install marsilea. ```. ```python. import scanpy as sc. adata = sc.datasets.pbmc68k_reduced(). adata.var[""gene_names""] = adata.var.index. t = sc.external.pl.MarsileaHeatmap(adata, vmin=-2, vmax=2, center=0, label=""Expression""). t.add_right(""bulk_labels"", ""colors"", size=.2, label=""Cell Lables""). t.add_dendrogram(""right"", add_base=False, add_meta=True). t.h_groupby(""bulk_labels""). t.add_top(""n_counts"", ""bar"", show_value=False, label=""Counts""). genes = ['CDK6', 'UQCRFS1', 'SH2D2A', 'CD63', 'CEP152',. 'SIAH2', 'LY86', 'HSPB1', 'POLR2G', 'IGBP1']. t.add_bottom(""gene_names"", ""annot"", mark=genes). t.add_title(""Expression Heatmap""). t.legend(). t.show(). ```. If everything goes well, it should display something like this:. ![download](https://github.com/scverse/scanpy/assets/23433306/eb089061-b9ef-4d49-a2e0-fb3adcf06cd7).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512
https://github.com/scverse/scanpy/pull/2512:1272,availability,down,download,1272,"Integration of Marsilea to create Heatmap from AnnData; This PR is for #2444 . The project ""heatgraphy"" mentioned in the issue is renamed to ""Marsilea"", https://github.com/Marsilea-viz/marsilea. The previous link to the image mentioned in the issue is not available anymore. Please refer to [this link](https://marsilea.readthedocs.io/en/latest/auto_examples/plot_pbmc3k.html). I added a minimum example in the `external` part. Here is a code example of using the plotting function. Remember to install Marsilea before proceeding:. ```shell. pip install marsilea. ```. ```python. import scanpy as sc. adata = sc.datasets.pbmc68k_reduced(). adata.var[""gene_names""] = adata.var.index. t = sc.external.pl.MarsileaHeatmap(adata, vmin=-2, vmax=2, center=0, label=""Expression""). t.add_right(""bulk_labels"", ""colors"", size=.2, label=""Cell Lables""). t.add_dendrogram(""right"", add_base=False, add_meta=True). t.h_groupby(""bulk_labels""). t.add_top(""n_counts"", ""bar"", show_value=False, label=""Counts""). genes = ['CDK6', 'UQCRFS1', 'SH2D2A', 'CD63', 'CEP152',. 'SIAH2', 'LY86', 'HSPB1', 'POLR2G', 'IGBP1']. t.add_bottom(""gene_names"", ""annot"", mark=genes). t.add_title(""Expression Heatmap""). t.legend(). t.show(). ```. If everything goes well, it should display something like this:. ![download](https://github.com/scverse/scanpy/assets/23433306/eb089061-b9ef-4d49-a2e0-fb3adcf06cd7).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512
https://github.com/scverse/scanpy/pull/2512:0,deployability,Integr,Integration,0,"Integration of Marsilea to create Heatmap from AnnData; This PR is for #2444 . The project ""heatgraphy"" mentioned in the issue is renamed to ""Marsilea"", https://github.com/Marsilea-viz/marsilea. The previous link to the image mentioned in the issue is not available anymore. Please refer to [this link](https://marsilea.readthedocs.io/en/latest/auto_examples/plot_pbmc3k.html). I added a minimum example in the `external` part. Here is a code example of using the plotting function. Remember to install Marsilea before proceeding:. ```shell. pip install marsilea. ```. ```python. import scanpy as sc. adata = sc.datasets.pbmc68k_reduced(). adata.var[""gene_names""] = adata.var.index. t = sc.external.pl.MarsileaHeatmap(adata, vmin=-2, vmax=2, center=0, label=""Expression""). t.add_right(""bulk_labels"", ""colors"", size=.2, label=""Cell Lables""). t.add_dendrogram(""right"", add_base=False, add_meta=True). t.h_groupby(""bulk_labels""). t.add_top(""n_counts"", ""bar"", show_value=False, label=""Counts""). genes = ['CDK6', 'UQCRFS1', 'SH2D2A', 'CD63', 'CEP152',. 'SIAH2', 'LY86', 'HSPB1', 'POLR2G', 'IGBP1']. t.add_bottom(""gene_names"", ""annot"", mark=genes). t.add_title(""Expression Heatmap""). t.legend(). t.show(). ```. If everything goes well, it should display something like this:. ![download](https://github.com/scverse/scanpy/assets/23433306/eb089061-b9ef-4d49-a2e0-fb3adcf06cd7).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512
https://github.com/scverse/scanpy/pull/2512:495,deployability,instal,install,495,"Integration of Marsilea to create Heatmap from AnnData; This PR is for #2444 . The project ""heatgraphy"" mentioned in the issue is renamed to ""Marsilea"", https://github.com/Marsilea-viz/marsilea. The previous link to the image mentioned in the issue is not available anymore. Please refer to [this link](https://marsilea.readthedocs.io/en/latest/auto_examples/plot_pbmc3k.html). I added a minimum example in the `external` part. Here is a code example of using the plotting function. Remember to install Marsilea before proceeding:. ```shell. pip install marsilea. ```. ```python. import scanpy as sc. adata = sc.datasets.pbmc68k_reduced(). adata.var[""gene_names""] = adata.var.index. t = sc.external.pl.MarsileaHeatmap(adata, vmin=-2, vmax=2, center=0, label=""Expression""). t.add_right(""bulk_labels"", ""colors"", size=.2, label=""Cell Lables""). t.add_dendrogram(""right"", add_base=False, add_meta=True). t.h_groupby(""bulk_labels""). t.add_top(""n_counts"", ""bar"", show_value=False, label=""Counts""). genes = ['CDK6', 'UQCRFS1', 'SH2D2A', 'CD63', 'CEP152',. 'SIAH2', 'LY86', 'HSPB1', 'POLR2G', 'IGBP1']. t.add_bottom(""gene_names"", ""annot"", mark=genes). t.add_title(""Expression Heatmap""). t.legend(). t.show(). ```. If everything goes well, it should display something like this:. ![download](https://github.com/scverse/scanpy/assets/23433306/eb089061-b9ef-4d49-a2e0-fb3adcf06cd7).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512
https://github.com/scverse/scanpy/pull/2512:546,deployability,instal,install,546,"Integration of Marsilea to create Heatmap from AnnData; This PR is for #2444 . The project ""heatgraphy"" mentioned in the issue is renamed to ""Marsilea"", https://github.com/Marsilea-viz/marsilea. The previous link to the image mentioned in the issue is not available anymore. Please refer to [this link](https://marsilea.readthedocs.io/en/latest/auto_examples/plot_pbmc3k.html). I added a minimum example in the `external` part. Here is a code example of using the plotting function. Remember to install Marsilea before proceeding:. ```shell. pip install marsilea. ```. ```python. import scanpy as sc. adata = sc.datasets.pbmc68k_reduced(). adata.var[""gene_names""] = adata.var.index. t = sc.external.pl.MarsileaHeatmap(adata, vmin=-2, vmax=2, center=0, label=""Expression""). t.add_right(""bulk_labels"", ""colors"", size=.2, label=""Cell Lables""). t.add_dendrogram(""right"", add_base=False, add_meta=True). t.h_groupby(""bulk_labels""). t.add_top(""n_counts"", ""bar"", show_value=False, label=""Counts""). genes = ['CDK6', 'UQCRFS1', 'SH2D2A', 'CD63', 'CEP152',. 'SIAH2', 'LY86', 'HSPB1', 'POLR2G', 'IGBP1']. t.add_bottom(""gene_names"", ""annot"", mark=genes). t.add_title(""Expression Heatmap""). t.legend(). t.show(). ```. If everything goes well, it should display something like this:. ![download](https://github.com/scverse/scanpy/assets/23433306/eb089061-b9ef-4d49-a2e0-fb3adcf06cd7).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512
https://github.com/scverse/scanpy/pull/2512:34,energy efficiency,Heat,Heatmap,34,"Integration of Marsilea to create Heatmap from AnnData; This PR is for #2444 . The project ""heatgraphy"" mentioned in the issue is renamed to ""Marsilea"", https://github.com/Marsilea-viz/marsilea. The previous link to the image mentioned in the issue is not available anymore. Please refer to [this link](https://marsilea.readthedocs.io/en/latest/auto_examples/plot_pbmc3k.html). I added a minimum example in the `external` part. Here is a code example of using the plotting function. Remember to install Marsilea before proceeding:. ```shell. pip install marsilea. ```. ```python. import scanpy as sc. adata = sc.datasets.pbmc68k_reduced(). adata.var[""gene_names""] = adata.var.index. t = sc.external.pl.MarsileaHeatmap(adata, vmin=-2, vmax=2, center=0, label=""Expression""). t.add_right(""bulk_labels"", ""colors"", size=.2, label=""Cell Lables""). t.add_dendrogram(""right"", add_base=False, add_meta=True). t.h_groupby(""bulk_labels""). t.add_top(""n_counts"", ""bar"", show_value=False, label=""Counts""). genes = ['CDK6', 'UQCRFS1', 'SH2D2A', 'CD63', 'CEP152',. 'SIAH2', 'LY86', 'HSPB1', 'POLR2G', 'IGBP1']. t.add_bottom(""gene_names"", ""annot"", mark=genes). t.add_title(""Expression Heatmap""). t.legend(). t.show(). ```. If everything goes well, it should display something like this:. ![download](https://github.com/scverse/scanpy/assets/23433306/eb089061-b9ef-4d49-a2e0-fb3adcf06cd7).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512
https://github.com/scverse/scanpy/pull/2512:92,energy efficiency,heat,heatgraphy,92,"Integration of Marsilea to create Heatmap from AnnData; This PR is for #2444 . The project ""heatgraphy"" mentioned in the issue is renamed to ""Marsilea"", https://github.com/Marsilea-viz/marsilea. The previous link to the image mentioned in the issue is not available anymore. Please refer to [this link](https://marsilea.readthedocs.io/en/latest/auto_examples/plot_pbmc3k.html). I added a minimum example in the `external` part. Here is a code example of using the plotting function. Remember to install Marsilea before proceeding:. ```shell. pip install marsilea. ```. ```python. import scanpy as sc. adata = sc.datasets.pbmc68k_reduced(). adata.var[""gene_names""] = adata.var.index. t = sc.external.pl.MarsileaHeatmap(adata, vmin=-2, vmax=2, center=0, label=""Expression""). t.add_right(""bulk_labels"", ""colors"", size=.2, label=""Cell Lables""). t.add_dendrogram(""right"", add_base=False, add_meta=True). t.h_groupby(""bulk_labels""). t.add_top(""n_counts"", ""bar"", show_value=False, label=""Counts""). genes = ['CDK6', 'UQCRFS1', 'SH2D2A', 'CD63', 'CEP152',. 'SIAH2', 'LY86', 'HSPB1', 'POLR2G', 'IGBP1']. t.add_bottom(""gene_names"", ""annot"", mark=genes). t.add_title(""Expression Heatmap""). t.legend(). t.show(). ```. If everything goes well, it should display something like this:. ![download](https://github.com/scverse/scanpy/assets/23433306/eb089061-b9ef-4d49-a2e0-fb3adcf06cd7).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512
https://github.com/scverse/scanpy/pull/2512:1167,energy efficiency,Heat,Heatmap,1167,"Integration of Marsilea to create Heatmap from AnnData; This PR is for #2444 . The project ""heatgraphy"" mentioned in the issue is renamed to ""Marsilea"", https://github.com/Marsilea-viz/marsilea. The previous link to the image mentioned in the issue is not available anymore. Please refer to [this link](https://marsilea.readthedocs.io/en/latest/auto_examples/plot_pbmc3k.html). I added a minimum example in the `external` part. Here is a code example of using the plotting function. Remember to install Marsilea before proceeding:. ```shell. pip install marsilea. ```. ```python. import scanpy as sc. adata = sc.datasets.pbmc68k_reduced(). adata.var[""gene_names""] = adata.var.index. t = sc.external.pl.MarsileaHeatmap(adata, vmin=-2, vmax=2, center=0, label=""Expression""). t.add_right(""bulk_labels"", ""colors"", size=.2, label=""Cell Lables""). t.add_dendrogram(""right"", add_base=False, add_meta=True). t.h_groupby(""bulk_labels""). t.add_top(""n_counts"", ""bar"", show_value=False, label=""Counts""). genes = ['CDK6', 'UQCRFS1', 'SH2D2A', 'CD63', 'CEP152',. 'SIAH2', 'LY86', 'HSPB1', 'POLR2G', 'IGBP1']. t.add_bottom(""gene_names"", ""annot"", mark=genes). t.add_title(""Expression Heatmap""). t.legend(). t.show(). ```. If everything goes well, it should display something like this:. ![download](https://github.com/scverse/scanpy/assets/23433306/eb089061-b9ef-4d49-a2e0-fb3adcf06cd7).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512
https://github.com/scverse/scanpy/pull/2512:0,integrability,Integr,Integration,0,"Integration of Marsilea to create Heatmap from AnnData; This PR is for #2444 . The project ""heatgraphy"" mentioned in the issue is renamed to ""Marsilea"", https://github.com/Marsilea-viz/marsilea. The previous link to the image mentioned in the issue is not available anymore. Please refer to [this link](https://marsilea.readthedocs.io/en/latest/auto_examples/plot_pbmc3k.html). I added a minimum example in the `external` part. Here is a code example of using the plotting function. Remember to install Marsilea before proceeding:. ```shell. pip install marsilea. ```. ```python. import scanpy as sc. adata = sc.datasets.pbmc68k_reduced(). adata.var[""gene_names""] = adata.var.index. t = sc.external.pl.MarsileaHeatmap(adata, vmin=-2, vmax=2, center=0, label=""Expression""). t.add_right(""bulk_labels"", ""colors"", size=.2, label=""Cell Lables""). t.add_dendrogram(""right"", add_base=False, add_meta=True). t.h_groupby(""bulk_labels""). t.add_top(""n_counts"", ""bar"", show_value=False, label=""Counts""). genes = ['CDK6', 'UQCRFS1', 'SH2D2A', 'CD63', 'CEP152',. 'SIAH2', 'LY86', 'HSPB1', 'POLR2G', 'IGBP1']. t.add_bottom(""gene_names"", ""annot"", mark=genes). t.add_title(""Expression Heatmap""). t.legend(). t.show(). ```. If everything goes well, it should display something like this:. ![download](https://github.com/scverse/scanpy/assets/23433306/eb089061-b9ef-4d49-a2e0-fb3adcf06cd7).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512
https://github.com/scverse/scanpy/pull/2512:0,interoperability,Integr,Integration,0,"Integration of Marsilea to create Heatmap from AnnData; This PR is for #2444 . The project ""heatgraphy"" mentioned in the issue is renamed to ""Marsilea"", https://github.com/Marsilea-viz/marsilea. The previous link to the image mentioned in the issue is not available anymore. Please refer to [this link](https://marsilea.readthedocs.io/en/latest/auto_examples/plot_pbmc3k.html). I added a minimum example in the `external` part. Here is a code example of using the plotting function. Remember to install Marsilea before proceeding:. ```shell. pip install marsilea. ```. ```python. import scanpy as sc. adata = sc.datasets.pbmc68k_reduced(). adata.var[""gene_names""] = adata.var.index. t = sc.external.pl.MarsileaHeatmap(adata, vmin=-2, vmax=2, center=0, label=""Expression""). t.add_right(""bulk_labels"", ""colors"", size=.2, label=""Cell Lables""). t.add_dendrogram(""right"", add_base=False, add_meta=True). t.h_groupby(""bulk_labels""). t.add_top(""n_counts"", ""bar"", show_value=False, label=""Counts""). genes = ['CDK6', 'UQCRFS1', 'SH2D2A', 'CD63', 'CEP152',. 'SIAH2', 'LY86', 'HSPB1', 'POLR2G', 'IGBP1']. t.add_bottom(""gene_names"", ""annot"", mark=genes). t.add_title(""Expression Heatmap""). t.legend(). t.show(). ```. If everything goes well, it should display something like this:. ![download](https://github.com/scverse/scanpy/assets/23433306/eb089061-b9ef-4d49-a2e0-fb3adcf06cd7).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512
https://github.com/scverse/scanpy/pull/2512:0,modifiability,Integr,Integration,0,"Integration of Marsilea to create Heatmap from AnnData; This PR is for #2444 . The project ""heatgraphy"" mentioned in the issue is renamed to ""Marsilea"", https://github.com/Marsilea-viz/marsilea. The previous link to the image mentioned in the issue is not available anymore. Please refer to [this link](https://marsilea.readthedocs.io/en/latest/auto_examples/plot_pbmc3k.html). I added a minimum example in the `external` part. Here is a code example of using the plotting function. Remember to install Marsilea before proceeding:. ```shell. pip install marsilea. ```. ```python. import scanpy as sc. adata = sc.datasets.pbmc68k_reduced(). adata.var[""gene_names""] = adata.var.index. t = sc.external.pl.MarsileaHeatmap(adata, vmin=-2, vmax=2, center=0, label=""Expression""). t.add_right(""bulk_labels"", ""colors"", size=.2, label=""Cell Lables""). t.add_dendrogram(""right"", add_base=False, add_meta=True). t.h_groupby(""bulk_labels""). t.add_top(""n_counts"", ""bar"", show_value=False, label=""Counts""). genes = ['CDK6', 'UQCRFS1', 'SH2D2A', 'CD63', 'CEP152',. 'SIAH2', 'LY86', 'HSPB1', 'POLR2G', 'IGBP1']. t.add_bottom(""gene_names"", ""annot"", mark=genes). t.add_title(""Expression Heatmap""). t.legend(). t.show(). ```. If everything goes well, it should display something like this:. ![download](https://github.com/scverse/scanpy/assets/23433306/eb089061-b9ef-4d49-a2e0-fb3adcf06cd7).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512
https://github.com/scverse/scanpy/pull/2512:0,reliability,Integr,Integration,0,"Integration of Marsilea to create Heatmap from AnnData; This PR is for #2444 . The project ""heatgraphy"" mentioned in the issue is renamed to ""Marsilea"", https://github.com/Marsilea-viz/marsilea. The previous link to the image mentioned in the issue is not available anymore. Please refer to [this link](https://marsilea.readthedocs.io/en/latest/auto_examples/plot_pbmc3k.html). I added a minimum example in the `external` part. Here is a code example of using the plotting function. Remember to install Marsilea before proceeding:. ```shell. pip install marsilea. ```. ```python. import scanpy as sc. adata = sc.datasets.pbmc68k_reduced(). adata.var[""gene_names""] = adata.var.index. t = sc.external.pl.MarsileaHeatmap(adata, vmin=-2, vmax=2, center=0, label=""Expression""). t.add_right(""bulk_labels"", ""colors"", size=.2, label=""Cell Lables""). t.add_dendrogram(""right"", add_base=False, add_meta=True). t.h_groupby(""bulk_labels""). t.add_top(""n_counts"", ""bar"", show_value=False, label=""Counts""). genes = ['CDK6', 'UQCRFS1', 'SH2D2A', 'CD63', 'CEP152',. 'SIAH2', 'LY86', 'HSPB1', 'POLR2G', 'IGBP1']. t.add_bottom(""gene_names"", ""annot"", mark=genes). t.add_title(""Expression Heatmap""). t.legend(). t.show(). ```. If everything goes well, it should display something like this:. ![download](https://github.com/scverse/scanpy/assets/23433306/eb089061-b9ef-4d49-a2e0-fb3adcf06cd7).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512
https://github.com/scverse/scanpy/pull/2512:256,reliability,availab,available,256,"Integration of Marsilea to create Heatmap from AnnData; This PR is for #2444 . The project ""heatgraphy"" mentioned in the issue is renamed to ""Marsilea"", https://github.com/Marsilea-viz/marsilea. The previous link to the image mentioned in the issue is not available anymore. Please refer to [this link](https://marsilea.readthedocs.io/en/latest/auto_examples/plot_pbmc3k.html). I added a minimum example in the `external` part. Here is a code example of using the plotting function. Remember to install Marsilea before proceeding:. ```shell. pip install marsilea. ```. ```python. import scanpy as sc. adata = sc.datasets.pbmc68k_reduced(). adata.var[""gene_names""] = adata.var.index. t = sc.external.pl.MarsileaHeatmap(adata, vmin=-2, vmax=2, center=0, label=""Expression""). t.add_right(""bulk_labels"", ""colors"", size=.2, label=""Cell Lables""). t.add_dendrogram(""right"", add_base=False, add_meta=True). t.h_groupby(""bulk_labels""). t.add_top(""n_counts"", ""bar"", show_value=False, label=""Counts""). genes = ['CDK6', 'UQCRFS1', 'SH2D2A', 'CD63', 'CEP152',. 'SIAH2', 'LY86', 'HSPB1', 'POLR2G', 'IGBP1']. t.add_bottom(""gene_names"", ""annot"", mark=genes). t.add_title(""Expression Heatmap""). t.legend(). t.show(). ```. If everything goes well, it should display something like this:. ![download](https://github.com/scverse/scanpy/assets/23433306/eb089061-b9ef-4d49-a2e0-fb3adcf06cd7).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512
https://github.com/scverse/scanpy/pull/2512:256,safety,avail,available,256,"Integration of Marsilea to create Heatmap from AnnData; This PR is for #2444 . The project ""heatgraphy"" mentioned in the issue is renamed to ""Marsilea"", https://github.com/Marsilea-viz/marsilea. The previous link to the image mentioned in the issue is not available anymore. Please refer to [this link](https://marsilea.readthedocs.io/en/latest/auto_examples/plot_pbmc3k.html). I added a minimum example in the `external` part. Here is a code example of using the plotting function. Remember to install Marsilea before proceeding:. ```shell. pip install marsilea. ```. ```python. import scanpy as sc. adata = sc.datasets.pbmc68k_reduced(). adata.var[""gene_names""] = adata.var.index. t = sc.external.pl.MarsileaHeatmap(adata, vmin=-2, vmax=2, center=0, label=""Expression""). t.add_right(""bulk_labels"", ""colors"", size=.2, label=""Cell Lables""). t.add_dendrogram(""right"", add_base=False, add_meta=True). t.h_groupby(""bulk_labels""). t.add_top(""n_counts"", ""bar"", show_value=False, label=""Counts""). genes = ['CDK6', 'UQCRFS1', 'SH2D2A', 'CD63', 'CEP152',. 'SIAH2', 'LY86', 'HSPB1', 'POLR2G', 'IGBP1']. t.add_bottom(""gene_names"", ""annot"", mark=genes). t.add_title(""Expression Heatmap""). t.legend(). t.show(). ```. If everything goes well, it should display something like this:. ![download](https://github.com/scverse/scanpy/assets/23433306/eb089061-b9ef-4d49-a2e0-fb3adcf06cd7).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512
https://github.com/scverse/scanpy/pull/2512:483,safety,Reme,Remember,483,"Integration of Marsilea to create Heatmap from AnnData; This PR is for #2444 . The project ""heatgraphy"" mentioned in the issue is renamed to ""Marsilea"", https://github.com/Marsilea-viz/marsilea. The previous link to the image mentioned in the issue is not available anymore. Please refer to [this link](https://marsilea.readthedocs.io/en/latest/auto_examples/plot_pbmc3k.html). I added a minimum example in the `external` part. Here is a code example of using the plotting function. Remember to install Marsilea before proceeding:. ```shell. pip install marsilea. ```. ```python. import scanpy as sc. adata = sc.datasets.pbmc68k_reduced(). adata.var[""gene_names""] = adata.var.index. t = sc.external.pl.MarsileaHeatmap(adata, vmin=-2, vmax=2, center=0, label=""Expression""). t.add_right(""bulk_labels"", ""colors"", size=.2, label=""Cell Lables""). t.add_dendrogram(""right"", add_base=False, add_meta=True). t.h_groupby(""bulk_labels""). t.add_top(""n_counts"", ""bar"", show_value=False, label=""Counts""). genes = ['CDK6', 'UQCRFS1', 'SH2D2A', 'CD63', 'CEP152',. 'SIAH2', 'LY86', 'HSPB1', 'POLR2G', 'IGBP1']. t.add_bottom(""gene_names"", ""annot"", mark=genes). t.add_title(""Expression Heatmap""). t.legend(). t.show(). ```. If everything goes well, it should display something like this:. ![download](https://github.com/scverse/scanpy/assets/23433306/eb089061-b9ef-4d49-a2e0-fb3adcf06cd7).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512
https://github.com/scverse/scanpy/pull/2512:0,security,Integr,Integration,0,"Integration of Marsilea to create Heatmap from AnnData; This PR is for #2444 . The project ""heatgraphy"" mentioned in the issue is renamed to ""Marsilea"", https://github.com/Marsilea-viz/marsilea. The previous link to the image mentioned in the issue is not available anymore. Please refer to [this link](https://marsilea.readthedocs.io/en/latest/auto_examples/plot_pbmc3k.html). I added a minimum example in the `external` part. Here is a code example of using the plotting function. Remember to install Marsilea before proceeding:. ```shell. pip install marsilea. ```. ```python. import scanpy as sc. adata = sc.datasets.pbmc68k_reduced(). adata.var[""gene_names""] = adata.var.index. t = sc.external.pl.MarsileaHeatmap(adata, vmin=-2, vmax=2, center=0, label=""Expression""). t.add_right(""bulk_labels"", ""colors"", size=.2, label=""Cell Lables""). t.add_dendrogram(""right"", add_base=False, add_meta=True). t.h_groupby(""bulk_labels""). t.add_top(""n_counts"", ""bar"", show_value=False, label=""Counts""). genes = ['CDK6', 'UQCRFS1', 'SH2D2A', 'CD63', 'CEP152',. 'SIAH2', 'LY86', 'HSPB1', 'POLR2G', 'IGBP1']. t.add_bottom(""gene_names"", ""annot"", mark=genes). t.add_title(""Expression Heatmap""). t.legend(). t.show(). ```. If everything goes well, it should display something like this:. ![download](https://github.com/scverse/scanpy/assets/23433306/eb089061-b9ef-4d49-a2e0-fb3adcf06cd7).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512
https://github.com/scverse/scanpy/pull/2512:256,security,availab,available,256,"Integration of Marsilea to create Heatmap from AnnData; This PR is for #2444 . The project ""heatgraphy"" mentioned in the issue is renamed to ""Marsilea"", https://github.com/Marsilea-viz/marsilea. The previous link to the image mentioned in the issue is not available anymore. Please refer to [this link](https://marsilea.readthedocs.io/en/latest/auto_examples/plot_pbmc3k.html). I added a minimum example in the `external` part. Here is a code example of using the plotting function. Remember to install Marsilea before proceeding:. ```shell. pip install marsilea. ```. ```python. import scanpy as sc. adata = sc.datasets.pbmc68k_reduced(). adata.var[""gene_names""] = adata.var.index. t = sc.external.pl.MarsileaHeatmap(adata, vmin=-2, vmax=2, center=0, label=""Expression""). t.add_right(""bulk_labels"", ""colors"", size=.2, label=""Cell Lables""). t.add_dendrogram(""right"", add_base=False, add_meta=True). t.h_groupby(""bulk_labels""). t.add_top(""n_counts"", ""bar"", show_value=False, label=""Counts""). genes = ['CDK6', 'UQCRFS1', 'SH2D2A', 'CD63', 'CEP152',. 'SIAH2', 'LY86', 'HSPB1', 'POLR2G', 'IGBP1']. t.add_bottom(""gene_names"", ""annot"", mark=genes). t.add_title(""Expression Heatmap""). t.legend(). t.show(). ```. If everything goes well, it should display something like this:. ![download](https://github.com/scverse/scanpy/assets/23433306/eb089061-b9ef-4d49-a2e0-fb3adcf06cd7).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512
https://github.com/scverse/scanpy/pull/2512:0,testability,Integr,Integration,0,"Integration of Marsilea to create Heatmap from AnnData; This PR is for #2444 . The project ""heatgraphy"" mentioned in the issue is renamed to ""Marsilea"", https://github.com/Marsilea-viz/marsilea. The previous link to the image mentioned in the issue is not available anymore. Please refer to [this link](https://marsilea.readthedocs.io/en/latest/auto_examples/plot_pbmc3k.html). I added a minimum example in the `external` part. Here is a code example of using the plotting function. Remember to install Marsilea before proceeding:. ```shell. pip install marsilea. ```. ```python. import scanpy as sc. adata = sc.datasets.pbmc68k_reduced(). adata.var[""gene_names""] = adata.var.index. t = sc.external.pl.MarsileaHeatmap(adata, vmin=-2, vmax=2, center=0, label=""Expression""). t.add_right(""bulk_labels"", ""colors"", size=.2, label=""Cell Lables""). t.add_dendrogram(""right"", add_base=False, add_meta=True). t.h_groupby(""bulk_labels""). t.add_top(""n_counts"", ""bar"", show_value=False, label=""Counts""). genes = ['CDK6', 'UQCRFS1', 'SH2D2A', 'CD63', 'CEP152',. 'SIAH2', 'LY86', 'HSPB1', 'POLR2G', 'IGBP1']. t.add_bottom(""gene_names"", ""annot"", mark=genes). t.add_title(""Expression Heatmap""). t.legend(). t.show(). ```. If everything goes well, it should display something like this:. ![download](https://github.com/scverse/scanpy/assets/23433306/eb089061-b9ef-4d49-a2e0-fb3adcf06cd7).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512
https://github.com/scverse/scanpy/pull/2512:388,usability,minim,minimum,388,"Integration of Marsilea to create Heatmap from AnnData; This PR is for #2444 . The project ""heatgraphy"" mentioned in the issue is renamed to ""Marsilea"", https://github.com/Marsilea-viz/marsilea. The previous link to the image mentioned in the issue is not available anymore. Please refer to [this link](https://marsilea.readthedocs.io/en/latest/auto_examples/plot_pbmc3k.html). I added a minimum example in the `external` part. Here is a code example of using the plotting function. Remember to install Marsilea before proceeding:. ```shell. pip install marsilea. ```. ```python. import scanpy as sc. adata = sc.datasets.pbmc68k_reduced(). adata.var[""gene_names""] = adata.var.index. t = sc.external.pl.MarsileaHeatmap(adata, vmin=-2, vmax=2, center=0, label=""Expression""). t.add_right(""bulk_labels"", ""colors"", size=.2, label=""Cell Lables""). t.add_dendrogram(""right"", add_base=False, add_meta=True). t.h_groupby(""bulk_labels""). t.add_top(""n_counts"", ""bar"", show_value=False, label=""Counts""). genes = ['CDK6', 'UQCRFS1', 'SH2D2A', 'CD63', 'CEP152',. 'SIAH2', 'LY86', 'HSPB1', 'POLR2G', 'IGBP1']. t.add_bottom(""gene_names"", ""annot"", mark=genes). t.add_title(""Expression Heatmap""). t.legend(). t.show(). ```. If everything goes well, it should display something like this:. ![download](https://github.com/scverse/scanpy/assets/23433306/eb089061-b9ef-4d49-a2e0-fb3adcf06cd7).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512
https://github.com/scverse/scanpy/issues/2513:160,availability,consist,consistency,160,"Switch to ruff; ### What kind of feature would you like to request? Other? ### Please describe your wishes. Like anndata, scanpy can benefit from the speed and consistency of ruff.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2513
https://github.com/scverse/scanpy/issues/2513:160,usability,consist,consistency,160,"Switch to ruff; ### What kind of feature would you like to request? Other? ### Please describe your wishes. Like anndata, scanpy can benefit from the speed and consistency of ruff.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2513
https://github.com/scverse/scanpy/pull/2514:179,safety,accid,accidentally,179,Switch to ruff; Fixes #2513. Compare https://github.com/scverse/anndata/pull/891 and https://github.com/scverse/anndata/commit/14e0cd4795730673e55d14639d562fbe7691965b (sorry for accidentally pushing to main),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2514
https://github.com/scverse/scanpy/pull/2515:33,safety,test,tests,33,Fix for sklearn 1.3; Fix the dev tests. `square_distances` are not longer an argument in sklearn 1.3,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2515
https://github.com/scverse/scanpy/pull/2515:33,testability,test,tests,33,Fix for sklearn 1.3; Fix the dev tests. `square_distances` are not longer an argument in sklearn 1.3,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2515
https://github.com/scverse/scanpy/pull/2517:61,deployability,version,version,61,Backport PR #2447 and #2468 on branch 1.9.x (Increase ubuntu version & python version for azure CI); Backport PR #2447: Increase ubuntu version & python version for azure CI,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2517
https://github.com/scverse/scanpy/pull/2517:78,deployability,version,version,78,Backport PR #2447 and #2468 on branch 1.9.x (Increase ubuntu version & python version for azure CI); Backport PR #2447: Increase ubuntu version & python version for azure CI,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2517
https://github.com/scverse/scanpy/pull/2517:136,deployability,version,version,136,Backport PR #2447 and #2468 on branch 1.9.x (Increase ubuntu version & python version for azure CI); Backport PR #2447: Increase ubuntu version & python version for azure CI,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2517
https://github.com/scverse/scanpy/pull/2517:153,deployability,version,version,153,Backport PR #2447 and #2468 on branch 1.9.x (Increase ubuntu version & python version for azure CI); Backport PR #2447: Increase ubuntu version & python version for azure CI,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2517
https://github.com/scverse/scanpy/pull/2517:61,integrability,version,version,61,Backport PR #2447 and #2468 on branch 1.9.x (Increase ubuntu version & python version for azure CI); Backport PR #2447: Increase ubuntu version & python version for azure CI,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2517
https://github.com/scverse/scanpy/pull/2517:78,integrability,version,version,78,Backport PR #2447 and #2468 on branch 1.9.x (Increase ubuntu version & python version for azure CI); Backport PR #2447: Increase ubuntu version & python version for azure CI,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2517
https://github.com/scverse/scanpy/pull/2517:136,integrability,version,version,136,Backport PR #2447 and #2468 on branch 1.9.x (Increase ubuntu version & python version for azure CI); Backport PR #2447: Increase ubuntu version & python version for azure CI,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2517
https://github.com/scverse/scanpy/pull/2517:153,integrability,version,version,153,Backport PR #2447 and #2468 on branch 1.9.x (Increase ubuntu version & python version for azure CI); Backport PR #2447: Increase ubuntu version & python version for azure CI,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2517
https://github.com/scverse/scanpy/pull/2517:61,modifiability,version,version,61,Backport PR #2447 and #2468 on branch 1.9.x (Increase ubuntu version & python version for azure CI); Backport PR #2447: Increase ubuntu version & python version for azure CI,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2517
https://github.com/scverse/scanpy/pull/2517:78,modifiability,version,version,78,Backport PR #2447 and #2468 on branch 1.9.x (Increase ubuntu version & python version for azure CI); Backport PR #2447: Increase ubuntu version & python version for azure CI,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2517
https://github.com/scverse/scanpy/pull/2517:136,modifiability,version,version,136,Backport PR #2447 and #2468 on branch 1.9.x (Increase ubuntu version & python version for azure CI); Backport PR #2447: Increase ubuntu version & python version for azure CI,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2517
https://github.com/scverse/scanpy/pull/2517:153,modifiability,version,version,153,Backport PR #2447 and #2468 on branch 1.9.x (Increase ubuntu version & python version for azure CI); Backport PR #2447: Increase ubuntu version & python version for azure CI,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2517
https://github.com/scverse/scanpy/pull/2518:113,deployability,releas,release,113,"Check PRs for milestones; So we cant forget adding them, thereby ensuring bugfixes make it into the next bugfix release (including CI fixes)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2518
https://github.com/scverse/scanpy/issues/2519:2160,availability,slo,slow,2160,"edocs.io/en/latest/api.html#umap.umap_.nearest_neighbors). - if `method == 'gauss'`: use umap distances, overwrite its connectivities. ## Evaluating options. See [ann-benchmarks.com](https://ann-benchmarks.com/index.html#datasets). Build time vs query time is not straightforward, see https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180389432 and https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180747770. > If, however, you are simply interested in knn-graph construction then you can get that from pynndescent in less time than even the index construction time (since the prepare phase isn't required, but is a non-trivial part of the index construction time). Plots for index building are on individual dataset pages, like [glove-100-angular](https://ann-benchmarks.com/glove-100-angular_10_angular.html). <details>. <summary>Used metrics</summary>. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L35-L56. </details>. Interesting:. Name | Demo | wheels: Platforms | wheels: Python | Search speed | Index build. --- | --- | --- | --- | --- | ---. [NMSLIB](https://github.com/nmslib/nmslib) | [6 notebooks](https://github.com/nmslib/nmslib/blob/master/python_bindings/notebooks/README.md) | [Linux wheels](https://pypi.org/project/nmslib/2.1.1/#files) |[Outdated (3.9)](https://github.com/nmslib/nmslib/issues/529) | top | slow. [ScaNN](https://github.com/google-research/google-research/tree/master/scann#readme) | [notebook](https://github.com/google-research/google-research/blob/master/scann/docs/example.ipynb) | [All wheels](https://pypi.org/project/scann/1.2.9/#files) | [Slightly outdated (3.10)](https://github.com/google-research/google-research/issues/1590) | very fast | middling. [Faiss](https://github.com/facebookresearch/faiss) | [wiki](https://github.com/facebookresearch/faiss/wiki/Getting-started) | * | * | middling to very fast | faiss-ivf: top. *no official package on PyPI",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2519:2416,availability,Sli,Slightly,2416,"edocs.io/en/latest/api.html#umap.umap_.nearest_neighbors). - if `method == 'gauss'`: use umap distances, overwrite its connectivities. ## Evaluating options. See [ann-benchmarks.com](https://ann-benchmarks.com/index.html#datasets). Build time vs query time is not straightforward, see https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180389432 and https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180747770. > If, however, you are simply interested in knn-graph construction then you can get that from pynndescent in less time than even the index construction time (since the prepare phase isn't required, but is a non-trivial part of the index construction time). Plots for index building are on individual dataset pages, like [glove-100-angular](https://ann-benchmarks.com/glove-100-angular_10_angular.html). <details>. <summary>Used metrics</summary>. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L35-L56. </details>. Interesting:. Name | Demo | wheels: Platforms | wheels: Python | Search speed | Index build. --- | --- | --- | --- | --- | ---. [NMSLIB](https://github.com/nmslib/nmslib) | [6 notebooks](https://github.com/nmslib/nmslib/blob/master/python_bindings/notebooks/README.md) | [Linux wheels](https://pypi.org/project/nmslib/2.1.1/#files) |[Outdated (3.9)](https://github.com/nmslib/nmslib/issues/529) | top | slow. [ScaNN](https://github.com/google-research/google-research/tree/master/scann#readme) | [notebook](https://github.com/google-research/google-research/blob/master/scann/docs/example.ipynb) | [All wheels](https://pypi.org/project/scann/1.2.9/#files) | [Slightly outdated (3.10)](https://github.com/google-research/google-research/issues/1590) | very fast | middling. [Faiss](https://github.com/facebookresearch/faiss) | [wiki](https://github.com/facebookresearch/faiss/wiki/Getting-started) | * | * | middling to very fast | faiss-ivf: top. *no official package on PyPI",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2519:97,deployability,API,API,97,"Add approximate nearest neighbor backend; Goals:. - flexibility in creation. - Neighbor querying API. - Harmonize storage of backends querying indices. 	- use it in doublet detection and scanorama reimplementations. Current solution: `method` arg. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L32. - if `use_dense_distances`: [`sklearn.metrics.pairwise_distances`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html). - if `method == 'rapids'`: [`cuml.neighbors.NearestNeighbors`](https://docs.rapids.ai/api/cuml/stable/api/#cuml.neighbors.NearestNeighbors). - otherwise: [`umap.umap_.nearest_neighbors`](https://umap-learn.readthedocs.io/en/latest/api.html#umap.umap_.nearest_neighbors). - if `method == 'gauss'`: use umap distances, overwrite its connectivities. ## Evaluating options. See [ann-benchmarks.com](https://ann-benchmarks.com/index.html#datasets). Build time vs query time is not straightforward, see https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180389432 and https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180747770. > If, however, you are simply interested in knn-graph construction then you can get that from pynndescent in less time than even the index construction time (since the prepare phase isn't required, but is a non-trivial part of the index construction time). Plots for index building are on individual dataset pages, like [glove-100-angular](https://ann-benchmarks.com/glove-100-angular_10_angular.html). <details>. <summary>Used metrics</summary>. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L35-L56. </details>. Interesting:. Name | Demo | wheels: Platforms | wheels: Python | Search speed | Index build. --- | --- | --- | --- | --- | ---. [NMSLIB](https://github.com/nmslib/nmslib) | [6 notebooks](https://github.com/nmslib/nmslib/blob/master/python_bind",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2519:462,deployability,modul,modules,462,"Add approximate nearest neighbor backend; Goals:. - flexibility in creation. - Neighbor querying API. - Harmonize storage of backends querying indices. 	- use it in doublet detection and scanorama reimplementations. Current solution: `method` arg. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L32. - if `use_dense_distances`: [`sklearn.metrics.pairwise_distances`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html). - if `method == 'rapids'`: [`cuml.neighbors.NearestNeighbors`](https://docs.rapids.ai/api/cuml/stable/api/#cuml.neighbors.NearestNeighbors). - otherwise: [`umap.umap_.nearest_neighbors`](https://umap-learn.readthedocs.io/en/latest/api.html#umap.umap_.nearest_neighbors). - if `method == 'gauss'`: use umap distances, overwrite its connectivities. ## Evaluating options. See [ann-benchmarks.com](https://ann-benchmarks.com/index.html#datasets). Build time vs query time is not straightforward, see https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180389432 and https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180747770. > If, however, you are simply interested in knn-graph construction then you can get that from pynndescent in less time than even the index construction time (since the prepare phase isn't required, but is a non-trivial part of the index construction time). Plots for index building are on individual dataset pages, like [glove-100-angular](https://ann-benchmarks.com/glove-100-angular_10_angular.html). <details>. <summary>Used metrics</summary>. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L35-L56. </details>. Interesting:. Name | Demo | wheels: Platforms | wheels: Python | Search speed | Index build. --- | --- | --- | --- | --- | ---. [NMSLIB](https://github.com/nmslib/nmslib) | [6 notebooks](https://github.com/nmslib/nmslib/blob/master/python_bind",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2519:608,deployability,api,api,608,"Add approximate nearest neighbor backend; Goals:. - flexibility in creation. - Neighbor querying API. - Harmonize storage of backends querying indices. 	- use it in doublet detection and scanorama reimplementations. Current solution: `method` arg. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L32. - if `use_dense_distances`: [`sklearn.metrics.pairwise_distances`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html). - if `method == 'rapids'`: [`cuml.neighbors.NearestNeighbors`](https://docs.rapids.ai/api/cuml/stable/api/#cuml.neighbors.NearestNeighbors). - otherwise: [`umap.umap_.nearest_neighbors`](https://umap-learn.readthedocs.io/en/latest/api.html#umap.umap_.nearest_neighbors). - if `method == 'gauss'`: use umap distances, overwrite its connectivities. ## Evaluating options. See [ann-benchmarks.com](https://ann-benchmarks.com/index.html#datasets). Build time vs query time is not straightforward, see https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180389432 and https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180747770. > If, however, you are simply interested in knn-graph construction then you can get that from pynndescent in less time than even the index construction time (since the prepare phase isn't required, but is a non-trivial part of the index construction time). Plots for index building are on individual dataset pages, like [glove-100-angular](https://ann-benchmarks.com/glove-100-angular_10_angular.html). <details>. <summary>Used metrics</summary>. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L35-L56. </details>. Interesting:. Name | Demo | wheels: Platforms | wheels: Python | Search speed | Index build. --- | --- | --- | --- | --- | ---. [NMSLIB](https://github.com/nmslib/nmslib) | [6 notebooks](https://github.com/nmslib/nmslib/blob/master/python_bind",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2519:624,deployability,api,api,624,"Add approximate nearest neighbor backend; Goals:. - flexibility in creation. - Neighbor querying API. - Harmonize storage of backends querying indices. 	- use it in doublet detection and scanorama reimplementations. Current solution: `method` arg. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L32. - if `use_dense_distances`: [`sklearn.metrics.pairwise_distances`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html). - if `method == 'rapids'`: [`cuml.neighbors.NearestNeighbors`](https://docs.rapids.ai/api/cuml/stable/api/#cuml.neighbors.NearestNeighbors). - otherwise: [`umap.umap_.nearest_neighbors`](https://umap-learn.readthedocs.io/en/latest/api.html#umap.umap_.nearest_neighbors). - if `method == 'gauss'`: use umap distances, overwrite its connectivities. ## Evaluating options. See [ann-benchmarks.com](https://ann-benchmarks.com/index.html#datasets). Build time vs query time is not straightforward, see https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180389432 and https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180747770. > If, however, you are simply interested in knn-graph construction then you can get that from pynndescent in less time than even the index construction time (since the prepare phase isn't required, but is a non-trivial part of the index construction time). Plots for index building are on individual dataset pages, like [glove-100-angular](https://ann-benchmarks.com/glove-100-angular_10_angular.html). <details>. <summary>Used metrics</summary>. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L35-L56. </details>. Interesting:. Name | Demo | wheels: Platforms | wheels: Python | Search speed | Index build. --- | --- | --- | --- | --- | ---. [NMSLIB](https://github.com/nmslib/nmslib) | [6 notebooks](https://github.com/nmslib/nmslib/blob/master/python_bind",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2519:753,deployability,api,api,753,"Add approximate nearest neighbor backend; Goals:. - flexibility in creation. - Neighbor querying API. - Harmonize storage of backends querying indices. 	- use it in doublet detection and scanorama reimplementations. Current solution: `method` arg. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L32. - if `use_dense_distances`: [`sklearn.metrics.pairwise_distances`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html). - if `method == 'rapids'`: [`cuml.neighbors.NearestNeighbors`](https://docs.rapids.ai/api/cuml/stable/api/#cuml.neighbors.NearestNeighbors). - otherwise: [`umap.umap_.nearest_neighbors`](https://umap-learn.readthedocs.io/en/latest/api.html#umap.umap_.nearest_neighbors). - if `method == 'gauss'`: use umap distances, overwrite its connectivities. ## Evaluating options. See [ann-benchmarks.com](https://ann-benchmarks.com/index.html#datasets). Build time vs query time is not straightforward, see https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180389432 and https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180747770. > If, however, you are simply interested in knn-graph construction then you can get that from pynndescent in less time than even the index construction time (since the prepare phase isn't required, but is a non-trivial part of the index construction time). Plots for index building are on individual dataset pages, like [glove-100-angular](https://ann-benchmarks.com/glove-100-angular_10_angular.html). <details>. <summary>Used metrics</summary>. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L35-L56. </details>. Interesting:. Name | Demo | wheels: Platforms | wheels: Python | Search speed | Index build. --- | --- | --- | --- | --- | ---. [NMSLIB](https://github.com/nmslib/nmslib) | [6 notebooks](https://github.com/nmslib/nmslib/blob/master/python_bind",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2519:966,deployability,Build,Build,966,"Add approximate nearest neighbor backend; Goals:. - flexibility in creation. - Neighbor querying API. - Harmonize storage of backends querying indices. 	- use it in doublet detection and scanorama reimplementations. Current solution: `method` arg. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L32. - if `use_dense_distances`: [`sklearn.metrics.pairwise_distances`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html). - if `method == 'rapids'`: [`cuml.neighbors.NearestNeighbors`](https://docs.rapids.ai/api/cuml/stable/api/#cuml.neighbors.NearestNeighbors). - otherwise: [`umap.umap_.nearest_neighbors`](https://umap-learn.readthedocs.io/en/latest/api.html#umap.umap_.nearest_neighbors). - if `method == 'gauss'`: use umap distances, overwrite its connectivities. ## Evaluating options. See [ann-benchmarks.com](https://ann-benchmarks.com/index.html#datasets). Build time vs query time is not straightforward, see https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180389432 and https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180747770. > If, however, you are simply interested in knn-graph construction then you can get that from pynndescent in less time than even the index construction time (since the prepare phase isn't required, but is a non-trivial part of the index construction time). Plots for index building are on individual dataset pages, like [glove-100-angular](https://ann-benchmarks.com/glove-100-angular_10_angular.html). <details>. <summary>Used metrics</summary>. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L35-L56. </details>. Interesting:. Name | Demo | wheels: Platforms | wheels: Python | Search speed | Index build. --- | --- | --- | --- | --- | ---. [NMSLIB](https://github.com/nmslib/nmslib) | [6 notebooks](https://github.com/nmslib/nmslib/blob/master/python_bind",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2519:1453,deployability,build,building,1453,"able/modules/generated/sklearn.metrics.pairwise_distances.html). - if `method == 'rapids'`: [`cuml.neighbors.NearestNeighbors`](https://docs.rapids.ai/api/cuml/stable/api/#cuml.neighbors.NearestNeighbors). - otherwise: [`umap.umap_.nearest_neighbors`](https://umap-learn.readthedocs.io/en/latest/api.html#umap.umap_.nearest_neighbors). - if `method == 'gauss'`: use umap distances, overwrite its connectivities. ## Evaluating options. See [ann-benchmarks.com](https://ann-benchmarks.com/index.html#datasets). Build time vs query time is not straightforward, see https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180389432 and https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180747770. > If, however, you are simply interested in knn-graph construction then you can get that from pynndescent in less time than even the index construction time (since the prepare phase isn't required, but is a non-trivial part of the index construction time). Plots for index building are on individual dataset pages, like [glove-100-angular](https://ann-benchmarks.com/glove-100-angular_10_angular.html). <details>. <summary>Used metrics</summary>. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L35-L56. </details>. Interesting:. Name | Demo | wheels: Platforms | wheels: Python | Search speed | Index build. --- | --- | --- | --- | --- | ---. [NMSLIB](https://github.com/nmslib/nmslib) | [6 notebooks](https://github.com/nmslib/nmslib/blob/master/python_bindings/notebooks/README.md) | [Linux wheels](https://pypi.org/project/nmslib/2.1.1/#files) |[Outdated (3.9)](https://github.com/nmslib/nmslib/issues/529) | top | slow. [ScaNN](https://github.com/google-research/google-research/tree/master/scann#readme) | [notebook](https://github.com/google-research/google-research/blob/master/scann/docs/example.ipynb) | [All wheels](https://pypi.org/project/scann/1.2.9/#files) | [Slightly outdated (3.10)](https://github.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2519:1843,deployability,build,build,1843,"edocs.io/en/latest/api.html#umap.umap_.nearest_neighbors). - if `method == 'gauss'`: use umap distances, overwrite its connectivities. ## Evaluating options. See [ann-benchmarks.com](https://ann-benchmarks.com/index.html#datasets). Build time vs query time is not straightforward, see https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180389432 and https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180747770. > If, however, you are simply interested in knn-graph construction then you can get that from pynndescent in less time than even the index construction time (since the prepare phase isn't required, but is a non-trivial part of the index construction time). Plots for index building are on individual dataset pages, like [glove-100-angular](https://ann-benchmarks.com/glove-100-angular_10_angular.html). <details>. <summary>Used metrics</summary>. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L35-L56. </details>. Interesting:. Name | Demo | wheels: Platforms | wheels: Python | Search speed | Index build. --- | --- | --- | --- | --- | ---. [NMSLIB](https://github.com/nmslib/nmslib) | [6 notebooks](https://github.com/nmslib/nmslib/blob/master/python_bindings/notebooks/README.md) | [Linux wheels](https://pypi.org/project/nmslib/2.1.1/#files) |[Outdated (3.9)](https://github.com/nmslib/nmslib/issues/529) | top | slow. [ScaNN](https://github.com/google-research/google-research/tree/master/scann#readme) | [notebook](https://github.com/google-research/google-research/blob/master/scann/docs/example.ipynb) | [All wheels](https://pypi.org/project/scann/1.2.9/#files) | [Slightly outdated (3.10)](https://github.com/google-research/google-research/issues/1590) | very fast | middling. [Faiss](https://github.com/facebookresearch/faiss) | [wiki](https://github.com/facebookresearch/faiss/wiki/Getting-started) | * | * | middling to very fast | faiss-ivf: top. *no official package on PyPI",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2519:217,energy efficiency,Current,Current,217,"Add approximate nearest neighbor backend; Goals:. - flexibility in creation. - Neighbor querying API. - Harmonize storage of backends querying indices. 	- use it in doublet detection and scanorama reimplementations. Current solution: `method` arg. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L32. - if `use_dense_distances`: [`sklearn.metrics.pairwise_distances`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html). - if `method == 'rapids'`: [`cuml.neighbors.NearestNeighbors`](https://docs.rapids.ai/api/cuml/stable/api/#cuml.neighbors.NearestNeighbors). - otherwise: [`umap.umap_.nearest_neighbors`](https://umap-learn.readthedocs.io/en/latest/api.html#umap.umap_.nearest_neighbors). - if `method == 'gauss'`: use umap distances, overwrite its connectivities. ## Evaluating options. See [ann-benchmarks.com](https://ann-benchmarks.com/index.html#datasets). Build time vs query time is not straightforward, see https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180389432 and https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180747770. > If, however, you are simply interested in knn-graph construction then you can get that from pynndescent in less time than even the index construction time (since the prepare phase isn't required, but is a non-trivial part of the index construction time). Plots for index building are on individual dataset pages, like [glove-100-angular](https://ann-benchmarks.com/glove-100-angular_10_angular.html). <details>. <summary>Used metrics</summary>. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L35-L56. </details>. Interesting:. Name | Demo | wheels: Platforms | wheels: Python | Search speed | Index build. --- | --- | --- | --- | --- | ---. [NMSLIB](https://github.com/nmslib/nmslib) | [6 notebooks](https://github.com/nmslib/nmslib/blob/master/python_bind",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2519:97,integrability,API,API,97,"Add approximate nearest neighbor backend; Goals:. - flexibility in creation. - Neighbor querying API. - Harmonize storage of backends querying indices. 	- use it in doublet detection and scanorama reimplementations. Current solution: `method` arg. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L32. - if `use_dense_distances`: [`sklearn.metrics.pairwise_distances`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html). - if `method == 'rapids'`: [`cuml.neighbors.NearestNeighbors`](https://docs.rapids.ai/api/cuml/stable/api/#cuml.neighbors.NearestNeighbors). - otherwise: [`umap.umap_.nearest_neighbors`](https://umap-learn.readthedocs.io/en/latest/api.html#umap.umap_.nearest_neighbors). - if `method == 'gauss'`: use umap distances, overwrite its connectivities. ## Evaluating options. See [ann-benchmarks.com](https://ann-benchmarks.com/index.html#datasets). Build time vs query time is not straightforward, see https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180389432 and https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180747770. > If, however, you are simply interested in knn-graph construction then you can get that from pynndescent in less time than even the index construction time (since the prepare phase isn't required, but is a non-trivial part of the index construction time). Plots for index building are on individual dataset pages, like [glove-100-angular](https://ann-benchmarks.com/glove-100-angular_10_angular.html). <details>. <summary>Used metrics</summary>. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L35-L56. </details>. Interesting:. Name | Demo | wheels: Platforms | wheels: Python | Search speed | Index build. --- | --- | --- | --- | --- | ---. [NMSLIB](https://github.com/nmslib/nmslib) | [6 notebooks](https://github.com/nmslib/nmslib/blob/master/python_bind",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2519:608,integrability,api,api,608,"Add approximate nearest neighbor backend; Goals:. - flexibility in creation. - Neighbor querying API. - Harmonize storage of backends querying indices. 	- use it in doublet detection and scanorama reimplementations. Current solution: `method` arg. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L32. - if `use_dense_distances`: [`sklearn.metrics.pairwise_distances`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html). - if `method == 'rapids'`: [`cuml.neighbors.NearestNeighbors`](https://docs.rapids.ai/api/cuml/stable/api/#cuml.neighbors.NearestNeighbors). - otherwise: [`umap.umap_.nearest_neighbors`](https://umap-learn.readthedocs.io/en/latest/api.html#umap.umap_.nearest_neighbors). - if `method == 'gauss'`: use umap distances, overwrite its connectivities. ## Evaluating options. See [ann-benchmarks.com](https://ann-benchmarks.com/index.html#datasets). Build time vs query time is not straightforward, see https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180389432 and https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180747770. > If, however, you are simply interested in knn-graph construction then you can get that from pynndescent in less time than even the index construction time (since the prepare phase isn't required, but is a non-trivial part of the index construction time). Plots for index building are on individual dataset pages, like [glove-100-angular](https://ann-benchmarks.com/glove-100-angular_10_angular.html). <details>. <summary>Used metrics</summary>. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L35-L56. </details>. Interesting:. Name | Demo | wheels: Platforms | wheels: Python | Search speed | Index build. --- | --- | --- | --- | --- | ---. [NMSLIB](https://github.com/nmslib/nmslib) | [6 notebooks](https://github.com/nmslib/nmslib/blob/master/python_bind",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2519:624,integrability,api,api,624,"Add approximate nearest neighbor backend; Goals:. - flexibility in creation. - Neighbor querying API. - Harmonize storage of backends querying indices. 	- use it in doublet detection and scanorama reimplementations. Current solution: `method` arg. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L32. - if `use_dense_distances`: [`sklearn.metrics.pairwise_distances`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html). - if `method == 'rapids'`: [`cuml.neighbors.NearestNeighbors`](https://docs.rapids.ai/api/cuml/stable/api/#cuml.neighbors.NearestNeighbors). - otherwise: [`umap.umap_.nearest_neighbors`](https://umap-learn.readthedocs.io/en/latest/api.html#umap.umap_.nearest_neighbors). - if `method == 'gauss'`: use umap distances, overwrite its connectivities. ## Evaluating options. See [ann-benchmarks.com](https://ann-benchmarks.com/index.html#datasets). Build time vs query time is not straightforward, see https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180389432 and https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180747770. > If, however, you are simply interested in knn-graph construction then you can get that from pynndescent in less time than even the index construction time (since the prepare phase isn't required, but is a non-trivial part of the index construction time). Plots for index building are on individual dataset pages, like [glove-100-angular](https://ann-benchmarks.com/glove-100-angular_10_angular.html). <details>. <summary>Used metrics</summary>. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L35-L56. </details>. Interesting:. Name | Demo | wheels: Platforms | wheels: Python | Search speed | Index build. --- | --- | --- | --- | --- | ---. [NMSLIB](https://github.com/nmslib/nmslib) | [6 notebooks](https://github.com/nmslib/nmslib/blob/master/python_bind",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2519:753,integrability,api,api,753,"Add approximate nearest neighbor backend; Goals:. - flexibility in creation. - Neighbor querying API. - Harmonize storage of backends querying indices. 	- use it in doublet detection and scanorama reimplementations. Current solution: `method` arg. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L32. - if `use_dense_distances`: [`sklearn.metrics.pairwise_distances`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html). - if `method == 'rapids'`: [`cuml.neighbors.NearestNeighbors`](https://docs.rapids.ai/api/cuml/stable/api/#cuml.neighbors.NearestNeighbors). - otherwise: [`umap.umap_.nearest_neighbors`](https://umap-learn.readthedocs.io/en/latest/api.html#umap.umap_.nearest_neighbors). - if `method == 'gauss'`: use umap distances, overwrite its connectivities. ## Evaluating options. See [ann-benchmarks.com](https://ann-benchmarks.com/index.html#datasets). Build time vs query time is not straightforward, see https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180389432 and https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180747770. > If, however, you are simply interested in knn-graph construction then you can get that from pynndescent in less time than even the index construction time (since the prepare phase isn't required, but is a non-trivial part of the index construction time). Plots for index building are on individual dataset pages, like [glove-100-angular](https://ann-benchmarks.com/glove-100-angular_10_angular.html). <details>. <summary>Used metrics</summary>. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L35-L56. </details>. Interesting:. Name | Demo | wheels: Platforms | wheels: Python | Search speed | Index build. --- | --- | --- | --- | --- | ---. [NMSLIB](https://github.com/nmslib/nmslib) | [6 notebooks](https://github.com/nmslib/nmslib/blob/master/python_bind",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2519:97,interoperability,API,API,97,"Add approximate nearest neighbor backend; Goals:. - flexibility in creation. - Neighbor querying API. - Harmonize storage of backends querying indices. 	- use it in doublet detection and scanorama reimplementations. Current solution: `method` arg. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L32. - if `use_dense_distances`: [`sklearn.metrics.pairwise_distances`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html). - if `method == 'rapids'`: [`cuml.neighbors.NearestNeighbors`](https://docs.rapids.ai/api/cuml/stable/api/#cuml.neighbors.NearestNeighbors). - otherwise: [`umap.umap_.nearest_neighbors`](https://umap-learn.readthedocs.io/en/latest/api.html#umap.umap_.nearest_neighbors). - if `method == 'gauss'`: use umap distances, overwrite its connectivities. ## Evaluating options. See [ann-benchmarks.com](https://ann-benchmarks.com/index.html#datasets). Build time vs query time is not straightforward, see https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180389432 and https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180747770. > If, however, you are simply interested in knn-graph construction then you can get that from pynndescent in less time than even the index construction time (since the prepare phase isn't required, but is a non-trivial part of the index construction time). Plots for index building are on individual dataset pages, like [glove-100-angular](https://ann-benchmarks.com/glove-100-angular_10_angular.html). <details>. <summary>Used metrics</summary>. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L35-L56. </details>. Interesting:. Name | Demo | wheels: Platforms | wheels: Python | Search speed | Index build. --- | --- | --- | --- | --- | ---. [NMSLIB](https://github.com/nmslib/nmslib) | [6 notebooks](https://github.com/nmslib/nmslib/blob/master/python_bind",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2519:608,interoperability,api,api,608,"Add approximate nearest neighbor backend; Goals:. - flexibility in creation. - Neighbor querying API. - Harmonize storage of backends querying indices. 	- use it in doublet detection and scanorama reimplementations. Current solution: `method` arg. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L32. - if `use_dense_distances`: [`sklearn.metrics.pairwise_distances`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html). - if `method == 'rapids'`: [`cuml.neighbors.NearestNeighbors`](https://docs.rapids.ai/api/cuml/stable/api/#cuml.neighbors.NearestNeighbors). - otherwise: [`umap.umap_.nearest_neighbors`](https://umap-learn.readthedocs.io/en/latest/api.html#umap.umap_.nearest_neighbors). - if `method == 'gauss'`: use umap distances, overwrite its connectivities. ## Evaluating options. See [ann-benchmarks.com](https://ann-benchmarks.com/index.html#datasets). Build time vs query time is not straightforward, see https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180389432 and https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180747770. > If, however, you are simply interested in knn-graph construction then you can get that from pynndescent in less time than even the index construction time (since the prepare phase isn't required, but is a non-trivial part of the index construction time). Plots for index building are on individual dataset pages, like [glove-100-angular](https://ann-benchmarks.com/glove-100-angular_10_angular.html). <details>. <summary>Used metrics</summary>. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L35-L56. </details>. Interesting:. Name | Demo | wheels: Platforms | wheels: Python | Search speed | Index build. --- | --- | --- | --- | --- | ---. [NMSLIB](https://github.com/nmslib/nmslib) | [6 notebooks](https://github.com/nmslib/nmslib/blob/master/python_bind",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2519:624,interoperability,api,api,624,"Add approximate nearest neighbor backend; Goals:. - flexibility in creation. - Neighbor querying API. - Harmonize storage of backends querying indices. 	- use it in doublet detection and scanorama reimplementations. Current solution: `method` arg. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L32. - if `use_dense_distances`: [`sklearn.metrics.pairwise_distances`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html). - if `method == 'rapids'`: [`cuml.neighbors.NearestNeighbors`](https://docs.rapids.ai/api/cuml/stable/api/#cuml.neighbors.NearestNeighbors). - otherwise: [`umap.umap_.nearest_neighbors`](https://umap-learn.readthedocs.io/en/latest/api.html#umap.umap_.nearest_neighbors). - if `method == 'gauss'`: use umap distances, overwrite its connectivities. ## Evaluating options. See [ann-benchmarks.com](https://ann-benchmarks.com/index.html#datasets). Build time vs query time is not straightforward, see https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180389432 and https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180747770. > If, however, you are simply interested in knn-graph construction then you can get that from pynndescent in less time than even the index construction time (since the prepare phase isn't required, but is a non-trivial part of the index construction time). Plots for index building are on individual dataset pages, like [glove-100-angular](https://ann-benchmarks.com/glove-100-angular_10_angular.html). <details>. <summary>Used metrics</summary>. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L35-L56. </details>. Interesting:. Name | Demo | wheels: Platforms | wheels: Python | Search speed | Index build. --- | --- | --- | --- | --- | ---. [NMSLIB](https://github.com/nmslib/nmslib) | [6 notebooks](https://github.com/nmslib/nmslib/blob/master/python_bind",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2519:753,interoperability,api,api,753,"Add approximate nearest neighbor backend; Goals:. - flexibility in creation. - Neighbor querying API. - Harmonize storage of backends querying indices. 	- use it in doublet detection and scanorama reimplementations. Current solution: `method` arg. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L32. - if `use_dense_distances`: [`sklearn.metrics.pairwise_distances`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html). - if `method == 'rapids'`: [`cuml.neighbors.NearestNeighbors`](https://docs.rapids.ai/api/cuml/stable/api/#cuml.neighbors.NearestNeighbors). - otherwise: [`umap.umap_.nearest_neighbors`](https://umap-learn.readthedocs.io/en/latest/api.html#umap.umap_.nearest_neighbors). - if `method == 'gauss'`: use umap distances, overwrite its connectivities. ## Evaluating options. See [ann-benchmarks.com](https://ann-benchmarks.com/index.html#datasets). Build time vs query time is not straightforward, see https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180389432 and https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180747770. > If, however, you are simply interested in knn-graph construction then you can get that from pynndescent in less time than even the index construction time (since the prepare phase isn't required, but is a non-trivial part of the index construction time). Plots for index building are on individual dataset pages, like [glove-100-angular](https://ann-benchmarks.com/glove-100-angular_10_angular.html). <details>. <summary>Used metrics</summary>. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L35-L56. </details>. Interesting:. Name | Demo | wheels: Platforms | wheels: Python | Search speed | Index build. --- | --- | --- | --- | --- | ---. [NMSLIB](https://github.com/nmslib/nmslib) | [6 notebooks](https://github.com/nmslib/nmslib/blob/master/python_bind",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2519:1793,interoperability,Platform,Platforms,1793,"edocs.io/en/latest/api.html#umap.umap_.nearest_neighbors). - if `method == 'gauss'`: use umap distances, overwrite its connectivities. ## Evaluating options. See [ann-benchmarks.com](https://ann-benchmarks.com/index.html#datasets). Build time vs query time is not straightforward, see https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180389432 and https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180747770. > If, however, you are simply interested in knn-graph construction then you can get that from pynndescent in less time than even the index construction time (since the prepare phase isn't required, but is a non-trivial part of the index construction time). Plots for index building are on individual dataset pages, like [glove-100-angular](https://ann-benchmarks.com/glove-100-angular_10_angular.html). <details>. <summary>Used metrics</summary>. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L35-L56. </details>. Interesting:. Name | Demo | wheels: Platforms | wheels: Python | Search speed | Index build. --- | --- | --- | --- | --- | ---. [NMSLIB](https://github.com/nmslib/nmslib) | [6 notebooks](https://github.com/nmslib/nmslib/blob/master/python_bindings/notebooks/README.md) | [Linux wheels](https://pypi.org/project/nmslib/2.1.1/#files) |[Outdated (3.9)](https://github.com/nmslib/nmslib/issues/529) | top | slow. [ScaNN](https://github.com/google-research/google-research/tree/master/scann#readme) | [notebook](https://github.com/google-research/google-research/blob/master/scann/docs/example.ipynb) | [All wheels](https://pypi.org/project/scann/1.2.9/#files) | [Slightly outdated (3.10)](https://github.com/google-research/google-research/issues/1590) | very fast | middling. [Faiss](https://github.com/facebookresearch/faiss) | [wiki](https://github.com/facebookresearch/faiss/wiki/Getting-started) | * | * | middling to very fast | faiss-ivf: top. *no official package on PyPI",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2519:462,modifiability,modul,modules,462,"Add approximate nearest neighbor backend; Goals:. - flexibility in creation. - Neighbor querying API. - Harmonize storage of backends querying indices. 	- use it in doublet detection and scanorama reimplementations. Current solution: `method` arg. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L32. - if `use_dense_distances`: [`sklearn.metrics.pairwise_distances`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html). - if `method == 'rapids'`: [`cuml.neighbors.NearestNeighbors`](https://docs.rapids.ai/api/cuml/stable/api/#cuml.neighbors.NearestNeighbors). - otherwise: [`umap.umap_.nearest_neighbors`](https://umap-learn.readthedocs.io/en/latest/api.html#umap.umap_.nearest_neighbors). - if `method == 'gauss'`: use umap distances, overwrite its connectivities. ## Evaluating options. See [ann-benchmarks.com](https://ann-benchmarks.com/index.html#datasets). Build time vs query time is not straightforward, see https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180389432 and https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180747770. > If, however, you are simply interested in knn-graph construction then you can get that from pynndescent in less time than even the index construction time (since the prepare phase isn't required, but is a non-trivial part of the index construction time). Plots for index building are on individual dataset pages, like [glove-100-angular](https://ann-benchmarks.com/glove-100-angular_10_angular.html). <details>. <summary>Used metrics</summary>. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L35-L56. </details>. Interesting:. Name | Demo | wheels: Platforms | wheels: Python | Search speed | Index build. --- | --- | --- | --- | --- | ---. [NMSLIB](https://github.com/nmslib/nmslib) | [6 notebooks](https://github.com/nmslib/nmslib/blob/master/python_bind",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2519:2719,modifiability,pac,package,2719,"edocs.io/en/latest/api.html#umap.umap_.nearest_neighbors). - if `method == 'gauss'`: use umap distances, overwrite its connectivities. ## Evaluating options. See [ann-benchmarks.com](https://ann-benchmarks.com/index.html#datasets). Build time vs query time is not straightforward, see https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180389432 and https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180747770. > If, however, you are simply interested in knn-graph construction then you can get that from pynndescent in less time than even the index construction time (since the prepare phase isn't required, but is a non-trivial part of the index construction time). Plots for index building are on individual dataset pages, like [glove-100-angular](https://ann-benchmarks.com/glove-100-angular_10_angular.html). <details>. <summary>Used metrics</summary>. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L35-L56. </details>. Interesting:. Name | Demo | wheels: Platforms | wheels: Python | Search speed | Index build. --- | --- | --- | --- | --- | ---. [NMSLIB](https://github.com/nmslib/nmslib) | [6 notebooks](https://github.com/nmslib/nmslib/blob/master/python_bindings/notebooks/README.md) | [Linux wheels](https://pypi.org/project/nmslib/2.1.1/#files) |[Outdated (3.9)](https://github.com/nmslib/nmslib/issues/529) | top | slow. [ScaNN](https://github.com/google-research/google-research/tree/master/scann#readme) | [notebook](https://github.com/google-research/google-research/blob/master/scann/docs/example.ipynb) | [All wheels](https://pypi.org/project/scann/1.2.9/#files) | [Slightly outdated (3.10)](https://github.com/google-research/google-research/issues/1590) | very fast | middling. [Faiss](https://github.com/facebookresearch/faiss) | [wiki](https://github.com/facebookresearch/faiss/wiki/Getting-started) | * | * | middling to very fast | faiss-ivf: top. *no official package on PyPI",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2519:972,performance,time,time,972,"Add approximate nearest neighbor backend; Goals:. - flexibility in creation. - Neighbor querying API. - Harmonize storage of backends querying indices. 	- use it in doublet detection and scanorama reimplementations. Current solution: `method` arg. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L32. - if `use_dense_distances`: [`sklearn.metrics.pairwise_distances`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html). - if `method == 'rapids'`: [`cuml.neighbors.NearestNeighbors`](https://docs.rapids.ai/api/cuml/stable/api/#cuml.neighbors.NearestNeighbors). - otherwise: [`umap.umap_.nearest_neighbors`](https://umap-learn.readthedocs.io/en/latest/api.html#umap.umap_.nearest_neighbors). - if `method == 'gauss'`: use umap distances, overwrite its connectivities. ## Evaluating options. See [ann-benchmarks.com](https://ann-benchmarks.com/index.html#datasets). Build time vs query time is not straightforward, see https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180389432 and https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180747770. > If, however, you are simply interested in knn-graph construction then you can get that from pynndescent in less time than even the index construction time (since the prepare phase isn't required, but is a non-trivial part of the index construction time). Plots for index building are on individual dataset pages, like [glove-100-angular](https://ann-benchmarks.com/glove-100-angular_10_angular.html). <details>. <summary>Used metrics</summary>. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L35-L56. </details>. Interesting:. Name | Demo | wheels: Platforms | wheels: Python | Search speed | Index build. --- | --- | --- | --- | --- | ---. [NMSLIB](https://github.com/nmslib/nmslib) | [6 notebooks](https://github.com/nmslib/nmslib/blob/master/python_bind",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2519:986,performance,time,time,986,"Add approximate nearest neighbor backend; Goals:. - flexibility in creation. - Neighbor querying API. - Harmonize storage of backends querying indices. 	- use it in doublet detection and scanorama reimplementations. Current solution: `method` arg. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L32. - if `use_dense_distances`: [`sklearn.metrics.pairwise_distances`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html). - if `method == 'rapids'`: [`cuml.neighbors.NearestNeighbors`](https://docs.rapids.ai/api/cuml/stable/api/#cuml.neighbors.NearestNeighbors). - otherwise: [`umap.umap_.nearest_neighbors`](https://umap-learn.readthedocs.io/en/latest/api.html#umap.umap_.nearest_neighbors). - if `method == 'gauss'`: use umap distances, overwrite its connectivities. ## Evaluating options. See [ann-benchmarks.com](https://ann-benchmarks.com/index.html#datasets). Build time vs query time is not straightforward, see https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180389432 and https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180747770. > If, however, you are simply interested in knn-graph construction then you can get that from pynndescent in less time than even the index construction time (since the prepare phase isn't required, but is a non-trivial part of the index construction time). Plots for index building are on individual dataset pages, like [glove-100-angular](https://ann-benchmarks.com/glove-100-angular_10_angular.html). <details>. <summary>Used metrics</summary>. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L35-L56. </details>. Interesting:. Name | Demo | wheels: Platforms | wheels: Python | Search speed | Index build. --- | --- | --- | --- | --- | ---. [NMSLIB](https://github.com/nmslib/nmslib) | [6 notebooks](https://github.com/nmslib/nmslib/blob/master/python_bind",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2519:1294,performance,time,time,1294,"d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L32. - if `use_dense_distances`: [`sklearn.metrics.pairwise_distances`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html). - if `method == 'rapids'`: [`cuml.neighbors.NearestNeighbors`](https://docs.rapids.ai/api/cuml/stable/api/#cuml.neighbors.NearestNeighbors). - otherwise: [`umap.umap_.nearest_neighbors`](https://umap-learn.readthedocs.io/en/latest/api.html#umap.umap_.nearest_neighbors). - if `method == 'gauss'`: use umap distances, overwrite its connectivities. ## Evaluating options. See [ann-benchmarks.com](https://ann-benchmarks.com/index.html#datasets). Build time vs query time is not straightforward, see https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180389432 and https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180747770. > If, however, you are simply interested in knn-graph construction then you can get that from pynndescent in less time than even the index construction time (since the prepare phase isn't required, but is a non-trivial part of the index construction time). Plots for index building are on individual dataset pages, like [glove-100-angular](https://ann-benchmarks.com/glove-100-angular_10_angular.html). <details>. <summary>Used metrics</summary>. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L35-L56. </details>. Interesting:. Name | Demo | wheels: Platforms | wheels: Python | Search speed | Index build. --- | --- | --- | --- | --- | ---. [NMSLIB](https://github.com/nmslib/nmslib) | [6 notebooks](https://github.com/nmslib/nmslib/blob/master/python_bindings/notebooks/README.md) | [Linux wheels](https://pypi.org/project/nmslib/2.1.1/#files) |[Outdated (3.9)](https://github.com/nmslib/nmslib/issues/529) | top | slow. [ScaNN](https://github.com/google-research/google-research/tree/master/scann#readme) | [notebook](https://github.com/google-resear",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2519:1332,performance,time,time,1332,"y/neighbors/__init__.py#L32. - if `use_dense_distances`: [`sklearn.metrics.pairwise_distances`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html). - if `method == 'rapids'`: [`cuml.neighbors.NearestNeighbors`](https://docs.rapids.ai/api/cuml/stable/api/#cuml.neighbors.NearestNeighbors). - otherwise: [`umap.umap_.nearest_neighbors`](https://umap-learn.readthedocs.io/en/latest/api.html#umap.umap_.nearest_neighbors). - if `method == 'gauss'`: use umap distances, overwrite its connectivities. ## Evaluating options. See [ann-benchmarks.com](https://ann-benchmarks.com/index.html#datasets). Build time vs query time is not straightforward, see https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180389432 and https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180747770. > If, however, you are simply interested in knn-graph construction then you can get that from pynndescent in less time than even the index construction time (since the prepare phase isn't required, but is a non-trivial part of the index construction time). Plots for index building are on individual dataset pages, like [glove-100-angular](https://ann-benchmarks.com/glove-100-angular_10_angular.html). <details>. <summary>Used metrics</summary>. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L35-L56. </details>. Interesting:. Name | Demo | wheels: Platforms | wheels: Python | Search speed | Index build. --- | --- | --- | --- | --- | ---. [NMSLIB](https://github.com/nmslib/nmslib) | [6 notebooks](https://github.com/nmslib/nmslib/blob/master/python_bindings/notebooks/README.md) | [Linux wheels](https://pypi.org/project/nmslib/2.1.1/#files) |[Outdated (3.9)](https://github.com/nmslib/nmslib/issues/529) | top | slow. [ScaNN](https://github.com/google-research/google-research/tree/master/scann#readme) | [notebook](https://github.com/google-research/google-research/blob/master/scann/d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2519:1430,performance,time,time,1430,"tps://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html). - if `method == 'rapids'`: [`cuml.neighbors.NearestNeighbors`](https://docs.rapids.ai/api/cuml/stable/api/#cuml.neighbors.NearestNeighbors). - otherwise: [`umap.umap_.nearest_neighbors`](https://umap-learn.readthedocs.io/en/latest/api.html#umap.umap_.nearest_neighbors). - if `method == 'gauss'`: use umap distances, overwrite its connectivities. ## Evaluating options. See [ann-benchmarks.com](https://ann-benchmarks.com/index.html#datasets). Build time vs query time is not straightforward, see https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180389432 and https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180747770. > If, however, you are simply interested in knn-graph construction then you can get that from pynndescent in less time than even the index construction time (since the prepare phase isn't required, but is a non-trivial part of the index construction time). Plots for index building are on individual dataset pages, like [glove-100-angular](https://ann-benchmarks.com/glove-100-angular_10_angular.html). <details>. <summary>Used metrics</summary>. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L35-L56. </details>. Interesting:. Name | Demo | wheels: Platforms | wheels: Python | Search speed | Index build. --- | --- | --- | --- | --- | ---. [NMSLIB](https://github.com/nmslib/nmslib) | [6 notebooks](https://github.com/nmslib/nmslib/blob/master/python_bindings/notebooks/README.md) | [Linux wheels](https://pypi.org/project/nmslib/2.1.1/#files) |[Outdated (3.9)](https://github.com/nmslib/nmslib/issues/529) | top | slow. [ScaNN](https://github.com/google-research/google-research/tree/master/scann#readme) | [notebook](https://github.com/google-research/google-research/blob/master/scann/docs/example.ipynb) | [All wheels](https://pypi.org/project/scann/1.2.9/#files) | [Slightly outdate",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2519:2160,reliability,slo,slow,2160,"edocs.io/en/latest/api.html#umap.umap_.nearest_neighbors). - if `method == 'gauss'`: use umap distances, overwrite its connectivities. ## Evaluating options. See [ann-benchmarks.com](https://ann-benchmarks.com/index.html#datasets). Build time vs query time is not straightforward, see https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180389432 and https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180747770. > If, however, you are simply interested in knn-graph construction then you can get that from pynndescent in less time than even the index construction time (since the prepare phase isn't required, but is a non-trivial part of the index construction time). Plots for index building are on individual dataset pages, like [glove-100-angular](https://ann-benchmarks.com/glove-100-angular_10_angular.html). <details>. <summary>Used metrics</summary>. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L35-L56. </details>. Interesting:. Name | Demo | wheels: Platforms | wheels: Python | Search speed | Index build. --- | --- | --- | --- | --- | ---. [NMSLIB](https://github.com/nmslib/nmslib) | [6 notebooks](https://github.com/nmslib/nmslib/blob/master/python_bindings/notebooks/README.md) | [Linux wheels](https://pypi.org/project/nmslib/2.1.1/#files) |[Outdated (3.9)](https://github.com/nmslib/nmslib/issues/529) | top | slow. [ScaNN](https://github.com/google-research/google-research/tree/master/scann#readme) | [notebook](https://github.com/google-research/google-research/blob/master/scann/docs/example.ipynb) | [All wheels](https://pypi.org/project/scann/1.2.9/#files) | [Slightly outdated (3.10)](https://github.com/google-research/google-research/issues/1590) | very fast | middling. [Faiss](https://github.com/facebookresearch/faiss) | [wiki](https://github.com/facebookresearch/faiss/wiki/Getting-started) | * | * | middling to very fast | faiss-ivf: top. *no official package on PyPI",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2519:2416,reliability,Sli,Slightly,2416,"edocs.io/en/latest/api.html#umap.umap_.nearest_neighbors). - if `method == 'gauss'`: use umap distances, overwrite its connectivities. ## Evaluating options. See [ann-benchmarks.com](https://ann-benchmarks.com/index.html#datasets). Build time vs query time is not straightforward, see https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180389432 and https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180747770. > If, however, you are simply interested in knn-graph construction then you can get that from pynndescent in less time than even the index construction time (since the prepare phase isn't required, but is a non-trivial part of the index construction time). Plots for index building are on individual dataset pages, like [glove-100-angular](https://ann-benchmarks.com/glove-100-angular_10_angular.html). <details>. <summary>Used metrics</summary>. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L35-L56. </details>. Interesting:. Name | Demo | wheels: Platforms | wheels: Python | Search speed | Index build. --- | --- | --- | --- | --- | ---. [NMSLIB](https://github.com/nmslib/nmslib) | [6 notebooks](https://github.com/nmslib/nmslib/blob/master/python_bindings/notebooks/README.md) | [Linux wheels](https://pypi.org/project/nmslib/2.1.1/#files) |[Outdated (3.9)](https://github.com/nmslib/nmslib/issues/529) | top | slow. [ScaNN](https://github.com/google-research/google-research/tree/master/scann#readme) | [notebook](https://github.com/google-research/google-research/blob/master/scann/docs/example.ipynb) | [All wheels](https://pypi.org/project/scann/1.2.9/#files) | [Slightly outdated (3.10)](https://github.com/google-research/google-research/issues/1590) | very fast | middling. [Faiss](https://github.com/facebookresearch/faiss) | [wiki](https://github.com/facebookresearch/faiss/wiki/Getting-started) | * | * | middling to very fast | faiss-ivf: top. *no official package on PyPI",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2519:174,safety,detect,detection,174,"Add approximate nearest neighbor backend; Goals:. - flexibility in creation. - Neighbor querying API. - Harmonize storage of backends querying indices. 	- use it in doublet detection and scanorama reimplementations. Current solution: `method` arg. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L32. - if `use_dense_distances`: [`sklearn.metrics.pairwise_distances`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html). - if `method == 'rapids'`: [`cuml.neighbors.NearestNeighbors`](https://docs.rapids.ai/api/cuml/stable/api/#cuml.neighbors.NearestNeighbors). - otherwise: [`umap.umap_.nearest_neighbors`](https://umap-learn.readthedocs.io/en/latest/api.html#umap.umap_.nearest_neighbors). - if `method == 'gauss'`: use umap distances, overwrite its connectivities. ## Evaluating options. See [ann-benchmarks.com](https://ann-benchmarks.com/index.html#datasets). Build time vs query time is not straightforward, see https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180389432 and https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180747770. > If, however, you are simply interested in knn-graph construction then you can get that from pynndescent in less time than even the index construction time (since the prepare phase isn't required, but is a non-trivial part of the index construction time). Plots for index building are on individual dataset pages, like [glove-100-angular](https://ann-benchmarks.com/glove-100-angular_10_angular.html). <details>. <summary>Used metrics</summary>. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L35-L56. </details>. Interesting:. Name | Demo | wheels: Platforms | wheels: Python | Search speed | Index build. --- | --- | --- | --- | --- | ---. [NMSLIB](https://github.com/nmslib/nmslib) | [6 notebooks](https://github.com/nmslib/nmslib/blob/master/python_bind",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2519:462,safety,modul,modules,462,"Add approximate nearest neighbor backend; Goals:. - flexibility in creation. - Neighbor querying API. - Harmonize storage of backends querying indices. 	- use it in doublet detection and scanorama reimplementations. Current solution: `method` arg. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L32. - if `use_dense_distances`: [`sklearn.metrics.pairwise_distances`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html). - if `method == 'rapids'`: [`cuml.neighbors.NearestNeighbors`](https://docs.rapids.ai/api/cuml/stable/api/#cuml.neighbors.NearestNeighbors). - otherwise: [`umap.umap_.nearest_neighbors`](https://umap-learn.readthedocs.io/en/latest/api.html#umap.umap_.nearest_neighbors). - if `method == 'gauss'`: use umap distances, overwrite its connectivities. ## Evaluating options. See [ann-benchmarks.com](https://ann-benchmarks.com/index.html#datasets). Build time vs query time is not straightforward, see https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180389432 and https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180747770. > If, however, you are simply interested in knn-graph construction then you can get that from pynndescent in less time than even the index construction time (since the prepare phase isn't required, but is a non-trivial part of the index construction time). Plots for index building are on individual dataset pages, like [glove-100-angular](https://ann-benchmarks.com/glove-100-angular_10_angular.html). <details>. <summary>Used metrics</summary>. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L35-L56. </details>. Interesting:. Name | Demo | wheels: Platforms | wheels: Python | Search speed | Index build. --- | --- | --- | --- | --- | ---. [NMSLIB](https://github.com/nmslib/nmslib) | [6 notebooks](https://github.com/nmslib/nmslib/blob/master/python_bind",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2519:174,security,detect,detection,174,"Add approximate nearest neighbor backend; Goals:. - flexibility in creation. - Neighbor querying API. - Harmonize storage of backends querying indices. 	- use it in doublet detection and scanorama reimplementations. Current solution: `method` arg. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L32. - if `use_dense_distances`: [`sklearn.metrics.pairwise_distances`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html). - if `method == 'rapids'`: [`cuml.neighbors.NearestNeighbors`](https://docs.rapids.ai/api/cuml/stable/api/#cuml.neighbors.NearestNeighbors). - otherwise: [`umap.umap_.nearest_neighbors`](https://umap-learn.readthedocs.io/en/latest/api.html#umap.umap_.nearest_neighbors). - if `method == 'gauss'`: use umap distances, overwrite its connectivities. ## Evaluating options. See [ann-benchmarks.com](https://ann-benchmarks.com/index.html#datasets). Build time vs query time is not straightforward, see https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180389432 and https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180747770. > If, however, you are simply interested in knn-graph construction then you can get that from pynndescent in less time than even the index construction time (since the prepare phase isn't required, but is a non-trivial part of the index construction time). Plots for index building are on individual dataset pages, like [glove-100-angular](https://ann-benchmarks.com/glove-100-angular_10_angular.html). <details>. <summary>Used metrics</summary>. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L35-L56. </details>. Interesting:. Name | Demo | wheels: Platforms | wheels: Python | Search speed | Index build. --- | --- | --- | --- | --- | ---. [NMSLIB](https://github.com/nmslib/nmslib) | [6 notebooks](https://github.com/nmslib/nmslib/blob/master/python_bind",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2519:1203,testability,simpl,simply,1203,"entations. Current solution: `method` arg. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L32. - if `use_dense_distances`: [`sklearn.metrics.pairwise_distances`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html). - if `method == 'rapids'`: [`cuml.neighbors.NearestNeighbors`](https://docs.rapids.ai/api/cuml/stable/api/#cuml.neighbors.NearestNeighbors). - otherwise: [`umap.umap_.nearest_neighbors`](https://umap-learn.readthedocs.io/en/latest/api.html#umap.umap_.nearest_neighbors). - if `method == 'gauss'`: use umap distances, overwrite its connectivities. ## Evaluating options. See [ann-benchmarks.com](https://ann-benchmarks.com/index.html#datasets). Build time vs query time is not straightforward, see https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180389432 and https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180747770. > If, however, you are simply interested in knn-graph construction then you can get that from pynndescent in less time than even the index construction time (since the prepare phase isn't required, but is a non-trivial part of the index construction time). Plots for index building are on individual dataset pages, like [glove-100-angular](https://ann-benchmarks.com/glove-100-angular_10_angular.html). <details>. <summary>Used metrics</summary>. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L35-L56. </details>. Interesting:. Name | Demo | wheels: Platforms | wheels: Python | Search speed | Index build. --- | --- | --- | --- | --- | ---. [NMSLIB](https://github.com/nmslib/nmslib) | [6 notebooks](https://github.com/nmslib/nmslib/blob/master/python_bindings/notebooks/README.md) | [Linux wheels](https://pypi.org/project/nmslib/2.1.1/#files) |[Outdated (3.9)](https://github.com/nmslib/nmslib/issues/529) | top | slow. [ScaNN](https://github.com/google-resear",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2519:445,usability,learn,learn,445,"Add approximate nearest neighbor backend; Goals:. - flexibility in creation. - Neighbor querying API. - Harmonize storage of backends querying indices. 	- use it in doublet detection and scanorama reimplementations. Current solution: `method` arg. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L32. - if `use_dense_distances`: [`sklearn.metrics.pairwise_distances`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html). - if `method == 'rapids'`: [`cuml.neighbors.NearestNeighbors`](https://docs.rapids.ai/api/cuml/stable/api/#cuml.neighbors.NearestNeighbors). - otherwise: [`umap.umap_.nearest_neighbors`](https://umap-learn.readthedocs.io/en/latest/api.html#umap.umap_.nearest_neighbors). - if `method == 'gauss'`: use umap distances, overwrite its connectivities. ## Evaluating options. See [ann-benchmarks.com](https://ann-benchmarks.com/index.html#datasets). Build time vs query time is not straightforward, see https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180389432 and https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180747770. > If, however, you are simply interested in knn-graph construction then you can get that from pynndescent in less time than even the index construction time (since the prepare phase isn't required, but is a non-trivial part of the index construction time). Plots for index building are on individual dataset pages, like [glove-100-angular](https://ann-benchmarks.com/glove-100-angular_10_angular.html). <details>. <summary>Used metrics</summary>. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L35-L56. </details>. Interesting:. Name | Demo | wheels: Platforms | wheels: Python | Search speed | Index build. --- | --- | --- | --- | --- | ---. [NMSLIB](https://github.com/nmslib/nmslib) | [6 notebooks](https://github.com/nmslib/nmslib/blob/master/python_bind",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2519:722,usability,learn,learn,722,"Add approximate nearest neighbor backend; Goals:. - flexibility in creation. - Neighbor querying API. - Harmonize storage of backends querying indices. 	- use it in doublet detection and scanorama reimplementations. Current solution: `method` arg. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L32. - if `use_dense_distances`: [`sklearn.metrics.pairwise_distances`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html). - if `method == 'rapids'`: [`cuml.neighbors.NearestNeighbors`](https://docs.rapids.ai/api/cuml/stable/api/#cuml.neighbors.NearestNeighbors). - otherwise: [`umap.umap_.nearest_neighbors`](https://umap-learn.readthedocs.io/en/latest/api.html#umap.umap_.nearest_neighbors). - if `method == 'gauss'`: use umap distances, overwrite its connectivities. ## Evaluating options. See [ann-benchmarks.com](https://ann-benchmarks.com/index.html#datasets). Build time vs query time is not straightforward, see https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180389432 and https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180747770. > If, however, you are simply interested in knn-graph construction then you can get that from pynndescent in less time than even the index construction time (since the prepare phase isn't required, but is a non-trivial part of the index construction time). Plots for index building are on individual dataset pages, like [glove-100-angular](https://ann-benchmarks.com/glove-100-angular_10_angular.html). <details>. <summary>Used metrics</summary>. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L35-L56. </details>. Interesting:. Name | Demo | wheels: Platforms | wheels: Python | Search speed | Index build. --- | --- | --- | --- | --- | ---. [NMSLIB](https://github.com/nmslib/nmslib) | [6 notebooks](https://github.com/nmslib/nmslib/blob/master/python_bind",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2519:1203,usability,simpl,simply,1203,"entations. Current solution: `method` arg. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L32. - if `use_dense_distances`: [`sklearn.metrics.pairwise_distances`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html). - if `method == 'rapids'`: [`cuml.neighbors.NearestNeighbors`](https://docs.rapids.ai/api/cuml/stable/api/#cuml.neighbors.NearestNeighbors). - otherwise: [`umap.umap_.nearest_neighbors`](https://umap-learn.readthedocs.io/en/latest/api.html#umap.umap_.nearest_neighbors). - if `method == 'gauss'`: use umap distances, overwrite its connectivities. ## Evaluating options. See [ann-benchmarks.com](https://ann-benchmarks.com/index.html#datasets). Build time vs query time is not straightforward, see https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180389432 and https://github.com/erikbern/ann-benchmarks/issues/207#issuecomment-1180747770. > If, however, you are simply interested in knn-graph construction then you can get that from pynndescent in less time than even the index construction time (since the prepare phase isn't required, but is a non-trivial part of the index construction time). Plots for index building are on individual dataset pages, like [glove-100-angular](https://ann-benchmarks.com/glove-100-angular_10_angular.html). <details>. <summary>Used metrics</summary>. https://github.com/scverse/scanpy/blob/ed8e1401d39068782f2435d258b33fce4f7b4a9e/scanpy/neighbors/__init__.py#L35-L56. </details>. Interesting:. Name | Demo | wheels: Platforms | wheels: Python | Search speed | Index build. --- | --- | --- | --- | --- | ---. [NMSLIB](https://github.com/nmslib/nmslib) | [6 notebooks](https://github.com/nmslib/nmslib/blob/master/python_bindings/notebooks/README.md) | [Linux wheels](https://pypi.org/project/nmslib/2.1.1/#files) |[Outdated (3.9)](https://github.com/nmslib/nmslib/issues/529) | top | slow. [ScaNN](https://github.com/google-resear",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519
https://github.com/scverse/scanpy/issues/2520:112,modifiability,paramet,parameters,112,"Swap axes of image using sc.pl.spatial; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hi, I want to swap the axes of plots when using sc.pl.spatial, but set ""dimension = (1, 0)"" parameter only swaps the spatial axes. Is there any solutions? ![spatial](https://github.com/scverse/scanpy/assets/38309419/e5dba2b9-8dcd-434a-9ee9-08f6bdf9a3cf).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2520
https://github.com/scverse/scanpy/issues/2520:292,modifiability,paramet,parameter,292,"Swap axes of image using sc.pl.spatial; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hi, I want to swap the axes of plots when using sc.pl.spatial, but set ""dimension = (1, 0)"" parameter only swaps the spatial axes. Is there any solutions? ![spatial](https://github.com/scverse/scanpy/assets/38309419/e5dba2b9-8dcd-434a-9ee9-08f6bdf9a3cf).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2520
https://github.com/scverse/scanpy/pull/2521:49,deployability,fail,fail,49,"Fix paga tests; In the sense that they no longer fail, not that they make sense. Thats what #2235 is for.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2521
https://github.com/scverse/scanpy/pull/2521:49,reliability,fail,fail,49,"Fix paga tests; In the sense that they no longer fail, not that they make sense. Thats what #2235 is for.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2521
https://github.com/scverse/scanpy/pull/2521:9,safety,test,tests,9,"Fix paga tests; In the sense that they no longer fail, not that they make sense. Thats what #2235 is for.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2521
https://github.com/scverse/scanpy/pull/2521:9,testability,test,tests,9,"Fix paga tests; In the sense that they no longer fail, not that they make sense. Thats what #2235 is for.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2521
https://github.com/scverse/scanpy/pull/2522:42,usability,prefer,prefer,42,Use functions for data fixtures; IDK if I prefer this.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2522
https://github.com/scverse/scanpy/issues/2524:16,interoperability,coordinat,coordinates,16,get discretized coordinates from spatial function; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Obtain the discretized coordinates from using scanpy.pl.spatial. https://scanpy.readthedocs.io/en/stable/generated/scanpy.pl.spatial.html,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2524
https://github.com/scverse/scanpy/issues/2524:234,interoperability,coordinat,coordinates,234,get discretized coordinates from spatial function; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Obtain the discretized coordinates from using scanpy.pl.spatial. https://scanpy.readthedocs.io/en/stable/generated/scanpy.pl.spatial.html,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2524
https://github.com/scverse/scanpy/issues/2524:123,modifiability,paramet,parameters,123,get discretized coordinates from spatial function; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Obtain the discretized coordinates from using scanpy.pl.spatial. https://scanpy.readthedocs.io/en/stable/generated/scanpy.pl.spatial.html,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2524
https://github.com/scverse/scanpy/pull/2525:44,safety,test,tests,44,Backport PR #2521 on branch 1.9.x (Fix paga tests); Backport PR #2521: Fix paga tests,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2525
https://github.com/scverse/scanpy/pull/2525:80,safety,test,tests,80,Backport PR #2521 on branch 1.9.x (Fix paga tests); Backport PR #2521: Fix paga tests,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2525
https://github.com/scverse/scanpy/pull/2525:44,testability,test,tests,44,Backport PR #2521 on branch 1.9.x (Fix paga tests); Backport PR #2521: Fix paga tests,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2525
https://github.com/scverse/scanpy/pull/2525:80,testability,test,tests,80,Backport PR #2521 on branch 1.9.x (Fix paga tests); Backport PR #2521: Fix paga tests,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2525
https://github.com/scverse/scanpy/issues/2526:500,availability,avail,available,500,"Broken test: `TestPreprocessingDistributed::test_normalize_per_cell[dask]`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In #2235, more gradually enabled some so far disabled tests. Before, all tests in `TestPreprocessingDistributed` were disabled with the available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. ___________________________________",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:1017,availability,Error,Error,1017,"eprocessingDistributed::test_normalize_per_cell[dask]`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In #2235, more gradually enabled some so far disabled tests. Before, all tests in `TestPreprocessingDistributed` were disabled with the available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. _______________________________________________________",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:1847,availability,FAILUR,FAILURES,1847,"tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. __________________________________________________________________________________ TestPreprocessingDistributed.test_normalize_per_cell[dask] __________________________________________________________________________________. self = <scanpy.tests.test_preprocessing_distributed.TestPreprocessingDistributed object at 0x7fdd21f2e9d0>, adata = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. adata_dist = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. uns: 'dist-mode'. def test_normalize_per_cell(self, adata, adata_dist):. if adata_dist.uns[""dist-mode""] == ""dask"":. pytest.xfail(""TODO: Test broken for dask""). normalize_per_cell(adata_dist). result = materialize_as_ndarray(adata_dist.X). normalize_per_cell(adata). assert result.shape == adata.shape. assert result.shape == (adata.n_obs, adata.n_vars). > npt.ass",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:2917,availability,toler,tolerance,2917,"=========================================. __________________________________________________________________________________ TestPreprocessingDistributed.test_normalize_per_cell[dask] __________________________________________________________________________________. self = <scanpy.tests.test_preprocessing_distributed.TestPreprocessingDistributed object at 0x7fdd21f2e9d0>, adata = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. adata_dist = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. uns: 'dist-mode'. def test_normalize_per_cell(self, adata, adata_dist):. if adata_dist.uns[""dist-mode""] == ""dask"":. pytest.xfail(""TODO: Test broken for dask""). normalize_per_cell(adata_dist). result = materialize_as_ndarray(adata_dist.X). normalize_per_cell(adata). assert result.shape == adata.shape. assert result.shape == (adata.n_obs, adata.n_vars). > npt.assert_allclose(result, adata.X). E AssertionError: . E Not equal to tolerance rtol=1e-07, atol=0. E . E Mismatched elements: 688287 / 9999000 (6.88%). E Max absolute difference: 573.4154. E Max relative difference: 11.335767. E x: array([[0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],... E y: array([[0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],... scanpy/tests/test_preprocessing_distributed.py:64: AssertionError. ----------------------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------------------. normalizing by total count per cell. filtered out dask.array<sum-aggregate, shape=(), dtype=int64, chunksize=(), chunktype=numpy.ndarray> cells that have less than 1 counts. finished (0:00:00): normalized adata.X and added 'n_counts', counts per cell before normalization (adata.obs). normalizing by total count per cell. filt",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:244,deployability,version,version,244,"Broken test: `TestPreprocessingDistributed::test_normalize_per_cell[dask]`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In #2235, more gradually enabled some so far disabled tests. Before, all tests in `TestPreprocessingDistributed` were disabled with the available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. ___________________________________",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:519,deployability,depend,dependencies,519,"Broken test: `TestPreprocessingDistributed::test_normalize_per_cell[dask]`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In #2235, more gradually enabled some so far disabled tests. Before, all tests in `TestPreprocessingDistributed` were disabled with the available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. ___________________________________",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:1724,deployability,FAIL,FAILED,1724," only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. __________________________________________________________________________________ TestPreprocessingDistributed.test_normalize_per_cell[dask] __________________________________________________________________________________. self = <scanpy.tests.test_preprocessing_distributed.TestPreprocessingDistributed object at 0x7fdd21f2e9d0>, adata = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. adata_dist = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. uns: 'dist-mode'. def test_normalize_per_cell(self, adata, adata_dist):. if adata_dist.uns[""dist-mode""] == ""dask"":. pytest.xfail(""TODO: Test broken for dask""). normalize_per_cell(adata_dist). result = materialize_as_ndarray(adata_dist.X). ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:1847,deployability,FAIL,FAILURES,1847,"tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. __________________________________________________________________________________ TestPreprocessingDistributed.test_normalize_per_cell[dask] __________________________________________________________________________________. self = <scanpy.tests.test_preprocessing_distributed.TestPreprocessingDistributed object at 0x7fdd21f2e9d0>, adata = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. adata_dist = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. uns: 'dist-mode'. def test_normalize_per_cell(self, adata, adata_dist):. if adata_dist.uns[""dist-mode""] == ""dask"":. pytest.xfail(""TODO: Test broken for dask""). normalize_per_cell(adata_dist). result = materialize_as_ndarray(adata_dist.X). normalize_per_cell(adata). assert result.shape == adata.shape. assert result.shape == (adata.n_obs, adata.n_vars). > npt.ass",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:4306,deployability,FAIL,FAILED,4306,"/tests/test_preprocessing_distributed.py:64: AssertionError. ----------------------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------------------. normalizing by total count per cell. filtered out dask.array<sum-aggregate, shape=(), dtype=int64, chunksize=(), chunktype=numpy.ndarray> cells that have less than 1 counts. finished (0:00:00): normalized adata.X and added 'n_counts', counts per cell before normalization (adata.obs). normalizing by total count per cell. filtered out 1 cells that have less than 1 counts. finished (0:00:00): normalized adata.X and added 'n_counts', counts per cell before normalization (adata.obs). =================================================================================================== short test summary info ====================================================================================================. FAILED scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] - AssertionError: . ======================================================================================== 1 failed, 985 deselected, 11 warnings in 3.74s ========================================================================================. ```. ### Versions. <details>. ```. -----. anndata 0.9.0rc2.dev43+g21a76088. scanpy 1.10.0.dev117+g6b9e734f. -----. PIL 9.1.1. asciitree NA. awkward 2.2.1. awkward_cpp NA. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cloudpickle 2.2.1. cycler 0.10.0. cython_runtime NA. dask 2023.5.0. dateutil 2.8.2. defusedxml 0.7.1. entrypoints 0.4. fasteners 0.17.3. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. jinja2 3.1.2. joblib 1.1.0. kiwisolver 1.4.3. leidenalg 0.9.1. llvmlite 0.38.1. markupsafe 2.1.1. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.1.0. nbinom_ufunc NA. numba 0.55.2. numcodecs 0.10.2. numpy 1.22.4. packagin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:4532,deployability,fail,failed,4532,"-----------------------------------------------------------. normalizing by total count per cell. filtered out dask.array<sum-aggregate, shape=(), dtype=int64, chunksize=(), chunktype=numpy.ndarray> cells that have less than 1 counts. finished (0:00:00): normalized adata.X and added 'n_counts', counts per cell before normalization (adata.obs). normalizing by total count per cell. filtered out 1 cells that have less than 1 counts. finished (0:00:00): normalized adata.X and added 'n_counts', counts per cell before normalization (adata.obs). =================================================================================================== short test summary info ====================================================================================================. FAILED scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] - AssertionError: . ======================================================================================== 1 failed, 985 deselected, 11 warnings in 3.74s ========================================================================================. ```. ### Versions. <details>. ```. -----. anndata 0.9.0rc2.dev43+g21a76088. scanpy 1.10.0.dev117+g6b9e734f. -----. PIL 9.1.1. asciitree NA. awkward 2.2.1. awkward_cpp NA. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cloudpickle 2.2.1. cycler 0.10.0. cython_runtime NA. dask 2023.5.0. dateutil 2.8.2. defusedxml 0.7.1. entrypoints 0.4. fasteners 0.17.3. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. jinja2 3.1.2. joblib 1.1.0. kiwisolver 1.4.3. leidenalg 0.9.1. llvmlite 0.38.1. markupsafe 2.1.1. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.1.0. nbinom_ufunc NA. numba 0.55.2. numcodecs 0.10.2. numpy 1.22.4. packaging 21.3. pandas 2.0.2. pkg_resources NA. psutil 5.9.1. pyparsing 3.0.9. pytz 2022.1. scipy 1.8.1. session_info 1.0.0. setuptools 67.8.0. setuptools_scm NA. six 1.16.0. sklearn 1.1.1. sphinxcontrib NA. texttable 1.6.7. threadpoo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:4676,deployability,Version,Versions,4676,", dtype=int64, chunksize=(), chunktype=numpy.ndarray> cells that have less than 1 counts. finished (0:00:00): normalized adata.X and added 'n_counts', counts per cell before normalization (adata.obs). normalizing by total count per cell. filtered out 1 cells that have less than 1 counts. finished (0:00:00): normalized adata.X and added 'n_counts', counts per cell before normalization (adata.obs). =================================================================================================== short test summary info ====================================================================================================. FAILED scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] - AssertionError: . ======================================================================================== 1 failed, 985 deselected, 11 warnings in 3.74s ========================================================================================. ```. ### Versions. <details>. ```. -----. anndata 0.9.0rc2.dev43+g21a76088. scanpy 1.10.0.dev117+g6b9e734f. -----. PIL 9.1.1. asciitree NA. awkward 2.2.1. awkward_cpp NA. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cloudpickle 2.2.1. cycler 0.10.0. cython_runtime NA. dask 2023.5.0. dateutil 2.8.2. defusedxml 0.7.1. entrypoints 0.4. fasteners 0.17.3. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. jinja2 3.1.2. joblib 1.1.0. kiwisolver 1.4.3. leidenalg 0.9.1. llvmlite 0.38.1. markupsafe 2.1.1. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.1.0. nbinom_ufunc NA. numba 0.55.2. numcodecs 0.10.2. numpy 1.22.4. packaging 21.3. pandas 2.0.2. pkg_resources NA. psutil 5.9.1. pyparsing 3.0.9. pytz 2022.1. scipy 1.8.1. session_info 1.0.0. setuptools 67.8.0. setuptools_scm NA. six 1.16.0. sklearn 1.1.1. sphinxcontrib NA. texttable 1.6.7. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. typing_extensions NA. wcwidth 0.2.5. yaml 6.0. zarr 2.12.0. zipp NA. -----. Python 3.8.17 (default, Jun 17 ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:5792,deployability,updat,updated,5792," per cell before normalization (adata.obs). normalizing by total count per cell. filtered out 1 cells that have less than 1 counts. finished (0:00:00): normalized adata.X and added 'n_counts', counts per cell before normalization (adata.obs). =================================================================================================== short test summary info ====================================================================================================. FAILED scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] - AssertionError: . ======================================================================================== 1 failed, 985 deselected, 11 warnings in 3.74s ========================================================================================. ```. ### Versions. <details>. ```. -----. anndata 0.9.0rc2.dev43+g21a76088. scanpy 1.10.0.dev117+g6b9e734f. -----. PIL 9.1.1. asciitree NA. awkward 2.2.1. awkward_cpp NA. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cloudpickle 2.2.1. cycler 0.10.0. cython_runtime NA. dask 2023.5.0. dateutil 2.8.2. defusedxml 0.7.1. entrypoints 0.4. fasteners 0.17.3. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. jinja2 3.1.2. joblib 1.1.0. kiwisolver 1.4.3. leidenalg 0.9.1. llvmlite 0.38.1. markupsafe 2.1.1. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.1.0. nbinom_ufunc NA. numba 0.55.2. numcodecs 0.10.2. numpy 1.22.4. packaging 21.3. pandas 2.0.2. pkg_resources NA. psutil 5.9.1. pyparsing 3.0.9. pytz 2022.1. scipy 1.8.1. session_info 1.0.0. setuptools 67.8.0. setuptools_scm NA. six 1.16.0. sklearn 1.1.1. sphinxcontrib NA. texttable 1.6.7. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. typing_extensions NA. wcwidth 0.2.5. yaml 6.0. zarr 2.12.0. zipp NA. -----. Python 3.8.17 (default, Jun 17 2023, 20:09:37) [GCC 13.1.1 20230429]. Linux-6.3.8-zen1-1-zen-x86_64-with-glibc2.34. -----. Session information updated at 2023-06-23 14:29. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:4882,energy efficiency,cloud,cloudpickle,4882," per cell before normalization (adata.obs). normalizing by total count per cell. filtered out 1 cells that have less than 1 counts. finished (0:00:00): normalized adata.X and added 'n_counts', counts per cell before normalization (adata.obs). =================================================================================================== short test summary info ====================================================================================================. FAILED scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] - AssertionError: . ======================================================================================== 1 failed, 985 deselected, 11 warnings in 3.74s ========================================================================================. ```. ### Versions. <details>. ```. -----. anndata 0.9.0rc2.dev43+g21a76088. scanpy 1.10.0.dev117+g6b9e734f. -----. PIL 9.1.1. asciitree NA. awkward 2.2.1. awkward_cpp NA. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cloudpickle 2.2.1. cycler 0.10.0. cython_runtime NA. dask 2023.5.0. dateutil 2.8.2. defusedxml 0.7.1. entrypoints 0.4. fasteners 0.17.3. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. jinja2 3.1.2. joblib 1.1.0. kiwisolver 1.4.3. leidenalg 0.9.1. llvmlite 0.38.1. markupsafe 2.1.1. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.1.0. nbinom_ufunc NA. numba 0.55.2. numcodecs 0.10.2. numpy 1.22.4. packaging 21.3. pandas 2.0.2. pkg_resources NA. psutil 5.9.1. pyparsing 3.0.9. pytz 2022.1. scipy 1.8.1. session_info 1.0.0. setuptools 67.8.0. setuptools_scm NA. six 1.16.0. sklearn 1.1.1. sphinxcontrib NA. texttable 1.6.7. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. typing_extensions NA. wcwidth 0.2.5. yaml 6.0. zarr 2.12.0. zipp NA. -----. Python 3.8.17 (default, Jun 17 2023, 20:09:37) [GCC 13.1.1 20230429]. Linux-6.3.8-zen1-1-zen-x86_64-with-glibc2.34. -----. Session information updated at 2023-06-23 14:29. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:244,integrability,version,version,244,"Broken test: `TestPreprocessingDistributed::test_normalize_per_cell[dask]`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In #2235, more gradually enabled some so far disabled tests. Before, all tests in `TestPreprocessingDistributed` were disabled with the available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. ___________________________________",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:519,integrability,depend,dependencies,519,"Broken test: `TestPreprocessingDistributed::test_normalize_per_cell[dask]`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In #2235, more gradually enabled some so far disabled tests. Before, all tests in `TestPreprocessingDistributed` were disabled with the available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. ___________________________________",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:3633,integrability,filter,filtered,3633,"or dask""). normalize_per_cell(adata_dist). result = materialize_as_ndarray(adata_dist.X). normalize_per_cell(adata). assert result.shape == adata.shape. assert result.shape == (adata.n_obs, adata.n_vars). > npt.assert_allclose(result, adata.X). E AssertionError: . E Not equal to tolerance rtol=1e-07, atol=0. E . E Mismatched elements: 688287 / 9999000 (6.88%). E Max absolute difference: 573.4154. E Max relative difference: 11.335767. E x: array([[0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],... E y: array([[0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],... scanpy/tests/test_preprocessing_distributed.py:64: AssertionError. ----------------------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------------------. normalizing by total count per cell. filtered out dask.array<sum-aggregate, shape=(), dtype=int64, chunksize=(), chunktype=numpy.ndarray> cells that have less than 1 counts. finished (0:00:00): normalized adata.X and added 'n_counts', counts per cell before normalization (adata.obs). normalizing by total count per cell. filtered out 1 cells that have less than 1 counts. finished (0:00:00): normalized adata.X and added 'n_counts', counts per cell before normalization (adata.obs). =================================================================================================== short test summary info ====================================================================================================. FAILED scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] - AssertionError: . ======================================================================================== 1 failed, 985 deselected, 11 warnings in 3.74s ============================================================",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:3918,integrability,filter,filtered,3918,"ance rtol=1e-07, atol=0. E . E Mismatched elements: 688287 / 9999000 (6.88%). E Max absolute difference: 573.4154. E Max relative difference: 11.335767. E x: array([[0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],... E y: array([[0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],... scanpy/tests/test_preprocessing_distributed.py:64: AssertionError. ----------------------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------------------. normalizing by total count per cell. filtered out dask.array<sum-aggregate, shape=(), dtype=int64, chunksize=(), chunktype=numpy.ndarray> cells that have less than 1 counts. finished (0:00:00): normalized adata.X and added 'n_counts', counts per cell before normalization (adata.obs). normalizing by total count per cell. filtered out 1 cells that have less than 1 counts. finished (0:00:00): normalized adata.X and added 'n_counts', counts per cell before normalization (adata.obs). =================================================================================================== short test summary info ====================================================================================================. FAILED scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] - AssertionError: . ======================================================================================== 1 failed, 985 deselected, 11 warnings in 3.74s ========================================================================================. ```. ### Versions. <details>. ```. -----. anndata 0.9.0rc2.dev43+g21a76088. scanpy 1.10.0.dev117+g6b9e734f. -----. PIL 9.1.1. asciitree NA. awkward 2.2.1. awkward_cpp NA. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cloudpickle 2.2.1. cycler 0.10.0. cython",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:4676,integrability,Version,Versions,4676,", dtype=int64, chunksize=(), chunktype=numpy.ndarray> cells that have less than 1 counts. finished (0:00:00): normalized adata.X and added 'n_counts', counts per cell before normalization (adata.obs). normalizing by total count per cell. filtered out 1 cells that have less than 1 counts. finished (0:00:00): normalized adata.X and added 'n_counts', counts per cell before normalization (adata.obs). =================================================================================================== short test summary info ====================================================================================================. FAILED scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] - AssertionError: . ======================================================================================== 1 failed, 985 deselected, 11 warnings in 3.74s ========================================================================================. ```. ### Versions. <details>. ```. -----. anndata 0.9.0rc2.dev43+g21a76088. scanpy 1.10.0.dev117+g6b9e734f. -----. PIL 9.1.1. asciitree NA. awkward 2.2.1. awkward_cpp NA. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cloudpickle 2.2.1. cycler 0.10.0. cython_runtime NA. dask 2023.5.0. dateutil 2.8.2. defusedxml 0.7.1. entrypoints 0.4. fasteners 0.17.3. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. jinja2 3.1.2. joblib 1.1.0. kiwisolver 1.4.3. leidenalg 0.9.1. llvmlite 0.38.1. markupsafe 2.1.1. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.1.0. nbinom_ufunc NA. numba 0.55.2. numcodecs 0.10.2. numpy 1.22.4. packaging 21.3. pandas 2.0.2. pkg_resources NA. psutil 5.9.1. pyparsing 3.0.9. pytz 2022.1. scipy 1.8.1. session_info 1.0.0. setuptools 67.8.0. setuptools_scm NA. six 1.16.0. sklearn 1.1.1. sphinxcontrib NA. texttable 1.6.7. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. typing_extensions NA. wcwidth 0.2.5. yaml 6.0. zarr 2.12.0. zipp NA. -----. Python 3.8.17 (default, Jun 17 ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:1266,interoperability,platform,platform,1266,"optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In #2235, more gradually enabled some so far disabled tests. Before, all tests in `TestPreprocessingDistributed` were disabled with the available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. __________________________________________________________________________________ TestPreprocessingDistributed.test_normalize_per_cell[dask] __________________________________________________________________________________. self = <scanpy.tests.test_preprocessing_distributed.TestPreprocessingDistribute",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:1313,interoperability,plug,pluggy-,1313,"he master branch of scanpy. ### What happened? In #2235, more gradually enabled some so far disabled tests. Before, all tests in `TestPreprocessingDistributed` were disabled with the available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. __________________________________________________________________________________ TestPreprocessingDistributed.test_normalize_per_cell[dask] __________________________________________________________________________________. self = <scanpy.tests.test_preprocessing_distributed.TestPreprocessingDistributed object at 0x7fdd21f2e9d0>, adata = AnnData ob",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:1504,interoperability,plug,plugins,1504,"e optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. __________________________________________________________________________________ TestPreprocessingDistributed.test_normalize_per_cell[dask] __________________________________________________________________________________. self = <scanpy.tests.test_preprocessing_distributed.TestPreprocessingDistributed object at 0x7fdd21f2e9d0>, adata = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. adata_dist = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. uns: 'dist-mode'. de",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:2953,interoperability,Mismatch,Mismatched,2953,"=====. __________________________________________________________________________________ TestPreprocessingDistributed.test_normalize_per_cell[dask] __________________________________________________________________________________. self = <scanpy.tests.test_preprocessing_distributed.TestPreprocessingDistributed object at 0x7fdd21f2e9d0>, adata = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. adata_dist = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. uns: 'dist-mode'. def test_normalize_per_cell(self, adata, adata_dist):. if adata_dist.uns[""dist-mode""] == ""dask"":. pytest.xfail(""TODO: Test broken for dask""). normalize_per_cell(adata_dist). result = materialize_as_ndarray(adata_dist.X). normalize_per_cell(adata). assert result.shape == adata.shape. assert result.shape == (adata.n_obs, adata.n_vars). > npt.assert_allclose(result, adata.X). E AssertionError: . E Not equal to tolerance rtol=1e-07, atol=0. E . E Mismatched elements: 688287 / 9999000 (6.88%). E Max absolute difference: 573.4154. E Max relative difference: 11.335767. E x: array([[0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],... E y: array([[0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],... scanpy/tests/test_preprocessing_distributed.py:64: AssertionError. ----------------------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------------------. normalizing by total count per cell. filtered out dask.array<sum-aggregate, shape=(), dtype=int64, chunksize=(), chunktype=numpy.ndarray> cells that have less than 1 counts. finished (0:00:00): normalized adata.X and added 'n_counts', counts per cell before normalization (adata.obs). normalizing by total count per cell. filtered out 1 cells that have less than",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:244,modifiability,version,version,244,"Broken test: `TestPreprocessingDistributed::test_normalize_per_cell[dask]`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In #2235, more gradually enabled some so far disabled tests. Before, all tests in `TestPreprocessingDistributed` were disabled with the available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. ___________________________________",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:519,modifiability,depend,dependencies,519,"Broken test: `TestPreprocessingDistributed::test_normalize_per_cell[dask]`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In #2235, more gradually enabled some so far disabled tests. Before, all tests in `TestPreprocessingDistributed` were disabled with the available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. ___________________________________",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:4676,modifiability,Version,Versions,4676,", dtype=int64, chunksize=(), chunktype=numpy.ndarray> cells that have less than 1 counts. finished (0:00:00): normalized adata.X and added 'n_counts', counts per cell before normalization (adata.obs). normalizing by total count per cell. filtered out 1 cells that have less than 1 counts. finished (0:00:00): normalized adata.X and added 'n_counts', counts per cell before normalization (adata.obs). =================================================================================================== short test summary info ====================================================================================================. FAILED scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] - AssertionError: . ======================================================================================== 1 failed, 985 deselected, 11 warnings in 3.74s ========================================================================================. ```. ### Versions. <details>. ```. -----. anndata 0.9.0rc2.dev43+g21a76088. scanpy 1.10.0.dev117+g6b9e734f. -----. PIL 9.1.1. asciitree NA. awkward 2.2.1. awkward_cpp NA. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cloudpickle 2.2.1. cycler 0.10.0. cython_runtime NA. dask 2023.5.0. dateutil 2.8.2. defusedxml 0.7.1. entrypoints 0.4. fasteners 0.17.3. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. jinja2 3.1.2. joblib 1.1.0. kiwisolver 1.4.3. leidenalg 0.9.1. llvmlite 0.38.1. markupsafe 2.1.1. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.1.0. nbinom_ufunc NA. numba 0.55.2. numcodecs 0.10.2. numpy 1.22.4. packaging 21.3. pandas 2.0.2. pkg_resources NA. psutil 5.9.1. pyparsing 3.0.9. pytz 2022.1. scipy 1.8.1. session_info 1.0.0. setuptools 67.8.0. setuptools_scm NA. six 1.16.0. sklearn 1.1.1. sphinxcontrib NA. texttable 1.6.7. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. typing_extensions NA. wcwidth 0.2.5. yaml 6.0. zarr 2.12.0. zipp NA. -----. Python 3.8.17 (default, Jun 17 ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:5301,modifiability,pac,packaging,5301," per cell before normalization (adata.obs). normalizing by total count per cell. filtered out 1 cells that have less than 1 counts. finished (0:00:00): normalized adata.X and added 'n_counts', counts per cell before normalization (adata.obs). =================================================================================================== short test summary info ====================================================================================================. FAILED scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] - AssertionError: . ======================================================================================== 1 failed, 985 deselected, 11 warnings in 3.74s ========================================================================================. ```. ### Versions. <details>. ```. -----. anndata 0.9.0rc2.dev43+g21a76088. scanpy 1.10.0.dev117+g6b9e734f. -----. PIL 9.1.1. asciitree NA. awkward 2.2.1. awkward_cpp NA. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cloudpickle 2.2.1. cycler 0.10.0. cython_runtime NA. dask 2023.5.0. dateutil 2.8.2. defusedxml 0.7.1. entrypoints 0.4. fasteners 0.17.3. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. jinja2 3.1.2. joblib 1.1.0. kiwisolver 1.4.3. leidenalg 0.9.1. llvmlite 0.38.1. markupsafe 2.1.1. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.1.0. nbinom_ufunc NA. numba 0.55.2. numcodecs 0.10.2. numpy 1.22.4. packaging 21.3. pandas 2.0.2. pkg_resources NA. psutil 5.9.1. pyparsing 3.0.9. pytz 2022.1. scipy 1.8.1. session_info 1.0.0. setuptools 67.8.0. setuptools_scm NA. six 1.16.0. sklearn 1.1.1. sphinxcontrib NA. texttable 1.6.7. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. typing_extensions NA. wcwidth 0.2.5. yaml 6.0. zarr 2.12.0. zipp NA. -----. Python 3.8.17 (default, Jun 17 2023, 20:09:37) [GCC 13.1.1 20230429]. Linux-6.3.8-zen1-1-zen-x86_64-with-glibc2.34. -----. Session information updated at 2023-06-23 14:29. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:1017,performance,Error,Error,1017,"eprocessingDistributed::test_normalize_per_cell[dask]`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In #2235, more gradually enabled some so far disabled tests. Before, all tests in `TestPreprocessingDistributed` were disabled with the available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. _______________________________________________________",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:1381,performance,cach,cachedir,1381,"lly enabled some so far disabled tests. Before, all tests in `TestPreprocessingDistributed` were disabled with the available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. __________________________________________________________________________________ TestPreprocessingDistributed.test_normalize_per_cell[dask] __________________________________________________________________________________. self = <scanpy.tests.test_preprocessing_distributed.TestPreprocessingDistributed object at 0x7fdd21f2e9d0>, adata = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:1847,performance,FAILUR,FAILURES,1847,"tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. __________________________________________________________________________________ TestPreprocessingDistributed.test_normalize_per_cell[dask] __________________________________________________________________________________. self = <scanpy.tests.test_preprocessing_distributed.TestPreprocessingDistributed object at 0x7fdd21f2e9d0>, adata = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. adata_dist = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. uns: 'dist-mode'. def test_normalize_per_cell(self, adata, adata_dist):. if adata_dist.uns[""dist-mode""] == ""dask"":. pytest.xfail(""TODO: Test broken for dask""). normalize_per_cell(adata_dist). result = materialize_as_ndarray(adata_dist.X). normalize_per_cell(adata). assert result.shape == adata.shape. assert result.shape == (adata.n_obs, adata.n_vars). > npt.ass",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:500,reliability,availab,available,500,"Broken test: `TestPreprocessingDistributed::test_normalize_per_cell[dask]`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In #2235, more gradually enabled some so far disabled tests. Before, all tests in `TestPreprocessingDistributed` were disabled with the available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. ___________________________________",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:1724,reliability,FAIL,FAILED,1724," only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. __________________________________________________________________________________ TestPreprocessingDistributed.test_normalize_per_cell[dask] __________________________________________________________________________________. self = <scanpy.tests.test_preprocessing_distributed.TestPreprocessingDistributed object at 0x7fdd21f2e9d0>, adata = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. adata_dist = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. uns: 'dist-mode'. def test_normalize_per_cell(self, adata, adata_dist):. if adata_dist.uns[""dist-mode""] == ""dask"":. pytest.xfail(""TODO: Test broken for dask""). normalize_per_cell(adata_dist). result = materialize_as_ndarray(adata_dist.X). ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:1847,reliability,FAIL,FAILURES,1847,"tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. __________________________________________________________________________________ TestPreprocessingDistributed.test_normalize_per_cell[dask] __________________________________________________________________________________. self = <scanpy.tests.test_preprocessing_distributed.TestPreprocessingDistributed object at 0x7fdd21f2e9d0>, adata = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. adata_dist = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. uns: 'dist-mode'. def test_normalize_per_cell(self, adata, adata_dist):. if adata_dist.uns[""dist-mode""] == ""dask"":. pytest.xfail(""TODO: Test broken for dask""). normalize_per_cell(adata_dist). result = materialize_as_ndarray(adata_dist.X). normalize_per_cell(adata). assert result.shape == adata.shape. assert result.shape == (adata.n_obs, adata.n_vars). > npt.ass",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:2917,reliability,toleran,tolerance,2917,"=========================================. __________________________________________________________________________________ TestPreprocessingDistributed.test_normalize_per_cell[dask] __________________________________________________________________________________. self = <scanpy.tests.test_preprocessing_distributed.TestPreprocessingDistributed object at 0x7fdd21f2e9d0>, adata = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. adata_dist = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. uns: 'dist-mode'. def test_normalize_per_cell(self, adata, adata_dist):. if adata_dist.uns[""dist-mode""] == ""dask"":. pytest.xfail(""TODO: Test broken for dask""). normalize_per_cell(adata_dist). result = materialize_as_ndarray(adata_dist.X). normalize_per_cell(adata). assert result.shape == adata.shape. assert result.shape == (adata.n_obs, adata.n_vars). > npt.assert_allclose(result, adata.X). E AssertionError: . E Not equal to tolerance rtol=1e-07, atol=0. E . E Mismatched elements: 688287 / 9999000 (6.88%). E Max absolute difference: 573.4154. E Max relative difference: 11.335767. E x: array([[0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],... E y: array([[0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],... scanpy/tests/test_preprocessing_distributed.py:64: AssertionError. ----------------------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------------------. normalizing by total count per cell. filtered out dask.array<sum-aggregate, shape=(), dtype=int64, chunksize=(), chunktype=numpy.ndarray> cells that have less than 1 counts. finished (0:00:00): normalized adata.X and added 'n_counts', counts per cell before normalization (adata.obs). normalizing by total count per cell. filt",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:2927,reliability,rto,rtol,2927,"==================================. __________________________________________________________________________________ TestPreprocessingDistributed.test_normalize_per_cell[dask] __________________________________________________________________________________. self = <scanpy.tests.test_preprocessing_distributed.TestPreprocessingDistributed object at 0x7fdd21f2e9d0>, adata = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. adata_dist = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. uns: 'dist-mode'. def test_normalize_per_cell(self, adata, adata_dist):. if adata_dist.uns[""dist-mode""] == ""dask"":. pytest.xfail(""TODO: Test broken for dask""). normalize_per_cell(adata_dist). result = materialize_as_ndarray(adata_dist.X). normalize_per_cell(adata). assert result.shape == adata.shape. assert result.shape == (adata.n_obs, adata.n_vars). > npt.assert_allclose(result, adata.X). E AssertionError: . E Not equal to tolerance rtol=1e-07, atol=0. E . E Mismatched elements: 688287 / 9999000 (6.88%). E Max absolute difference: 573.4154. E Max relative difference: 11.335767. E x: array([[0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],... E y: array([[0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],... scanpy/tests/test_preprocessing_distributed.py:64: AssertionError. ----------------------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------------------. normalizing by total count per cell. filtered out dask.array<sum-aggregate, shape=(), dtype=int64, chunksize=(), chunktype=numpy.ndarray> cells that have less than 1 counts. finished (0:00:00): normalized adata.X and added 'n_counts', counts per cell before normalization (adata.obs). normalizing by total count per cell. filtered ou",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:4306,reliability,FAIL,FAILED,4306,"/tests/test_preprocessing_distributed.py:64: AssertionError. ----------------------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------------------. normalizing by total count per cell. filtered out dask.array<sum-aggregate, shape=(), dtype=int64, chunksize=(), chunktype=numpy.ndarray> cells that have less than 1 counts. finished (0:00:00): normalized adata.X and added 'n_counts', counts per cell before normalization (adata.obs). normalizing by total count per cell. filtered out 1 cells that have less than 1 counts. finished (0:00:00): normalized adata.X and added 'n_counts', counts per cell before normalization (adata.obs). =================================================================================================== short test summary info ====================================================================================================. FAILED scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] - AssertionError: . ======================================================================================== 1 failed, 985 deselected, 11 warnings in 3.74s ========================================================================================. ```. ### Versions. <details>. ```. -----. anndata 0.9.0rc2.dev43+g21a76088. scanpy 1.10.0.dev117+g6b9e734f. -----. PIL 9.1.1. asciitree NA. awkward 2.2.1. awkward_cpp NA. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cloudpickle 2.2.1. cycler 0.10.0. cython_runtime NA. dask 2023.5.0. dateutil 2.8.2. defusedxml 0.7.1. entrypoints 0.4. fasteners 0.17.3. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. jinja2 3.1.2. joblib 1.1.0. kiwisolver 1.4.3. leidenalg 0.9.1. llvmlite 0.38.1. markupsafe 2.1.1. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.1.0. nbinom_ufunc NA. numba 0.55.2. numcodecs 0.10.2. numpy 1.22.4. packagin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:4532,reliability,fail,failed,4532,"-----------------------------------------------------------. normalizing by total count per cell. filtered out dask.array<sum-aggregate, shape=(), dtype=int64, chunksize=(), chunktype=numpy.ndarray> cells that have less than 1 counts. finished (0:00:00): normalized adata.X and added 'n_counts', counts per cell before normalization (adata.obs). normalizing by total count per cell. filtered out 1 cells that have less than 1 counts. finished (0:00:00): normalized adata.X and added 'n_counts', counts per cell before normalization (adata.obs). =================================================================================================== short test summary info ====================================================================================================. FAILED scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] - AssertionError: . ======================================================================================== 1 failed, 985 deselected, 11 warnings in 3.74s ========================================================================================. ```. ### Versions. <details>. ```. -----. anndata 0.9.0rc2.dev43+g21a76088. scanpy 1.10.0.dev117+g6b9e734f. -----. PIL 9.1.1. asciitree NA. awkward 2.2.1. awkward_cpp NA. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cloudpickle 2.2.1. cycler 0.10.0. cython_runtime NA. dask 2023.5.0. dateutil 2.8.2. defusedxml 0.7.1. entrypoints 0.4. fasteners 0.17.3. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. jinja2 3.1.2. joblib 1.1.0. kiwisolver 1.4.3. leidenalg 0.9.1. llvmlite 0.38.1. markupsafe 2.1.1. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.1.0. nbinom_ufunc NA. numba 0.55.2. numcodecs 0.10.2. numpy 1.22.4. packaging 21.3. pandas 2.0.2. pkg_resources NA. psutil 5.9.1. pyparsing 3.0.9. pytz 2022.1. scipy 1.8.1. session_info 1.0.0. setuptools 67.8.0. setuptools_scm NA. six 1.16.0. sklearn 1.1.1. sphinxcontrib NA. texttable 1.6.7. threadpoo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:7,safety,test,test,7,"Broken test: `TestPreprocessingDistributed::test_normalize_per_cell[dask]`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In #2235, more gradually enabled some so far disabled tests. Before, all tests in `TestPreprocessingDistributed` were disabled with the available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. ___________________________________",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:14,safety,Test,TestPreprocessingDistributed,14,"Broken test: `TestPreprocessingDistributed::test_normalize_per_cell[dask]`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In #2235, more gradually enabled some so far disabled tests. Before, all tests in `TestPreprocessingDistributed` were disabled with the available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. ___________________________________",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:418,safety,test,tests,418,"Broken test: `TestPreprocessingDistributed::test_normalize_per_cell[dask]`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In #2235, more gradually enabled some so far disabled tests. Before, all tests in `TestPreprocessingDistributed` were disabled with the available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. ___________________________________",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:437,safety,test,tests,437,"Broken test: `TestPreprocessingDistributed::test_normalize_per_cell[dask]`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In #2235, more gradually enabled some so far disabled tests. Before, all tests in `TestPreprocessingDistributed` were disabled with the available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. ___________________________________",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:447,safety,Test,TestPreprocessingDistributed,447,"Broken test: `TestPreprocessingDistributed::test_normalize_per_cell[dask]`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In #2235, more gradually enabled some so far disabled tests. Before, all tests in `TestPreprocessingDistributed` were disabled with the available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. ___________________________________",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:500,safety,avail,available,500,"Broken test: `TestPreprocessingDistributed::test_normalize_per_cell[dask]`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In #2235, more gradually enabled some so far disabled tests. Before, all tests in `TestPreprocessingDistributed` were disabled with the available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. ___________________________________",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:519,safety,depend,dependencies,519,"Broken test: `TestPreprocessingDistributed::test_normalize_per_cell[dask]`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In #2235, more gradually enabled some so far disabled tests. Before, all tests in `TestPreprocessingDistributed` were disabled with the available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. ___________________________________",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:543,safety,test,tests,543,"Broken test: `TestPreprocessingDistributed::test_normalize_per_cell[dask]`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In #2235, more gradually enabled some so far disabled tests. Before, all tests in `TestPreprocessingDistributed` were disabled with the available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. ___________________________________",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:643,safety,test,tests,643,"Broken test: `TestPreprocessingDistributed::test_normalize_per_cell[dask]`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In #2235, more gradually enabled some so far disabled tests. Before, all tests in `TestPreprocessingDistributed` were disabled with the available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. ___________________________________",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:706,safety,test,tests,706,"Broken test: `TestPreprocessingDistributed::test_normalize_per_cell[dask]`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In #2235, more gradually enabled some so far disabled tests. Before, all tests in `TestPreprocessingDistributed` were disabled with the available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. ___________________________________",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:743,safety,test,tests,743,"Broken test: `TestPreprocessingDistributed::test_normalize_per_cell[dask]`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In #2235, more gradually enabled some so far disabled tests. Before, all tests in `TestPreprocessingDistributed` were disabled with the available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. ___________________________________",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:851,safety,test,tests,851,"Broken test: `TestPreprocessingDistributed::test_normalize_per_cell[dask]`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In #2235, more gradually enabled some so far disabled tests. Before, all tests in `TestPreprocessingDistributed` were disabled with the available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. ___________________________________",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:1017,safety,Error,Error,1017,"eprocessingDistributed::test_normalize_per_cell[dask]`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In #2235, more gradually enabled some so far disabled tests. Before, all tests in `TestPreprocessingDistributed` were disabled with the available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. _______________________________________________________",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:1142,safety,test,test,1142,"that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In #2235, more gradually enabled some so far disabled tests. Before, all tests in `TestPreprocessingDistributed` were disabled with the available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. __________________________________________________________________________________ TestPreprocessingDistributed.test_normalize_per_cell[dask] _____________________________________",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:1485,safety,test,testpaths,1485," with the available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. __________________________________________________________________________________ TestPreprocessingDistributed.test_normalize_per_cell[dask] __________________________________________________________________________________. self = <scanpy.tests.test_preprocessing_distributed.TestPreprocessingDistributed object at 0x7fdd21f2e9d0>, adata = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. adata_dist = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. un",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:1623,safety,test,tests,1623,"c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. __________________________________________________________________________________ TestPreprocessingDistributed.test_normalize_per_cell[dask] __________________________________________________________________________________. self = <scanpy.tests.test_preprocessing_distributed.TestPreprocessingDistributed object at 0x7fdd21f2e9d0>, adata = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. adata_dist = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. uns: 'dist-mode'. def test_normalize_per_cell(self, adata, adata_dist):. if adata_dist.uns[""dist-mode""] == ""dask"":. pytest.xfail(""TODO: Te",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:1664,safety,Test,TestPreprocessingDistributed,1664,"d.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. __________________________________________________________________________________ TestPreprocessingDistributed.test_normalize_per_cell[dask] __________________________________________________________________________________. self = <scanpy.tests.test_preprocessing_distributed.TestPreprocessingDistributed object at 0x7fdd21f2e9d0>, adata = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. adata_dist = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. uns: 'dist-mode'. def test_normalize_per_cell(self, adata, adata_dist):. if adata_dist.uns[""dist-mode""] == ""dask"":. pytest.xfail(""TODO: Test broken for dask""). normalize_per_cell(adata_dist)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:2048,safety,Test,TestPreprocessingDistributed,2048,"=============================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. __________________________________________________________________________________ TestPreprocessingDistributed.test_normalize_per_cell[dask] __________________________________________________________________________________. self = <scanpy.tests.test_preprocessing_distributed.TestPreprocessingDistributed object at 0x7fdd21f2e9d0>, adata = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. adata_dist = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. uns: 'dist-mode'. def test_normalize_per_cell(self, adata, adata_dist):. if adata_dist.uns[""dist-mode""] == ""dask"":. pytest.xfail(""TODO: Test broken for dask""). normalize_per_cell(adata_dist). result = materialize_as_ndarray(adata_dist.X). normalize_per_cell(adata). assert result.shape == adata.shape. assert result.shape == (adata.n_obs, adata.n_vars). > npt.assert_allclose(result, adata.X). E AssertionError: . E Not equal to tolerance rtol=1e-07, atol=0. E . E Mismatched elements: 688287 / 9999000 (6.88%). E Max absolute difference: 573.4154. E Max relative difference",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:2206,safety,test,tests,2206,"=======================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. __________________________________________________________________________________ TestPreprocessingDistributed.test_normalize_per_cell[dask] __________________________________________________________________________________. self = <scanpy.tests.test_preprocessing_distributed.TestPreprocessingDistributed object at 0x7fdd21f2e9d0>, adata = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. adata_dist = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. uns: 'dist-mode'. def test_normalize_per_cell(self, adata, adata_dist):. if adata_dist.uns[""dist-mode""] == ""dask"":. pytest.xfail(""TODO: Test broken for dask""). normalize_per_cell(adata_dist). result = materialize_as_ndarray(adata_dist.X). normalize_per_cell(adata). assert result.shape == adata.shape. assert result.shape == (adata.n_obs, adata.n_vars). > npt.assert_allclose(result, adata.X). E AssertionError: . E Not equal to tolerance rtol=1e-07, atol=0. E . E Mismatched elements: 688287 / 9999000 (6.88%). E Max absolute difference: 573.4154. E Max relative difference: 11.335767. E x: array([[0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],... E y: array([[0., 0.,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:2243,safety,Test,TestPreprocessingDistributed,2243,"=======. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. __________________________________________________________________________________ TestPreprocessingDistributed.test_normalize_per_cell[dask] __________________________________________________________________________________. self = <scanpy.tests.test_preprocessing_distributed.TestPreprocessingDistributed object at 0x7fdd21f2e9d0>, adata = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. adata_dist = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. uns: 'dist-mode'. def test_normalize_per_cell(self, adata, adata_dist):. if adata_dist.uns[""dist-mode""] == ""dask"":. pytest.xfail(""TODO: Test broken for dask""). normalize_per_cell(adata_dist). result = materialize_as_ndarray(adata_dist.X). normalize_per_cell(adata). assert result.shape == adata.shape. assert result.shape == (adata.n_obs, adata.n_vars). > npt.assert_allclose(result, adata.X). E AssertionError: . E Not equal to tolerance rtol=1e-07, atol=0. E . E Mismatched elements: 688287 / 9999000 (6.88%). E Max absolute difference: 573.4154. E Max relative difference: 11.335767. E x: array([[0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],... E y: array([[0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:2624,safety,Test,Test,2624,"ts/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. __________________________________________________________________________________ TestPreprocessingDistributed.test_normalize_per_cell[dask] __________________________________________________________________________________. self = <scanpy.tests.test_preprocessing_distributed.TestPreprocessingDistributed object at 0x7fdd21f2e9d0>, adata = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. adata_dist = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. uns: 'dist-mode'. def test_normalize_per_cell(self, adata, adata_dist):. if adata_dist.uns[""dist-mode""] == ""dask"":. pytest.xfail(""TODO: Test broken for dask""). normalize_per_cell(adata_dist). result = materialize_as_ndarray(adata_dist.X). normalize_per_cell(adata). assert result.shape == adata.shape. assert result.shape == (adata.n_obs, adata.n_vars). > npt.assert_allclose(result, adata.X). E AssertionError: . E Not equal to tolerance rtol=1e-07, atol=0. E . E Mismatched elements: 688287 / 9999000 (6.88%). E Max absolute difference: 573.4154. E Max relative difference: 11.335767. E x: array([[0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],... E y: array([[0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],... scanpy/tests/test_preprocessing_distributed.py:64: AssertionError. ----------------------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------------------. normalizing by total count per",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:3310,safety,test,tests,3310,"a object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. adata_dist = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. uns: 'dist-mode'. def test_normalize_per_cell(self, adata, adata_dist):. if adata_dist.uns[""dist-mode""] == ""dask"":. pytest.xfail(""TODO: Test broken for dask""). normalize_per_cell(adata_dist). result = materialize_as_ndarray(adata_dist.X). normalize_per_cell(adata). assert result.shape == adata.shape. assert result.shape == (adata.n_obs, adata.n_vars). > npt.assert_allclose(result, adata.X). E AssertionError: . E Not equal to tolerance rtol=1e-07, atol=0. E . E Mismatched elements: 688287 / 9999000 (6.88%). E Max absolute difference: 573.4154. E Max relative difference: 11.335767. E x: array([[0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],... E y: array([[0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],... scanpy/tests/test_preprocessing_distributed.py:64: AssertionError. ----------------------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------------------. normalizing by total count per cell. filtered out dask.array<sum-aggregate, shape=(), dtype=int64, chunksize=(), chunktype=numpy.ndarray> cells that have less than 1 counts. finished (0:00:00): normalized adata.X and added 'n_counts', counts per cell before normalization (adata.obs). normalizing by total count per cell. filtered out 1 cells that have less than 1 counts. finished (0:00:00): normalized adata.X and added 'n_counts', counts per cell before normalization (adata.obs). =================================================================================================== short test summary info ====================================================================================================. FAILED ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:4186,safety,test,test,4186," E y: array([[0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],... scanpy/tests/test_preprocessing_distributed.py:64: AssertionError. ----------------------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------------------. normalizing by total count per cell. filtered out dask.array<sum-aggregate, shape=(), dtype=int64, chunksize=(), chunktype=numpy.ndarray> cells that have less than 1 counts. finished (0:00:00): normalized adata.X and added 'n_counts', counts per cell before normalization (adata.obs). normalizing by total count per cell. filtered out 1 cells that have less than 1 counts. finished (0:00:00): normalized adata.X and added 'n_counts', counts per cell before normalization (adata.obs). =================================================================================================== short test summary info ====================================================================================================. FAILED scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] - AssertionError: . ======================================================================================== 1 failed, 985 deselected, 11 warnings in 3.74s ========================================================================================. ```. ### Versions. <details>. ```. -----. anndata 0.9.0rc2.dev43+g21a76088. scanpy 1.10.0.dev117+g6b9e734f. -----. PIL 9.1.1. asciitree NA. awkward 2.2.1. awkward_cpp NA. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cloudpickle 2.2.1. cycler 0.10.0. cython_runtime NA. dask 2023.5.0. dateutil 2.8.2. defusedxml 0.7.1. entrypoints 0.4. fasteners 0.17.3. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. jinja2 3.1.2. joblib 1.1.0. kiwisolver 1.4.3. leidenalg 0.9.1. llvmlite 0.38.1. markupsafe 2.1.1. ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:4320,safety,test,tests,4320,"eprocessing_distributed.py:64: AssertionError. ----------------------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------------------. normalizing by total count per cell. filtered out dask.array<sum-aggregate, shape=(), dtype=int64, chunksize=(), chunktype=numpy.ndarray> cells that have less than 1 counts. finished (0:00:00): normalized adata.X and added 'n_counts', counts per cell before normalization (adata.obs). normalizing by total count per cell. filtered out 1 cells that have less than 1 counts. finished (0:00:00): normalized adata.X and added 'n_counts', counts per cell before normalization (adata.obs). =================================================================================================== short test summary info ====================================================================================================. FAILED scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] - AssertionError: . ======================================================================================== 1 failed, 985 deselected, 11 warnings in 3.74s ========================================================================================. ```. ### Versions. <details>. ```. -----. anndata 0.9.0rc2.dev43+g21a76088. scanpy 1.10.0.dev117+g6b9e734f. -----. PIL 9.1.1. asciitree NA. awkward 2.2.1. awkward_cpp NA. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cloudpickle 2.2.1. cycler 0.10.0. cython_runtime NA. dask 2023.5.0. dateutil 2.8.2. defusedxml 0.7.1. entrypoints 0.4. fasteners 0.17.3. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. jinja2 3.1.2. joblib 1.1.0. kiwisolver 1.4.3. leidenalg 0.9.1. llvmlite 0.38.1. markupsafe 2.1.1. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.1.0. nbinom_ufunc NA. numba 0.55.2. numcodecs 0.10.2. numpy 1.22.4. packaging 21.3. pandas",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:4361,safety,Test,TestPreprocessingDistributed,4361,"------------------------------------------------------------------------------------------------ Captured stderr call -----------------------------------------------------------------------------------------------------. normalizing by total count per cell. filtered out dask.array<sum-aggregate, shape=(), dtype=int64, chunksize=(), chunktype=numpy.ndarray> cells that have less than 1 counts. finished (0:00:00): normalized adata.X and added 'n_counts', counts per cell before normalization (adata.obs). normalizing by total count per cell. filtered out 1 cells that have less than 1 counts. finished (0:00:00): normalized adata.X and added 'n_counts', counts per cell before normalization (adata.obs). =================================================================================================== short test summary info ====================================================================================================. FAILED scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] - AssertionError: . ======================================================================================== 1 failed, 985 deselected, 11 warnings in 3.74s ========================================================================================. ```. ### Versions. <details>. ```. -----. anndata 0.9.0rc2.dev43+g21a76088. scanpy 1.10.0.dev117+g6b9e734f. -----. PIL 9.1.1. asciitree NA. awkward 2.2.1. awkward_cpp NA. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cloudpickle 2.2.1. cycler 0.10.0. cython_runtime NA. dask 2023.5.0. dateutil 2.8.2. defusedxml 0.7.1. entrypoints 0.4. fasteners 0.17.3. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. jinja2 3.1.2. joblib 1.1.0. kiwisolver 1.4.3. leidenalg 0.9.1. llvmlite 0.38.1. markupsafe 2.1.1. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.1.0. nbinom_ufunc NA. numba 0.55.2. numcodecs 0.10.2. numpy 1.22.4. packaging 21.3. pandas 2.0.2. pkg_resources NA. psutil 5.9.1. pyparsing 3.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:5792,safety,updat,updated,5792," per cell before normalization (adata.obs). normalizing by total count per cell. filtered out 1 cells that have less than 1 counts. finished (0:00:00): normalized adata.X and added 'n_counts', counts per cell before normalization (adata.obs). =================================================================================================== short test summary info ====================================================================================================. FAILED scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] - AssertionError: . ======================================================================================== 1 failed, 985 deselected, 11 warnings in 3.74s ========================================================================================. ```. ### Versions. <details>. ```. -----. anndata 0.9.0rc2.dev43+g21a76088. scanpy 1.10.0.dev117+g6b9e734f. -----. PIL 9.1.1. asciitree NA. awkward 2.2.1. awkward_cpp NA. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cloudpickle 2.2.1. cycler 0.10.0. cython_runtime NA. dask 2023.5.0. dateutil 2.8.2. defusedxml 0.7.1. entrypoints 0.4. fasteners 0.17.3. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. jinja2 3.1.2. joblib 1.1.0. kiwisolver 1.4.3. leidenalg 0.9.1. llvmlite 0.38.1. markupsafe 2.1.1. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.1.0. nbinom_ufunc NA. numba 0.55.2. numcodecs 0.10.2. numpy 1.22.4. packaging 21.3. pandas 2.0.2. pkg_resources NA. psutil 5.9.1. pyparsing 3.0.9. pytz 2022.1. scipy 1.8.1. session_info 1.0.0. setuptools 67.8.0. setuptools_scm NA. six 1.16.0. sklearn 1.1.1. sphinxcontrib NA. texttable 1.6.7. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. typing_extensions NA. wcwidth 0.2.5. yaml 6.0. zarr 2.12.0. zipp NA. -----. Python 3.8.17 (default, Jun 17 2023, 20:09:37) [GCC 13.1.1 20230429]. Linux-6.3.8-zen1-1-zen-x86_64-with-glibc2.34. -----. Session information updated at 2023-06-23 14:29. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:500,security,availab,available,500,"Broken test: `TestPreprocessingDistributed::test_normalize_per_cell[dask]`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In #2235, more gradually enabled some so far disabled tests. Before, all tests in `TestPreprocessingDistributed` were disabled with the available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. ___________________________________",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:1147,security,session,session,1147,"is issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In #2235, more gradually enabled some so far disabled tests. Before, all tests in `TestPreprocessingDistributed` were disabled with the available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. __________________________________________________________________________________ TestPreprocessingDistributed.test_normalize_per_cell[dask] ____________________________________________",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:5772,security,Session,Session,5772," per cell before normalization (adata.obs). normalizing by total count per cell. filtered out 1 cells that have less than 1 counts. finished (0:00:00): normalized adata.X and added 'n_counts', counts per cell before normalization (adata.obs). =================================================================================================== short test summary info ====================================================================================================. FAILED scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] - AssertionError: . ======================================================================================== 1 failed, 985 deselected, 11 warnings in 3.74s ========================================================================================. ```. ### Versions. <details>. ```. -----. anndata 0.9.0rc2.dev43+g21a76088. scanpy 1.10.0.dev117+g6b9e734f. -----. PIL 9.1.1. asciitree NA. awkward 2.2.1. awkward_cpp NA. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cloudpickle 2.2.1. cycler 0.10.0. cython_runtime NA. dask 2023.5.0. dateutil 2.8.2. defusedxml 0.7.1. entrypoints 0.4. fasteners 0.17.3. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. jinja2 3.1.2. joblib 1.1.0. kiwisolver 1.4.3. leidenalg 0.9.1. llvmlite 0.38.1. markupsafe 2.1.1. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.1.0. nbinom_ufunc NA. numba 0.55.2. numcodecs 0.10.2. numpy 1.22.4. packaging 21.3. pandas 2.0.2. pkg_resources NA. psutil 5.9.1. pyparsing 3.0.9. pytz 2022.1. scipy 1.8.1. session_info 1.0.0. setuptools 67.8.0. setuptools_scm NA. six 1.16.0. sklearn 1.1.1. sphinxcontrib NA. texttable 1.6.7. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. typing_extensions NA. wcwidth 0.2.5. yaml 6.0. zarr 2.12.0. zipp NA. -----. Python 3.8.17 (default, Jun 17 2023, 20:09:37) [GCC 13.1.1 20230429]. Linux-6.3.8-zen1-1-zen-x86_64-with-glibc2.34. -----. Session information updated at 2023-06-23 14:29. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:5792,security,updat,updated,5792," per cell before normalization (adata.obs). normalizing by total count per cell. filtered out 1 cells that have less than 1 counts. finished (0:00:00): normalized adata.X and added 'n_counts', counts per cell before normalization (adata.obs). =================================================================================================== short test summary info ====================================================================================================. FAILED scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] - AssertionError: . ======================================================================================== 1 failed, 985 deselected, 11 warnings in 3.74s ========================================================================================. ```. ### Versions. <details>. ```. -----. anndata 0.9.0rc2.dev43+g21a76088. scanpy 1.10.0.dev117+g6b9e734f. -----. PIL 9.1.1. asciitree NA. awkward 2.2.1. awkward_cpp NA. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cloudpickle 2.2.1. cycler 0.10.0. cython_runtime NA. dask 2023.5.0. dateutil 2.8.2. defusedxml 0.7.1. entrypoints 0.4. fasteners 0.17.3. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. jinja2 3.1.2. joblib 1.1.0. kiwisolver 1.4.3. leidenalg 0.9.1. llvmlite 0.38.1. markupsafe 2.1.1. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.1.0. nbinom_ufunc NA. numba 0.55.2. numcodecs 0.10.2. numpy 1.22.4. packaging 21.3. pandas 2.0.2. pkg_resources NA. psutil 5.9.1. pyparsing 3.0.9. pytz 2022.1. scipy 1.8.1. session_info 1.0.0. setuptools 67.8.0. setuptools_scm NA. six 1.16.0. sklearn 1.1.1. sphinxcontrib NA. texttable 1.6.7. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. typing_extensions NA. wcwidth 0.2.5. yaml 6.0. zarr 2.12.0. zipp NA. -----. Python 3.8.17 (default, Jun 17 2023, 20:09:37) [GCC 13.1.1 20230429]. Linux-6.3.8-zen1-1-zen-x86_64-with-glibc2.34. -----. Session information updated at 2023-06-23 14:29. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:7,testability,test,test,7,"Broken test: `TestPreprocessingDistributed::test_normalize_per_cell[dask]`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In #2235, more gradually enabled some so far disabled tests. Before, all tests in `TestPreprocessingDistributed` were disabled with the available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. ___________________________________",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:14,testability,Test,TestPreprocessingDistributed,14,"Broken test: `TestPreprocessingDistributed::test_normalize_per_cell[dask]`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In #2235, more gradually enabled some so far disabled tests. Before, all tests in `TestPreprocessingDistributed` were disabled with the available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. ___________________________________",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:418,testability,test,tests,418,"Broken test: `TestPreprocessingDistributed::test_normalize_per_cell[dask]`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In #2235, more gradually enabled some so far disabled tests. Before, all tests in `TestPreprocessingDistributed` were disabled with the available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. ___________________________________",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:437,testability,test,tests,437,"Broken test: `TestPreprocessingDistributed::test_normalize_per_cell[dask]`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In #2235, more gradually enabled some so far disabled tests. Before, all tests in `TestPreprocessingDistributed` were disabled with the available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. ___________________________________",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:447,testability,Test,TestPreprocessingDistributed,447,"Broken test: `TestPreprocessingDistributed::test_normalize_per_cell[dask]`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In #2235, more gradually enabled some so far disabled tests. Before, all tests in `TestPreprocessingDistributed` were disabled with the available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. ___________________________________",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:519,testability,depend,dependencies,519,"Broken test: `TestPreprocessingDistributed::test_normalize_per_cell[dask]`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In #2235, more gradually enabled some so far disabled tests. Before, all tests in `TestPreprocessingDistributed` were disabled with the available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. ___________________________________",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:543,testability,test,tests,543,"Broken test: `TestPreprocessingDistributed::test_normalize_per_cell[dask]`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In #2235, more gradually enabled some so far disabled tests. Before, all tests in `TestPreprocessingDistributed` were disabled with the available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. ___________________________________",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:643,testability,test,tests,643,"Broken test: `TestPreprocessingDistributed::test_normalize_per_cell[dask]`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In #2235, more gradually enabled some so far disabled tests. Before, all tests in `TestPreprocessingDistributed` were disabled with the available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. ___________________________________",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:706,testability,test,tests,706,"Broken test: `TestPreprocessingDistributed::test_normalize_per_cell[dask]`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In #2235, more gradually enabled some so far disabled tests. Before, all tests in `TestPreprocessingDistributed` were disabled with the available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. ___________________________________",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:743,testability,test,tests,743,"Broken test: `TestPreprocessingDistributed::test_normalize_per_cell[dask]`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In #2235, more gradually enabled some so far disabled tests. Before, all tests in `TestPreprocessingDistributed` were disabled with the available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. ___________________________________",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:851,testability,test,tests,851,"Broken test: `TestPreprocessingDistributed::test_normalize_per_cell[dask]`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In #2235, more gradually enabled some so far disabled tests. Before, all tests in `TestPreprocessingDistributed` were disabled with the available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. ___________________________________",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:1142,testability,test,test,1142,"that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In #2235, more gradually enabled some so far disabled tests. Before, all tests in `TestPreprocessingDistributed` were disabled with the available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. __________________________________________________________________________________ TestPreprocessingDistributed.test_normalize_per_cell[dask] _____________________________________",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:1485,testability,test,testpaths,1485," with the available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. __________________________________________________________________________________ TestPreprocessingDistributed.test_normalize_per_cell[dask] __________________________________________________________________________________. self = <scanpy.tests.test_preprocessing_distributed.TestPreprocessingDistributed object at 0x7fdd21f2e9d0>, adata = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. adata_dist = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. un",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:1623,testability,test,tests,1623,"c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. __________________________________________________________________________________ TestPreprocessingDistributed.test_normalize_per_cell[dask] __________________________________________________________________________________. self = <scanpy.tests.test_preprocessing_distributed.TestPreprocessingDistributed object at 0x7fdd21f2e9d0>, adata = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. adata_dist = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. uns: 'dist-mode'. def test_normalize_per_cell(self, adata, adata_dist):. if adata_dist.uns[""dist-mode""] == ""dask"":. pytest.xfail(""TODO: Te",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:1664,testability,Test,TestPreprocessingDistributed,1664,"d.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. __________________________________________________________________________________ TestPreprocessingDistributed.test_normalize_per_cell[dask] __________________________________________________________________________________. self = <scanpy.tests.test_preprocessing_distributed.TestPreprocessingDistributed object at 0x7fdd21f2e9d0>, adata = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. adata_dist = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. uns: 'dist-mode'. def test_normalize_per_cell(self, adata, adata_dist):. if adata_dist.uns[""dist-mode""] == ""dask"":. pytest.xfail(""TODO: Test broken for dask""). normalize_per_cell(adata_dist)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:2048,testability,Test,TestPreprocessingDistributed,2048,"=============================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. __________________________________________________________________________________ TestPreprocessingDistributed.test_normalize_per_cell[dask] __________________________________________________________________________________. self = <scanpy.tests.test_preprocessing_distributed.TestPreprocessingDistributed object at 0x7fdd21f2e9d0>, adata = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. adata_dist = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. uns: 'dist-mode'. def test_normalize_per_cell(self, adata, adata_dist):. if adata_dist.uns[""dist-mode""] == ""dask"":. pytest.xfail(""TODO: Test broken for dask""). normalize_per_cell(adata_dist). result = materialize_as_ndarray(adata_dist.X). normalize_per_cell(adata). assert result.shape == adata.shape. assert result.shape == (adata.n_obs, adata.n_vars). > npt.assert_allclose(result, adata.X). E AssertionError: . E Not equal to tolerance rtol=1e-07, atol=0. E . E Mismatched elements: 688287 / 9999000 (6.88%). E Max absolute difference: 573.4154. E Max relative difference",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:2206,testability,test,tests,2206,"=======================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. __________________________________________________________________________________ TestPreprocessingDistributed.test_normalize_per_cell[dask] __________________________________________________________________________________. self = <scanpy.tests.test_preprocessing_distributed.TestPreprocessingDistributed object at 0x7fdd21f2e9d0>, adata = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. adata_dist = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. uns: 'dist-mode'. def test_normalize_per_cell(self, adata, adata_dist):. if adata_dist.uns[""dist-mode""] == ""dask"":. pytest.xfail(""TODO: Test broken for dask""). normalize_per_cell(adata_dist). result = materialize_as_ndarray(adata_dist.X). normalize_per_cell(adata). assert result.shape == adata.shape. assert result.shape == (adata.n_obs, adata.n_vars). > npt.assert_allclose(result, adata.X). E AssertionError: . E Not equal to tolerance rtol=1e-07, atol=0. E . E Mismatched elements: 688287 / 9999000 (6.88%). E Max absolute difference: 573.4154. E Max relative difference: 11.335767. E x: array([[0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],... E y: array([[0., 0.,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:2243,testability,Test,TestPreprocessingDistributed,2243,"=======. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. __________________________________________________________________________________ TestPreprocessingDistributed.test_normalize_per_cell[dask] __________________________________________________________________________________. self = <scanpy.tests.test_preprocessing_distributed.TestPreprocessingDistributed object at 0x7fdd21f2e9d0>, adata = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. adata_dist = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. uns: 'dist-mode'. def test_normalize_per_cell(self, adata, adata_dist):. if adata_dist.uns[""dist-mode""] == ""dask"":. pytest.xfail(""TODO: Test broken for dask""). normalize_per_cell(adata_dist). result = materialize_as_ndarray(adata_dist.X). normalize_per_cell(adata). assert result.shape == adata.shape. assert result.shape == (adata.n_obs, adata.n_vars). > npt.assert_allclose(result, adata.X). E AssertionError: . E Not equal to tolerance rtol=1e-07, atol=0. E . E Mismatched elements: 688287 / 9999000 (6.88%). E Max absolute difference: 573.4154. E Max relative difference: 11.335767. E x: array([[0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],... E y: array([[0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:2624,testability,Test,Test,2624,"ts/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. __________________________________________________________________________________ TestPreprocessingDistributed.test_normalize_per_cell[dask] __________________________________________________________________________________. self = <scanpy.tests.test_preprocessing_distributed.TestPreprocessingDistributed object at 0x7fdd21f2e9d0>, adata = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. adata_dist = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. uns: 'dist-mode'. def test_normalize_per_cell(self, adata, adata_dist):. if adata_dist.uns[""dist-mode""] == ""dask"":. pytest.xfail(""TODO: Test broken for dask""). normalize_per_cell(adata_dist). result = materialize_as_ndarray(adata_dist.X). normalize_per_cell(adata). assert result.shape == adata.shape. assert result.shape == (adata.n_obs, adata.n_vars). > npt.assert_allclose(result, adata.X). E AssertionError: . E Not equal to tolerance rtol=1e-07, atol=0. E . E Mismatched elements: 688287 / 9999000 (6.88%). E Max absolute difference: 573.4154. E Max relative difference: 11.335767. E x: array([[0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],... E y: array([[0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],... scanpy/tests/test_preprocessing_distributed.py:64: AssertionError. ----------------------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------------------. normalizing by total count per",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:2754,testability,assert,assert,2754,"========================================================================================= FAILURES ===========================================================================================================. __________________________________________________________________________________ TestPreprocessingDistributed.test_normalize_per_cell[dask] __________________________________________________________________________________. self = <scanpy.tests.test_preprocessing_distributed.TestPreprocessingDistributed object at 0x7fdd21f2e9d0>, adata = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. adata_dist = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. uns: 'dist-mode'. def test_normalize_per_cell(self, adata, adata_dist):. if adata_dist.uns[""dist-mode""] == ""dask"":. pytest.xfail(""TODO: Test broken for dask""). normalize_per_cell(adata_dist). result = materialize_as_ndarray(adata_dist.X). normalize_per_cell(adata). assert result.shape == adata.shape. assert result.shape == (adata.n_obs, adata.n_vars). > npt.assert_allclose(result, adata.X). E AssertionError: . E Not equal to tolerance rtol=1e-07, atol=0. E . E Mismatched elements: 688287 / 9999000 (6.88%). E Max absolute difference: 573.4154. E Max relative difference: 11.335767. E x: array([[0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],... E y: array([[0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],... scanpy/tests/test_preprocessing_distributed.py:64: AssertionError. ----------------------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------------------. normalizing by total count per cell. filtered out dask.array<sum-aggregate, shape=(), dtype=int64, chunksize=(), chunktype=numpy.ndarray> cells that have less th",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:2790,testability,assert,assert,2790,"===================================================== FAILURES ===========================================================================================================. __________________________________________________________________________________ TestPreprocessingDistributed.test_normalize_per_cell[dask] __________________________________________________________________________________. self = <scanpy.tests.test_preprocessing_distributed.TestPreprocessingDistributed object at 0x7fdd21f2e9d0>, adata = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. adata_dist = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. uns: 'dist-mode'. def test_normalize_per_cell(self, adata, adata_dist):. if adata_dist.uns[""dist-mode""] == ""dask"":. pytest.xfail(""TODO: Test broken for dask""). normalize_per_cell(adata_dist). result = materialize_as_ndarray(adata_dist.X). normalize_per_cell(adata). assert result.shape == adata.shape. assert result.shape == (adata.n_obs, adata.n_vars). > npt.assert_allclose(result, adata.X). E AssertionError: . E Not equal to tolerance rtol=1e-07, atol=0. E . E Mismatched elements: 688287 / 9999000 (6.88%). E Max absolute difference: 573.4154. E Max relative difference: 11.335767. E x: array([[0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],... E y: array([[0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],... scanpy/tests/test_preprocessing_distributed.py:64: AssertionError. ----------------------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------------------. normalizing by total count per cell. filtered out dask.array<sum-aggregate, shape=(), dtype=int64, chunksize=(), chunktype=numpy.ndarray> cells that have less than 1 counts. finished (0:00:00): nor",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:2884,testability,Assert,AssertionError,2884,"========================================================================. __________________________________________________________________________________ TestPreprocessingDistributed.test_normalize_per_cell[dask] __________________________________________________________________________________. self = <scanpy.tests.test_preprocessing_distributed.TestPreprocessingDistributed object at 0x7fdd21f2e9d0>, adata = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. adata_dist = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. uns: 'dist-mode'. def test_normalize_per_cell(self, adata, adata_dist):. if adata_dist.uns[""dist-mode""] == ""dask"":. pytest.xfail(""TODO: Test broken for dask""). normalize_per_cell(adata_dist). result = materialize_as_ndarray(adata_dist.X). normalize_per_cell(adata). assert result.shape == adata.shape. assert result.shape == (adata.n_obs, adata.n_vars). > npt.assert_allclose(result, adata.X). E AssertionError: . E Not equal to tolerance rtol=1e-07, atol=0. E . E Mismatched elements: 688287 / 9999000 (6.88%). E Max absolute difference: 573.4154. E Max relative difference: 11.335767. E x: array([[0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],... E y: array([[0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],... scanpy/tests/test_preprocessing_distributed.py:64: AssertionError. ----------------------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------------------. normalizing by total count per cell. filtered out dask.array<sum-aggregate, shape=(), dtype=int64, chunksize=(), chunktype=numpy.ndarray> cells that have less than 1 counts. finished (0:00:00): normalized adata.X and added 'n_counts', counts per cell before normalization (adata.obs). normalizin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:3310,testability,test,tests,3310,"a object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. adata_dist = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. uns: 'dist-mode'. def test_normalize_per_cell(self, adata, adata_dist):. if adata_dist.uns[""dist-mode""] == ""dask"":. pytest.xfail(""TODO: Test broken for dask""). normalize_per_cell(adata_dist). result = materialize_as_ndarray(adata_dist.X). normalize_per_cell(adata). assert result.shape == adata.shape. assert result.shape == (adata.n_obs, adata.n_vars). > npt.assert_allclose(result, adata.X). E AssertionError: . E Not equal to tolerance rtol=1e-07, atol=0. E . E Mismatched elements: 688287 / 9999000 (6.88%). E Max absolute difference: 573.4154. E Max relative difference: 11.335767. E x: array([[0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],... E y: array([[0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],... scanpy/tests/test_preprocessing_distributed.py:64: AssertionError. ----------------------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------------------. normalizing by total count per cell. filtered out dask.array<sum-aggregate, shape=(), dtype=int64, chunksize=(), chunktype=numpy.ndarray> cells that have less than 1 counts. finished (0:00:00): normalized adata.X and added 'n_counts', counts per cell before normalization (adata.obs). normalizing by total count per cell. filtered out 1 cells that have less than 1 counts. finished (0:00:00): normalized adata.X and added 'n_counts', counts per cell before normalization (adata.obs). =================================================================================================== short test summary info ====================================================================================================. FAILED ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:3354,testability,Assert,AssertionError,3354," 'n_counts'. var: 'gene_ids'. adata_dist = AnnData object with n_obs  n_vars = 9999  1000. obs: 'n_counts'. var: 'gene_ids'. uns: 'dist-mode'. def test_normalize_per_cell(self, adata, adata_dist):. if adata_dist.uns[""dist-mode""] == ""dask"":. pytest.xfail(""TODO: Test broken for dask""). normalize_per_cell(adata_dist). result = materialize_as_ndarray(adata_dist.X). normalize_per_cell(adata). assert result.shape == adata.shape. assert result.shape == (adata.n_obs, adata.n_vars). > npt.assert_allclose(result, adata.X). E AssertionError: . E Not equal to tolerance rtol=1e-07, atol=0. E . E Mismatched elements: 688287 / 9999000 (6.88%). E Max absolute difference: 573.4154. E Max relative difference: 11.335767. E x: array([[0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],... E y: array([[0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],... scanpy/tests/test_preprocessing_distributed.py:64: AssertionError. ----------------------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------------------. normalizing by total count per cell. filtered out dask.array<sum-aggregate, shape=(), dtype=int64, chunksize=(), chunktype=numpy.ndarray> cells that have less than 1 counts. finished (0:00:00): normalized adata.X and added 'n_counts', counts per cell before normalization (adata.obs). normalizing by total count per cell. filtered out 1 cells that have less than 1 counts. finished (0:00:00): normalized adata.X and added 'n_counts', counts per cell before normalization (adata.obs). =================================================================================================== short test summary info ====================================================================================================. FAILED scanpy/tests/test_preprocessing_distributed.py::",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:4186,testability,test,test,4186," E y: array([[0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],. E [0., 0., 0., ..., 0., 0., 0.],... scanpy/tests/test_preprocessing_distributed.py:64: AssertionError. ----------------------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------------------. normalizing by total count per cell. filtered out dask.array<sum-aggregate, shape=(), dtype=int64, chunksize=(), chunktype=numpy.ndarray> cells that have less than 1 counts. finished (0:00:00): normalized adata.X and added 'n_counts', counts per cell before normalization (adata.obs). normalizing by total count per cell. filtered out 1 cells that have less than 1 counts. finished (0:00:00): normalized adata.X and added 'n_counts', counts per cell before normalization (adata.obs). =================================================================================================== short test summary info ====================================================================================================. FAILED scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] - AssertionError: . ======================================================================================== 1 failed, 985 deselected, 11 warnings in 3.74s ========================================================================================. ```. ### Versions. <details>. ```. -----. anndata 0.9.0rc2.dev43+g21a76088. scanpy 1.10.0.dev117+g6b9e734f. -----. PIL 9.1.1. asciitree NA. awkward 2.2.1. awkward_cpp NA. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cloudpickle 2.2.1. cycler 0.10.0. cython_runtime NA. dask 2023.5.0. dateutil 2.8.2. defusedxml 0.7.1. entrypoints 0.4. fasteners 0.17.3. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. jinja2 3.1.2. joblib 1.1.0. kiwisolver 1.4.3. leidenalg 0.9.1. llvmlite 0.38.1. markupsafe 2.1.1. ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:4320,testability,test,tests,4320,"eprocessing_distributed.py:64: AssertionError. ----------------------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------------------. normalizing by total count per cell. filtered out dask.array<sum-aggregate, shape=(), dtype=int64, chunksize=(), chunktype=numpy.ndarray> cells that have less than 1 counts. finished (0:00:00): normalized adata.X and added 'n_counts', counts per cell before normalization (adata.obs). normalizing by total count per cell. filtered out 1 cells that have less than 1 counts. finished (0:00:00): normalized adata.X and added 'n_counts', counts per cell before normalization (adata.obs). =================================================================================================== short test summary info ====================================================================================================. FAILED scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] - AssertionError: . ======================================================================================== 1 failed, 985 deselected, 11 warnings in 3.74s ========================================================================================. ```. ### Versions. <details>. ```. -----. anndata 0.9.0rc2.dev43+g21a76088. scanpy 1.10.0.dev117+g6b9e734f. -----. PIL 9.1.1. asciitree NA. awkward 2.2.1. awkward_cpp NA. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cloudpickle 2.2.1. cycler 0.10.0. cython_runtime NA. dask 2023.5.0. dateutil 2.8.2. defusedxml 0.7.1. entrypoints 0.4. fasteners 0.17.3. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. jinja2 3.1.2. joblib 1.1.0. kiwisolver 1.4.3. leidenalg 0.9.1. llvmlite 0.38.1. markupsafe 2.1.1. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.1.0. nbinom_ufunc NA. numba 0.55.2. numcodecs 0.10.2. numpy 1.22.4. packaging 21.3. pandas",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:4361,testability,Test,TestPreprocessingDistributed,4361,"------------------------------------------------------------------------------------------------ Captured stderr call -----------------------------------------------------------------------------------------------------. normalizing by total count per cell. filtered out dask.array<sum-aggregate, shape=(), dtype=int64, chunksize=(), chunktype=numpy.ndarray> cells that have less than 1 counts. finished (0:00:00): normalized adata.X and added 'n_counts', counts per cell before normalization (adata.obs). normalizing by total count per cell. filtered out 1 cells that have less than 1 counts. finished (0:00:00): normalized adata.X and added 'n_counts', counts per cell before normalization (adata.obs). =================================================================================================== short test summary info ====================================================================================================. FAILED scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] - AssertionError: . ======================================================================================== 1 failed, 985 deselected, 11 warnings in 3.74s ========================================================================================. ```. ### Versions. <details>. ```. -----. anndata 0.9.0rc2.dev43+g21a76088. scanpy 1.10.0.dev117+g6b9e734f. -----. PIL 9.1.1. asciitree NA. awkward 2.2.1. awkward_cpp NA. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cloudpickle 2.2.1. cycler 0.10.0. cython_runtime NA. dask 2023.5.0. dateutil 2.8.2. defusedxml 0.7.1. entrypoints 0.4. fasteners 0.17.3. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. jinja2 3.1.2. joblib 1.1.0. kiwisolver 1.4.3. leidenalg 0.9.1. llvmlite 0.38.1. markupsafe 2.1.1. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.1.0. nbinom_ufunc NA. numba 0.55.2. numcodecs 0.10.2. numpy 1.22.4. packaging 21.3. pandas 2.0.2. pkg_resources NA. psutil 5.9.1. pyparsing 3.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:4423,testability,Assert,AssertionError,4423,"----------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------------------. normalizing by total count per cell. filtered out dask.array<sum-aggregate, shape=(), dtype=int64, chunksize=(), chunktype=numpy.ndarray> cells that have less than 1 counts. finished (0:00:00): normalized adata.X and added 'n_counts', counts per cell before normalization (adata.obs). normalizing by total count per cell. filtered out 1 cells that have less than 1 counts. finished (0:00:00): normalized adata.X and added 'n_counts', counts per cell before normalization (adata.obs). =================================================================================================== short test summary info ====================================================================================================. FAILED scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] - AssertionError: . ======================================================================================== 1 failed, 985 deselected, 11 warnings in 3.74s ========================================================================================. ```. ### Versions. <details>. ```. -----. anndata 0.9.0rc2.dev43+g21a76088. scanpy 1.10.0.dev117+g6b9e734f. -----. PIL 9.1.1. asciitree NA. awkward 2.2.1. awkward_cpp NA. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cloudpickle 2.2.1. cycler 0.10.0. cython_runtime NA. dask 2023.5.0. dateutil 2.8.2. defusedxml 0.7.1. entrypoints 0.4. fasteners 0.17.3. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. jinja2 3.1.2. joblib 1.1.0. kiwisolver 1.4.3. leidenalg 0.9.1. llvmlite 0.38.1. markupsafe 2.1.1. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.1.0. nbinom_ufunc NA. numba 0.55.2. numcodecs 0.10.2. numpy 1.22.4. packaging 21.3. pandas 2.0.2. pkg_resources NA. psutil 5.9.1. pyparsing 3.0.9. pytz 2022.1. scipy 1.8.1. session_info 1.0.0. setu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:204,usability,confirm,confirmed,204,"Broken test: `TestPreprocessingDistributed::test_normalize_per_cell[dask]`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In #2235, more gradually enabled some so far disabled tests. Before, all tests in `TestPreprocessingDistributed` were disabled with the available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. ___________________________________",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:287,usability,confirm,confirmed,287,"Broken test: `TestPreprocessingDistributed::test_normalize_per_cell[dask]`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In #2235, more gradually enabled some so far disabled tests. Before, all tests in `TestPreprocessingDistributed` were disabled with the available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. ___________________________________",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:904,usability,Minim,Minimal,904,"Broken test: `TestPreprocessingDistributed::test_normalize_per_cell[dask]`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In #2235, more gradually enabled some so far disabled tests. Before, all tests in `TestPreprocessingDistributed` were disabled with the available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. ___________________________________",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:1017,usability,Error,Error,1017,"eprocessingDistributed::test_normalize_per_cell[dask]`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In #2235, more gradually enabled some so far disabled tests. Before, all tests in `TestPreprocessingDistributed` were disabled with the available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash. pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail. ```. ### Error output. ```pytb. ===================================================================================================== test session starts ======================================================================================================. platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python. cachedir: .pytest_cache. rootdir: /home/phil/Dev/Python/Single Cell/scanpy. configfile: pyproject.toml. testpaths: scanpy. plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1. collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================. _______________________________________________________",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/issues/2526:5559,usability,tool,toolz,5559," per cell before normalization (adata.obs). normalizing by total count per cell. filtered out 1 cells that have less than 1 counts. finished (0:00:00): normalized adata.X and added 'n_counts', counts per cell before normalization (adata.obs). =================================================================================================== short test summary info ====================================================================================================. FAILED scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] - AssertionError: . ======================================================================================== 1 failed, 985 deselected, 11 warnings in 3.74s ========================================================================================. ```. ### Versions. <details>. ```. -----. anndata 0.9.0rc2.dev43+g21a76088. scanpy 1.10.0.dev117+g6b9e734f. -----. PIL 9.1.1. asciitree NA. awkward 2.2.1. awkward_cpp NA. beta_ufunc NA. binom_ufunc NA. cffi 1.15.0. cloudpickle 2.2.1. cycler 0.10.0. cython_runtime NA. dask 2023.5.0. dateutil 2.8.2. defusedxml 0.7.1. entrypoints 0.4. fasteners 0.17.3. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. jinja2 3.1.2. joblib 1.1.0. kiwisolver 1.4.3. leidenalg 0.9.1. llvmlite 0.38.1. markupsafe 2.1.1. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.1.0. nbinom_ufunc NA. numba 0.55.2. numcodecs 0.10.2. numpy 1.22.4. packaging 21.3. pandas 2.0.2. pkg_resources NA. psutil 5.9.1. pyparsing 3.0.9. pytz 2022.1. scipy 1.8.1. session_info 1.0.0. setuptools 67.8.0. setuptools_scm NA. six 1.16.0. sklearn 1.1.1. sphinxcontrib NA. texttable 1.6.7. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. typing_extensions NA. wcwidth 0.2.5. yaml 6.0. zarr 2.12.0. zipp NA. -----. Python 3.8.17 (default, Jun 17 2023, 20:09:37) [GCC 13.1.1 20230429]. Linux-6.3.8-zen1-1-zen-x86_64-with-glibc2.34. -----. Session information updated at 2023-06-23 14:29. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526
https://github.com/scverse/scanpy/pull/2527:10,deployability,version,versions,10,Make the `versions` field more helpful; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2527
https://github.com/scverse/scanpy/pull/2527:10,integrability,version,versions,10,Make the `versions` field more helpful; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2527
https://github.com/scverse/scanpy/pull/2527:10,modifiability,version,versions,10,Make the `versions` field more helpful; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2527
https://github.com/scverse/scanpy/pull/2527:259,safety,review,review,259,Make the `versions` field more helpful; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2527
https://github.com/scverse/scanpy/pull/2527:259,testability,review,review,259,Make the `versions` field more helpful; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2527
https://github.com/scverse/scanpy/pull/2527:31,usability,help,helpful,31,Make the `versions` field more helpful; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2527
https://github.com/scverse/scanpy/pull/2527:110,usability,guid,guidelines,110,Make the `versions` field more helpful; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2527
https://github.com/scverse/scanpy/pull/2527:141,usability,guid,guide,141,Make the `versions` field more helpful; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2527
https://github.com/scverse/scanpy/pull/2527:237,usability,workflow,workflow,237,Make the `versions` field more helpful; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2527
https://github.com/scverse/scanpy/pull/2528:55,safety,test,test,55,Backport PRs #2478 and #2235 on branch 1.9.x (Separate test utils from tests); <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2528
https://github.com/scverse/scanpy/pull/2528:71,safety,test,tests,71,Backport PRs #2478 and #2235 on branch 1.9.x (Separate test utils from tests); <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2528
https://github.com/scverse/scanpy/pull/2528:298,safety,review,review,298,Backport PRs #2478 and #2235 on branch 1.9.x (Separate test utils from tests); <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2528
https://github.com/scverse/scanpy/pull/2528:55,testability,test,test,55,Backport PRs #2478 and #2235 on branch 1.9.x (Separate test utils from tests); <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2528
https://github.com/scverse/scanpy/pull/2528:71,testability,test,tests,71,Backport PRs #2478 and #2235 on branch 1.9.x (Separate test utils from tests); <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2528
https://github.com/scverse/scanpy/pull/2528:298,testability,review,review,298,Backport PRs #2478 and #2235 on branch 1.9.x (Separate test utils from tests); <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2528
https://github.com/scverse/scanpy/pull/2528:149,usability,guid,guidelines,149,Backport PRs #2478 and #2235 on branch 1.9.x (Separate test utils from tests); <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2528
https://github.com/scverse/scanpy/pull/2528:180,usability,guid,guide,180,Backport PRs #2478 and #2235 on branch 1.9.x (Separate test utils from tests); <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2528
https://github.com/scverse/scanpy/pull/2528:276,usability,workflow,workflow,276,Backport PRs #2478 and #2235 on branch 1.9.x (Separate test utils from tests); <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2528
https://github.com/scverse/scanpy/pull/2529:29,deployability,version,versions,29,Backport PR #2527: Make the `versions` field more helpful; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2529
https://github.com/scverse/scanpy/pull/2529:29,integrability,version,versions,29,Backport PR #2527: Make the `versions` field more helpful; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2529
https://github.com/scverse/scanpy/pull/2529:29,modifiability,version,versions,29,Backport PR #2527: Make the `versions` field more helpful; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2529
https://github.com/scverse/scanpy/pull/2529:278,safety,review,review,278,Backport PR #2527: Make the `versions` field more helpful; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2529
https://github.com/scverse/scanpy/pull/2529:278,testability,review,review,278,Backport PR #2527: Make the `versions` field more helpful; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2529
https://github.com/scverse/scanpy/pull/2529:50,usability,help,helpful,50,Backport PR #2527: Make the `versions` field more helpful; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2529
https://github.com/scverse/scanpy/pull/2529:129,usability,guid,guidelines,129,Backport PR #2527: Make the `versions` field more helpful; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2529
https://github.com/scverse/scanpy/pull/2529:160,usability,guid,guide,160,Backport PR #2527: Make the `versions` field more helpful; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2529
https://github.com/scverse/scanpy/pull/2529:256,usability,workflow,workflow,256,Backport PR #2527: Make the `versions` field more helpful; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2529
https://github.com/scverse/scanpy/issues/2530:20,security,rotat,rotate,20,How can I delete or rotate the legend of `sc.pl.stacked_violin`?; . Hello:. How can I delete or rotate or change the position of the legend of `sc.pl.stacked_violin`? ![image](https://github.com/scverse/scanpy/assets/33963919/c38e40c0-680f-4241-8c18-e8d1e972f2f8). Thanks,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2530
https://github.com/scverse/scanpy/issues/2530:96,security,rotat,rotate,96,How can I delete or rotate the legend of `sc.pl.stacked_violin`?; . Hello:. How can I delete or rotate or change the position of the legend of `sc.pl.stacked_violin`? ![image](https://github.com/scverse/scanpy/assets/33963919/c38e40c0-680f-4241-8c18-e8d1e972f2f8). Thanks,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2530
https://github.com/scverse/scanpy/issues/2531:1857,availability,Error,Error,1857,"hen using scanpy 1.9.1. Here is the output from the header logger:. scanpy==1.9.3 anndata==0.9.1 umap==0.5.3 numpy==1.24.3 scipy==1.9.1 pandas==2.0.2 scikit-learn==1.2.2 statsmodels==0.14.0 python-igraph==0.10.4 louvain==0.8.0 pynndescent==0.5.10. ### Minimal code sample. ```python. #Follow the same as the tutorial, or as a minimum example:. import numpy as np. import pandas as pd. import scanpy as sc. adata = sc.datasets.pbmc3k(). adata.var_names_make_unique() . sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3). adata.var['mt'] = adata.var_names.str.startswith('MT-'). sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). adata = adata[adata.obs.n_genes_by_counts < 2500, :]. adata = adata[adata.obs.pct_counts_mt < 5, :]. sc.pp.normalize_total(adata, target_sum=1e4). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). sc.tl.pca(adata, svd_solver='arpack'). ```. ### Error output. No error output, it just hangs. ```pytb. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. CoreFoundation NA. Foundation NA. PIL 9.5.0. PyObjCTools NA. anyio NA. appnope 0.1.3. argcomplete NA. asttokens NA. attr 23.1.0. babel 2.12.1. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. fastjsonschema NA. google NA. h5py 3.9.0. hypergeom_ufunc NA. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.2. ipython_genutils 0.2.0. ipywidgets 8.0.6. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.23.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. louvain 0.8.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:1874,availability,error,error,1874,"1.9.1. Here is the output from the header logger:. scanpy==1.9.3 anndata==0.9.1 umap==0.5.3 numpy==1.24.3 scipy==1.9.1 pandas==2.0.2 scikit-learn==1.2.2 statsmodels==0.14.0 python-igraph==0.10.4 louvain==0.8.0 pynndescent==0.5.10. ### Minimal code sample. ```python. #Follow the same as the tutorial, or as a minimum example:. import numpy as np. import pandas as pd. import scanpy as sc. adata = sc.datasets.pbmc3k(). adata.var_names_make_unique() . sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3). adata.var['mt'] = adata.var_names.str.startswith('MT-'). sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). adata = adata[adata.obs.n_genes_by_counts < 2500, :]. adata = adata[adata.obs.pct_counts_mt < 5, :]. sc.pp.normalize_total(adata, target_sum=1e4). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). sc.tl.pca(adata, svd_solver='arpack'). ```. ### Error output. No error output, it just hangs. ```pytb. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. CoreFoundation NA. Foundation NA. PIL 9.5.0. PyObjCTools NA. anyio NA. appnope 0.1.3. argcomplete NA. asttokens NA. attr 23.1.0. babel 2.12.1. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. fastjsonschema NA. google NA. h5py 3.9.0. hypergeom_ufunc NA. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.2. ipython_genutils 0.2.0. ipywidgets 8.0.6. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.23.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. louvain 0.8.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.4.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:208,deployability,version,version,208,"PCA is hanging and taking way too long; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I just made a new environment with the latest version of scanpy (1.9.3), and was trying to run some basic analysis and the pca function is hanging and taking forever, when it used to be basically instantaneous. I tried reproducing it with the the simple tutorial and it is still hanging when I try to run PCA (takes longer than 10 minutes on the simple example dataset in the tutorial, I eventually just have to kill the process). . Not sure at all what the issue is, but wanted to bring it to your attention. This does not happen when using scanpy 1.9.1. Here is the output from the header logger:. scanpy==1.9.3 anndata==0.9.1 umap==0.5.3 numpy==1.24.3 scipy==1.9.1 pandas==2.0.2 scikit-learn==1.2.2 statsmodels==0.14.0 python-igraph==0.10.4 louvain==0.8.0 pynndescent==0.5.10. ### Minimal code sample. ```python. #Follow the same as the tutorial, or as a minimum example:. import numpy as np. import pandas as pd. import scanpy as sc. adata = sc.datasets.pbmc3k(). adata.var_names_make_unique() . sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3). adata.var['mt'] = adata.var_names.str.startswith('MT-'). sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). adata = adata[adata.obs.n_genes_by_counts < 2500, :]. adata = adata[adata.obs.pct_counts_mt < 5, :]. sc.pp.normalize_total(adata, target_sum=1e4). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). sc.tl.pca(adata, svd_solver='arpack'). ```. ### Error output. No error output, it just hangs. ```pytb. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. CoreFounda",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:374,deployability,version,version,374,"PCA is hanging and taking way too long; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I just made a new environment with the latest version of scanpy (1.9.3), and was trying to run some basic analysis and the pca function is hanging and taking forever, when it used to be basically instantaneous. I tried reproducing it with the the simple tutorial and it is still hanging when I try to run PCA (takes longer than 10 minutes on the simple example dataset in the tutorial, I eventually just have to kill the process). . Not sure at all what the issue is, but wanted to bring it to your attention. This does not happen when using scanpy 1.9.1. Here is the output from the header logger:. scanpy==1.9.3 anndata==0.9.1 umap==0.5.3 numpy==1.24.3 scipy==1.9.1 pandas==2.0.2 scikit-learn==1.2.2 statsmodels==0.14.0 python-igraph==0.10.4 louvain==0.8.0 pynndescent==0.5.10. ### Minimal code sample. ```python. #Follow the same as the tutorial, or as a minimum example:. import numpy as np. import pandas as pd. import scanpy as sc. adata = sc.datasets.pbmc3k(). adata.var_names_make_unique() . sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3). adata.var['mt'] = adata.var_names.str.startswith('MT-'). sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). adata = adata[adata.obs.n_genes_by_counts < 2500, :]. adata = adata[adata.obs.pct_counts_mt < 5, :]. sc.pp.normalize_total(adata, target_sum=1e4). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). sc.tl.pca(adata, svd_solver='arpack'). ```. ### Error output. No error output, it just hangs. ```pytb. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. CoreFounda",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:919,deployability,log,logger,919,"PCA is hanging and taking way too long; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I just made a new environment with the latest version of scanpy (1.9.3), and was trying to run some basic analysis and the pca function is hanging and taking forever, when it used to be basically instantaneous. I tried reproducing it with the the simple tutorial and it is still hanging when I try to run PCA (takes longer than 10 minutes on the simple example dataset in the tutorial, I eventually just have to kill the process). . Not sure at all what the issue is, but wanted to bring it to your attention. This does not happen when using scanpy 1.9.1. Here is the output from the header logger:. scanpy==1.9.3 anndata==0.9.1 umap==0.5.3 numpy==1.24.3 scipy==1.9.1 pandas==2.0.2 scikit-learn==1.2.2 statsmodels==0.14.0 python-igraph==0.10.4 louvain==0.8.0 pynndescent==0.5.10. ### Minimal code sample. ```python. #Follow the same as the tutorial, or as a minimum example:. import numpy as np. import pandas as pd. import scanpy as sc. adata = sc.datasets.pbmc3k(). adata.var_names_make_unique() . sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3). adata.var['mt'] = adata.var_names.str.startswith('MT-'). sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). adata = adata[adata.obs.n_genes_by_counts < 2500, :]. adata = adata[adata.obs.pct_counts_mt < 5, :]. sc.pp.normalize_total(adata, target_sum=1e4). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). sc.tl.pca(adata, svd_solver='arpack'). ```. ### Error output. No error output, it just hangs. ```pytb. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. CoreFounda",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:1921,deployability,Version,Versions,1921,":. scanpy==1.9.3 anndata==0.9.1 umap==0.5.3 numpy==1.24.3 scipy==1.9.1 pandas==2.0.2 scikit-learn==1.2.2 statsmodels==0.14.0 python-igraph==0.10.4 louvain==0.8.0 pynndescent==0.5.10. ### Minimal code sample. ```python. #Follow the same as the tutorial, or as a minimum example:. import numpy as np. import pandas as pd. import scanpy as sc. adata = sc.datasets.pbmc3k(). adata.var_names_make_unique() . sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3). adata.var['mt'] = adata.var_names.str.startswith('MT-'). sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). adata = adata[adata.obs.n_genes_by_counts < 2500, :]. adata = adata[adata.obs.pct_counts_mt < 5, :]. sc.pp.normalize_total(adata, target_sum=1e4). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). sc.tl.pca(adata, svd_solver='arpack'). ```. ### Error output. No error output, it just hangs. ```pytb. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. CoreFoundation NA. Foundation NA. PIL 9.5.0. PyObjCTools NA. anyio NA. appnope 0.1.3. argcomplete NA. asttokens NA. attr 23.1.0. babel 2.12.1. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. fastjsonschema NA. google NA. h5py 3.9.0. hypergeom_ufunc NA. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.2. ipython_genutils 0.2.0. ipywidgets 8.0.6. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.23.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. louvain 0.8.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.4.0. nbformat 5.9.0. nbinom_ufunc NA. ncf_ufunc NA",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:4104,deployability,updat,updated,4104,"beta_ufunc NA. binom_ufunc NA. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. fastjsonschema NA. google NA. h5py 3.9.0. hypergeom_ufunc NA. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.2. ipython_genutils 0.2.0. ipywidgets 8.0.6. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.23.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. louvain 0.8.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.4.0. nbformat 5.9.0. nbinom_ufunc NA. ncf_ufunc NA. numba 0.57.0. numexpr 2.8.4. numpy 1.24.3. objc 9.2. overrides NA. packaging 23.1. pandas 2.0.2. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.6.0. prometheus_client NA. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pvectorc NA. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.1.0. pyrsistent NA. pythonjsonlogger NA. pytz 2023.3. requests 2.31.0. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. scipy 1.9.1. send2trash NA. session_info 1.0.0. setuptools 68.0.0. six 1.16.0. sklearn 1.2.2. sniffio 1.3.0. socks 1.7.1. stack_data 0.6.2. texttable 1.6.7. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. torch 1.12.1. tornado 6.3.2. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. urllib3 2.0.3. wcwidth 0.2.6. websocket 1.6.0. yaml 6.0. zipp NA. zmq 25.1.0. zoneinfo NA. -----. IPython 8.14.0. jupyter_client 8.2.0. jupyter_core 5.3.1. jupyterlab 4.0.2. notebook 6.5.4. -----. Python 3.9.16 | packaged by conda-forge | (main, Feb 1 2023, 21:42:20) [Clang 14.0.6 ]. macOS-12.6.6-x86_64-i386-64bit. -----. Session information updated at 2023-06-23 18:00. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:1990,energy efficiency,Core,CoreFoundation,1990,"andas==2.0.2 scikit-learn==1.2.2 statsmodels==0.14.0 python-igraph==0.10.4 louvain==0.8.0 pynndescent==0.5.10. ### Minimal code sample. ```python. #Follow the same as the tutorial, or as a minimum example:. import numpy as np. import pandas as pd. import scanpy as sc. adata = sc.datasets.pbmc3k(). adata.var_names_make_unique() . sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3). adata.var['mt'] = adata.var_names.str.startswith('MT-'). sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). adata = adata[adata.obs.n_genes_by_counts < 2500, :]. adata = adata[adata.obs.pct_counts_mt < 5, :]. sc.pp.normalize_total(adata, target_sum=1e4). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). sc.tl.pca(adata, svd_solver='arpack'). ```. ### Error output. No error output, it just hangs. ```pytb. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. CoreFoundation NA. Foundation NA. PIL 9.5.0. PyObjCTools NA. anyio NA. appnope 0.1.3. argcomplete NA. asttokens NA. attr 23.1.0. babel 2.12.1. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. fastjsonschema NA. google NA. h5py 3.9.0. hypergeom_ufunc NA. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.2. ipython_genutils 0.2.0. ipywidgets 8.0.6. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.23.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. louvain 0.8.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.4.0. nbformat 5.9.0. nbinom_ufunc NA. ncf_ufunc NA. numba 0.57.0. numexpr 2.8.4. numpy 1.24.3. objc 9.2. overrides NA. pac",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:2239,energy efficiency,cloud,cloudpickle,2239,"import scanpy as sc. adata = sc.datasets.pbmc3k(). adata.var_names_make_unique() . sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3). adata.var['mt'] = adata.var_names.str.startswith('MT-'). sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). adata = adata[adata.obs.n_genes_by_counts < 2500, :]. adata = adata[adata.obs.pct_counts_mt < 5, :]. sc.pp.normalize_total(adata, target_sum=1e4). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). sc.tl.pca(adata, svd_solver='arpack'). ```. ### Error output. No error output, it just hangs. ```pytb. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. CoreFoundation NA. Foundation NA. PIL 9.5.0. PyObjCTools NA. anyio NA. appnope 0.1.3. argcomplete NA. asttokens NA. attr 23.1.0. babel 2.12.1. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. fastjsonschema NA. google NA. h5py 3.9.0. hypergeom_ufunc NA. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.2. ipython_genutils 0.2.0. ipywidgets 8.0.6. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.23.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. louvain 0.8.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.4.0. nbformat 5.9.0. nbinom_ufunc NA. ncf_ufunc NA. numba 0.57.0. numexpr 2.8.4. numpy 1.24.3. objc 9.2. overrides NA. packaging 23.1. pandas 2.0.2. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.6.0. prometheus_client NA. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pvectorc NA. pydev_ipython NA. pydevconsol",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:208,integrability,version,version,208,"PCA is hanging and taking way too long; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I just made a new environment with the latest version of scanpy (1.9.3), and was trying to run some basic analysis and the pca function is hanging and taking forever, when it used to be basically instantaneous. I tried reproducing it with the the simple tutorial and it is still hanging when I try to run PCA (takes longer than 10 minutes on the simple example dataset in the tutorial, I eventually just have to kill the process). . Not sure at all what the issue is, but wanted to bring it to your attention. This does not happen when using scanpy 1.9.1. Here is the output from the header logger:. scanpy==1.9.3 anndata==0.9.1 umap==0.5.3 numpy==1.24.3 scipy==1.9.1 pandas==2.0.2 scikit-learn==1.2.2 statsmodels==0.14.0 python-igraph==0.10.4 louvain==0.8.0 pynndescent==0.5.10. ### Minimal code sample. ```python. #Follow the same as the tutorial, or as a minimum example:. import numpy as np. import pandas as pd. import scanpy as sc. adata = sc.datasets.pbmc3k(). adata.var_names_make_unique() . sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3). adata.var['mt'] = adata.var_names.str.startswith('MT-'). sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). adata = adata[adata.obs.n_genes_by_counts < 2500, :]. adata = adata[adata.obs.pct_counts_mt < 5, :]. sc.pp.normalize_total(adata, target_sum=1e4). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). sc.tl.pca(adata, svd_solver='arpack'). ```. ### Error output. No error output, it just hangs. ```pytb. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. CoreFounda",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:374,integrability,version,version,374,"PCA is hanging and taking way too long; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I just made a new environment with the latest version of scanpy (1.9.3), and was trying to run some basic analysis and the pca function is hanging and taking forever, when it used to be basically instantaneous. I tried reproducing it with the the simple tutorial and it is still hanging when I try to run PCA (takes longer than 10 minutes on the simple example dataset in the tutorial, I eventually just have to kill the process). . Not sure at all what the issue is, but wanted to bring it to your attention. This does not happen when using scanpy 1.9.1. Here is the output from the header logger:. scanpy==1.9.3 anndata==0.9.1 umap==0.5.3 numpy==1.24.3 scipy==1.9.1 pandas==2.0.2 scikit-learn==1.2.2 statsmodels==0.14.0 python-igraph==0.10.4 louvain==0.8.0 pynndescent==0.5.10. ### Minimal code sample. ```python. #Follow the same as the tutorial, or as a minimum example:. import numpy as np. import pandas as pd. import scanpy as sc. adata = sc.datasets.pbmc3k(). adata.var_names_make_unique() . sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3). adata.var['mt'] = adata.var_names.str.startswith('MT-'). sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). adata = adata[adata.obs.n_genes_by_counts < 2500, :]. adata = adata[adata.obs.pct_counts_mt < 5, :]. sc.pp.normalize_total(adata, target_sum=1e4). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). sc.tl.pca(adata, svd_solver='arpack'). ```. ### Error output. No error output, it just hangs. ```pytb. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. CoreFounda",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:716,integrability,event,eventually,716,"PCA is hanging and taking way too long; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I just made a new environment with the latest version of scanpy (1.9.3), and was trying to run some basic analysis and the pca function is hanging and taking forever, when it used to be basically instantaneous. I tried reproducing it with the the simple tutorial and it is still hanging when I try to run PCA (takes longer than 10 minutes on the simple example dataset in the tutorial, I eventually just have to kill the process). . Not sure at all what the issue is, but wanted to bring it to your attention. This does not happen when using scanpy 1.9.1. Here is the output from the header logger:. scanpy==1.9.3 anndata==0.9.1 umap==0.5.3 numpy==1.24.3 scipy==1.9.1 pandas==2.0.2 scikit-learn==1.2.2 statsmodels==0.14.0 python-igraph==0.10.4 louvain==0.8.0 pynndescent==0.5.10. ### Minimal code sample. ```python. #Follow the same as the tutorial, or as a minimum example:. import numpy as np. import pandas as pd. import scanpy as sc. adata = sc.datasets.pbmc3k(). adata.var_names_make_unique() . sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3). adata.var['mt'] = adata.var_names.str.startswith('MT-'). sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). adata = adata[adata.obs.n_genes_by_counts < 2500, :]. adata = adata[adata.obs.pct_counts_mt < 5, :]. sc.pp.normalize_total(adata, target_sum=1e4). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). sc.tl.pca(adata, svd_solver='arpack'). ```. ### Error output. No error output, it just hangs. ```pytb. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. CoreFounda",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:1921,integrability,Version,Versions,1921,":. scanpy==1.9.3 anndata==0.9.1 umap==0.5.3 numpy==1.24.3 scipy==1.9.1 pandas==2.0.2 scikit-learn==1.2.2 statsmodels==0.14.0 python-igraph==0.10.4 louvain==0.8.0 pynndescent==0.5.10. ### Minimal code sample. ```python. #Follow the same as the tutorial, or as a minimum example:. import numpy as np. import pandas as pd. import scanpy as sc. adata = sc.datasets.pbmc3k(). adata.var_names_make_unique() . sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3). adata.var['mt'] = adata.var_names.str.startswith('MT-'). sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). adata = adata[adata.obs.n_genes_by_counts < 2500, :]. adata = adata[adata.obs.pct_counts_mt < 5, :]. sc.pp.normalize_total(adata, target_sum=1e4). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). sc.tl.pca(adata, svd_solver='arpack'). ```. ### Error output. No error output, it just hangs. ```pytb. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. CoreFoundation NA. Foundation NA. PIL 9.5.0. PyObjCTools NA. anyio NA. appnope 0.1.3. argcomplete NA. asttokens NA. attr 23.1.0. babel 2.12.1. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. fastjsonschema NA. google NA. h5py 3.9.0. hypergeom_ufunc NA. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.2. ipython_genutils 0.2.0. ipywidgets 8.0.6. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.23.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. louvain 0.8.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.4.0. nbformat 5.9.0. nbinom_ufunc NA. ncf_ufunc NA",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:3089,interoperability,platform,platformdirs,3089,"tokens NA. attr 23.1.0. babel 2.12.1. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. fastjsonschema NA. google NA. h5py 3.9.0. hypergeom_ufunc NA. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.2. ipython_genutils 0.2.0. ipywidgets 8.0.6. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.23.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. louvain 0.8.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.4.0. nbformat 5.9.0. nbinom_ufunc NA. ncf_ufunc NA. numba 0.57.0. numexpr 2.8.4. numpy 1.24.3. objc 9.2. overrides NA. packaging 23.1. pandas 2.0.2. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.6.0. prometheus_client NA. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pvectorc NA. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.1.0. pyrsistent NA. pythonjsonlogger NA. pytz 2023.3. requests 2.31.0. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. scipy 1.9.1. send2trash NA. session_info 1.0.0. setuptools 68.0.0. six 1.16.0. sklearn 1.2.2. sniffio 1.3.0. socks 1.7.1. stack_data 0.6.2. texttable 1.6.7. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. torch 1.12.1. tornado 6.3.2. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. urllib3 2.0.3. wcwidth 0.2.6. websocket 1.6.0. yaml 6.0. zipp NA. zmq 25.1.0. zoneinfo NA. -----. IPython 8.14.0. jupyter_client 8.2.0. jupyter_core 5.3.1. jupyterlab 4.0.2. notebook 6.5.4. -----. Python 3.9.16 | packaged by conda-forge | (main, Feb 1 2023, 21:42:20) [Clang 14.0.6 ]. macOS-12.6.6-x86_64-i386-64bit. -----. Session inf",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:208,modifiability,version,version,208,"PCA is hanging and taking way too long; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I just made a new environment with the latest version of scanpy (1.9.3), and was trying to run some basic analysis and the pca function is hanging and taking forever, when it used to be basically instantaneous. I tried reproducing it with the the simple tutorial and it is still hanging when I try to run PCA (takes longer than 10 minutes on the simple example dataset in the tutorial, I eventually just have to kill the process). . Not sure at all what the issue is, but wanted to bring it to your attention. This does not happen when using scanpy 1.9.1. Here is the output from the header logger:. scanpy==1.9.3 anndata==0.9.1 umap==0.5.3 numpy==1.24.3 scipy==1.9.1 pandas==2.0.2 scikit-learn==1.2.2 statsmodels==0.14.0 python-igraph==0.10.4 louvain==0.8.0 pynndescent==0.5.10. ### Minimal code sample. ```python. #Follow the same as the tutorial, or as a minimum example:. import numpy as np. import pandas as pd. import scanpy as sc. adata = sc.datasets.pbmc3k(). adata.var_names_make_unique() . sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3). adata.var['mt'] = adata.var_names.str.startswith('MT-'). sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). adata = adata[adata.obs.n_genes_by_counts < 2500, :]. adata = adata[adata.obs.pct_counts_mt < 5, :]. sc.pp.normalize_total(adata, target_sum=1e4). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). sc.tl.pca(adata, svd_solver='arpack'). ```. ### Error output. No error output, it just hangs. ```pytb. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. CoreFounda",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:374,modifiability,version,version,374,"PCA is hanging and taking way too long; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I just made a new environment with the latest version of scanpy (1.9.3), and was trying to run some basic analysis and the pca function is hanging and taking forever, when it used to be basically instantaneous. I tried reproducing it with the the simple tutorial and it is still hanging when I try to run PCA (takes longer than 10 minutes on the simple example dataset in the tutorial, I eventually just have to kill the process). . Not sure at all what the issue is, but wanted to bring it to your attention. This does not happen when using scanpy 1.9.1. Here is the output from the header logger:. scanpy==1.9.3 anndata==0.9.1 umap==0.5.3 numpy==1.24.3 scipy==1.9.1 pandas==2.0.2 scikit-learn==1.2.2 statsmodels==0.14.0 python-igraph==0.10.4 louvain==0.8.0 pynndescent==0.5.10. ### Minimal code sample. ```python. #Follow the same as the tutorial, or as a minimum example:. import numpy as np. import pandas as pd. import scanpy as sc. adata = sc.datasets.pbmc3k(). adata.var_names_make_unique() . sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3). adata.var['mt'] = adata.var_names.str.startswith('MT-'). sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). adata = adata[adata.obs.n_genes_by_counts < 2500, :]. adata = adata[adata.obs.pct_counts_mt < 5, :]. sc.pp.normalize_total(adata, target_sum=1e4). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). sc.tl.pca(adata, svd_solver='arpack'). ```. ### Error output. No error output, it just hangs. ```pytb. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. CoreFounda",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:1921,modifiability,Version,Versions,1921,":. scanpy==1.9.3 anndata==0.9.1 umap==0.5.3 numpy==1.24.3 scipy==1.9.1 pandas==2.0.2 scikit-learn==1.2.2 statsmodels==0.14.0 python-igraph==0.10.4 louvain==0.8.0 pynndescent==0.5.10. ### Minimal code sample. ```python. #Follow the same as the tutorial, or as a minimum example:. import numpy as np. import pandas as pd. import scanpy as sc. adata = sc.datasets.pbmc3k(). adata.var_names_make_unique() . sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3). adata.var['mt'] = adata.var_names.str.startswith('MT-'). sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). adata = adata[adata.obs.n_genes_by_counts < 2500, :]. adata = adata[adata.obs.pct_counts_mt < 5, :]. sc.pp.normalize_total(adata, target_sum=1e4). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). sc.tl.pca(adata, svd_solver='arpack'). ```. ### Error output. No error output, it just hangs. ```pytb. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. CoreFoundation NA. Foundation NA. PIL 9.5.0. PyObjCTools NA. anyio NA. appnope 0.1.3. argcomplete NA. asttokens NA. attr 23.1.0. babel 2.12.1. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. fastjsonschema NA. google NA. h5py 3.9.0. hypergeom_ufunc NA. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.2. ipython_genutils 0.2.0. ipywidgets 8.0.6. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.23.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. louvain 0.8.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.4.0. nbformat 5.9.0. nbinom_ufunc NA. ncf_ufunc NA",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:2382,modifiability,deco,decorator,2382,"s(adata, min_cells=3). adata.var['mt'] = adata.var_names.str.startswith('MT-'). sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). adata = adata[adata.obs.n_genes_by_counts < 2500, :]. adata = adata[adata.obs.pct_counts_mt < 5, :]. sc.pp.normalize_total(adata, target_sum=1e4). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). sc.tl.pca(adata, svd_solver='arpack'). ```. ### Error output. No error output, it just hangs. ```pytb. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. CoreFoundation NA. Foundation NA. PIL 9.5.0. PyObjCTools NA. anyio NA. appnope 0.1.3. argcomplete NA. asttokens NA. attr 23.1.0. babel 2.12.1. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. fastjsonschema NA. google NA. h5py 3.9.0. hypergeom_ufunc NA. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.2. ipython_genutils 0.2.0. ipywidgets 8.0.6. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.23.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. louvain 0.8.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.4.0. nbformat 5.9.0. nbinom_ufunc NA. ncf_ufunc NA. numba 0.57.0. numexpr 2.8.4. numpy 1.24.3. objc 9.2. overrides NA. packaging 23.1. pandas 2.0.2. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.6.0. prometheus_client NA. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pvectorc NA. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.1.0. pyrsistent NA. pythonjsonlog",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:2994,modifiability,pac,packaging,2994,ation NA. Foundation NA. PIL 9.5.0. PyObjCTools NA. anyio NA. appnope 0.1.3. argcomplete NA. asttokens NA. attr 23.1.0. babel 2.12.1. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. fastjsonschema NA. google NA. h5py 3.9.0. hypergeom_ufunc NA. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.2. ipython_genutils 0.2.0. ipywidgets 8.0.6. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.23.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. louvain 0.8.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.4.0. nbformat 5.9.0. nbinom_ufunc NA. ncf_ufunc NA. numba 0.57.0. numexpr 2.8.4. numpy 1.24.3. objc 9.2. overrides NA. packaging 23.1. pandas 2.0.2. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.6.0. prometheus_client NA. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pvectorc NA. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.1.0. pyrsistent NA. pythonjsonlogger NA. pytz 2023.3. requests 2.31.0. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. scipy 1.9.1. send2trash NA. session_info 1.0.0. setuptools 68.0.0. six 1.16.0. sklearn 1.2.2. sniffio 1.3.0. socks 1.7.1. stack_data 0.6.2. texttable 1.6.7. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. torch 1.12.1. tornado 6.3.2. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. urllib3 2.0.3. wcwidth 0.2.6. websocket 1.6.0. yaml 6.0. zipp NA. zmq 25.1.0. zoneinfo NA. -----. IPython 8.14.0. jupyter_client 8.2.0. jupyter_core 5.3.1. jupyterlab 4.0.2. notebook 6.5.4. -----. Python 3.9.16 | packaged by conda-forge | ,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:3973,modifiability,pac,packaged,3973,"beta_ufunc NA. binom_ufunc NA. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. fastjsonschema NA. google NA. h5py 3.9.0. hypergeom_ufunc NA. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.2. ipython_genutils 0.2.0. ipywidgets 8.0.6. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.23.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. louvain 0.8.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.4.0. nbformat 5.9.0. nbinom_ufunc NA. ncf_ufunc NA. numba 0.57.0. numexpr 2.8.4. numpy 1.24.3. objc 9.2. overrides NA. packaging 23.1. pandas 2.0.2. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.6.0. prometheus_client NA. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pvectorc NA. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.1.0. pyrsistent NA. pythonjsonlogger NA. pytz 2023.3. requests 2.31.0. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. scipy 1.9.1. send2trash NA. session_info 1.0.0. setuptools 68.0.0. six 1.16.0. sklearn 1.2.2. sniffio 1.3.0. socks 1.7.1. stack_data 0.6.2. texttable 1.6.7. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. torch 1.12.1. tornado 6.3.2. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. urllib3 2.0.3. wcwidth 0.2.6. websocket 1.6.0. yaml 6.0. zipp NA. zmq 25.1.0. zoneinfo NA. -----. IPython 8.14.0. jupyter_client 8.2.0. jupyter_core 5.3.1. jupyterlab 4.0.2. notebook 6.5.4. -----. Python 3.9.16 | packaged by conda-forge | (main, Feb 1 2023, 21:42:20) [Clang 14.0.6 ]. macOS-12.6.6-x86_64-i386-64bit. -----. Session information updated at 2023-06-23 18:00. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:1857,performance,Error,Error,1857,"hen using scanpy 1.9.1. Here is the output from the header logger:. scanpy==1.9.3 anndata==0.9.1 umap==0.5.3 numpy==1.24.3 scipy==1.9.1 pandas==2.0.2 scikit-learn==1.2.2 statsmodels==0.14.0 python-igraph==0.10.4 louvain==0.8.0 pynndescent==0.5.10. ### Minimal code sample. ```python. #Follow the same as the tutorial, or as a minimum example:. import numpy as np. import pandas as pd. import scanpy as sc. adata = sc.datasets.pbmc3k(). adata.var_names_make_unique() . sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3). adata.var['mt'] = adata.var_names.str.startswith('MT-'). sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). adata = adata[adata.obs.n_genes_by_counts < 2500, :]. adata = adata[adata.obs.pct_counts_mt < 5, :]. sc.pp.normalize_total(adata, target_sum=1e4). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). sc.tl.pca(adata, svd_solver='arpack'). ```. ### Error output. No error output, it just hangs. ```pytb. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. CoreFoundation NA. Foundation NA. PIL 9.5.0. PyObjCTools NA. anyio NA. appnope 0.1.3. argcomplete NA. asttokens NA. attr 23.1.0. babel 2.12.1. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. fastjsonschema NA. google NA. h5py 3.9.0. hypergeom_ufunc NA. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.2. ipython_genutils 0.2.0. ipywidgets 8.0.6. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.23.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. louvain 0.8.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:1874,performance,error,error,1874,"1.9.1. Here is the output from the header logger:. scanpy==1.9.3 anndata==0.9.1 umap==0.5.3 numpy==1.24.3 scipy==1.9.1 pandas==2.0.2 scikit-learn==1.2.2 statsmodels==0.14.0 python-igraph==0.10.4 louvain==0.8.0 pynndescent==0.5.10. ### Minimal code sample. ```python. #Follow the same as the tutorial, or as a minimum example:. import numpy as np. import pandas as pd. import scanpy as sc. adata = sc.datasets.pbmc3k(). adata.var_names_make_unique() . sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3). adata.var['mt'] = adata.var_names.str.startswith('MT-'). sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). adata = adata[adata.obs.n_genes_by_counts < 2500, :]. adata = adata[adata.obs.pct_counts_mt < 5, :]. sc.pp.normalize_total(adata, target_sum=1e4). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). sc.tl.pca(adata, svd_solver='arpack'). ```. ### Error output. No error output, it just hangs. ```pytb. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. CoreFoundation NA. Foundation NA. PIL 9.5.0. PyObjCTools NA. anyio NA. appnope 0.1.3. argcomplete NA. asttokens NA. attr 23.1.0. babel 2.12.1. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. fastjsonschema NA. google NA. h5py 3.9.0. hypergeom_ufunc NA. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.2. ipython_genutils 0.2.0. ipywidgets 8.0.6. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.23.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. louvain 0.8.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.4.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:843,reliability,doe,does,843,"PCA is hanging and taking way too long; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I just made a new environment with the latest version of scanpy (1.9.3), and was trying to run some basic analysis and the pca function is hanging and taking forever, when it used to be basically instantaneous. I tried reproducing it with the the simple tutorial and it is still hanging when I try to run PCA (takes longer than 10 minutes on the simple example dataset in the tutorial, I eventually just have to kill the process). . Not sure at all what the issue is, but wanted to bring it to your attention. This does not happen when using scanpy 1.9.1. Here is the output from the header logger:. scanpy==1.9.3 anndata==0.9.1 umap==0.5.3 numpy==1.24.3 scipy==1.9.1 pandas==2.0.2 scikit-learn==1.2.2 statsmodels==0.14.0 python-igraph==0.10.4 louvain==0.8.0 pynndescent==0.5.10. ### Minimal code sample. ```python. #Follow the same as the tutorial, or as a minimum example:. import numpy as np. import pandas as pd. import scanpy as sc. adata = sc.datasets.pbmc3k(). adata.var_names_make_unique() . sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3). adata.var['mt'] = adata.var_names.str.startswith('MT-'). sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). adata = adata[adata.obs.n_genes_by_counts < 2500, :]. adata = adata[adata.obs.pct_counts_mt < 5, :]. sc.pp.normalize_total(adata, target_sum=1e4). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). sc.tl.pca(adata, svd_solver='arpack'). ```. ### Error output. No error output, it just hangs. ```pytb. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. CoreFounda",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:919,safety,log,logger,919,"PCA is hanging and taking way too long; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I just made a new environment with the latest version of scanpy (1.9.3), and was trying to run some basic analysis and the pca function is hanging and taking forever, when it used to be basically instantaneous. I tried reproducing it with the the simple tutorial and it is still hanging when I try to run PCA (takes longer than 10 minutes on the simple example dataset in the tutorial, I eventually just have to kill the process). . Not sure at all what the issue is, but wanted to bring it to your attention. This does not happen when using scanpy 1.9.1. Here is the output from the header logger:. scanpy==1.9.3 anndata==0.9.1 umap==0.5.3 numpy==1.24.3 scipy==1.9.1 pandas==2.0.2 scikit-learn==1.2.2 statsmodels==0.14.0 python-igraph==0.10.4 louvain==0.8.0 pynndescent==0.5.10. ### Minimal code sample. ```python. #Follow the same as the tutorial, or as a minimum example:. import numpy as np. import pandas as pd. import scanpy as sc. adata = sc.datasets.pbmc3k(). adata.var_names_make_unique() . sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3). adata.var['mt'] = adata.var_names.str.startswith('MT-'). sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). adata = adata[adata.obs.n_genes_by_counts < 2500, :]. adata = adata[adata.obs.pct_counts_mt < 5, :]. sc.pp.normalize_total(adata, target_sum=1e4). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). sc.tl.pca(adata, svd_solver='arpack'). ```. ### Error output. No error output, it just hangs. ```pytb. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. CoreFounda",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:1857,safety,Error,Error,1857,"hen using scanpy 1.9.1. Here is the output from the header logger:. scanpy==1.9.3 anndata==0.9.1 umap==0.5.3 numpy==1.24.3 scipy==1.9.1 pandas==2.0.2 scikit-learn==1.2.2 statsmodels==0.14.0 python-igraph==0.10.4 louvain==0.8.0 pynndescent==0.5.10. ### Minimal code sample. ```python. #Follow the same as the tutorial, or as a minimum example:. import numpy as np. import pandas as pd. import scanpy as sc. adata = sc.datasets.pbmc3k(). adata.var_names_make_unique() . sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3). adata.var['mt'] = adata.var_names.str.startswith('MT-'). sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). adata = adata[adata.obs.n_genes_by_counts < 2500, :]. adata = adata[adata.obs.pct_counts_mt < 5, :]. sc.pp.normalize_total(adata, target_sum=1e4). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). sc.tl.pca(adata, svd_solver='arpack'). ```. ### Error output. No error output, it just hangs. ```pytb. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. CoreFoundation NA. Foundation NA. PIL 9.5.0. PyObjCTools NA. anyio NA. appnope 0.1.3. argcomplete NA. asttokens NA. attr 23.1.0. babel 2.12.1. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. fastjsonschema NA. google NA. h5py 3.9.0. hypergeom_ufunc NA. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.2. ipython_genutils 0.2.0. ipywidgets 8.0.6. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.23.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. louvain 0.8.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:1874,safety,error,error,1874,"1.9.1. Here is the output from the header logger:. scanpy==1.9.3 anndata==0.9.1 umap==0.5.3 numpy==1.24.3 scipy==1.9.1 pandas==2.0.2 scikit-learn==1.2.2 statsmodels==0.14.0 python-igraph==0.10.4 louvain==0.8.0 pynndescent==0.5.10. ### Minimal code sample. ```python. #Follow the same as the tutorial, or as a minimum example:. import numpy as np. import pandas as pd. import scanpy as sc. adata = sc.datasets.pbmc3k(). adata.var_names_make_unique() . sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3). adata.var['mt'] = adata.var_names.str.startswith('MT-'). sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). adata = adata[adata.obs.n_genes_by_counts < 2500, :]. adata = adata[adata.obs.pct_counts_mt < 5, :]. sc.pp.normalize_total(adata, target_sum=1e4). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). sc.tl.pca(adata, svd_solver='arpack'). ```. ### Error output. No error output, it just hangs. ```pytb. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. CoreFoundation NA. Foundation NA. PIL 9.5.0. PyObjCTools NA. anyio NA. appnope 0.1.3. argcomplete NA. asttokens NA. attr 23.1.0. babel 2.12.1. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. fastjsonschema NA. google NA. h5py 3.9.0. hypergeom_ufunc NA. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.2. ipython_genutils 0.2.0. ipywidgets 8.0.6. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.23.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. louvain 0.8.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.4.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:4104,safety,updat,updated,4104,"beta_ufunc NA. binom_ufunc NA. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. fastjsonschema NA. google NA. h5py 3.9.0. hypergeom_ufunc NA. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.2. ipython_genutils 0.2.0. ipywidgets 8.0.6. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.23.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. louvain 0.8.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.4.0. nbformat 5.9.0. nbinom_ufunc NA. ncf_ufunc NA. numba 0.57.0. numexpr 2.8.4. numpy 1.24.3. objc 9.2. overrides NA. packaging 23.1. pandas 2.0.2. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.6.0. prometheus_client NA. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pvectorc NA. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.1.0. pyrsistent NA. pythonjsonlogger NA. pytz 2023.3. requests 2.31.0. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. scipy 1.9.1. send2trash NA. session_info 1.0.0. setuptools 68.0.0. six 1.16.0. sklearn 1.2.2. sniffio 1.3.0. socks 1.7.1. stack_data 0.6.2. texttable 1.6.7. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. torch 1.12.1. tornado 6.3.2. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. urllib3 2.0.3. wcwidth 0.2.6. websocket 1.6.0. yaml 6.0. zipp NA. zmq 25.1.0. zoneinfo NA. -----. IPython 8.14.0. jupyter_client 8.2.0. jupyter_core 5.3.1. jupyterlab 4.0.2. notebook 6.5.4. -----. Python 3.9.16 | packaged by conda-forge | (main, Feb 1 2023, 21:42:20) [Clang 14.0.6 ]. macOS-12.6.6-x86_64-i386-64bit. -----. Session information updated at 2023-06-23 18:00. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:919,security,log,logger,919,"PCA is hanging and taking way too long; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I just made a new environment with the latest version of scanpy (1.9.3), and was trying to run some basic analysis and the pca function is hanging and taking forever, when it used to be basically instantaneous. I tried reproducing it with the the simple tutorial and it is still hanging when I try to run PCA (takes longer than 10 minutes on the simple example dataset in the tutorial, I eventually just have to kill the process). . Not sure at all what the issue is, but wanted to bring it to your attention. This does not happen when using scanpy 1.9.1. Here is the output from the header logger:. scanpy==1.9.3 anndata==0.9.1 umap==0.5.3 numpy==1.24.3 scipy==1.9.1 pandas==2.0.2 scikit-learn==1.2.2 statsmodels==0.14.0 python-igraph==0.10.4 louvain==0.8.0 pynndescent==0.5.10. ### Minimal code sample. ```python. #Follow the same as the tutorial, or as a minimum example:. import numpy as np. import pandas as pd. import scanpy as sc. adata = sc.datasets.pbmc3k(). adata.var_names_make_unique() . sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3). adata.var['mt'] = adata.var_names.str.startswith('MT-'). sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). adata = adata[adata.obs.n_genes_by_counts < 2500, :]. adata = adata[adata.obs.pct_counts_mt < 5, :]. sc.pp.normalize_total(adata, target_sum=1e4). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). sc.tl.pca(adata, svd_solver='arpack'). ```. ### Error output. No error output, it just hangs. ```pytb. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. CoreFounda",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:2180,security,certif,certifi,2180,"a minimum example:. import numpy as np. import pandas as pd. import scanpy as sc. adata = sc.datasets.pbmc3k(). adata.var_names_make_unique() . sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3). adata.var['mt'] = adata.var_names.str.startswith('MT-'). sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). adata = adata[adata.obs.n_genes_by_counts < 2500, :]. adata = adata[adata.obs.pct_counts_mt < 5, :]. sc.pp.normalize_total(adata, target_sum=1e4). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). sc.tl.pca(adata, svd_solver='arpack'). ```. ### Error output. No error output, it just hangs. ```pytb. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. CoreFoundation NA. Foundation NA. PIL 9.5.0. PyObjCTools NA. anyio NA. appnope 0.1.3. argcomplete NA. asttokens NA. attr 23.1.0. babel 2.12.1. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. fastjsonschema NA. google NA. h5py 3.9.0. hypergeom_ufunc NA. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.2. ipython_genutils 0.2.0. ipywidgets 8.0.6. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.23.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. louvain 0.8.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.4.0. nbformat 5.9.0. nbinom_ufunc NA. ncf_ufunc NA. numba 0.57.0. numexpr 2.8.4. numpy 1.24.3. objc 9.2. overrides NA. packaging 23.1. pandas 2.0.2. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.6.0. prometheus_client NA. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:3584,security,soc,socks,3584,"beta_ufunc NA. binom_ufunc NA. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. fastjsonschema NA. google NA. h5py 3.9.0. hypergeom_ufunc NA. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.2. ipython_genutils 0.2.0. ipywidgets 8.0.6. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.23.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. louvain 0.8.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.4.0. nbformat 5.9.0. nbinom_ufunc NA. ncf_ufunc NA. numba 0.57.0. numexpr 2.8.4. numpy 1.24.3. objc 9.2. overrides NA. packaging 23.1. pandas 2.0.2. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.6.0. prometheus_client NA. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pvectorc NA. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.1.0. pyrsistent NA. pythonjsonlogger NA. pytz 2023.3. requests 2.31.0. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. scipy 1.9.1. send2trash NA. session_info 1.0.0. setuptools 68.0.0. six 1.16.0. sklearn 1.2.2. sniffio 1.3.0. socks 1.7.1. stack_data 0.6.2. texttable 1.6.7. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. torch 1.12.1. tornado 6.3.2. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. urllib3 2.0.3. wcwidth 0.2.6. websocket 1.6.0. yaml 6.0. zipp NA. zmq 25.1.0. zoneinfo NA. -----. IPython 8.14.0. jupyter_client 8.2.0. jupyter_core 5.3.1. jupyterlab 4.0.2. notebook 6.5.4. -----. Python 3.9.16 | packaged by conda-forge | (main, Feb 1 2023, 21:42:20) [Clang 14.0.6 ]. macOS-12.6.6-x86_64-i386-64bit. -----. Session information updated at 2023-06-23 18:00. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:4084,security,Session,Session,4084,"beta_ufunc NA. binom_ufunc NA. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. fastjsonschema NA. google NA. h5py 3.9.0. hypergeom_ufunc NA. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.2. ipython_genutils 0.2.0. ipywidgets 8.0.6. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.23.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. louvain 0.8.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.4.0. nbformat 5.9.0. nbinom_ufunc NA. ncf_ufunc NA. numba 0.57.0. numexpr 2.8.4. numpy 1.24.3. objc 9.2. overrides NA. packaging 23.1. pandas 2.0.2. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.6.0. prometheus_client NA. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pvectorc NA. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.1.0. pyrsistent NA. pythonjsonlogger NA. pytz 2023.3. requests 2.31.0. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. scipy 1.9.1. send2trash NA. session_info 1.0.0. setuptools 68.0.0. six 1.16.0. sklearn 1.2.2. sniffio 1.3.0. socks 1.7.1. stack_data 0.6.2. texttable 1.6.7. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. torch 1.12.1. tornado 6.3.2. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. urllib3 2.0.3. wcwidth 0.2.6. websocket 1.6.0. yaml 6.0. zipp NA. zmq 25.1.0. zoneinfo NA. -----. IPython 8.14.0. jupyter_client 8.2.0. jupyter_core 5.3.1. jupyterlab 4.0.2. notebook 6.5.4. -----. Python 3.9.16 | packaged by conda-forge | (main, Feb 1 2023, 21:42:20) [Clang 14.0.6 ]. macOS-12.6.6-x86_64-i386-64bit. -----. Session information updated at 2023-06-23 18:00. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:4104,security,updat,updated,4104,"beta_ufunc NA. binom_ufunc NA. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. fastjsonschema NA. google NA. h5py 3.9.0. hypergeom_ufunc NA. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.2. ipython_genutils 0.2.0. ipywidgets 8.0.6. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.23.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. louvain 0.8.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.4.0. nbformat 5.9.0. nbinom_ufunc NA. ncf_ufunc NA. numba 0.57.0. numexpr 2.8.4. numpy 1.24.3. objc 9.2. overrides NA. packaging 23.1. pandas 2.0.2. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.6.0. prometheus_client NA. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pvectorc NA. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.1.0. pyrsistent NA. pythonjsonlogger NA. pytz 2023.3. requests 2.31.0. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. scipy 1.9.1. send2trash NA. session_info 1.0.0. setuptools 68.0.0. six 1.16.0. sklearn 1.2.2. sniffio 1.3.0. socks 1.7.1. stack_data 0.6.2. texttable 1.6.7. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. torch 1.12.1. tornado 6.3.2. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. urllib3 2.0.3. wcwidth 0.2.6. websocket 1.6.0. yaml 6.0. zipp NA. zmq 25.1.0. zoneinfo NA. -----. IPython 8.14.0. jupyter_client 8.2.0. jupyter_core 5.3.1. jupyterlab 4.0.2. notebook 6.5.4. -----. Python 3.9.16 | packaged by conda-forge | (main, Feb 1 2023, 21:42:20) [Clang 14.0.6 ]. macOS-12.6.6-x86_64-i386-64bit. -----. Session information updated at 2023-06-23 18:00. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:575,testability,simpl,simple,575,"PCA is hanging and taking way too long; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I just made a new environment with the latest version of scanpy (1.9.3), and was trying to run some basic analysis and the pca function is hanging and taking forever, when it used to be basically instantaneous. I tried reproducing it with the the simple tutorial and it is still hanging when I try to run PCA (takes longer than 10 minutes on the simple example dataset in the tutorial, I eventually just have to kill the process). . Not sure at all what the issue is, but wanted to bring it to your attention. This does not happen when using scanpy 1.9.1. Here is the output from the header logger:. scanpy==1.9.3 anndata==0.9.1 umap==0.5.3 numpy==1.24.3 scipy==1.9.1 pandas==2.0.2 scikit-learn==1.2.2 statsmodels==0.14.0 python-igraph==0.10.4 louvain==0.8.0 pynndescent==0.5.10. ### Minimal code sample. ```python. #Follow the same as the tutorial, or as a minimum example:. import numpy as np. import pandas as pd. import scanpy as sc. adata = sc.datasets.pbmc3k(). adata.var_names_make_unique() . sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3). adata.var['mt'] = adata.var_names.str.startswith('MT-'). sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). adata = adata[adata.obs.n_genes_by_counts < 2500, :]. adata = adata[adata.obs.pct_counts_mt < 5, :]. sc.pp.normalize_total(adata, target_sum=1e4). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). sc.tl.pca(adata, svd_solver='arpack'). ```. ### Error output. No error output, it just hangs. ```pytb. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. CoreFounda",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:674,testability,simpl,simple,674,"PCA is hanging and taking way too long; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I just made a new environment with the latest version of scanpy (1.9.3), and was trying to run some basic analysis and the pca function is hanging and taking forever, when it used to be basically instantaneous. I tried reproducing it with the the simple tutorial and it is still hanging when I try to run PCA (takes longer than 10 minutes on the simple example dataset in the tutorial, I eventually just have to kill the process). . Not sure at all what the issue is, but wanted to bring it to your attention. This does not happen when using scanpy 1.9.1. Here is the output from the header logger:. scanpy==1.9.3 anndata==0.9.1 umap==0.5.3 numpy==1.24.3 scipy==1.9.1 pandas==2.0.2 scikit-learn==1.2.2 statsmodels==0.14.0 python-igraph==0.10.4 louvain==0.8.0 pynndescent==0.5.10. ### Minimal code sample. ```python. #Follow the same as the tutorial, or as a minimum example:. import numpy as np. import pandas as pd. import scanpy as sc. adata = sc.datasets.pbmc3k(). adata.var_names_make_unique() . sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3). adata.var['mt'] = adata.var_names.str.startswith('MT-'). sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). adata = adata[adata.obs.n_genes_by_counts < 2500, :]. adata = adata[adata.obs.pct_counts_mt < 5, :]. sc.pp.normalize_total(adata, target_sum=1e4). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). sc.tl.pca(adata, svd_solver='arpack'). ```. ### Error output. No error output, it just hangs. ```pytb. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. CoreFounda",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:919,testability,log,logger,919,"PCA is hanging and taking way too long; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I just made a new environment with the latest version of scanpy (1.9.3), and was trying to run some basic analysis and the pca function is hanging and taking forever, when it used to be basically instantaneous. I tried reproducing it with the the simple tutorial and it is still hanging when I try to run PCA (takes longer than 10 minutes on the simple example dataset in the tutorial, I eventually just have to kill the process). . Not sure at all what the issue is, but wanted to bring it to your attention. This does not happen when using scanpy 1.9.1. Here is the output from the header logger:. scanpy==1.9.3 anndata==0.9.1 umap==0.5.3 numpy==1.24.3 scipy==1.9.1 pandas==2.0.2 scikit-learn==1.2.2 statsmodels==0.14.0 python-igraph==0.10.4 louvain==0.8.0 pynndescent==0.5.10. ### Minimal code sample. ```python. #Follow the same as the tutorial, or as a minimum example:. import numpy as np. import pandas as pd. import scanpy as sc. adata = sc.datasets.pbmc3k(). adata.var_names_make_unique() . sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3). adata.var['mt'] = adata.var_names.str.startswith('MT-'). sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). adata = adata[adata.obs.n_genes_by_counts < 2500, :]. adata = adata[adata.obs.pct_counts_mt < 5, :]. sc.pp.normalize_total(adata, target_sum=1e4). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). sc.tl.pca(adata, svd_solver='arpack'). ```. ### Error output. No error output, it just hangs. ```pytb. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. CoreFounda",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:168,usability,confirm,confirmed,168,"PCA is hanging and taking way too long; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I just made a new environment with the latest version of scanpy (1.9.3), and was trying to run some basic analysis and the pca function is hanging and taking forever, when it used to be basically instantaneous. I tried reproducing it with the the simple tutorial and it is still hanging when I try to run PCA (takes longer than 10 minutes on the simple example dataset in the tutorial, I eventually just have to kill the process). . Not sure at all what the issue is, but wanted to bring it to your attention. This does not happen when using scanpy 1.9.1. Here is the output from the header logger:. scanpy==1.9.3 anndata==0.9.1 umap==0.5.3 numpy==1.24.3 scipy==1.9.1 pandas==2.0.2 scikit-learn==1.2.2 statsmodels==0.14.0 python-igraph==0.10.4 louvain==0.8.0 pynndescent==0.5.10. ### Minimal code sample. ```python. #Follow the same as the tutorial, or as a minimum example:. import numpy as np. import pandas as pd. import scanpy as sc. adata = sc.datasets.pbmc3k(). adata.var_names_make_unique() . sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3). adata.var['mt'] = adata.var_names.str.startswith('MT-'). sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). adata = adata[adata.obs.n_genes_by_counts < 2500, :]. adata = adata[adata.obs.pct_counts_mt < 5, :]. sc.pp.normalize_total(adata, target_sum=1e4). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). sc.tl.pca(adata, svd_solver='arpack'). ```. ### Error output. No error output, it just hangs. ```pytb. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. CoreFounda",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:251,usability,confirm,confirmed,251,"PCA is hanging and taking way too long; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I just made a new environment with the latest version of scanpy (1.9.3), and was trying to run some basic analysis and the pca function is hanging and taking forever, when it used to be basically instantaneous. I tried reproducing it with the the simple tutorial and it is still hanging when I try to run PCA (takes longer than 10 minutes on the simple example dataset in the tutorial, I eventually just have to kill the process). . Not sure at all what the issue is, but wanted to bring it to your attention. This does not happen when using scanpy 1.9.1. Here is the output from the header logger:. scanpy==1.9.3 anndata==0.9.1 umap==0.5.3 numpy==1.24.3 scipy==1.9.1 pandas==2.0.2 scikit-learn==1.2.2 statsmodels==0.14.0 python-igraph==0.10.4 louvain==0.8.0 pynndescent==0.5.10. ### Minimal code sample. ```python. #Follow the same as the tutorial, or as a minimum example:. import numpy as np. import pandas as pd. import scanpy as sc. adata = sc.datasets.pbmc3k(). adata.var_names_make_unique() . sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3). adata.var['mt'] = adata.var_names.str.startswith('MT-'). sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). adata = adata[adata.obs.n_genes_by_counts < 2500, :]. adata = adata[adata.obs.pct_counts_mt < 5, :]. sc.pp.normalize_total(adata, target_sum=1e4). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). sc.tl.pca(adata, svd_solver='arpack'). ```. ### Error output. No error output, it just hangs. ```pytb. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. CoreFounda",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:575,usability,simpl,simple,575,"PCA is hanging and taking way too long; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I just made a new environment with the latest version of scanpy (1.9.3), and was trying to run some basic analysis and the pca function is hanging and taking forever, when it used to be basically instantaneous. I tried reproducing it with the the simple tutorial and it is still hanging when I try to run PCA (takes longer than 10 minutes on the simple example dataset in the tutorial, I eventually just have to kill the process). . Not sure at all what the issue is, but wanted to bring it to your attention. This does not happen when using scanpy 1.9.1. Here is the output from the header logger:. scanpy==1.9.3 anndata==0.9.1 umap==0.5.3 numpy==1.24.3 scipy==1.9.1 pandas==2.0.2 scikit-learn==1.2.2 statsmodels==0.14.0 python-igraph==0.10.4 louvain==0.8.0 pynndescent==0.5.10. ### Minimal code sample. ```python. #Follow the same as the tutorial, or as a minimum example:. import numpy as np. import pandas as pd. import scanpy as sc. adata = sc.datasets.pbmc3k(). adata.var_names_make_unique() . sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3). adata.var['mt'] = adata.var_names.str.startswith('MT-'). sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). adata = adata[adata.obs.n_genes_by_counts < 2500, :]. adata = adata[adata.obs.pct_counts_mt < 5, :]. sc.pp.normalize_total(adata, target_sum=1e4). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). sc.tl.pca(adata, svd_solver='arpack'). ```. ### Error output. No error output, it just hangs. ```pytb. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. CoreFounda",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:674,usability,simpl,simple,674,"PCA is hanging and taking way too long; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I just made a new environment with the latest version of scanpy (1.9.3), and was trying to run some basic analysis and the pca function is hanging and taking forever, when it used to be basically instantaneous. I tried reproducing it with the the simple tutorial and it is still hanging when I try to run PCA (takes longer than 10 minutes on the simple example dataset in the tutorial, I eventually just have to kill the process). . Not sure at all what the issue is, but wanted to bring it to your attention. This does not happen when using scanpy 1.9.1. Here is the output from the header logger:. scanpy==1.9.3 anndata==0.9.1 umap==0.5.3 numpy==1.24.3 scipy==1.9.1 pandas==2.0.2 scikit-learn==1.2.2 statsmodels==0.14.0 python-igraph==0.10.4 louvain==0.8.0 pynndescent==0.5.10. ### Minimal code sample. ```python. #Follow the same as the tutorial, or as a minimum example:. import numpy as np. import pandas as pd. import scanpy as sc. adata = sc.datasets.pbmc3k(). adata.var_names_make_unique() . sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3). adata.var['mt'] = adata.var_names.str.startswith('MT-'). sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). adata = adata[adata.obs.n_genes_by_counts < 2500, :]. adata = adata[adata.obs.pct_counts_mt < 5, :]. sc.pp.normalize_total(adata, target_sum=1e4). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). sc.tl.pca(adata, svd_solver='arpack'). ```. ### Error output. No error output, it just hangs. ```pytb. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. CoreFounda",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:1017,usability,learn,learn,1017,"aking way too long; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I just made a new environment with the latest version of scanpy (1.9.3), and was trying to run some basic analysis and the pca function is hanging and taking forever, when it used to be basically instantaneous. I tried reproducing it with the the simple tutorial and it is still hanging when I try to run PCA (takes longer than 10 minutes on the simple example dataset in the tutorial, I eventually just have to kill the process). . Not sure at all what the issue is, but wanted to bring it to your attention. This does not happen when using scanpy 1.9.1. Here is the output from the header logger:. scanpy==1.9.3 anndata==0.9.1 umap==0.5.3 numpy==1.24.3 scipy==1.9.1 pandas==2.0.2 scikit-learn==1.2.2 statsmodels==0.14.0 python-igraph==0.10.4 louvain==0.8.0 pynndescent==0.5.10. ### Minimal code sample. ```python. #Follow the same as the tutorial, or as a minimum example:. import numpy as np. import pandas as pd. import scanpy as sc. adata = sc.datasets.pbmc3k(). adata.var_names_make_unique() . sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3). adata.var['mt'] = adata.var_names.str.startswith('MT-'). sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). adata = adata[adata.obs.n_genes_by_counts < 2500, :]. adata = adata[adata.obs.pct_counts_mt < 5, :]. sc.pp.normalize_total(adata, target_sum=1e4). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). sc.tl.pca(adata, svd_solver='arpack'). ```. ### Error output. No error output, it just hangs. ```pytb. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. CoreFoundation NA. Foundation ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:1112,usability,Minim,Minimal,1112,"s issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I just made a new environment with the latest version of scanpy (1.9.3), and was trying to run some basic analysis and the pca function is hanging and taking forever, when it used to be basically instantaneous. I tried reproducing it with the the simple tutorial and it is still hanging when I try to run PCA (takes longer than 10 minutes on the simple example dataset in the tutorial, I eventually just have to kill the process). . Not sure at all what the issue is, but wanted to bring it to your attention. This does not happen when using scanpy 1.9.1. Here is the output from the header logger:. scanpy==1.9.3 anndata==0.9.1 umap==0.5.3 numpy==1.24.3 scipy==1.9.1 pandas==2.0.2 scikit-learn==1.2.2 statsmodels==0.14.0 python-igraph==0.10.4 louvain==0.8.0 pynndescent==0.5.10. ### Minimal code sample. ```python. #Follow the same as the tutorial, or as a minimum example:. import numpy as np. import pandas as pd. import scanpy as sc. adata = sc.datasets.pbmc3k(). adata.var_names_make_unique() . sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3). adata.var['mt'] = adata.var_names.str.startswith('MT-'). sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). adata = adata[adata.obs.n_genes_by_counts < 2500, :]. adata = adata[adata.obs.pct_counts_mt < 5, :]. sc.pp.normalize_total(adata, target_sum=1e4). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). sc.tl.pca(adata, svd_solver='arpack'). ```. ### Error output. No error output, it just hangs. ```pytb. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. CoreFoundation NA. Foundation NA. PIL 9.5.0. PyObjCTools NA. anyio NA. appnope 0.1.3. argcomplete NA. asttokens NA. attr 23.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:1186,usability,minim,minimum,1186,"sts on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I just made a new environment with the latest version of scanpy (1.9.3), and was trying to run some basic analysis and the pca function is hanging and taking forever, when it used to be basically instantaneous. I tried reproducing it with the the simple tutorial and it is still hanging when I try to run PCA (takes longer than 10 minutes on the simple example dataset in the tutorial, I eventually just have to kill the process). . Not sure at all what the issue is, but wanted to bring it to your attention. This does not happen when using scanpy 1.9.1. Here is the output from the header logger:. scanpy==1.9.3 anndata==0.9.1 umap==0.5.3 numpy==1.24.3 scipy==1.9.1 pandas==2.0.2 scikit-learn==1.2.2 statsmodels==0.14.0 python-igraph==0.10.4 louvain==0.8.0 pynndescent==0.5.10. ### Minimal code sample. ```python. #Follow the same as the tutorial, or as a minimum example:. import numpy as np. import pandas as pd. import scanpy as sc. adata = sc.datasets.pbmc3k(). adata.var_names_make_unique() . sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3). adata.var['mt'] = adata.var_names.str.startswith('MT-'). sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). adata = adata[adata.obs.n_genes_by_counts < 2500, :]. adata = adata[adata.obs.pct_counts_mt < 5, :]. sc.pp.normalize_total(adata, target_sum=1e4). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). sc.tl.pca(adata, svd_solver='arpack'). ```. ### Error output. No error output, it just hangs. ```pytb. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. CoreFoundation NA. Foundation NA. PIL 9.5.0. PyObjCTools NA. anyio NA. appnope 0.1.3. argcomplete NA. asttokens NA. attr 23.1.0. babel 2.12.1. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. certifi 20",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:1857,usability,Error,Error,1857,"hen using scanpy 1.9.1. Here is the output from the header logger:. scanpy==1.9.3 anndata==0.9.1 umap==0.5.3 numpy==1.24.3 scipy==1.9.1 pandas==2.0.2 scikit-learn==1.2.2 statsmodels==0.14.0 python-igraph==0.10.4 louvain==0.8.0 pynndescent==0.5.10. ### Minimal code sample. ```python. #Follow the same as the tutorial, or as a minimum example:. import numpy as np. import pandas as pd. import scanpy as sc. adata = sc.datasets.pbmc3k(). adata.var_names_make_unique() . sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3). adata.var['mt'] = adata.var_names.str.startswith('MT-'). sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). adata = adata[adata.obs.n_genes_by_counts < 2500, :]. adata = adata[adata.obs.pct_counts_mt < 5, :]. sc.pp.normalize_total(adata, target_sum=1e4). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). sc.tl.pca(adata, svd_solver='arpack'). ```. ### Error output. No error output, it just hangs. ```pytb. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. CoreFoundation NA. Foundation NA. PIL 9.5.0. PyObjCTools NA. anyio NA. appnope 0.1.3. argcomplete NA. asttokens NA. attr 23.1.0. babel 2.12.1. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. fastjsonschema NA. google NA. h5py 3.9.0. hypergeom_ufunc NA. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.2. ipython_genutils 0.2.0. ipywidgets 8.0.6. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.23.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. louvain 0.8.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:1874,usability,error,error,1874,"1.9.1. Here is the output from the header logger:. scanpy==1.9.3 anndata==0.9.1 umap==0.5.3 numpy==1.24.3 scipy==1.9.1 pandas==2.0.2 scikit-learn==1.2.2 statsmodels==0.14.0 python-igraph==0.10.4 louvain==0.8.0 pynndescent==0.5.10. ### Minimal code sample. ```python. #Follow the same as the tutorial, or as a minimum example:. import numpy as np. import pandas as pd. import scanpy as sc. adata = sc.datasets.pbmc3k(). adata.var_names_make_unique() . sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3). adata.var['mt'] = adata.var_names.str.startswith('MT-'). sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). adata = adata[adata.obs.n_genes_by_counts < 2500, :]. adata = adata[adata.obs.pct_counts_mt < 5, :]. sc.pp.normalize_total(adata, target_sum=1e4). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). sc.tl.pca(adata, svd_solver='arpack'). ```. ### Error output. No error output, it just hangs. ```pytb. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. CoreFoundation NA. Foundation NA. PIL 9.5.0. PyObjCTools NA. anyio NA. appnope 0.1.3. argcomplete NA. asttokens NA. attr 23.1.0. babel 2.12.1. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. fastjsonschema NA. google NA. h5py 3.9.0. hypergeom_ufunc NA. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.2. ipython_genutils 0.2.0. ipywidgets 8.0.6. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.23.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. louvain 0.8.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.4.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/issues/2531:3665,usability,tool,toolz,3665,"beta_ufunc NA. binom_ufunc NA. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. fastjsonschema NA. google NA. h5py 3.9.0. hypergeom_ufunc NA. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.2. ipython_genutils 0.2.0. ipywidgets 8.0.6. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.23.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. louvain 0.8.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. natsort 8.4.0. nbformat 5.9.0. nbinom_ufunc NA. ncf_ufunc NA. numba 0.57.0. numexpr 2.8.4. numpy 1.24.3. objc 9.2. overrides NA. packaging 23.1. pandas 2.0.2. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.6.0. prometheus_client NA. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pvectorc NA. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.1.0. pyrsistent NA. pythonjsonlogger NA. pytz 2023.3. requests 2.31.0. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. scipy 1.9.1. send2trash NA. session_info 1.0.0. setuptools 68.0.0. six 1.16.0. sklearn 1.2.2. sniffio 1.3.0. socks 1.7.1. stack_data 0.6.2. texttable 1.6.7. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. torch 1.12.1. tornado 6.3.2. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. urllib3 2.0.3. wcwidth 0.2.6. websocket 1.6.0. yaml 6.0. zipp NA. zmq 25.1.0. zoneinfo NA. -----. IPython 8.14.0. jupyter_client 8.2.0. jupyter_core 5.3.1. jupyterlab 4.0.2. notebook 6.5.4. -----. Python 3.9.16 | packaged by conda-forge | (main, Feb 1 2023, 21:42:20) [Clang 14.0.6 ]. macOS-12.6.6-x86_64-i386-64bit. -----. Session information updated at 2023-06-23 18:00. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531
https://github.com/scverse/scanpy/pull/2532:4,safety,test,test,4,"Add test for reading visium 2; See #2499, #2345",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2532
https://github.com/scverse/scanpy/pull/2532:4,testability,test,test,4,"Add test for reading visium 2; See #2499, #2345",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2532
https://github.com/scverse/scanpy/pull/2536:1960,availability,mask,mask,1960,"alse`*? It would make sense for e.g. the scipy knn transformer with `algorithm='kd_tree'`  . https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L614-L617. - I also changed `method` to only mean connectivity method. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR. - [x] figure out what the `_more_tags` methods are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the logic shouldnt have changed*. - pre-computed in umap transformer; handling of disconnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont tes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:64,deployability,API,APIs,64,"Make `neighbors` work based on KNeighborsTransformer-compatible APIs; [`sklearn.neighbors.KNeighborsTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsTransformer.html) is a good abstraction for KNN calculations. We could follow the suggestion here, then finish this PR https://github.com/frankier/sklearn-ann/issues/8#issuecomment-1609553421. ### For reviewing. - most functions were moved unchanged. - I extensively changed https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L471. - This uncovered very grown-over-time logic: https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L522-L523 . *Should we change that logic for more predictability at the expense of backwards compatibility*? Especially the euclidean condition makes not much sense IMHO. - Another piece of weird logic: *Where to allow`knn=False`*? It would make sense for e.g. the scipy knn transformer with `algorithm='kd_tree'`  . https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L614-L617. - I also changed `method` to only mean connectivity method. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR. - [x] figure out what the `_more_tags` methods are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the log",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:146,deployability,modul,modules,146,"Make `neighbors` work based on KNeighborsTransformer-compatible APIs; [`sklearn.neighbors.KNeighborsTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsTransformer.html) is a good abstraction for KNN calculations. We could follow the suggestion here, then finish this PR https://github.com/frankier/sklearn-ann/issues/8#issuecomment-1609553421. ### For reviewing. - most functions were moved unchanged. - I extensively changed https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L471. - This uncovered very grown-over-time logic: https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L522-L523 . *Should we change that logic for more predictability at the expense of backwards compatibility*? Especially the euclidean condition makes not much sense IMHO. - Another piece of weird logic: *Where to allow`knn=False`*? It would make sense for e.g. the scipy knn transformer with `algorithm='kd_tree'`  . https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L614-L617. - I also changed `method` to only mean connectivity method. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR. - [x] figure out what the `_more_tags` methods are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the log",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:620,deployability,log,logic,620,"Make `neighbors` work based on KNeighborsTransformer-compatible APIs; [`sklearn.neighbors.KNeighborsTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsTransformer.html) is a good abstraction for KNN calculations. We could follow the suggestion here, then finish this PR https://github.com/frankier/sklearn-ann/issues/8#issuecomment-1609553421. ### For reviewing. - most functions were moved unchanged. - I extensively changed https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L471. - This uncovered very grown-over-time logic: https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L522-L523 . *Should we change that logic for more predictability at the expense of backwards compatibility*? Especially the euclidean condition makes not much sense IMHO. - Another piece of weird logic: *Where to allow`knn=False`*? It would make sense for e.g. the scipy knn transformer with `algorithm='kd_tree'`  . https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L614-L617. - I also changed `method` to only mean connectivity method. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR. - [x] figure out what the `_more_tags` methods are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the log",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:771,deployability,log,logic,771,"Make `neighbors` work based on KNeighborsTransformer-compatible APIs; [`sklearn.neighbors.KNeighborsTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsTransformer.html) is a good abstraction for KNN calculations. We could follow the suggestion here, then finish this PR https://github.com/frankier/sklearn-ann/issues/8#issuecomment-1609553421. ### For reviewing. - most functions were moved unchanged. - I extensively changed https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L471. - This uncovered very grown-over-time logic: https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L522-L523 . *Should we change that logic for more predictability at the expense of backwards compatibility*? Especially the euclidean condition makes not much sense IMHO. - Another piece of weird logic: *Where to allow`knn=False`*? It would make sense for e.g. the scipy knn transformer with `algorithm='kd_tree'`  . https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L614-L617. - I also changed `method` to only mean connectivity method. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR. - [x] figure out what the `_more_tags` methods are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the log",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:934,deployability,log,logic,934,"Make `neighbors` work based on KNeighborsTransformer-compatible APIs; [`sklearn.neighbors.KNeighborsTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsTransformer.html) is a good abstraction for KNN calculations. We could follow the suggestion here, then finish this PR https://github.com/frankier/sklearn-ann/issues/8#issuecomment-1609553421. ### For reviewing. - most functions were moved unchanged. - I extensively changed https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L471. - This uncovered very grown-over-time logic: https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L522-L523 . *Should we change that logic for more predictability at the expense of backwards compatibility*? Especially the euclidean condition makes not much sense IMHO. - Another piece of weird logic: *Where to allow`knn=False`*? It would make sense for e.g. the scipy knn transformer with `algorithm='kd_tree'`  . https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L614-L617. - I also changed `method` to only mean connectivity method. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR. - [x] figure out what the `_more_tags` methods are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the log",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:1942,deployability,build,build,1942,"re to allow`knn=False`*? It would make sense for e.g. the scipy knn transformer with `algorithm='kd_tree'`  . https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L614-L617. - I also changed `method` to only mean connectivity method. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR. - [x] figure out what the `_more_tags` methods are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the logic shouldnt have changed*. - pre-computed in umap transformer; handling of disconnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, w",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:1997,deployability,log,logic,1997,"he scipy knn transformer with `algorithm='kd_tree'`  . https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L614-L617. - I also changed `method` to only mean connectivity method. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR. - [x] figure out what the `_more_tags` methods are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the logic shouldnt have changed*. - pre-computed in umap transformer; handling of disconnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont test for: . ![grafik](https://github.com/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:2219,deployability,depend,depend,2219,"tivity method. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR. - [x] figure out what the `_more_tags` methods are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the logic shouldnt have changed*. - pre-computed in umap transformer; handling of disconnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont test for: . ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972fad65a3e). The logic was:. - if small data (<8000) and euclidean metric or knn==False, calculate `pairwise_distances`. - if knn=True, then sparsify that matrix by pu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:3072,deployability,log,logic,3072," disconnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont test for: . ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972fad65a3e). The logic was:. - if small data (<8000) and euclidean metric or knn==False, calculate `pairwise_distances`. - if knn=True, then sparsify that matrix by pulling out KNN using `_get_indices_distances_from_dense_matrix` and converting that into a sparse one. - if not, run `umap.nearest_neighbors`. - if even smaller data (<4000), calculate `pairwise_distances` like above and run `umap.nearest_neighbors` with `metric='precomputed'`. its internal logic then does the same as `_get_indices_distances_from_dense_matrix`. - else run `umap.nearest_neighbors` directly, which simply runs PyNNDescent with `n_trees` and `n_iters` set with a heuristic. ### now the logic is. - if small data (for either definition of small), calculate `pairwise_distances`, sparsify on demand. (this covers both our previous optimization and the umap `'precomputed'` code path). - else run a KNN transformer. if the old `'umap'` method was chosen, pick the same parameters mentioned aboveto the `PyNNDescent` transformer. ---. Fixes ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:3513,deployability,log,logic,3513,"onnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont test for: . ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972fad65a3e). The logic was:. - if small data (<8000) and euclidean metric or knn==False, calculate `pairwise_distances`. - if knn=True, then sparsify that matrix by pulling out KNN using `_get_indices_distances_from_dense_matrix` and converting that into a sparse one. - if not, run `umap.nearest_neighbors`. - if even smaller data (<4000), calculate `pairwise_distances` like above and run `umap.nearest_neighbors` with `metric='precomputed'`. its internal logic then does the same as `_get_indices_distances_from_dense_matrix`. - else run `umap.nearest_neighbors` directly, which simply runs PyNNDescent with `n_trees` and `n_iters` set with a heuristic. ### now the logic is. - if small data (for either definition of small), calculate `pairwise_distances`, sparsify on demand. (this covers both our previous optimization and the umap `'precomputed'` code path). - else run a KNN transformer. if the old `'umap'` method was chosen, pick the same parameters mentioned aboveto the `PyNNDescent` transformer. ---. Fixes #2519",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:3724,deployability,log,logic,3724,"onnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont test for: . ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972fad65a3e). The logic was:. - if small data (<8000) and euclidean metric or knn==False, calculate `pairwise_distances`. - if knn=True, then sparsify that matrix by pulling out KNN using `_get_indices_distances_from_dense_matrix` and converting that into a sparse one. - if not, run `umap.nearest_neighbors`. - if even smaller data (<4000), calculate `pairwise_distances` like above and run `umap.nearest_neighbors` with `metric='precomputed'`. its internal logic then does the same as `_get_indices_distances_from_dense_matrix`. - else run `umap.nearest_neighbors` directly, which simply runs PyNNDescent with `n_trees` and `n_iters` set with a heuristic. ### now the logic is. - if small data (for either definition of small), calculate `pairwise_distances`, sparsify on demand. (this covers both our previous optimization and the umap `'precomputed'` code path). - else run a KNN transformer. if the old `'umap'` method was chosen, pick the same parameters mentioned aboveto the `PyNNDescent` transformer. ---. Fixes #2519",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:786,energy efficiency,predict,predictability,786,"Make `neighbors` work based on KNeighborsTransformer-compatible APIs; [`sklearn.neighbors.KNeighborsTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsTransformer.html) is a good abstraction for KNN calculations. We could follow the suggestion here, then finish this PR https://github.com/frankier/sklearn-ann/issues/8#issuecomment-1609553421. ### For reviewing. - most functions were moved unchanged. - I extensively changed https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L471. - This uncovered very grown-over-time logic: https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L522-L523 . *Should we change that logic for more predictability at the expense of backwards compatibility*? Especially the euclidean condition makes not much sense IMHO. - Another piece of weird logic: *Where to allow`knn=False`*? It would make sense for e.g. the scipy knn transformer with `algorithm='kd_tree'`  . https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L614-L617. - I also changed `method` to only mean connectivity method. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR. - [x] figure out what the `_more_tags` methods are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the log",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:2725,energy efficiency,reduc,reduced,2725,"e be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the logic shouldnt have changed*. - pre-computed in umap transformer; handling of disconnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont test for: . ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972fad65a3e). The logic was:. - if small data (<8000) and euclidean metric or knn==False, calculate `pairwise_distances`. - if knn=True, then sparsify that matrix by pulling out KNN using `_get_indices_distances_from_dense_matrix` and converting that into a sparse one. - if not, run `umap.nearest_neighbors`. - if even smaller data (<4000), calculate `pairwise_distances` like above and run `umap.nearest_neighbors` with `metric='precomputed'`. its internal logic then does the same as `_get_indices_distances_from_dense_matrix`. - else run `umap.nearest_neighbors` directly, which simply runs PyNNDescent with `n_trees` and `n_iters` set with a heuristic. ### now the logic",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:3867,energy efficiency,optim,optimization,3867,"onnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont test for: . ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972fad65a3e). The logic was:. - if small data (<8000) and euclidean metric or knn==False, calculate `pairwise_distances`. - if knn=True, then sparsify that matrix by pulling out KNN using `_get_indices_distances_from_dense_matrix` and converting that into a sparse one. - if not, run `umap.nearest_neighbors`. - if even smaller data (<4000), calculate `pairwise_distances` like above and run `umap.nearest_neighbors` with `metric='precomputed'`. its internal logic then does the same as `_get_indices_distances_from_dense_matrix`. - else run `umap.nearest_neighbors` directly, which simply runs PyNNDescent with `n_trees` and `n_iters` set with a heuristic. ### now the logic is. - if small data (for either definition of small), calculate `pairwise_distances`, sparsify on demand. (this covers both our previous optimization and the umap `'precomputed'` code path). - else run a KNN transformer. if the old `'umap'` method was chosen, pick the same parameters mentioned aboveto the `PyNNDescent` transformer. ---. Fixes #2519",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:64,integrability,API,APIs,64,"Make `neighbors` work based on KNeighborsTransformer-compatible APIs; [`sklearn.neighbors.KNeighborsTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsTransformer.html) is a good abstraction for KNN calculations. We could follow the suggestion here, then finish this PR https://github.com/frankier/sklearn-ann/issues/8#issuecomment-1609553421. ### For reviewing. - most functions were moved unchanged. - I extensively changed https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L471. - This uncovered very grown-over-time logic: https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L522-L523 . *Should we change that logic for more predictability at the expense of backwards compatibility*? Especially the euclidean condition makes not much sense IMHO. - Another piece of weird logic: *Where to allow`knn=False`*? It would make sense for e.g. the scipy knn transformer with `algorithm='kd_tree'`  . https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L614-L617. - I also changed `method` to only mean connectivity method. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR. - [x] figure out what the `_more_tags` methods are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the log",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:220,integrability,abstract,abstraction,220,"Make `neighbors` work based on KNeighborsTransformer-compatible APIs; [`sklearn.neighbors.KNeighborsTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsTransformer.html) is a good abstraction for KNN calculations. We could follow the suggestion here, then finish this PR https://github.com/frankier/sklearn-ann/issues/8#issuecomment-1609553421. ### For reviewing. - most functions were moved unchanged. - I extensively changed https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L471. - This uncovered very grown-over-time logic: https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L522-L523 . *Should we change that logic for more predictability at the expense of backwards compatibility*? Especially the euclidean condition makes not much sense IMHO. - Another piece of weird logic: *Where to allow`knn=False`*? It would make sense for e.g. the scipy knn transformer with `algorithm='kd_tree'`  . https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L614-L617. - I also changed `method` to only mean connectivity method. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR. - [x] figure out what the `_more_tags` methods are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the log",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:1013,integrability,transform,transformer,1013,"rk based on KNeighborsTransformer-compatible APIs; [`sklearn.neighbors.KNeighborsTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsTransformer.html) is a good abstraction for KNN calculations. We could follow the suggestion here, then finish this PR https://github.com/frankier/sklearn-ann/issues/8#issuecomment-1609553421. ### For reviewing. - most functions were moved unchanged. - I extensively changed https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L471. - This uncovered very grown-over-time logic: https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L522-L523 . *Should we change that logic for more predictability at the expense of backwards compatibility*? Especially the euclidean condition makes not much sense IMHO. - Another piece of weird logic: *Where to allow`knn=False`*? It would make sense for e.g. the scipy knn transformer with `algorithm='kd_tree'`  . https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L614-L617. - I also changed `method` to only mean connectivity method. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR. - [x] figure out what the `_more_tags` methods are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the logic shouldnt have c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:2051,integrability,transform,transformer,2051,"ttps://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L614-L617. - I also changed `method` to only mean connectivity method. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR. - [x] figure out what the `_more_tags` methods are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the logic shouldnt have changed*. - pre-computed in umap transformer; handling of disconnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont test for: . ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:2089,integrability,compon,components,2089,"/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L614-L617. - I also changed `method` to only mean connectivity method. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR. - [x] figure out what the `_more_tags` methods are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the logic shouldnt have changed*. - pre-computed in umap transformer; handling of disconnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont test for: . ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972fad65a3e). The logic was:. - if small",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:2219,integrability,depend,depend,2219,"tivity method. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR. - [x] figure out what the `_more_tags` methods are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the logic shouldnt have changed*. - pre-computed in umap transformer; handling of disconnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont test for: . ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972fad65a3e). The logic was:. - if small data (<8000) and euclidean metric or knn==False, calculate `pairwise_distances`. - if knn=True, then sparsify that matrix by pu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:2406,integrability,transform,transformer,2406," are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the logic shouldnt have changed*. - pre-computed in umap transformer; handling of disconnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont test for: . ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972fad65a3e). The logic was:. - if small data (<8000) and euclidean metric or knn==False, calculate `pairwise_distances`. - if knn=True, then sparsify that matrix by pulling out KNN using `_get_indices_distances_from_dense_matrix` and converting that into a sparse one. - if not, run `umap.nearest_neighbors`. - if even smaller data (<4000), calculate `pairw",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:2526,integrability,wrap,wrapper,2526,". - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the logic shouldnt have changed*. - pre-computed in umap transformer; handling of disconnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont test for: . ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972fad65a3e). The logic was:. - if small data (<8000) and euclidean metric or knn==False, calculate `pairwise_distances`. - if knn=True, then sparsify that matrix by pulling out KNN using `_get_indices_distances_from_dense_matrix` and converting that into a sparse one. - if not, run `umap.nearest_neighbors`. - if even smaller data (<4000), calculate `pairwise_distances` like above and run `umap.nearest_neighbors` with `metric='precomputed'`. its internal logic then does t",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:2602,integrability,wrap,wrapper,2602,"gorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the logic shouldnt have changed*. - pre-computed in umap transformer; handling of disconnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont test for: . ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972fad65a3e). The logic was:. - if small data (<8000) and euclidean metric or knn==False, calculate `pairwise_distances`. - if knn=True, then sparsify that matrix by pulling out KNN using `_get_indices_distances_from_dense_matrix` and converting that into a sparse one. - if not, run `umap.nearest_neighbors`. - if even smaller data (<4000), calculate `pairwise_distances` like above and run `umap.nearest_neighbors` with `metric='precomputed'`. its internal logic then does the same as `_get_indices_distances_from_dense_matrix`. - else run `umap.near",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:3938,integrability,transform,transformer,3938,"onnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont test for: . ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972fad65a3e). The logic was:. - if small data (<8000) and euclidean metric or knn==False, calculate `pairwise_distances`. - if knn=True, then sparsify that matrix by pulling out KNN using `_get_indices_distances_from_dense_matrix` and converting that into a sparse one. - if not, run `umap.nearest_neighbors`. - if even smaller data (<4000), calculate `pairwise_distances` like above and run `umap.nearest_neighbors` with `metric='precomputed'`. its internal logic then does the same as `_get_indices_distances_from_dense_matrix`. - else run `umap.nearest_neighbors` directly, which simply runs PyNNDescent with `n_trees` and `n_iters` set with a heuristic. ### now the logic is. - if small data (for either definition of small), calculate `pairwise_distances`, sparsify on demand. (this covers both our previous optimization and the umap `'precomputed'` code path). - else run a KNN transformer. if the old `'umap'` method was chosen, pick the same parameters mentioned aboveto the `PyNNDescent` transformer. ---. Fixes #2519",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:4051,integrability,transform,transformer,4051,"onnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont test for: . ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972fad65a3e). The logic was:. - if small data (<8000) and euclidean metric or knn==False, calculate `pairwise_distances`. - if knn=True, then sparsify that matrix by pulling out KNN using `_get_indices_distances_from_dense_matrix` and converting that into a sparse one. - if not, run `umap.nearest_neighbors`. - if even smaller data (<4000), calculate `pairwise_distances` like above and run `umap.nearest_neighbors` with `metric='precomputed'`. its internal logic then does the same as `_get_indices_distances_from_dense_matrix`. - else run `umap.nearest_neighbors` directly, which simply runs PyNNDescent with `n_trees` and `n_iters` set with a heuristic. ### now the logic is. - if small data (for either definition of small), calculate `pairwise_distances`, sparsify on demand. (this covers both our previous optimization and the umap `'precomputed'` code path). - else run a KNN transformer. if the old `'umap'` method was chosen, pick the same parameters mentioned aboveto the `PyNNDescent` transformer. ---. Fixes #2519",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:53,interoperability,compatib,compatible,53,"Make `neighbors` work based on KNeighborsTransformer-compatible APIs; [`sklearn.neighbors.KNeighborsTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsTransformer.html) is a good abstraction for KNN calculations. We could follow the suggestion here, then finish this PR https://github.com/frankier/sklearn-ann/issues/8#issuecomment-1609553421. ### For reviewing. - most functions were moved unchanged. - I extensively changed https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L471. - This uncovered very grown-over-time logic: https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L522-L523 . *Should we change that logic for more predictability at the expense of backwards compatibility*? Especially the euclidean condition makes not much sense IMHO. - Another piece of weird logic: *Where to allow`knn=False`*? It would make sense for e.g. the scipy knn transformer with `algorithm='kd_tree'`  . https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L614-L617. - I also changed `method` to only mean connectivity method. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR. - [x] figure out what the `_more_tags` methods are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the log",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:64,interoperability,API,APIs,64,"Make `neighbors` work based on KNeighborsTransformer-compatible APIs; [`sklearn.neighbors.KNeighborsTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsTransformer.html) is a good abstraction for KNN calculations. We could follow the suggestion here, then finish this PR https://github.com/frankier/sklearn-ann/issues/8#issuecomment-1609553421. ### For reviewing. - most functions were moved unchanged. - I extensively changed https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L471. - This uncovered very grown-over-time logic: https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L522-L523 . *Should we change that logic for more predictability at the expense of backwards compatibility*? Especially the euclidean condition makes not much sense IMHO. - Another piece of weird logic: *Where to allow`knn=False`*? It would make sense for e.g. the scipy knn transformer with `algorithm='kd_tree'`  . https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L614-L617. - I also changed `method` to only mean connectivity method. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR. - [x] figure out what the `_more_tags` methods are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the log",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:829,interoperability,compatib,compatibility,829,"Make `neighbors` work based on KNeighborsTransformer-compatible APIs; [`sklearn.neighbors.KNeighborsTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsTransformer.html) is a good abstraction for KNN calculations. We could follow the suggestion here, then finish this PR https://github.com/frankier/sklearn-ann/issues/8#issuecomment-1609553421. ### For reviewing. - most functions were moved unchanged. - I extensively changed https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L471. - This uncovered very grown-over-time logic: https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L522-L523 . *Should we change that logic for more predictability at the expense of backwards compatibility*? Especially the euclidean condition makes not much sense IMHO. - Another piece of weird logic: *Where to allow`knn=False`*? It would make sense for e.g. the scipy knn transformer with `algorithm='kd_tree'`  . https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L614-L617. - I also changed `method` to only mean connectivity method. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR. - [x] figure out what the `_more_tags` methods are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the log",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:1013,interoperability,transform,transformer,1013,"rk based on KNeighborsTransformer-compatible APIs; [`sklearn.neighbors.KNeighborsTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsTransformer.html) is a good abstraction for KNN calculations. We could follow the suggestion here, then finish this PR https://github.com/frankier/sklearn-ann/issues/8#issuecomment-1609553421. ### For reviewing. - most functions were moved unchanged. - I extensively changed https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L471. - This uncovered very grown-over-time logic: https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L522-L523 . *Should we change that logic for more predictability at the expense of backwards compatibility*? Especially the euclidean condition makes not much sense IMHO. - Another piece of weird logic: *Where to allow`knn=False`*? It would make sense for e.g. the scipy knn transformer with `algorithm='kd_tree'`  . https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L614-L617. - I also changed `method` to only mean connectivity method. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR. - [x] figure out what the `_more_tags` methods are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the logic shouldnt have c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:1431,interoperability,specif,specifying,1431,"anged. - I extensively changed https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L471. - This uncovered very grown-over-time logic: https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L522-L523 . *Should we change that logic for more predictability at the expense of backwards compatibility*? Especially the euclidean condition makes not much sense IMHO. - Another piece of weird logic: *Where to allow`knn=False`*? It would make sense for e.g. the scipy knn transformer with `algorithm='kd_tree'`  . https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L614-L617. - I also changed `method` to only mean connectivity method. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR. - [x] figure out what the `_more_tags` methods are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the logic shouldnt have changed*. - pre-computed in umap transformer; handling of disconnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:1858,interoperability,specif,specific,1858,"uclidean condition makes not much sense IMHO. - Another piece of weird logic: *Where to allow`knn=False`*? It would make sense for e.g. the scipy knn transformer with `algorithm='kd_tree'`  . https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L614-L617. - I also changed `method` to only mean connectivity method. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR. - [x] figure out what the `_more_tags` methods are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the logic shouldnt have changed*. - pre-computed in umap transformer; handling of disconnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:2051,interoperability,transform,transformer,2051,"ttps://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L614-L617. - I also changed `method` to only mean connectivity method. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR. - [x] figure out what the `_more_tags` methods are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the logic shouldnt have changed*. - pre-computed in umap transformer; handling of disconnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont test for: . ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:2089,interoperability,compon,components,2089,"/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L614-L617. - I also changed `method` to only mean connectivity method. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR. - [x] figure out what the `_more_tags` methods are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the logic shouldnt have changed*. - pre-computed in umap transformer; handling of disconnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont test for: . ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972fad65a3e). The logic was:. - if small",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:2406,interoperability,transform,transformer,2406," are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the logic shouldnt have changed*. - pre-computed in umap transformer; handling of disconnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont test for: . ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972fad65a3e). The logic was:. - if small data (<8000) and euclidean metric or knn==False, calculate `pairwise_distances`. - if knn=True, then sparsify that matrix by pulling out KNN using `_get_indices_distances_from_dense_matrix` and converting that into a sparse one. - if not, run `umap.nearest_neighbors`. - if even smaller data (<4000), calculate `pairw",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:2526,interoperability,wrapper,wrapper,2526,". - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the logic shouldnt have changed*. - pre-computed in umap transformer; handling of disconnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont test for: . ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972fad65a3e). The logic was:. - if small data (<8000) and euclidean metric or knn==False, calculate `pairwise_distances`. - if knn=True, then sparsify that matrix by pulling out KNN using `_get_indices_distances_from_dense_matrix` and converting that into a sparse one. - if not, run `umap.nearest_neighbors`. - if even smaller data (<4000), calculate `pairwise_distances` like above and run `umap.nearest_neighbors` with `metric='precomputed'`. its internal logic then does t",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:2602,interoperability,wrapper,wrapper,2602,"gorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the logic shouldnt have changed*. - pre-computed in umap transformer; handling of disconnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont test for: . ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972fad65a3e). The logic was:. - if small data (<8000) and euclidean metric or knn==False, calculate `pairwise_distances`. - if knn=True, then sparsify that matrix by pulling out KNN using `_get_indices_distances_from_dense_matrix` and converting that into a sparse one. - if not, run `umap.nearest_neighbors`. - if even smaller data (<4000), calculate `pairwise_distances` like above and run `umap.nearest_neighbors` with `metric='precomputed'`. its internal logic then does the same as `_get_indices_distances_from_dense_matrix`. - else run `umap.near",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:3938,interoperability,transform,transformer,3938,"onnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont test for: . ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972fad65a3e). The logic was:. - if small data (<8000) and euclidean metric or knn==False, calculate `pairwise_distances`. - if knn=True, then sparsify that matrix by pulling out KNN using `_get_indices_distances_from_dense_matrix` and converting that into a sparse one. - if not, run `umap.nearest_neighbors`. - if even smaller data (<4000), calculate `pairwise_distances` like above and run `umap.nearest_neighbors` with `metric='precomputed'`. its internal logic then does the same as `_get_indices_distances_from_dense_matrix`. - else run `umap.nearest_neighbors` directly, which simply runs PyNNDescent with `n_trees` and `n_iters` set with a heuristic. ### now the logic is. - if small data (for either definition of small), calculate `pairwise_distances`, sparsify on demand. (this covers both our previous optimization and the umap `'precomputed'` code path). - else run a KNN transformer. if the old `'umap'` method was chosen, pick the same parameters mentioned aboveto the `PyNNDescent` transformer. ---. Fixes #2519",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:4051,interoperability,transform,transformer,4051,"onnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont test for: . ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972fad65a3e). The logic was:. - if small data (<8000) and euclidean metric or knn==False, calculate `pairwise_distances`. - if knn=True, then sparsify that matrix by pulling out KNN using `_get_indices_distances_from_dense_matrix` and converting that into a sparse one. - if not, run `umap.nearest_neighbors`. - if even smaller data (<4000), calculate `pairwise_distances` like above and run `umap.nearest_neighbors` with `metric='precomputed'`. its internal logic then does the same as `_get_indices_distances_from_dense_matrix`. - else run `umap.nearest_neighbors` directly, which simply runs PyNNDescent with `n_trees` and `n_iters` set with a heuristic. ### now the logic is. - if small data (for either definition of small), calculate `pairwise_distances`, sparsify on demand. (this covers both our previous optimization and the umap `'precomputed'` code path). - else run a KNN transformer. if the old `'umap'` method was chosen, pick the same parameters mentioned aboveto the `PyNNDescent` transformer. ---. Fixes #2519",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:146,modifiability,modul,modules,146,"Make `neighbors` work based on KNeighborsTransformer-compatible APIs; [`sklearn.neighbors.KNeighborsTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsTransformer.html) is a good abstraction for KNN calculations. We could follow the suggestion here, then finish this PR https://github.com/frankier/sklearn-ann/issues/8#issuecomment-1609553421. ### For reviewing. - most functions were moved unchanged. - I extensively changed https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L471. - This uncovered very grown-over-time logic: https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L522-L523 . *Should we change that logic for more predictability at the expense of backwards compatibility*? Especially the euclidean condition makes not much sense IMHO. - Another piece of weird logic: *Where to allow`knn=False`*? It would make sense for e.g. the scipy knn transformer with `algorithm='kd_tree'`  . https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L614-L617. - I also changed `method` to only mean connectivity method. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR. - [x] figure out what the `_more_tags` methods are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the log",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:220,modifiability,abstract,abstraction,220,"Make `neighbors` work based on KNeighborsTransformer-compatible APIs; [`sklearn.neighbors.KNeighborsTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsTransformer.html) is a good abstraction for KNN calculations. We could follow the suggestion here, then finish this PR https://github.com/frankier/sklearn-ann/issues/8#issuecomment-1609553421. ### For reviewing. - most functions were moved unchanged. - I extensively changed https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L471. - This uncovered very grown-over-time logic: https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L522-L523 . *Should we change that logic for more predictability at the expense of backwards compatibility*? Especially the euclidean condition makes not much sense IMHO. - Another piece of weird logic: *Where to allow`knn=False`*? It would make sense for e.g. the scipy knn transformer with `algorithm='kd_tree'`  . https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L614-L617. - I also changed `method` to only mean connectivity method. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR. - [x] figure out what the `_more_tags` methods are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the log",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:447,modifiability,extens,extensively,447,"Make `neighbors` work based on KNeighborsTransformer-compatible APIs; [`sklearn.neighbors.KNeighborsTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsTransformer.html) is a good abstraction for KNN calculations. We could follow the suggestion here, then finish this PR https://github.com/frankier/sklearn-ann/issues/8#issuecomment-1609553421. ### For reviewing. - most functions were moved unchanged. - I extensively changed https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L471. - This uncovered very grown-over-time logic: https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L522-L523 . *Should we change that logic for more predictability at the expense of backwards compatibility*? Especially the euclidean condition makes not much sense IMHO. - Another piece of weird logic: *Where to allow`knn=False`*? It would make sense for e.g. the scipy knn transformer with `algorithm='kd_tree'`  . https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L614-L617. - I also changed `method` to only mean connectivity method. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR. - [x] figure out what the `_more_tags` methods are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the log",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:2089,modifiability,compon,components,2089,"/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L614-L617. - I also changed `method` to only mean connectivity method. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR. - [x] figure out what the `_more_tags` methods are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the logic shouldnt have changed*. - pre-computed in umap transformer; handling of disconnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont test for: . ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972fad65a3e). The logic was:. - if small",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:2219,modifiability,depend,depend,2219,"tivity method. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR. - [x] figure out what the `_more_tags` methods are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the logic shouldnt have changed*. - pre-computed in umap transformer; handling of disconnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont test for: . ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972fad65a3e). The logic was:. - if small data (<8000) and euclidean metric or knn==False, calculate `pairwise_distances`. - if knn=True, then sparsify that matrix by pu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:4004,modifiability,paramet,parameters,4004,"onnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont test for: . ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972fad65a3e). The logic was:. - if small data (<8000) and euclidean metric or knn==False, calculate `pairwise_distances`. - if knn=True, then sparsify that matrix by pulling out KNN using `_get_indices_distances_from_dense_matrix` and converting that into a sparse one. - if not, run `umap.nearest_neighbors`. - if even smaller data (<4000), calculate `pairwise_distances` like above and run `umap.nearest_neighbors` with `metric='precomputed'`. its internal logic then does the same as `_get_indices_distances_from_dense_matrix`. - else run `umap.nearest_neighbors` directly, which simply runs PyNNDescent with `n_trees` and `n_iters` set with a heuristic. ### now the logic is. - if small data (for either definition of small), calculate `pairwise_distances`, sparsify on demand. (this covers both our previous optimization and the umap `'precomputed'` code path). - else run a KNN transformer. if the old `'umap'` method was chosen, pick the same parameters mentioned aboveto the `PyNNDescent` transformer. ---. Fixes #2519",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:615,performance,time,time,615,"Make `neighbors` work based on KNeighborsTransformer-compatible APIs; [`sklearn.neighbors.KNeighborsTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsTransformer.html) is a good abstraction for KNN calculations. We could follow the suggestion here, then finish this PR https://github.com/frankier/sklearn-ann/issues/8#issuecomment-1609553421. ### For reviewing. - most functions were moved unchanged. - I extensively changed https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L471. - This uncovered very grown-over-time logic: https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L522-L523 . *Should we change that logic for more predictability at the expense of backwards compatibility*? Especially the euclidean condition makes not much sense IMHO. - Another piece of weird logic: *Where to allow`knn=False`*? It would make sense for e.g. the scipy knn transformer with `algorithm='kd_tree'`  . https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L614-L617. - I also changed `method` to only mean connectivity method. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR. - [x] figure out what the `_more_tags` methods are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the log",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:3867,performance,optimiz,optimization,3867,"onnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont test for: . ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972fad65a3e). The logic was:. - if small data (<8000) and euclidean metric or knn==False, calculate `pairwise_distances`. - if knn=True, then sparsify that matrix by pulling out KNN using `_get_indices_distances_from_dense_matrix` and converting that into a sparse one. - if not, run `umap.nearest_neighbors`. - if even smaller data (<4000), calculate `pairwise_distances` like above and run `umap.nearest_neighbors` with `metric='precomputed'`. its internal logic then does the same as `_get_indices_distances_from_dense_matrix`. - else run `umap.nearest_neighbors` directly, which simply runs PyNNDescent with `n_trees` and `n_iters` set with a heuristic. ### now the logic is. - if small data (for either definition of small), calculate `pairwise_distances`, sparsify on demand. (this covers both our previous optimization and the umap `'precomputed'` code path). - else run a KNN transformer. if the old `'umap'` method was chosen, pick the same parameters mentioned aboveto the `PyNNDescent` transformer. ---. Fixes #2519",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:2893,reliability,doe,does,2893,"hanged*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the logic shouldnt have changed*. - pre-computed in umap transformer; handling of disconnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont test for: . ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972fad65a3e). The logic was:. - if small data (<8000) and euclidean metric or knn==False, calculate `pairwise_distances`. - if knn=True, then sparsify that matrix by pulling out KNN using `_get_indices_distances_from_dense_matrix` and converting that into a sparse one. - if not, run `umap.nearest_neighbors`. - if even smaller data (<4000), calculate `pairwise_distances` like above and run `umap.nearest_neighbors` with `metric='precomputed'`. its internal logic then does the same as `_get_indices_distances_from_dense_matrix`. - else run `umap.nearest_neighbors` directly, which simply runs PyNNDescent with `n_trees` and `n_iters` set with a heuristic. ### now the logic is. - if small data (for either definition of small), calculate `pairwise_distances`, sparsify on demand. (this covers both our previous optimization and the umap `'",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:3524,reliability,doe,does,3524,"onnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont test for: . ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972fad65a3e). The logic was:. - if small data (<8000) and euclidean metric or knn==False, calculate `pairwise_distances`. - if knn=True, then sparsify that matrix by pulling out KNN using `_get_indices_distances_from_dense_matrix` and converting that into a sparse one. - if not, run `umap.nearest_neighbors`. - if even smaller data (<4000), calculate `pairwise_distances` like above and run `umap.nearest_neighbors` with `metric='precomputed'`. its internal logic then does the same as `_get_indices_distances_from_dense_matrix`. - else run `umap.nearest_neighbors` directly, which simply runs PyNNDescent with `n_trees` and `n_iters` set with a heuristic. ### now the logic is. - if small data (for either definition of small), calculate `pairwise_distances`, sparsify on demand. (this covers both our previous optimization and the umap `'precomputed'` code path). - else run a KNN transformer. if the old `'umap'` method was chosen, pick the same parameters mentioned aboveto the `PyNNDescent` transformer. ---. Fixes #2519",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:146,safety,modul,modules,146,"Make `neighbors` work based on KNeighborsTransformer-compatible APIs; [`sklearn.neighbors.KNeighborsTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsTransformer.html) is a good abstraction for KNN calculations. We could follow the suggestion here, then finish this PR https://github.com/frankier/sklearn-ann/issues/8#issuecomment-1609553421. ### For reviewing. - most functions were moved unchanged. - I extensively changed https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L471. - This uncovered very grown-over-time logic: https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L522-L523 . *Should we change that logic for more predictability at the expense of backwards compatibility*? Especially the euclidean condition makes not much sense IMHO. - Another piece of weird logic: *Where to allow`knn=False`*? It would make sense for e.g. the scipy knn transformer with `algorithm='kd_tree'`  . https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L614-L617. - I also changed `method` to only mean connectivity method. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR. - [x] figure out what the `_more_tags` methods are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the log",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:393,safety,review,reviewing,393,"Make `neighbors` work based on KNeighborsTransformer-compatible APIs; [`sklearn.neighbors.KNeighborsTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsTransformer.html) is a good abstraction for KNN calculations. We could follow the suggestion here, then finish this PR https://github.com/frankier/sklearn-ann/issues/8#issuecomment-1609553421. ### For reviewing. - most functions were moved unchanged. - I extensively changed https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L471. - This uncovered very grown-over-time logic: https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L522-L523 . *Should we change that logic for more predictability at the expense of backwards compatibility*? Especially the euclidean condition makes not much sense IMHO. - Another piece of weird logic: *Where to allow`knn=False`*? It would make sense for e.g. the scipy knn transformer with `algorithm='kd_tree'`  . https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L614-L617. - I also changed `method` to only mean connectivity method. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR. - [x] figure out what the `_more_tags` methods are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the log",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:620,safety,log,logic,620,"Make `neighbors` work based on KNeighborsTransformer-compatible APIs; [`sklearn.neighbors.KNeighborsTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsTransformer.html) is a good abstraction for KNN calculations. We could follow the suggestion here, then finish this PR https://github.com/frankier/sklearn-ann/issues/8#issuecomment-1609553421. ### For reviewing. - most functions were moved unchanged. - I extensively changed https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L471. - This uncovered very grown-over-time logic: https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L522-L523 . *Should we change that logic for more predictability at the expense of backwards compatibility*? Especially the euclidean condition makes not much sense IMHO. - Another piece of weird logic: *Where to allow`knn=False`*? It would make sense for e.g. the scipy knn transformer with `algorithm='kd_tree'`  . https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L614-L617. - I also changed `method` to only mean connectivity method. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR. - [x] figure out what the `_more_tags` methods are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the log",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:771,safety,log,logic,771,"Make `neighbors` work based on KNeighborsTransformer-compatible APIs; [`sklearn.neighbors.KNeighborsTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsTransformer.html) is a good abstraction for KNN calculations. We could follow the suggestion here, then finish this PR https://github.com/frankier/sklearn-ann/issues/8#issuecomment-1609553421. ### For reviewing. - most functions were moved unchanged. - I extensively changed https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L471. - This uncovered very grown-over-time logic: https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L522-L523 . *Should we change that logic for more predictability at the expense of backwards compatibility*? Especially the euclidean condition makes not much sense IMHO. - Another piece of weird logic: *Where to allow`knn=False`*? It would make sense for e.g. the scipy knn transformer with `algorithm='kd_tree'`  . https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L614-L617. - I also changed `method` to only mean connectivity method. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR. - [x] figure out what the `_more_tags` methods are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the log",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:786,safety,predict,predictability,786,"Make `neighbors` work based on KNeighborsTransformer-compatible APIs; [`sklearn.neighbors.KNeighborsTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsTransformer.html) is a good abstraction for KNN calculations. We could follow the suggestion here, then finish this PR https://github.com/frankier/sklearn-ann/issues/8#issuecomment-1609553421. ### For reviewing. - most functions were moved unchanged. - I extensively changed https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L471. - This uncovered very grown-over-time logic: https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L522-L523 . *Should we change that logic for more predictability at the expense of backwards compatibility*? Especially the euclidean condition makes not much sense IMHO. - Another piece of weird logic: *Where to allow`knn=False`*? It would make sense for e.g. the scipy knn transformer with `algorithm='kd_tree'`  . https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L614-L617. - I also changed `method` to only mean connectivity method. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR. - [x] figure out what the `_more_tags` methods are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the log",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:934,safety,log,logic,934,"Make `neighbors` work based on KNeighborsTransformer-compatible APIs; [`sklearn.neighbors.KNeighborsTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsTransformer.html) is a good abstraction for KNN calculations. We could follow the suggestion here, then finish this PR https://github.com/frankier/sklearn-ann/issues/8#issuecomment-1609553421. ### For reviewing. - most functions were moved unchanged. - I extensively changed https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L471. - This uncovered very grown-over-time logic: https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L522-L523 . *Should we change that logic for more predictability at the expense of backwards compatibility*? Especially the euclidean condition makes not much sense IMHO. - Another piece of weird logic: *Where to allow`knn=False`*? It would make sense for e.g. the scipy knn transformer with `algorithm='kd_tree'`  . https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L614-L617. - I also changed `method` to only mean connectivity method. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR. - [x] figure out what the `_more_tags` methods are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the log",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:1997,safety,log,logic,1997,"he scipy knn transformer with `algorithm='kd_tree'`  . https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L614-L617. - I also changed `method` to only mean connectivity method. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR. - [x] figure out what the `_more_tags` methods are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the logic shouldnt have changed*. - pre-computed in umap transformer; handling of disconnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont test for: . ![grafik](https://github.com/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:2181,safety,test,tests,2181," changed `method` to only mean connectivity method. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR. - [x] figure out what the `_more_tags` methods are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the logic shouldnt have changed*. - pre-computed in umap transformer; handling of disconnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont test for: . ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972fad65a3e). The logic was:. - if small data (<8000) and euclidean metric or knn==False, calculate `pairwise_distances`. - if knn",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:2219,safety,depend,depend,2219,"tivity method. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR. - [x] figure out what the `_more_tags` methods are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the logic shouldnt have changed*. - pre-computed in umap transformer; handling of disconnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont test for: . ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972fad65a3e). The logic was:. - if small data (<8000) and euclidean metric or knn==False, calculate `pairwise_distances`. - if knn=True, then sparsify that matrix by pu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:2737,safety,test,test,2737,"ed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the logic shouldnt have changed*. - pre-computed in umap transformer; handling of disconnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont test for: . ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972fad65a3e). The logic was:. - if small data (<8000) and euclidean metric or knn==False, calculate `pairwise_distances`. - if knn=True, then sparsify that matrix by pulling out KNN using `_get_indices_distances_from_dense_matrix` and converting that into a sparse one. - if not, run `umap.nearest_neighbors`. - if even smaller data (<4000), calculate `pairwise_distances` like above and run `umap.nearest_neighbors` with `metric='precomputed'`. its internal logic then does the same as `_get_indices_distances_from_dense_matrix`. - else run `umap.nearest_neighbors` directly, which simply runs PyNNDescent with `n_trees` and `n_iters` set with a heuristic. ### now the logic is. - if ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:2774,safety,test,test,2774,"onnectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the logic shouldnt have changed*. - pre-computed in umap transformer; handling of disconnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont test for: . ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972fad65a3e). The logic was:. - if small data (<8000) and euclidean metric or knn==False, calculate `pairwise_distances`. - if knn=True, then sparsify that matrix by pulling out KNN using `_get_indices_distances_from_dense_matrix` and converting that into a sparse one. - if not, run `umap.nearest_neighbors`. - if even smaller data (<4000), calculate `pairwise_distances` like above and run `umap.nearest_neighbors` with `metric='precomputed'`. its internal logic then does the same as `_get_indices_distances_from_dense_matrix`. - else run `umap.nearest_neighbors` directly, which simply runs PyNNDescent with `n_trees` and `n_iters` set with a heuristic. ### now the logic is. - if small data (for either definition of ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:2959,safety,test,test,2959,"ask, ) *not covered, but also the logic shouldnt have changed*. - pre-computed in umap transformer; handling of disconnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont test for: . ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972fad65a3e). The logic was:. - if small data (<8000) and euclidean metric or knn==False, calculate `pairwise_distances`. - if knn=True, then sparsify that matrix by pulling out KNN using `_get_indices_distances_from_dense_matrix` and converting that into a sparse one. - if not, run `umap.nearest_neighbors`. - if even smaller data (<4000), calculate `pairwise_distances` like above and run `umap.nearest_neighbors` with `metric='precomputed'`. its internal logic then does the same as `_get_indices_distances_from_dense_matrix`. - else run `umap.nearest_neighbors` directly, which simply runs PyNNDescent with `n_trees` and `n_iters` set with a heuristic. ### now the logic is. - if small data (for either definition of small), calculate `pairwise_distances`, sparsify on demand. (this covers both our previous optimization and the umap `'precomputed'` code path). - else run a KNN transformer. if the old",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:3072,safety,log,logic,3072," disconnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont test for: . ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972fad65a3e). The logic was:. - if small data (<8000) and euclidean metric or knn==False, calculate `pairwise_distances`. - if knn=True, then sparsify that matrix by pulling out KNN using `_get_indices_distances_from_dense_matrix` and converting that into a sparse one. - if not, run `umap.nearest_neighbors`. - if even smaller data (<4000), calculate `pairwise_distances` like above and run `umap.nearest_neighbors` with `metric='precomputed'`. its internal logic then does the same as `_get_indices_distances_from_dense_matrix`. - else run `umap.nearest_neighbors` directly, which simply runs PyNNDescent with `n_trees` and `n_iters` set with a heuristic. ### now the logic is. - if small data (for either definition of small), calculate `pairwise_distances`, sparsify on demand. (this covers both our previous optimization and the umap `'precomputed'` code path). - else run a KNN transformer. if the old `'umap'` method was chosen, pick the same parameters mentioned aboveto the `PyNNDescent` transformer. ---. Fixes ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:3513,safety,log,logic,3513,"onnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont test for: . ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972fad65a3e). The logic was:. - if small data (<8000) and euclidean metric or knn==False, calculate `pairwise_distances`. - if knn=True, then sparsify that matrix by pulling out KNN using `_get_indices_distances_from_dense_matrix` and converting that into a sparse one. - if not, run `umap.nearest_neighbors`. - if even smaller data (<4000), calculate `pairwise_distances` like above and run `umap.nearest_neighbors` with `metric='precomputed'`. its internal logic then does the same as `_get_indices_distances_from_dense_matrix`. - else run `umap.nearest_neighbors` directly, which simply runs PyNNDescent with `n_trees` and `n_iters` set with a heuristic. ### now the logic is. - if small data (for either definition of small), calculate `pairwise_distances`, sparsify on demand. (this covers both our previous optimization and the umap `'precomputed'` code path). - else run a KNN transformer. if the old `'umap'` method was chosen, pick the same parameters mentioned aboveto the `PyNNDescent` transformer. ---. Fixes #2519",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:3724,safety,log,logic,3724,"onnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont test for: . ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972fad65a3e). The logic was:. - if small data (<8000) and euclidean metric or knn==False, calculate `pairwise_distances`. - if knn=True, then sparsify that matrix by pulling out KNN using `_get_indices_distances_from_dense_matrix` and converting that into a sparse one. - if not, run `umap.nearest_neighbors`. - if even smaller data (<4000), calculate `pairwise_distances` like above and run `umap.nearest_neighbors` with `metric='precomputed'`. its internal logic then does the same as `_get_indices_distances_from_dense_matrix`. - else run `umap.nearest_neighbors` directly, which simply runs PyNNDescent with `n_trees` and `n_iters` set with a heuristic. ### now the logic is. - if small data (for either definition of small), calculate `pairwise_distances`, sparsify on demand. (this covers both our previous optimization and the umap `'precomputed'` code path). - else run a KNN transformer. if the old `'umap'` method was chosen, pick the same parameters mentioned aboveto the `PyNNDescent` transformer. ---. Fixes #2519",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:620,security,log,logic,620,"Make `neighbors` work based on KNeighborsTransformer-compatible APIs; [`sklearn.neighbors.KNeighborsTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsTransformer.html) is a good abstraction for KNN calculations. We could follow the suggestion here, then finish this PR https://github.com/frankier/sklearn-ann/issues/8#issuecomment-1609553421. ### For reviewing. - most functions were moved unchanged. - I extensively changed https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L471. - This uncovered very grown-over-time logic: https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L522-L523 . *Should we change that logic for more predictability at the expense of backwards compatibility*? Especially the euclidean condition makes not much sense IMHO. - Another piece of weird logic: *Where to allow`knn=False`*? It would make sense for e.g. the scipy knn transformer with `algorithm='kd_tree'`  . https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L614-L617. - I also changed `method` to only mean connectivity method. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR. - [x] figure out what the `_more_tags` methods are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the log",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:771,security,log,logic,771,"Make `neighbors` work based on KNeighborsTransformer-compatible APIs; [`sklearn.neighbors.KNeighborsTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsTransformer.html) is a good abstraction for KNN calculations. We could follow the suggestion here, then finish this PR https://github.com/frankier/sklearn-ann/issues/8#issuecomment-1609553421. ### For reviewing. - most functions were moved unchanged. - I extensively changed https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L471. - This uncovered very grown-over-time logic: https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L522-L523 . *Should we change that logic for more predictability at the expense of backwards compatibility*? Especially the euclidean condition makes not much sense IMHO. - Another piece of weird logic: *Where to allow`knn=False`*? It would make sense for e.g. the scipy knn transformer with `algorithm='kd_tree'`  . https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L614-L617. - I also changed `method` to only mean connectivity method. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR. - [x] figure out what the `_more_tags` methods are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the log",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:934,security,log,logic,934,"Make `neighbors` work based on KNeighborsTransformer-compatible APIs; [`sklearn.neighbors.KNeighborsTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsTransformer.html) is a good abstraction for KNN calculations. We could follow the suggestion here, then finish this PR https://github.com/frankier/sklearn-ann/issues/8#issuecomment-1609553421. ### For reviewing. - most functions were moved unchanged. - I extensively changed https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L471. - This uncovered very grown-over-time logic: https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L522-L523 . *Should we change that logic for more predictability at the expense of backwards compatibility*? Especially the euclidean condition makes not much sense IMHO. - Another piece of weird logic: *Where to allow`knn=False`*? It would make sense for e.g. the scipy knn transformer with `algorithm='kd_tree'`  . https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L614-L617. - I also changed `method` to only mean connectivity method. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR. - [x] figure out what the `_more_tags` methods are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the log",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:1997,security,log,logic,1997,"he scipy knn transformer with `algorithm='kd_tree'`  . https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L614-L617. - I also changed `method` to only mean connectivity method. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR. - [x] figure out what the `_more_tags` methods are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the logic shouldnt have changed*. - pre-computed in umap transformer; handling of disconnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont test for: . ![grafik](https://github.com/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:3072,security,log,logic,3072," disconnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont test for: . ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972fad65a3e). The logic was:. - if small data (<8000) and euclidean metric or knn==False, calculate `pairwise_distances`. - if knn=True, then sparsify that matrix by pulling out KNN using `_get_indices_distances_from_dense_matrix` and converting that into a sparse one. - if not, run `umap.nearest_neighbors`. - if even smaller data (<4000), calculate `pairwise_distances` like above and run `umap.nearest_neighbors` with `metric='precomputed'`. its internal logic then does the same as `_get_indices_distances_from_dense_matrix`. - else run `umap.nearest_neighbors` directly, which simply runs PyNNDescent with `n_trees` and `n_iters` set with a heuristic. ### now the logic is. - if small data (for either definition of small), calculate `pairwise_distances`, sparsify on demand. (this covers both our previous optimization and the umap `'precomputed'` code path). - else run a KNN transformer. if the old `'umap'` method was chosen, pick the same parameters mentioned aboveto the `PyNNDescent` transformer. ---. Fixes ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:3513,security,log,logic,3513,"onnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont test for: . ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972fad65a3e). The logic was:. - if small data (<8000) and euclidean metric or knn==False, calculate `pairwise_distances`. - if knn=True, then sparsify that matrix by pulling out KNN using `_get_indices_distances_from_dense_matrix` and converting that into a sparse one. - if not, run `umap.nearest_neighbors`. - if even smaller data (<4000), calculate `pairwise_distances` like above and run `umap.nearest_neighbors` with `metric='precomputed'`. its internal logic then does the same as `_get_indices_distances_from_dense_matrix`. - else run `umap.nearest_neighbors` directly, which simply runs PyNNDescent with `n_trees` and `n_iters` set with a heuristic. ### now the logic is. - if small data (for either definition of small), calculate `pairwise_distances`, sparsify on demand. (this covers both our previous optimization and the umap `'precomputed'` code path). - else run a KNN transformer. if the old `'umap'` method was chosen, pick the same parameters mentioned aboveto the `PyNNDescent` transformer. ---. Fixes #2519",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:3724,security,log,logic,3724,"onnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont test for: . ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972fad65a3e). The logic was:. - if small data (<8000) and euclidean metric or knn==False, calculate `pairwise_distances`. - if knn=True, then sparsify that matrix by pulling out KNN using `_get_indices_distances_from_dense_matrix` and converting that into a sparse one. - if not, run `umap.nearest_neighbors`. - if even smaller data (<4000), calculate `pairwise_distances` like above and run `umap.nearest_neighbors` with `metric='precomputed'`. its internal logic then does the same as `_get_indices_distances_from_dense_matrix`. - else run `umap.nearest_neighbors` directly, which simply runs PyNNDescent with `n_trees` and `n_iters` set with a heuristic. ### now the logic is. - if small data (for either definition of small), calculate `pairwise_distances`, sparsify on demand. (this covers both our previous optimization and the umap `'precomputed'` code path). - else run a KNN transformer. if the old `'umap'` method was chosen, pick the same parameters mentioned aboveto the `PyNNDescent` transformer. ---. Fixes #2519",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:393,testability,review,reviewing,393,"Make `neighbors` work based on KNeighborsTransformer-compatible APIs; [`sklearn.neighbors.KNeighborsTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsTransformer.html) is a good abstraction for KNN calculations. We could follow the suggestion here, then finish this PR https://github.com/frankier/sklearn-ann/issues/8#issuecomment-1609553421. ### For reviewing. - most functions were moved unchanged. - I extensively changed https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L471. - This uncovered very grown-over-time logic: https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L522-L523 . *Should we change that logic for more predictability at the expense of backwards compatibility*? Especially the euclidean condition makes not much sense IMHO. - Another piece of weird logic: *Where to allow`knn=False`*? It would make sense for e.g. the scipy knn transformer with `algorithm='kd_tree'`  . https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L614-L617. - I also changed `method` to only mean connectivity method. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR. - [x] figure out what the `_more_tags` methods are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the log",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:620,testability,log,logic,620,"Make `neighbors` work based on KNeighborsTransformer-compatible APIs; [`sklearn.neighbors.KNeighborsTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsTransformer.html) is a good abstraction for KNN calculations. We could follow the suggestion here, then finish this PR https://github.com/frankier/sklearn-ann/issues/8#issuecomment-1609553421. ### For reviewing. - most functions were moved unchanged. - I extensively changed https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L471. - This uncovered very grown-over-time logic: https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L522-L523 . *Should we change that logic for more predictability at the expense of backwards compatibility*? Especially the euclidean condition makes not much sense IMHO. - Another piece of weird logic: *Where to allow`knn=False`*? It would make sense for e.g. the scipy knn transformer with `algorithm='kd_tree'`  . https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L614-L617. - I also changed `method` to only mean connectivity method. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR. - [x] figure out what the `_more_tags` methods are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the log",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:771,testability,log,logic,771,"Make `neighbors` work based on KNeighborsTransformer-compatible APIs; [`sklearn.neighbors.KNeighborsTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsTransformer.html) is a good abstraction for KNN calculations. We could follow the suggestion here, then finish this PR https://github.com/frankier/sklearn-ann/issues/8#issuecomment-1609553421. ### For reviewing. - most functions were moved unchanged. - I extensively changed https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L471. - This uncovered very grown-over-time logic: https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L522-L523 . *Should we change that logic for more predictability at the expense of backwards compatibility*? Especially the euclidean condition makes not much sense IMHO. - Another piece of weird logic: *Where to allow`knn=False`*? It would make sense for e.g. the scipy knn transformer with `algorithm='kd_tree'`  . https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L614-L617. - I also changed `method` to only mean connectivity method. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR. - [x] figure out what the `_more_tags` methods are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the log",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:934,testability,log,logic,934,"Make `neighbors` work based on KNeighborsTransformer-compatible APIs; [`sklearn.neighbors.KNeighborsTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsTransformer.html) is a good abstraction for KNN calculations. We could follow the suggestion here, then finish this PR https://github.com/frankier/sklearn-ann/issues/8#issuecomment-1609553421. ### For reviewing. - most functions were moved unchanged. - I extensively changed https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L471. - This uncovered very grown-over-time logic: https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L522-L523 . *Should we change that logic for more predictability at the expense of backwards compatibility*? Especially the euclidean condition makes not much sense IMHO. - Another piece of weird logic: *Where to allow`knn=False`*? It would make sense for e.g. the scipy knn transformer with `algorithm='kd_tree'`  . https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L614-L617. - I also changed `method` to only mean connectivity method. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR. - [x] figure out what the `_more_tags` methods are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the log",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:1832,testability,coverag,coverage,1832,"bility*? Especially the euclidean condition makes not much sense IMHO. - Another piece of weird logic: *Where to allow`knn=False`*? It would make sense for e.g. the scipy knn transformer with `algorithm='kd_tree'`  . https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L614-L617. - I also changed `method` to only mean connectivity method. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR. - [x] figure out what the `_more_tags` methods are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the logic shouldnt have changed*. - pre-computed in umap transformer; handling of disconnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precom",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:1997,testability,log,logic,1997,"he scipy knn transformer with `algorithm='kd_tree'`  . https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L614-L617. - I also changed `method` to only mean connectivity method. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR. - [x] figure out what the `_more_tags` methods are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the logic shouldnt have changed*. - pre-computed in umap transformer; handling of disconnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont test for: . ![grafik](https://github.com/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:2181,testability,test,tests,2181," changed `method` to only mean connectivity method. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR. - [x] figure out what the `_more_tags` methods are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the logic shouldnt have changed*. - pre-computed in umap transformer; handling of disconnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont test for: . ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972fad65a3e). The logic was:. - if small data (<8000) and euclidean metric or knn==False, calculate `pairwise_distances`. - if knn",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:2219,testability,depend,depend,2219,"tivity method. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR. - [x] figure out what the `_more_tags` methods are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the logic shouldnt have changed*. - pre-computed in umap transformer; handling of disconnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont test for: . ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972fad65a3e). The logic was:. - if small data (<8000) and euclidean metric or knn==False, calculate `pairwise_distances`. - if knn=True, then sparsify that matrix by pu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:2737,testability,test,test,2737,"ed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the logic shouldnt have changed*. - pre-computed in umap transformer; handling of disconnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont test for: . ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972fad65a3e). The logic was:. - if small data (<8000) and euclidean metric or knn==False, calculate `pairwise_distances`. - if knn=True, then sparsify that matrix by pulling out KNN using `_get_indices_distances_from_dense_matrix` and converting that into a sparse one. - if not, run `umap.nearest_neighbors`. - if even smaller data (<4000), calculate `pairwise_distances` like above and run `umap.nearest_neighbors` with `metric='precomputed'`. its internal logic then does the same as `_get_indices_distances_from_dense_matrix`. - else run `umap.nearest_neighbors` directly, which simply runs PyNNDescent with `n_trees` and `n_iters` set with a heuristic. ### now the logic is. - if ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:2742,testability,coverag,coverage,2742,"r fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the logic shouldnt have changed*. - pre-computed in umap transformer; handling of disconnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont test for: . ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972fad65a3e). The logic was:. - if small data (<8000) and euclidean metric or knn==False, calculate `pairwise_distances`. - if knn=True, then sparsify that matrix by pulling out KNN using `_get_indices_distances_from_dense_matrix` and converting that into a sparse one. - if not, run `umap.nearest_neighbors`. - if even smaller data (<4000), calculate `pairwise_distances` like above and run `umap.nearest_neighbors` with `metric='precomputed'`. its internal logic then does the same as `_get_indices_distances_from_dense_matrix`. - else run `umap.nearest_neighbors` directly, which simply runs PyNNDescent with `n_trees` and `n_iters` set with a heuristic. ### now the logic is. - if small d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:2774,testability,test,test,2774,"onnectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the logic shouldnt have changed*. - pre-computed in umap transformer; handling of disconnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont test for: . ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972fad65a3e). The logic was:. - if small data (<8000) and euclidean metric or knn==False, calculate `pairwise_distances`. - if knn=True, then sparsify that matrix by pulling out KNN using `_get_indices_distances_from_dense_matrix` and converting that into a sparse one. - if not, run `umap.nearest_neighbors`. - if even smaller data (<4000), calculate `pairwise_distances` like above and run `umap.nearest_neighbors` with `metric='precomputed'`. its internal logic then does the same as `_get_indices_distances_from_dense_matrix`. - else run `umap.nearest_neighbors` directly, which simply runs PyNNDescent with `n_trees` and `n_iters` set with a heuristic. ### now the logic is. - if small data (for either definition of ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:2959,testability,test,test,2959,"ask, ) *not covered, but also the logic shouldnt have changed*. - pre-computed in umap transformer; handling of disconnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont test for: . ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972fad65a3e). The logic was:. - if small data (<8000) and euclidean metric or knn==False, calculate `pairwise_distances`. - if knn=True, then sparsify that matrix by pulling out KNN using `_get_indices_distances_from_dense_matrix` and converting that into a sparse one. - if not, run `umap.nearest_neighbors`. - if even smaller data (<4000), calculate `pairwise_distances` like above and run `umap.nearest_neighbors` with `metric='precomputed'`. its internal logic then does the same as `_get_indices_distances_from_dense_matrix`. - else run `umap.nearest_neighbors` directly, which simply runs PyNNDescent with `n_trees` and `n_iters` set with a heuristic. ### now the logic is. - if small data (for either definition of small), calculate `pairwise_distances`, sparsify on demand. (this covers both our previous optimization and the umap `'precomputed'` code path). - else run a KNN transformer. if the old",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:3072,testability,log,logic,3072," disconnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont test for: . ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972fad65a3e). The logic was:. - if small data (<8000) and euclidean metric or knn==False, calculate `pairwise_distances`. - if knn=True, then sparsify that matrix by pulling out KNN using `_get_indices_distances_from_dense_matrix` and converting that into a sparse one. - if not, run `umap.nearest_neighbors`. - if even smaller data (<4000), calculate `pairwise_distances` like above and run `umap.nearest_neighbors` with `metric='precomputed'`. its internal logic then does the same as `_get_indices_distances_from_dense_matrix`. - else run `umap.nearest_neighbors` directly, which simply runs PyNNDescent with `n_trees` and `n_iters` set with a heuristic. ### now the logic is. - if small data (for either definition of small), calculate `pairwise_distances`, sparsify on demand. (this covers both our previous optimization and the umap `'precomputed'` code path). - else run a KNN transformer. if the old `'umap'` method was chosen, pick the same parameters mentioned aboveto the `PyNNDescent` transformer. ---. Fixes ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:3513,testability,log,logic,3513,"onnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont test for: . ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972fad65a3e). The logic was:. - if small data (<8000) and euclidean metric or knn==False, calculate `pairwise_distances`. - if knn=True, then sparsify that matrix by pulling out KNN using `_get_indices_distances_from_dense_matrix` and converting that into a sparse one. - if not, run `umap.nearest_neighbors`. - if even smaller data (<4000), calculate `pairwise_distances` like above and run `umap.nearest_neighbors` with `metric='precomputed'`. its internal logic then does the same as `_get_indices_distances_from_dense_matrix`. - else run `umap.nearest_neighbors` directly, which simply runs PyNNDescent with `n_trees` and `n_iters` set with a heuristic. ### now the logic is. - if small data (for either definition of small), calculate `pairwise_distances`, sparsify on demand. (this covers both our previous optimization and the umap `'precomputed'` code path). - else run a KNN transformer. if the old `'umap'` method was chosen, pick the same parameters mentioned aboveto the `PyNNDescent` transformer. ---. Fixes #2519",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:3637,testability,simpl,simply,3637,"onnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont test for: . ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972fad65a3e). The logic was:. - if small data (<8000) and euclidean metric or knn==False, calculate `pairwise_distances`. - if knn=True, then sparsify that matrix by pulling out KNN using `_get_indices_distances_from_dense_matrix` and converting that into a sparse one. - if not, run `umap.nearest_neighbors`. - if even smaller data (<4000), calculate `pairwise_distances` like above and run `umap.nearest_neighbors` with `metric='precomputed'`. its internal logic then does the same as `_get_indices_distances_from_dense_matrix`. - else run `umap.nearest_neighbors` directly, which simply runs PyNNDescent with `n_trees` and `n_iters` set with a heuristic. ### now the logic is. - if small data (for either definition of small), calculate `pairwise_distances`, sparsify on demand. (this covers both our previous optimization and the umap `'precomputed'` code path). - else run a KNN transformer. if the old `'umap'` method was chosen, pick the same parameters mentioned aboveto the `PyNNDescent` transformer. ---. Fixes #2519",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:3724,testability,log,logic,3724,"onnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont test for: . ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972fad65a3e). The logic was:. - if small data (<8000) and euclidean metric or knn==False, calculate `pairwise_distances`. - if knn=True, then sparsify that matrix by pulling out KNN using `_get_indices_distances_from_dense_matrix` and converting that into a sparse one. - if not, run `umap.nearest_neighbors`. - if even smaller data (<4000), calculate `pairwise_distances` like above and run `umap.nearest_neighbors` with `metric='precomputed'`. its internal logic then does the same as `_get_indices_distances_from_dense_matrix`. - else run `umap.nearest_neighbors` directly, which simply runs PyNNDescent with `n_trees` and `n_iters` set with a heuristic. ### now the logic is. - if small data (for either definition of small), calculate `pairwise_distances`, sparsify on demand. (this covers both our previous optimization and the umap `'precomputed'` code path). - else run a KNN transformer. if the old `'umap'` method was chosen, pick the same parameters mentioned aboveto the `PyNNDescent` transformer. ---. Fixes #2519",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:129,usability,learn,learn,129,"Make `neighbors` work based on KNeighborsTransformer-compatible APIs; [`sklearn.neighbors.KNeighborsTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsTransformer.html) is a good abstraction for KNN calculations. We could follow the suggestion here, then finish this PR https://github.com/frankier/sklearn-ann/issues/8#issuecomment-1609553421. ### For reviewing. - most functions were moved unchanged. - I extensively changed https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L471. - This uncovered very grown-over-time logic: https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L522-L523 . *Should we change that logic for more predictability at the expense of backwards compatibility*? Especially the euclidean condition makes not much sense IMHO. - Another piece of weird logic: *Where to allow`knn=False`*? It would make sense for e.g. the scipy knn transformer with `algorithm='kd_tree'`  . https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L614-L617. - I also changed `method` to only mean connectivity method. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR. - [x] figure out what the `_more_tags` methods are . - [x] allow specifying algorithm and/or backend. - [x] revert 75c6670, move connectivities code out of backends. - [x] switch our stuff to KNeighborsTransformer. - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity). - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*. - [x] check out where we have coverage. - is there paga specific stuff? *not in the parts I changed*. - gauss: dense matrix when knn=True (build a symmetric mask, ) *not covered, but also the log",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2536:3637,usability,simpl,simply,3637,"onnected components in umap (indices == -1) *replaced with our own implementation, see below*. - add tests. - [x] pyknndescent (we already depend on it through umap). Maybe in another PR? - [ ] maybe store index in unified way? - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`? - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umaps `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we dont actually test umaps pynndescent codepath at all (just the fast `precomputed` path for small data). - umaps `precomputed` code does some weird things to its knn `indices` array, which we dont test for: . ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972fad65a3e). The logic was:. - if small data (<8000) and euclidean metric or knn==False, calculate `pairwise_distances`. - if knn=True, then sparsify that matrix by pulling out KNN using `_get_indices_distances_from_dense_matrix` and converting that into a sparse one. - if not, run `umap.nearest_neighbors`. - if even smaller data (<4000), calculate `pairwise_distances` like above and run `umap.nearest_neighbors` with `metric='precomputed'`. its internal logic then does the same as `_get_indices_distances_from_dense_matrix`. - else run `umap.nearest_neighbors` directly, which simply runs PyNNDescent with `n_trees` and `n_iters` set with a heuristic. ### now the logic is. - if small data (for either definition of small), calculate `pairwise_distances`, sparsify on demand. (this covers both our previous optimization and the umap `'precomputed'` code path). - else run a KNN transformer. if the old `'umap'` method was chosen, pick the same parameters mentioned aboveto the `PyNNDescent` transformer. ---. Fixes #2519",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536
https://github.com/scverse/scanpy/pull/2537:90,safety,review,review,90,"Fix pytest norecursedirs option; The CLI option is different from the config option . no review necessary, simple fix",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2537
https://github.com/scverse/scanpy/pull/2537:90,testability,review,review,90,"Fix pytest norecursedirs option; The CLI option is different from the config option . no review necessary, simple fix",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2537
https://github.com/scverse/scanpy/pull/2537:108,testability,simpl,simple,108,"Fix pytest norecursedirs option; The CLI option is different from the config option . no review necessary, simple fix",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2537
https://github.com/scverse/scanpy/pull/2537:108,usability,simpl,simple,108,"Fix pytest norecursedirs option; The CLI option is different from the config option . no review necessary, simple fix",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2537
https://github.com/scverse/scanpy/issues/2539:994,availability,Error,Error,994,"Inconsistent type hinting; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When working on one of the notebooks in `spatialdata-notebooks` I noticed that for `library_id` in `scatterplots.py` the type hints are inconsistent. Specifically here it is indicated to be either `Empty` or `str`:. https://github.com/scverse/scanpy/blob/cd7f6c57d4abb49947fa31a945e9a21b833e6e15/scanpy/plotting/_tools/scatterplots.py#L921. While here the type hint includes `None`:. https://github.com/scverse/scanpy/blob/cd7f6c57d4abb49947fa31a945e9a21b833e6e15/scanpy/plotting/_tools/scatterplots.py#L1287. I think the case should be that None should be included. Very minor, but just noticed it so I thought to open an issue. ### Minimal code sample. ```python. NA. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asciitree NA. cffi 1.15.1. cloudpickle 2.2.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dask 2023.5.1. dateutil 2.8.2. defusedxml 0.7.1. entrypoints 0.4. fasteners 0.18. h5py 3.8.0. jinja2 3.1.2. joblib 1.2.0. kiwisolver 1.4.4. llvmlite 0.40.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. msgpack 1.0.5. natsort 8.3.1. nt NA. numba 0.57.0. numcodecs 0.11.0. numpy 1.24.3. packaging 23.1. pandas 1.5.3. psutil 5.9.5. pyarrow 12.0.0. pyparsing 3.0.9. pythoncom NA. pytz 2023.3. pywintypes NA. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. sklearn 1.2.2. sphinxcontrib NA. tblib 1.7.0. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. typing_extensions NA. win32api NA. win32com NA. yaml 6.0. zarr 2.14.2. zipp NA. zoneinfo NA. -----. Python 3.10.11 | packaged by conda-forge | (main, May 10 2023, 18:51:25) [MSC v.1934 64 bit (AMD64)]. Windows-10-10.0.22621-SP0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2539
https://github.com/scverse/scanpy/issues/2539:195,deployability,version,version,195,"Inconsistent type hinting; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When working on one of the notebooks in `spatialdata-notebooks` I noticed that for `library_id` in `scatterplots.py` the type hints are inconsistent. Specifically here it is indicated to be either `Empty` or `str`:. https://github.com/scverse/scanpy/blob/cd7f6c57d4abb49947fa31a945e9a21b833e6e15/scanpy/plotting/_tools/scatterplots.py#L921. While here the type hint includes `None`:. https://github.com/scverse/scanpy/blob/cd7f6c57d4abb49947fa31a945e9a21b833e6e15/scanpy/plotting/_tools/scatterplots.py#L1287. I think the case should be that None should be included. Very minor, but just noticed it so I thought to open an issue. ### Minimal code sample. ```python. NA. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asciitree NA. cffi 1.15.1. cloudpickle 2.2.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dask 2023.5.1. dateutil 2.8.2. defusedxml 0.7.1. entrypoints 0.4. fasteners 0.18. h5py 3.8.0. jinja2 3.1.2. joblib 1.2.0. kiwisolver 1.4.4. llvmlite 0.40.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. msgpack 1.0.5. natsort 8.3.1. nt NA. numba 0.57.0. numcodecs 0.11.0. numpy 1.24.3. packaging 23.1. pandas 1.5.3. psutil 5.9.5. pyarrow 12.0.0. pyparsing 3.0.9. pythoncom NA. pytz 2023.3. pywintypes NA. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. sklearn 1.2.2. sphinxcontrib NA. tblib 1.7.0. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. typing_extensions NA. win32api NA. win32com NA. yaml 6.0. zarr 2.14.2. zipp NA. zoneinfo NA. -----. Python 3.10.11 | packaged by conda-forge | (main, May 10 2023, 18:51:25) [MSC v.1934 64 bit (AMD64)]. Windows-10-10.0.22621-SP0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2539
https://github.com/scverse/scanpy/issues/2539:1027,deployability,Version,Versions,1027,"hinting; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When working on one of the notebooks in `spatialdata-notebooks` I noticed that for `library_id` in `scatterplots.py` the type hints are inconsistent. Specifically here it is indicated to be either `Empty` or `str`:. https://github.com/scverse/scanpy/blob/cd7f6c57d4abb49947fa31a945e9a21b833e6e15/scanpy/plotting/_tools/scatterplots.py#L921. While here the type hint includes `None`:. https://github.com/scverse/scanpy/blob/cd7f6c57d4abb49947fa31a945e9a21b833e6e15/scanpy/plotting/_tools/scatterplots.py#L1287. I think the case should be that None should be included. Very minor, but just noticed it so I thought to open an issue. ### Minimal code sample. ```python. NA. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asciitree NA. cffi 1.15.1. cloudpickle 2.2.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dask 2023.5.1. dateutil 2.8.2. defusedxml 0.7.1. entrypoints 0.4. fasteners 0.18. h5py 3.8.0. jinja2 3.1.2. joblib 1.2.0. kiwisolver 1.4.4. llvmlite 0.40.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. msgpack 1.0.5. natsort 8.3.1. nt NA. numba 0.57.0. numcodecs 0.11.0. numpy 1.24.3. packaging 23.1. pandas 1.5.3. psutil 5.9.5. pyarrow 12.0.0. pyparsing 3.0.9. pythoncom NA. pytz 2023.3. pywintypes NA. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. sklearn 1.2.2. sphinxcontrib NA. tblib 1.7.0. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. typing_extensions NA. win32api NA. win32com NA. yaml 6.0. zarr 2.14.2. zipp NA. zoneinfo NA. -----. Python 3.10.11 | packaged by conda-forge | (main, May 10 2023, 18:51:25) [MSC v.1934 64 bit (AMD64)]. Windows-10-10.0.22621-SP0. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2539
https://github.com/scverse/scanpy/issues/2539:1134,energy efficiency,cloud,cloudpickle,1134,"hinting; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When working on one of the notebooks in `spatialdata-notebooks` I noticed that for `library_id` in `scatterplots.py` the type hints are inconsistent. Specifically here it is indicated to be either `Empty` or `str`:. https://github.com/scverse/scanpy/blob/cd7f6c57d4abb49947fa31a945e9a21b833e6e15/scanpy/plotting/_tools/scatterplots.py#L921. While here the type hint includes `None`:. https://github.com/scverse/scanpy/blob/cd7f6c57d4abb49947fa31a945e9a21b833e6e15/scanpy/plotting/_tools/scatterplots.py#L1287. I think the case should be that None should be included. Very minor, but just noticed it so I thought to open an issue. ### Minimal code sample. ```python. NA. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asciitree NA. cffi 1.15.1. cloudpickle 2.2.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dask 2023.5.1. dateutil 2.8.2. defusedxml 0.7.1. entrypoints 0.4. fasteners 0.18. h5py 3.8.0. jinja2 3.1.2. joblib 1.2.0. kiwisolver 1.4.4. llvmlite 0.40.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. msgpack 1.0.5. natsort 8.3.1. nt NA. numba 0.57.0. numcodecs 0.11.0. numpy 1.24.3. packaging 23.1. pandas 1.5.3. psutil 5.9.5. pyarrow 12.0.0. pyparsing 3.0.9. pythoncom NA. pytz 2023.3. pywintypes NA. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. sklearn 1.2.2. sphinxcontrib NA. tblib 1.7.0. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. typing_extensions NA. win32api NA. win32com NA. yaml 6.0. zarr 2.14.2. zipp NA. zoneinfo NA. -----. Python 3.10.11 | packaged by conda-forge | (main, May 10 2023, 18:51:25) [MSC v.1934 64 bit (AMD64)]. Windows-10-10.0.22621-SP0. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2539
https://github.com/scverse/scanpy/issues/2539:195,integrability,version,version,195,"Inconsistent type hinting; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When working on one of the notebooks in `spatialdata-notebooks` I noticed that for `library_id` in `scatterplots.py` the type hints are inconsistent. Specifically here it is indicated to be either `Empty` or `str`:. https://github.com/scverse/scanpy/blob/cd7f6c57d4abb49947fa31a945e9a21b833e6e15/scanpy/plotting/_tools/scatterplots.py#L921. While here the type hint includes `None`:. https://github.com/scverse/scanpy/blob/cd7f6c57d4abb49947fa31a945e9a21b833e6e15/scanpy/plotting/_tools/scatterplots.py#L1287. I think the case should be that None should be included. Very minor, but just noticed it so I thought to open an issue. ### Minimal code sample. ```python. NA. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asciitree NA. cffi 1.15.1. cloudpickle 2.2.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dask 2023.5.1. dateutil 2.8.2. defusedxml 0.7.1. entrypoints 0.4. fasteners 0.18. h5py 3.8.0. jinja2 3.1.2. joblib 1.2.0. kiwisolver 1.4.4. llvmlite 0.40.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. msgpack 1.0.5. natsort 8.3.1. nt NA. numba 0.57.0. numcodecs 0.11.0. numpy 1.24.3. packaging 23.1. pandas 1.5.3. psutil 5.9.5. pyarrow 12.0.0. pyparsing 3.0.9. pythoncom NA. pytz 2023.3. pywintypes NA. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. sklearn 1.2.2. sphinxcontrib NA. tblib 1.7.0. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. typing_extensions NA. win32api NA. win32com NA. yaml 6.0. zarr 2.14.2. zipp NA. zoneinfo NA. -----. Python 3.10.11 | packaged by conda-forge | (main, May 10 2023, 18:51:25) [MSC v.1934 64 bit (AMD64)]. Windows-10-10.0.22621-SP0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2539
https://github.com/scverse/scanpy/issues/2539:1027,integrability,Version,Versions,1027,"hinting; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When working on one of the notebooks in `spatialdata-notebooks` I noticed that for `library_id` in `scatterplots.py` the type hints are inconsistent. Specifically here it is indicated to be either `Empty` or `str`:. https://github.com/scverse/scanpy/blob/cd7f6c57d4abb49947fa31a945e9a21b833e6e15/scanpy/plotting/_tools/scatterplots.py#L921. While here the type hint includes `None`:. https://github.com/scverse/scanpy/blob/cd7f6c57d4abb49947fa31a945e9a21b833e6e15/scanpy/plotting/_tools/scatterplots.py#L1287. I think the case should be that None should be included. Very minor, but just noticed it so I thought to open an issue. ### Minimal code sample. ```python. NA. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asciitree NA. cffi 1.15.1. cloudpickle 2.2.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dask 2023.5.1. dateutil 2.8.2. defusedxml 0.7.1. entrypoints 0.4. fasteners 0.18. h5py 3.8.0. jinja2 3.1.2. joblib 1.2.0. kiwisolver 1.4.4. llvmlite 0.40.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. msgpack 1.0.5. natsort 8.3.1. nt NA. numba 0.57.0. numcodecs 0.11.0. numpy 1.24.3. packaging 23.1. pandas 1.5.3. psutil 5.9.5. pyarrow 12.0.0. pyparsing 3.0.9. pythoncom NA. pytz 2023.3. pywintypes NA. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. sklearn 1.2.2. sphinxcontrib NA. tblib 1.7.0. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. typing_extensions NA. win32api NA. win32com NA. yaml 6.0. zarr 2.14.2. zipp NA. zoneinfo NA. -----. Python 3.10.11 | packaged by conda-forge | (main, May 10 2023, 18:51:25) [MSC v.1934 64 bit (AMD64)]. Windows-10-10.0.22621-SP0. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2539
https://github.com/scverse/scanpy/issues/2539:465,interoperability,Specif,Specifically,465,"Inconsistent type hinting; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When working on one of the notebooks in `spatialdata-notebooks` I noticed that for `library_id` in `scatterplots.py` the type hints are inconsistent. Specifically here it is indicated to be either `Empty` or `str`:. https://github.com/scverse/scanpy/blob/cd7f6c57d4abb49947fa31a945e9a21b833e6e15/scanpy/plotting/_tools/scatterplots.py#L921. While here the type hint includes `None`:. https://github.com/scverse/scanpy/blob/cd7f6c57d4abb49947fa31a945e9a21b833e6e15/scanpy/plotting/_tools/scatterplots.py#L1287. I think the case should be that None should be included. Very minor, but just noticed it so I thought to open an issue. ### Minimal code sample. ```python. NA. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asciitree NA. cffi 1.15.1. cloudpickle 2.2.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dask 2023.5.1. dateutil 2.8.2. defusedxml 0.7.1. entrypoints 0.4. fasteners 0.18. h5py 3.8.0. jinja2 3.1.2. joblib 1.2.0. kiwisolver 1.4.4. llvmlite 0.40.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. msgpack 1.0.5. natsort 8.3.1. nt NA. numba 0.57.0. numcodecs 0.11.0. numpy 1.24.3. packaging 23.1. pandas 1.5.3. psutil 5.9.5. pyarrow 12.0.0. pyparsing 3.0.9. pythoncom NA. pytz 2023.3. pywintypes NA. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. sklearn 1.2.2. sphinxcontrib NA. tblib 1.7.0. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. typing_extensions NA. win32api NA. win32com NA. yaml 6.0. zarr 2.14.2. zipp NA. zoneinfo NA. -----. Python 3.10.11 | packaged by conda-forge | (main, May 10 2023, 18:51:25) [MSC v.1934 64 bit (AMD64)]. Windows-10-10.0.22621-SP0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2539
https://github.com/scverse/scanpy/issues/2539:195,modifiability,version,version,195,"Inconsistent type hinting; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When working on one of the notebooks in `spatialdata-notebooks` I noticed that for `library_id` in `scatterplots.py` the type hints are inconsistent. Specifically here it is indicated to be either `Empty` or `str`:. https://github.com/scverse/scanpy/blob/cd7f6c57d4abb49947fa31a945e9a21b833e6e15/scanpy/plotting/_tools/scatterplots.py#L921. While here the type hint includes `None`:. https://github.com/scverse/scanpy/blob/cd7f6c57d4abb49947fa31a945e9a21b833e6e15/scanpy/plotting/_tools/scatterplots.py#L1287. I think the case should be that None should be included. Very minor, but just noticed it so I thought to open an issue. ### Minimal code sample. ```python. NA. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asciitree NA. cffi 1.15.1. cloudpickle 2.2.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dask 2023.5.1. dateutil 2.8.2. defusedxml 0.7.1. entrypoints 0.4. fasteners 0.18. h5py 3.8.0. jinja2 3.1.2. joblib 1.2.0. kiwisolver 1.4.4. llvmlite 0.40.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. msgpack 1.0.5. natsort 8.3.1. nt NA. numba 0.57.0. numcodecs 0.11.0. numpy 1.24.3. packaging 23.1. pandas 1.5.3. psutil 5.9.5. pyarrow 12.0.0. pyparsing 3.0.9. pythoncom NA. pytz 2023.3. pywintypes NA. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. sklearn 1.2.2. sphinxcontrib NA. tblib 1.7.0. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. typing_extensions NA. win32api NA. win32com NA. yaml 6.0. zarr 2.14.2. zipp NA. zoneinfo NA. -----. Python 3.10.11 | packaged by conda-forge | (main, May 10 2023, 18:51:25) [MSC v.1934 64 bit (AMD64)]. Windows-10-10.0.22621-SP0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2539
https://github.com/scverse/scanpy/issues/2539:1027,modifiability,Version,Versions,1027,"hinting; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When working on one of the notebooks in `spatialdata-notebooks` I noticed that for `library_id` in `scatterplots.py` the type hints are inconsistent. Specifically here it is indicated to be either `Empty` or `str`:. https://github.com/scverse/scanpy/blob/cd7f6c57d4abb49947fa31a945e9a21b833e6e15/scanpy/plotting/_tools/scatterplots.py#L921. While here the type hint includes `None`:. https://github.com/scverse/scanpy/blob/cd7f6c57d4abb49947fa31a945e9a21b833e6e15/scanpy/plotting/_tools/scatterplots.py#L1287. I think the case should be that None should be included. Very minor, but just noticed it so I thought to open an issue. ### Minimal code sample. ```python. NA. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asciitree NA. cffi 1.15.1. cloudpickle 2.2.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dask 2023.5.1. dateutil 2.8.2. defusedxml 0.7.1. entrypoints 0.4. fasteners 0.18. h5py 3.8.0. jinja2 3.1.2. joblib 1.2.0. kiwisolver 1.4.4. llvmlite 0.40.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. msgpack 1.0.5. natsort 8.3.1. nt NA. numba 0.57.0. numcodecs 0.11.0. numpy 1.24.3. packaging 23.1. pandas 1.5.3. psutil 5.9.5. pyarrow 12.0.0. pyparsing 3.0.9. pythoncom NA. pytz 2023.3. pywintypes NA. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. sklearn 1.2.2. sphinxcontrib NA. tblib 1.7.0. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. typing_extensions NA. win32api NA. win32com NA. yaml 6.0. zarr 2.14.2. zipp NA. zoneinfo NA. -----. Python 3.10.11 | packaged by conda-forge | (main, May 10 2023, 18:51:25) [MSC v.1934 64 bit (AMD64)]. Windows-10-10.0.22621-SP0. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2539
https://github.com/scverse/scanpy/issues/2539:1496,modifiability,pac,packaging,1496,"hinting; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When working on one of the notebooks in `spatialdata-notebooks` I noticed that for `library_id` in `scatterplots.py` the type hints are inconsistent. Specifically here it is indicated to be either `Empty` or `str`:. https://github.com/scverse/scanpy/blob/cd7f6c57d4abb49947fa31a945e9a21b833e6e15/scanpy/plotting/_tools/scatterplots.py#L921. While here the type hint includes `None`:. https://github.com/scverse/scanpy/blob/cd7f6c57d4abb49947fa31a945e9a21b833e6e15/scanpy/plotting/_tools/scatterplots.py#L1287. I think the case should be that None should be included. Very minor, but just noticed it so I thought to open an issue. ### Minimal code sample. ```python. NA. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asciitree NA. cffi 1.15.1. cloudpickle 2.2.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dask 2023.5.1. dateutil 2.8.2. defusedxml 0.7.1. entrypoints 0.4. fasteners 0.18. h5py 3.8.0. jinja2 3.1.2. joblib 1.2.0. kiwisolver 1.4.4. llvmlite 0.40.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. msgpack 1.0.5. natsort 8.3.1. nt NA. numba 0.57.0. numcodecs 0.11.0. numpy 1.24.3. packaging 23.1. pandas 1.5.3. psutil 5.9.5. pyarrow 12.0.0. pyparsing 3.0.9. pythoncom NA. pytz 2023.3. pywintypes NA. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. sklearn 1.2.2. sphinxcontrib NA. tblib 1.7.0. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. typing_extensions NA. win32api NA. win32com NA. yaml 6.0. zarr 2.14.2. zipp NA. zoneinfo NA. -----. Python 3.10.11 | packaged by conda-forge | (main, May 10 2023, 18:51:25) [MSC v.1934 64 bit (AMD64)]. Windows-10-10.0.22621-SP0. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2539
https://github.com/scverse/scanpy/issues/2539:1890,modifiability,pac,packaged,1890,"hinting; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When working on one of the notebooks in `spatialdata-notebooks` I noticed that for `library_id` in `scatterplots.py` the type hints are inconsistent. Specifically here it is indicated to be either `Empty` or `str`:. https://github.com/scverse/scanpy/blob/cd7f6c57d4abb49947fa31a945e9a21b833e6e15/scanpy/plotting/_tools/scatterplots.py#L921. While here the type hint includes `None`:. https://github.com/scverse/scanpy/blob/cd7f6c57d4abb49947fa31a945e9a21b833e6e15/scanpy/plotting/_tools/scatterplots.py#L1287. I think the case should be that None should be included. Very minor, but just noticed it so I thought to open an issue. ### Minimal code sample. ```python. NA. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asciitree NA. cffi 1.15.1. cloudpickle 2.2.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dask 2023.5.1. dateutil 2.8.2. defusedxml 0.7.1. entrypoints 0.4. fasteners 0.18. h5py 3.8.0. jinja2 3.1.2. joblib 1.2.0. kiwisolver 1.4.4. llvmlite 0.40.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. msgpack 1.0.5. natsort 8.3.1. nt NA. numba 0.57.0. numcodecs 0.11.0. numpy 1.24.3. packaging 23.1. pandas 1.5.3. psutil 5.9.5. pyarrow 12.0.0. pyparsing 3.0.9. pythoncom NA. pytz 2023.3. pywintypes NA. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. sklearn 1.2.2. sphinxcontrib NA. tblib 1.7.0. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. typing_extensions NA. win32api NA. win32com NA. yaml 6.0. zarr 2.14.2. zipp NA. zoneinfo NA. -----. Python 3.10.11 | packaged by conda-forge | (main, May 10 2023, 18:51:25) [MSC v.1934 64 bit (AMD64)]. Windows-10-10.0.22621-SP0. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2539
https://github.com/scverse/scanpy/issues/2539:994,performance,Error,Error,994,"Inconsistent type hinting; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When working on one of the notebooks in `spatialdata-notebooks` I noticed that for `library_id` in `scatterplots.py` the type hints are inconsistent. Specifically here it is indicated to be either `Empty` or `str`:. https://github.com/scverse/scanpy/blob/cd7f6c57d4abb49947fa31a945e9a21b833e6e15/scanpy/plotting/_tools/scatterplots.py#L921. While here the type hint includes `None`:. https://github.com/scverse/scanpy/blob/cd7f6c57d4abb49947fa31a945e9a21b833e6e15/scanpy/plotting/_tools/scatterplots.py#L1287. I think the case should be that None should be included. Very minor, but just noticed it so I thought to open an issue. ### Minimal code sample. ```python. NA. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asciitree NA. cffi 1.15.1. cloudpickle 2.2.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dask 2023.5.1. dateutil 2.8.2. defusedxml 0.7.1. entrypoints 0.4. fasteners 0.18. h5py 3.8.0. jinja2 3.1.2. joblib 1.2.0. kiwisolver 1.4.4. llvmlite 0.40.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. msgpack 1.0.5. natsort 8.3.1. nt NA. numba 0.57.0. numcodecs 0.11.0. numpy 1.24.3. packaging 23.1. pandas 1.5.3. psutil 5.9.5. pyarrow 12.0.0. pyparsing 3.0.9. pythoncom NA. pytz 2023.3. pywintypes NA. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. sklearn 1.2.2. sphinxcontrib NA. tblib 1.7.0. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. typing_extensions NA. win32api NA. win32com NA. yaml 6.0. zarr 2.14.2. zipp NA. zoneinfo NA. -----. Python 3.10.11 | packaged by conda-forge | (main, May 10 2023, 18:51:25) [MSC v.1934 64 bit (AMD64)]. Windows-10-10.0.22621-SP0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2539
https://github.com/scverse/scanpy/issues/2539:994,safety,Error,Error,994,"Inconsistent type hinting; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When working on one of the notebooks in `spatialdata-notebooks` I noticed that for `library_id` in `scatterplots.py` the type hints are inconsistent. Specifically here it is indicated to be either `Empty` or `str`:. https://github.com/scverse/scanpy/blob/cd7f6c57d4abb49947fa31a945e9a21b833e6e15/scanpy/plotting/_tools/scatterplots.py#L921. While here the type hint includes `None`:. https://github.com/scverse/scanpy/blob/cd7f6c57d4abb49947fa31a945e9a21b833e6e15/scanpy/plotting/_tools/scatterplots.py#L1287. I think the case should be that None should be included. Very minor, but just noticed it so I thought to open an issue. ### Minimal code sample. ```python. NA. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asciitree NA. cffi 1.15.1. cloudpickle 2.2.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dask 2023.5.1. dateutil 2.8.2. defusedxml 0.7.1. entrypoints 0.4. fasteners 0.18. h5py 3.8.0. jinja2 3.1.2. joblib 1.2.0. kiwisolver 1.4.4. llvmlite 0.40.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. msgpack 1.0.5. natsort 8.3.1. nt NA. numba 0.57.0. numcodecs 0.11.0. numpy 1.24.3. packaging 23.1. pandas 1.5.3. psutil 5.9.5. pyarrow 12.0.0. pyparsing 3.0.9. pythoncom NA. pytz 2023.3. pywintypes NA. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. sklearn 1.2.2. sphinxcontrib NA. tblib 1.7.0. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. typing_extensions NA. win32api NA. win32com NA. yaml 6.0. zarr 2.14.2. zipp NA. zoneinfo NA. -----. Python 3.10.11 | packaged by conda-forge | (main, May 10 2023, 18:51:25) [MSC v.1934 64 bit (AMD64)]. Windows-10-10.0.22621-SP0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2539
https://github.com/scverse/scanpy/issues/2539:18,usability,hint,hinting,18,"Inconsistent type hinting; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When working on one of the notebooks in `spatialdata-notebooks` I noticed that for `library_id` in `scatterplots.py` the type hints are inconsistent. Specifically here it is indicated to be either `Empty` or `str`:. https://github.com/scverse/scanpy/blob/cd7f6c57d4abb49947fa31a945e9a21b833e6e15/scanpy/plotting/_tools/scatterplots.py#L921. While here the type hint includes `None`:. https://github.com/scverse/scanpy/blob/cd7f6c57d4abb49947fa31a945e9a21b833e6e15/scanpy/plotting/_tools/scatterplots.py#L1287. I think the case should be that None should be included. Very minor, but just noticed it so I thought to open an issue. ### Minimal code sample. ```python. NA. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asciitree NA. cffi 1.15.1. cloudpickle 2.2.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dask 2023.5.1. dateutil 2.8.2. defusedxml 0.7.1. entrypoints 0.4. fasteners 0.18. h5py 3.8.0. jinja2 3.1.2. joblib 1.2.0. kiwisolver 1.4.4. llvmlite 0.40.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. msgpack 1.0.5. natsort 8.3.1. nt NA. numba 0.57.0. numcodecs 0.11.0. numpy 1.24.3. packaging 23.1. pandas 1.5.3. psutil 5.9.5. pyarrow 12.0.0. pyparsing 3.0.9. pythoncom NA. pytz 2023.3. pywintypes NA. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. sklearn 1.2.2. sphinxcontrib NA. tblib 1.7.0. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. typing_extensions NA. win32api NA. win32com NA. yaml 6.0. zarr 2.14.2. zipp NA. zoneinfo NA. -----. Python 3.10.11 | packaged by conda-forge | (main, May 10 2023, 18:51:25) [MSC v.1934 64 bit (AMD64)]. Windows-10-10.0.22621-SP0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2539
https://github.com/scverse/scanpy/issues/2539:155,usability,confirm,confirmed,155,"Inconsistent type hinting; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When working on one of the notebooks in `spatialdata-notebooks` I noticed that for `library_id` in `scatterplots.py` the type hints are inconsistent. Specifically here it is indicated to be either `Empty` or `str`:. https://github.com/scverse/scanpy/blob/cd7f6c57d4abb49947fa31a945e9a21b833e6e15/scanpy/plotting/_tools/scatterplots.py#L921. While here the type hint includes `None`:. https://github.com/scverse/scanpy/blob/cd7f6c57d4abb49947fa31a945e9a21b833e6e15/scanpy/plotting/_tools/scatterplots.py#L1287. I think the case should be that None should be included. Very minor, but just noticed it so I thought to open an issue. ### Minimal code sample. ```python. NA. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asciitree NA. cffi 1.15.1. cloudpickle 2.2.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dask 2023.5.1. dateutil 2.8.2. defusedxml 0.7.1. entrypoints 0.4. fasteners 0.18. h5py 3.8.0. jinja2 3.1.2. joblib 1.2.0. kiwisolver 1.4.4. llvmlite 0.40.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. msgpack 1.0.5. natsort 8.3.1. nt NA. numba 0.57.0. numcodecs 0.11.0. numpy 1.24.3. packaging 23.1. pandas 1.5.3. psutil 5.9.5. pyarrow 12.0.0. pyparsing 3.0.9. pythoncom NA. pytz 2023.3. pywintypes NA. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. sklearn 1.2.2. sphinxcontrib NA. tblib 1.7.0. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. typing_extensions NA. win32api NA. win32com NA. yaml 6.0. zarr 2.14.2. zipp NA. zoneinfo NA. -----. Python 3.10.11 | packaged by conda-forge | (main, May 10 2023, 18:51:25) [MSC v.1934 64 bit (AMD64)]. Windows-10-10.0.22621-SP0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2539
https://github.com/scverse/scanpy/issues/2539:238,usability,confirm,confirmed,238,"Inconsistent type hinting; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When working on one of the notebooks in `spatialdata-notebooks` I noticed that for `library_id` in `scatterplots.py` the type hints are inconsistent. Specifically here it is indicated to be either `Empty` or `str`:. https://github.com/scverse/scanpy/blob/cd7f6c57d4abb49947fa31a945e9a21b833e6e15/scanpy/plotting/_tools/scatterplots.py#L921. While here the type hint includes `None`:. https://github.com/scverse/scanpy/blob/cd7f6c57d4abb49947fa31a945e9a21b833e6e15/scanpy/plotting/_tools/scatterplots.py#L1287. I think the case should be that None should be included. Very minor, but just noticed it so I thought to open an issue. ### Minimal code sample. ```python. NA. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asciitree NA. cffi 1.15.1. cloudpickle 2.2.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dask 2023.5.1. dateutil 2.8.2. defusedxml 0.7.1. entrypoints 0.4. fasteners 0.18. h5py 3.8.0. jinja2 3.1.2. joblib 1.2.0. kiwisolver 1.4.4. llvmlite 0.40.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. msgpack 1.0.5. natsort 8.3.1. nt NA. numba 0.57.0. numcodecs 0.11.0. numpy 1.24.3. packaging 23.1. pandas 1.5.3. psutil 5.9.5. pyarrow 12.0.0. pyparsing 3.0.9. pythoncom NA. pytz 2023.3. pywintypes NA. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. sklearn 1.2.2. sphinxcontrib NA. tblib 1.7.0. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. typing_extensions NA. win32api NA. win32com NA. yaml 6.0. zarr 2.14.2. zipp NA. zoneinfo NA. -----. Python 3.10.11 | packaged by conda-forge | (main, May 10 2023, 18:51:25) [MSC v.1934 64 bit (AMD64)]. Windows-10-10.0.22621-SP0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2539
https://github.com/scverse/scanpy/issues/2539:441,usability,hint,hints,441,"Inconsistent type hinting; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When working on one of the notebooks in `spatialdata-notebooks` I noticed that for `library_id` in `scatterplots.py` the type hints are inconsistent. Specifically here it is indicated to be either `Empty` or `str`:. https://github.com/scverse/scanpy/blob/cd7f6c57d4abb49947fa31a945e9a21b833e6e15/scanpy/plotting/_tools/scatterplots.py#L921. While here the type hint includes `None`:. https://github.com/scverse/scanpy/blob/cd7f6c57d4abb49947fa31a945e9a21b833e6e15/scanpy/plotting/_tools/scatterplots.py#L1287. I think the case should be that None should be included. Very minor, but just noticed it so I thought to open an issue. ### Minimal code sample. ```python. NA. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asciitree NA. cffi 1.15.1. cloudpickle 2.2.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dask 2023.5.1. dateutil 2.8.2. defusedxml 0.7.1. entrypoints 0.4. fasteners 0.18. h5py 3.8.0. jinja2 3.1.2. joblib 1.2.0. kiwisolver 1.4.4. llvmlite 0.40.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. msgpack 1.0.5. natsort 8.3.1. nt NA. numba 0.57.0. numcodecs 0.11.0. numpy 1.24.3. packaging 23.1. pandas 1.5.3. psutil 5.9.5. pyarrow 12.0.0. pyparsing 3.0.9. pythoncom NA. pytz 2023.3. pywintypes NA. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. sklearn 1.2.2. sphinxcontrib NA. tblib 1.7.0. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. typing_extensions NA. win32api NA. win32com NA. yaml 6.0. zarr 2.14.2. zipp NA. zoneinfo NA. -----. Python 3.10.11 | packaged by conda-forge | (main, May 10 2023, 18:51:25) [MSC v.1934 64 bit (AMD64)]. Windows-10-10.0.22621-SP0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2539
https://github.com/scverse/scanpy/issues/2539:489,usability,indicat,indicated,489,"Inconsistent type hinting; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When working on one of the notebooks in `spatialdata-notebooks` I noticed that for `library_id` in `scatterplots.py` the type hints are inconsistent. Specifically here it is indicated to be either `Empty` or `str`:. https://github.com/scverse/scanpy/blob/cd7f6c57d4abb49947fa31a945e9a21b833e6e15/scanpy/plotting/_tools/scatterplots.py#L921. While here the type hint includes `None`:. https://github.com/scverse/scanpy/blob/cd7f6c57d4abb49947fa31a945e9a21b833e6e15/scanpy/plotting/_tools/scatterplots.py#L1287. I think the case should be that None should be included. Very minor, but just noticed it so I thought to open an issue. ### Minimal code sample. ```python. NA. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asciitree NA. cffi 1.15.1. cloudpickle 2.2.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dask 2023.5.1. dateutil 2.8.2. defusedxml 0.7.1. entrypoints 0.4. fasteners 0.18. h5py 3.8.0. jinja2 3.1.2. joblib 1.2.0. kiwisolver 1.4.4. llvmlite 0.40.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. msgpack 1.0.5. natsort 8.3.1. nt NA. numba 0.57.0. numcodecs 0.11.0. numpy 1.24.3. packaging 23.1. pandas 1.5.3. psutil 5.9.5. pyarrow 12.0.0. pyparsing 3.0.9. pythoncom NA. pytz 2023.3. pywintypes NA. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. sklearn 1.2.2. sphinxcontrib NA. tblib 1.7.0. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. typing_extensions NA. win32api NA. win32com NA. yaml 6.0. zarr 2.14.2. zipp NA. zoneinfo NA. -----. Python 3.10.11 | packaged by conda-forge | (main, May 10 2023, 18:51:25) [MSC v.1934 64 bit (AMD64)]. Windows-10-10.0.22621-SP0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2539
https://github.com/scverse/scanpy/issues/2539:676,usability,hint,hint,676,"Inconsistent type hinting; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When working on one of the notebooks in `spatialdata-notebooks` I noticed that for `library_id` in `scatterplots.py` the type hints are inconsistent. Specifically here it is indicated to be either `Empty` or `str`:. https://github.com/scverse/scanpy/blob/cd7f6c57d4abb49947fa31a945e9a21b833e6e15/scanpy/plotting/_tools/scatterplots.py#L921. While here the type hint includes `None`:. https://github.com/scverse/scanpy/blob/cd7f6c57d4abb49947fa31a945e9a21b833e6e15/scanpy/plotting/_tools/scatterplots.py#L1287. I think the case should be that None should be included. Very minor, but just noticed it so I thought to open an issue. ### Minimal code sample. ```python. NA. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asciitree NA. cffi 1.15.1. cloudpickle 2.2.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dask 2023.5.1. dateutil 2.8.2. defusedxml 0.7.1. entrypoints 0.4. fasteners 0.18. h5py 3.8.0. jinja2 3.1.2. joblib 1.2.0. kiwisolver 1.4.4. llvmlite 0.40.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. msgpack 1.0.5. natsort 8.3.1. nt NA. numba 0.57.0. numcodecs 0.11.0. numpy 1.24.3. packaging 23.1. pandas 1.5.3. psutil 5.9.5. pyarrow 12.0.0. pyparsing 3.0.9. pythoncom NA. pytz 2023.3. pywintypes NA. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. sklearn 1.2.2. sphinxcontrib NA. tblib 1.7.0. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. typing_extensions NA. win32api NA. win32com NA. yaml 6.0. zarr 2.14.2. zipp NA. zoneinfo NA. -----. Python 3.10.11 | packaged by conda-forge | (main, May 10 2023, 18:51:25) [MSC v.1934 64 bit (AMD64)]. Windows-10-10.0.22621-SP0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2539
https://github.com/scverse/scanpy/issues/2539:949,usability,Minim,Minimal,949,"Inconsistent type hinting; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When working on one of the notebooks in `spatialdata-notebooks` I noticed that for `library_id` in `scatterplots.py` the type hints are inconsistent. Specifically here it is indicated to be either `Empty` or `str`:. https://github.com/scverse/scanpy/blob/cd7f6c57d4abb49947fa31a945e9a21b833e6e15/scanpy/plotting/_tools/scatterplots.py#L921. While here the type hint includes `None`:. https://github.com/scverse/scanpy/blob/cd7f6c57d4abb49947fa31a945e9a21b833e6e15/scanpy/plotting/_tools/scatterplots.py#L1287. I think the case should be that None should be included. Very minor, but just noticed it so I thought to open an issue. ### Minimal code sample. ```python. NA. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asciitree NA. cffi 1.15.1. cloudpickle 2.2.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dask 2023.5.1. dateutil 2.8.2. defusedxml 0.7.1. entrypoints 0.4. fasteners 0.18. h5py 3.8.0. jinja2 3.1.2. joblib 1.2.0. kiwisolver 1.4.4. llvmlite 0.40.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. msgpack 1.0.5. natsort 8.3.1. nt NA. numba 0.57.0. numcodecs 0.11.0. numpy 1.24.3. packaging 23.1. pandas 1.5.3. psutil 5.9.5. pyarrow 12.0.0. pyparsing 3.0.9. pythoncom NA. pytz 2023.3. pywintypes NA. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. sklearn 1.2.2. sphinxcontrib NA. tblib 1.7.0. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. typing_extensions NA. win32api NA. win32com NA. yaml 6.0. zarr 2.14.2. zipp NA. zoneinfo NA. -----. Python 3.10.11 | packaged by conda-forge | (main, May 10 2023, 18:51:25) [MSC v.1934 64 bit (AMD64)]. Windows-10-10.0.22621-SP0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2539
https://github.com/scverse/scanpy/issues/2539:994,usability,Error,Error,994,"Inconsistent type hinting; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When working on one of the notebooks in `spatialdata-notebooks` I noticed that for `library_id` in `scatterplots.py` the type hints are inconsistent. Specifically here it is indicated to be either `Empty` or `str`:. https://github.com/scverse/scanpy/blob/cd7f6c57d4abb49947fa31a945e9a21b833e6e15/scanpy/plotting/_tools/scatterplots.py#L921. While here the type hint includes `None`:. https://github.com/scverse/scanpy/blob/cd7f6c57d4abb49947fa31a945e9a21b833e6e15/scanpy/plotting/_tools/scatterplots.py#L1287. I think the case should be that None should be included. Very minor, but just noticed it so I thought to open an issue. ### Minimal code sample. ```python. NA. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asciitree NA. cffi 1.15.1. cloudpickle 2.2.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dask 2023.5.1. dateutil 2.8.2. defusedxml 0.7.1. entrypoints 0.4. fasteners 0.18. h5py 3.8.0. jinja2 3.1.2. joblib 1.2.0. kiwisolver 1.4.4. llvmlite 0.40.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. msgpack 1.0.5. natsort 8.3.1. nt NA. numba 0.57.0. numcodecs 0.11.0. numpy 1.24.3. packaging 23.1. pandas 1.5.3. psutil 5.9.5. pyarrow 12.0.0. pyparsing 3.0.9. pythoncom NA. pytz 2023.3. pywintypes NA. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. sklearn 1.2.2. sphinxcontrib NA. tblib 1.7.0. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. typing_extensions NA. win32api NA. win32com NA. yaml 6.0. zarr 2.14.2. zipp NA. zoneinfo NA. -----. Python 3.10.11 | packaged by conda-forge | (main, May 10 2023, 18:51:25) [MSC v.1934 64 bit (AMD64)]. Windows-10-10.0.22621-SP0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2539
https://github.com/scverse/scanpy/issues/2539:1759,usability,tool,toolz,1759,"hinting; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When working on one of the notebooks in `spatialdata-notebooks` I noticed that for `library_id` in `scatterplots.py` the type hints are inconsistent. Specifically here it is indicated to be either `Empty` or `str`:. https://github.com/scverse/scanpy/blob/cd7f6c57d4abb49947fa31a945e9a21b833e6e15/scanpy/plotting/_tools/scatterplots.py#L921. While here the type hint includes `None`:. https://github.com/scverse/scanpy/blob/cd7f6c57d4abb49947fa31a945e9a21b833e6e15/scanpy/plotting/_tools/scatterplots.py#L1287. I think the case should be that None should be included. Very minor, but just noticed it so I thought to open an issue. ### Minimal code sample. ```python. NA. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asciitree NA. cffi 1.15.1. cloudpickle 2.2.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dask 2023.5.1. dateutil 2.8.2. defusedxml 0.7.1. entrypoints 0.4. fasteners 0.18. h5py 3.8.0. jinja2 3.1.2. joblib 1.2.0. kiwisolver 1.4.4. llvmlite 0.40.0. markupsafe 2.1.3. matplotlib 3.7.1. mpl_toolkits NA. msgpack 1.0.5. natsort 8.3.1. nt NA. numba 0.57.0. numcodecs 0.11.0. numpy 1.24.3. packaging 23.1. pandas 1.5.3. psutil 5.9.5. pyarrow 12.0.0. pyparsing 3.0.9. pythoncom NA. pytz 2023.3. pywintypes NA. scipy 1.10.1. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. sklearn 1.2.2. sphinxcontrib NA. tblib 1.7.0. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. typing_extensions NA. win32api NA. win32com NA. yaml 6.0. zarr 2.14.2. zipp NA. zoneinfo NA. -----. Python 3.10.11 | packaged by conda-forge | (main, May 10 2023, 18:51:25) [MSC v.1934 64 bit (AMD64)]. Windows-10-10.0.22621-SP0. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2539
https://github.com/scverse/scanpy/issues/2540:809,availability,Error,Error,809,"pl.rank_genes_groups_violin() throws a KeyError when gene_symbols option is passed; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When trying to produce a violin plot using `pl.rank_genes_groups_violin()` of the ranked genes and passing `gene_symbols=""gene_symbols""` I get a `KeyError`. However `pl.rank_genes_groups()` works fine with the option `gene_symbols=""gene_symbols""` and the same `adata` object. ### Minimal code sample. ```python. sc.pl.rank_genes_groups_violin(. adata, groups=cell_types, n_genes=15, strip=False, gene_symbols=""gene_symbols"". ). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[258], line 1. ----> 1 sc.pl.rank_genes_groups_violin(. 2 adata, groups=cell_types, n_genes=15, strip=False, gene_symbols=""gene_symbols"". 3 ). File ~/.conda/envs/scirpy/lib/python3.10/site-packages/scanpy/plotting/_tools/__init__.py:1157, in rank_genes_groups_violin(adata, groups, n_genes, gene_names, gene_symbols, use_raw, key, split, scale, strip, jitter, size, ax, show, save). 1155 if isinstance(_gene_names, np.ndarray):. 1156 _gene_names = _gene_names.tolist(). -> 1157 df = obs_df(adata, _gene_names, use_raw=use_raw, gene_symbols=gene_symbols). 1158 new_gene_names = df.columns. 1159 df['hue'] = adata.obs[groups_key].astype(str).values. File ~/.conda/envs/scirpy/lib/python3.10/site-packages/scanpy/get/get.py:272, in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 269 else:. 270 alias_index = None. --> 272 obs_cols, var_idx_keys, var_symbols = _check_indices(. 273 adata.obs,. 274 var.index,. 275 ""obs"",. 276 keys,. 277 alias_index=alias_index,. 278 use_raw=use_raw,. 279 ). 281 # Make df. 282 df = pd.DataFrame(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2540
https://github.com/scverse/scanpy/issues/2540:252,deployability,version,version,252,"pl.rank_genes_groups_violin() throws a KeyError when gene_symbols option is passed; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When trying to produce a violin plot using `pl.rank_genes_groups_violin()` of the ranked genes and passing `gene_symbols=""gene_symbols""` I get a `KeyError`. However `pl.rank_genes_groups()` works fine with the option `gene_symbols=""gene_symbols""` and the same `adata` object. ### Minimal code sample. ```python. sc.pl.rank_genes_groups_violin(. adata, groups=cell_types, n_genes=15, strip=False, gene_symbols=""gene_symbols"". ). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[258], line 1. ----> 1 sc.pl.rank_genes_groups_violin(. 2 adata, groups=cell_types, n_genes=15, strip=False, gene_symbols=""gene_symbols"". 3 ). File ~/.conda/envs/scirpy/lib/python3.10/site-packages/scanpy/plotting/_tools/__init__.py:1157, in rank_genes_groups_violin(adata, groups, n_genes, gene_names, gene_symbols, use_raw, key, split, scale, strip, jitter, size, ax, show, save). 1155 if isinstance(_gene_names, np.ndarray):. 1156 _gene_names = _gene_names.tolist(). -> 1157 df = obs_df(adata, _gene_names, use_raw=use_raw, gene_symbols=gene_symbols). 1158 new_gene_names = df.columns. 1159 df['hue'] = adata.obs[groups_key].astype(str).values. File ~/.conda/envs/scirpy/lib/python3.10/site-packages/scanpy/get/get.py:272, in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 269 else:. 270 alias_index = None. --> 272 obs_cols, var_idx_keys, var_symbols = _check_indices(. 273 adata.obs,. 274 var.index,. 275 ""obs"",. 276 keys,. 277 alias_index=alias_index,. 278 use_raw=use_raw,. 279 ). 281 # Make df. 282 df = pd.DataFrame(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2540
https://github.com/scverse/scanpy/issues/2540:1298,deployability,scale,scale,1298,"med this bug exists on the master branch of scanpy. ### What happened? When trying to produce a violin plot using `pl.rank_genes_groups_violin()` of the ranked genes and passing `gene_symbols=""gene_symbols""` I get a `KeyError`. However `pl.rank_genes_groups()` works fine with the option `gene_symbols=""gene_symbols""` and the same `adata` object. ### Minimal code sample. ```python. sc.pl.rank_genes_groups_violin(. adata, groups=cell_types, n_genes=15, strip=False, gene_symbols=""gene_symbols"". ). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[258], line 1. ----> 1 sc.pl.rank_genes_groups_violin(. 2 adata, groups=cell_types, n_genes=15, strip=False, gene_symbols=""gene_symbols"". 3 ). File ~/.conda/envs/scirpy/lib/python3.10/site-packages/scanpy/plotting/_tools/__init__.py:1157, in rank_genes_groups_violin(adata, groups, n_genes, gene_names, gene_symbols, use_raw, key, split, scale, strip, jitter, size, ax, show, save). 1155 if isinstance(_gene_names, np.ndarray):. 1156 _gene_names = _gene_names.tolist(). -> 1157 df = obs_df(adata, _gene_names, use_raw=use_raw, gene_symbols=gene_symbols). 1158 new_gene_names = df.columns. 1159 df['hue'] = adata.obs[groups_key].astype(str).values. File ~/.conda/envs/scirpy/lib/python3.10/site-packages/scanpy/get/get.py:272, in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 269 else:. 270 alias_index = None. --> 272 obs_cols, var_idx_keys, var_symbols = _check_indices(. 273 adata.obs,. 274 var.index,. 275 ""obs"",. 276 keys,. 277 alias_index=alias_index,. 278 use_raw=use_raw,. 279 ). 281 # Make df. 282 df = pd.DataFrame(index=adata.obs_names). File ~/.conda/envs/scirpy/lib/python3.10/site-packages/scanpy/get/get.py:167, in _check_indices(dim_df, alt_index, dim, keys, alias_index, use_raw). 165 not_found.append(key). 166 if len(not_found) > 0:. --> 167 raise KeyError(. 168 f""Could not find keys '{not_found}' in colum",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2540
https://github.com/scverse/scanpy/issues/2540:2861,deployability,Version,Versions,2861,"274 var.index,. 275 ""obs"",. 276 keys,. 277 alias_index=alias_index,. 278 use_raw=use_raw,. 279 ). 281 # Make df. 282 df = pd.DataFrame(index=adata.obs_names). File ~/.conda/envs/scirpy/lib/python3.10/site-packages/scanpy/get/get.py:167, in _check_indices(dim_df, alt_index, dim, keys, alias_index, use_raw). 165 not_found.append(key). 166 if len(not_found) > 0:. --> 167 raise KeyError(. 168 f""Could not find keys '{not_found}' in columns of `adata.{dim}` or in"". 169 f"" {alt_repr}.{alt_search_repr}."". 170 ). 172 return col_keys, index_keys, index_aliases. KeyError: ""Could not find keys '['ENSMUSG00000003420', 'ENSMUSG00000022584', 'ENSMUSG00000024910', 'ENSMUSG00000028495', 'ENSMUSG00000032446', 'ENSMUSG00000035042', 'ENSMUSG00000045777', 'ENSMUSG00000051354', 'ENSMUSG00000054206', 'ENSMUSG00000056209', 'ENSMUSG00000056290', 'ENSMUSG00000057113', 'ENSMUSG00000057329', 'ENSMUSG00000064023', 'ENSMUSG00000076461']' in columns of `adata.obs` or in adata.raw.var['gene_symbols']."". ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. Levenshtein 0.21.0. PIL 9.5.0. adjustText NA. airr 1.4.1. asttokens NA. awkward 2.2.2. awkward_cpp NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. brotli NA. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.8.0. hypergeom_ufunc NA. idna 3.4. igraph 0.10.4. invgauss_ufunc NA. ipykernel 6.22.0. ipywidgets 8.0.6. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. mudata 0.2.3. muon 0.1.5. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. networkx 3.1. numba 0.56.4. numpy 1.23.5. packaging 23.1. pandas 1.5.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.2.0. pooch v1.7.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2540
https://github.com/scverse/scanpy/issues/2540:4794,deployability,updat,updated,4794,"symbols']."". ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. Levenshtein 0.21.0. PIL 9.5.0. adjustText NA. airr 1.4.1. asttokens NA. awkward 2.2.2. awkward_cpp NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. brotli NA. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.8.0. hypergeom_ufunc NA. idna 3.4. igraph 0.10.4. invgauss_ufunc NA. ipykernel 6.22.0. ipywidgets 8.0.6. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. mudata 0.2.3. muon 0.1.5. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. networkx 3.1. numba 0.56.4. numpy 1.23.5. packaging 23.1. pandas 1.5.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.2.0. pooch v1.7.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. rapidfuzz 2.15.1. requests 2.28.2. scipy 1.10.1. scirpy 0.13.1.dev2+g4ed908b. seaborn 0.12.2. session_info 1.0.0. setuptools 67.7.1. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. socks 1.7.1. squarify NA. stack_data 0.6.2. statsmodels 0.13.5. texttable 1.6.7. threadpoolctl 3.1.0. torch 1.12.1.post201. tornado 6.3. tqdm 4.65.0. tracerlib NA. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. urllib3 1.26.15. wcwidth 0.2.6. yaml 6.0. yamlordereddictloader NA. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.2. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]. Linux-3.10.0-1160.66.1.el7.x86_64-x86_64-with-glibc2.17. -----. Session information updated at 2023-06-28 18:02. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2540
https://github.com/scverse/scanpy/issues/2540:1298,energy efficiency,scale,scale,1298,"med this bug exists on the master branch of scanpy. ### What happened? When trying to produce a violin plot using `pl.rank_genes_groups_violin()` of the ranked genes and passing `gene_symbols=""gene_symbols""` I get a `KeyError`. However `pl.rank_genes_groups()` works fine with the option `gene_symbols=""gene_symbols""` and the same `adata` object. ### Minimal code sample. ```python. sc.pl.rank_genes_groups_violin(. adata, groups=cell_types, n_genes=15, strip=False, gene_symbols=""gene_symbols"". ). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[258], line 1. ----> 1 sc.pl.rank_genes_groups_violin(. 2 adata, groups=cell_types, n_genes=15, strip=False, gene_symbols=""gene_symbols"". 3 ). File ~/.conda/envs/scirpy/lib/python3.10/site-packages/scanpy/plotting/_tools/__init__.py:1157, in rank_genes_groups_violin(adata, groups, n_genes, gene_names, gene_symbols, use_raw, key, split, scale, strip, jitter, size, ax, show, save). 1155 if isinstance(_gene_names, np.ndarray):. 1156 _gene_names = _gene_names.tolist(). -> 1157 df = obs_df(adata, _gene_names, use_raw=use_raw, gene_symbols=gene_symbols). 1158 new_gene_names = df.columns. 1159 df['hue'] = adata.obs[groups_key].astype(str).values. File ~/.conda/envs/scirpy/lib/python3.10/site-packages/scanpy/get/get.py:272, in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 269 else:. 270 alias_index = None. --> 272 obs_cols, var_idx_keys, var_symbols = _check_indices(. 273 adata.obs,. 274 var.index,. 275 ""obs"",. 276 keys,. 277 alias_index=alias_index,. 278 use_raw=use_raw,. 279 ). 281 # Make df. 282 df = pd.DataFrame(index=adata.obs_names). File ~/.conda/envs/scirpy/lib/python3.10/site-packages/scanpy/get/get.py:167, in _check_indices(dim_df, alt_index, dim, keys, alias_index, use_raw). 165 not_found.append(key). 166 if len(not_found) > 0:. --> 167 raise KeyError(. 168 f""Could not find keys '{not_found}' in colum",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2540
https://github.com/scverse/scanpy/issues/2540:252,integrability,version,version,252,"pl.rank_genes_groups_violin() throws a KeyError when gene_symbols option is passed; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When trying to produce a violin plot using `pl.rank_genes_groups_violin()` of the ranked genes and passing `gene_symbols=""gene_symbols""` I get a `KeyError`. However `pl.rank_genes_groups()` works fine with the option `gene_symbols=""gene_symbols""` and the same `adata` object. ### Minimal code sample. ```python. sc.pl.rank_genes_groups_violin(. adata, groups=cell_types, n_genes=15, strip=False, gene_symbols=""gene_symbols"". ). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[258], line 1. ----> 1 sc.pl.rank_genes_groups_violin(. 2 adata, groups=cell_types, n_genes=15, strip=False, gene_symbols=""gene_symbols"". 3 ). File ~/.conda/envs/scirpy/lib/python3.10/site-packages/scanpy/plotting/_tools/__init__.py:1157, in rank_genes_groups_violin(adata, groups, n_genes, gene_names, gene_symbols, use_raw, key, split, scale, strip, jitter, size, ax, show, save). 1155 if isinstance(_gene_names, np.ndarray):. 1156 _gene_names = _gene_names.tolist(). -> 1157 df = obs_df(adata, _gene_names, use_raw=use_raw, gene_symbols=gene_symbols). 1158 new_gene_names = df.columns. 1159 df['hue'] = adata.obs[groups_key].astype(str).values. File ~/.conda/envs/scirpy/lib/python3.10/site-packages/scanpy/get/get.py:272, in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 269 else:. 270 alias_index = None. --> 272 obs_cols, var_idx_keys, var_symbols = _check_indices(. 273 adata.obs,. 274 var.index,. 275 ""obs"",. 276 keys,. 277 alias_index=alias_index,. 278 use_raw=use_raw,. 279 ). 281 # Make df. 282 df = pd.DataFrame(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2540
https://github.com/scverse/scanpy/issues/2540:2861,integrability,Version,Versions,2861,"274 var.index,. 275 ""obs"",. 276 keys,. 277 alias_index=alias_index,. 278 use_raw=use_raw,. 279 ). 281 # Make df. 282 df = pd.DataFrame(index=adata.obs_names). File ~/.conda/envs/scirpy/lib/python3.10/site-packages/scanpy/get/get.py:167, in _check_indices(dim_df, alt_index, dim, keys, alias_index, use_raw). 165 not_found.append(key). 166 if len(not_found) > 0:. --> 167 raise KeyError(. 168 f""Could not find keys '{not_found}' in columns of `adata.{dim}` or in"". 169 f"" {alt_repr}.{alt_search_repr}."". 170 ). 172 return col_keys, index_keys, index_aliases. KeyError: ""Could not find keys '['ENSMUSG00000003420', 'ENSMUSG00000022584', 'ENSMUSG00000024910', 'ENSMUSG00000028495', 'ENSMUSG00000032446', 'ENSMUSG00000035042', 'ENSMUSG00000045777', 'ENSMUSG00000051354', 'ENSMUSG00000054206', 'ENSMUSG00000056209', 'ENSMUSG00000056290', 'ENSMUSG00000057113', 'ENSMUSG00000057329', 'ENSMUSG00000064023', 'ENSMUSG00000076461']' in columns of `adata.obs` or in adata.raw.var['gene_symbols']."". ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. Levenshtein 0.21.0. PIL 9.5.0. adjustText NA. airr 1.4.1. asttokens NA. awkward 2.2.2. awkward_cpp NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. brotli NA. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.8.0. hypergeom_ufunc NA. idna 3.4. igraph 0.10.4. invgauss_ufunc NA. ipykernel 6.22.0. ipywidgets 8.0.6. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. mudata 0.2.3. muon 0.1.5. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. networkx 3.1. numba 0.56.4. numpy 1.23.5. packaging 23.1. pandas 1.5.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.2.0. pooch v1.7.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2540
https://github.com/scverse/scanpy/issues/2540:3766,interoperability,platform,platformdirs,3766,"00000076461']' in columns of `adata.obs` or in adata.raw.var['gene_symbols']."". ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. Levenshtein 0.21.0. PIL 9.5.0. adjustText NA. airr 1.4.1. asttokens NA. awkward 2.2.2. awkward_cpp NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. brotli NA. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.8.0. hypergeom_ufunc NA. idna 3.4. igraph 0.10.4. invgauss_ufunc NA. ipykernel 6.22.0. ipywidgets 8.0.6. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. mudata 0.2.3. muon 0.1.5. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. networkx 3.1. numba 0.56.4. numpy 1.23.5. packaging 23.1. pandas 1.5.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.2.0. pooch v1.7.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. rapidfuzz 2.15.1. requests 2.28.2. scipy 1.10.1. scirpy 0.13.1.dev2+g4ed908b. seaborn 0.12.2. session_info 1.0.0. setuptools 67.7.1. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. socks 1.7.1. squarify NA. stack_data 0.6.2. statsmodels 0.13.5. texttable 1.6.7. threadpoolctl 3.1.0. torch 1.12.1.post201. tornado 6.3. tqdm 4.65.0. tracerlib NA. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. urllib3 1.26.15. wcwidth 0.2.6. yaml 6.0. yamlordereddictloader NA. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.2. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]. Linux-3.10.0-1160.66.1.el7.x86_64-x86_64-with-glibc2.17. -----",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2540
https://github.com/scverse/scanpy/issues/2540:252,modifiability,version,version,252,"pl.rank_genes_groups_violin() throws a KeyError when gene_symbols option is passed; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When trying to produce a violin plot using `pl.rank_genes_groups_violin()` of the ranked genes and passing `gene_symbols=""gene_symbols""` I get a `KeyError`. However `pl.rank_genes_groups()` works fine with the option `gene_symbols=""gene_symbols""` and the same `adata` object. ### Minimal code sample. ```python. sc.pl.rank_genes_groups_violin(. adata, groups=cell_types, n_genes=15, strip=False, gene_symbols=""gene_symbols"". ). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[258], line 1. ----> 1 sc.pl.rank_genes_groups_violin(. 2 adata, groups=cell_types, n_genes=15, strip=False, gene_symbols=""gene_symbols"". 3 ). File ~/.conda/envs/scirpy/lib/python3.10/site-packages/scanpy/plotting/_tools/__init__.py:1157, in rank_genes_groups_violin(adata, groups, n_genes, gene_names, gene_symbols, use_raw, key, split, scale, strip, jitter, size, ax, show, save). 1155 if isinstance(_gene_names, np.ndarray):. 1156 _gene_names = _gene_names.tolist(). -> 1157 df = obs_df(adata, _gene_names, use_raw=use_raw, gene_symbols=gene_symbols). 1158 new_gene_names = df.columns. 1159 df['hue'] = adata.obs[groups_key].astype(str).values. File ~/.conda/envs/scirpy/lib/python3.10/site-packages/scanpy/get/get.py:272, in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 269 else:. 270 alias_index = None. --> 272 obs_cols, var_idx_keys, var_symbols = _check_indices(. 273 adata.obs,. 274 var.index,. 275 ""obs"",. 276 keys,. 277 alias_index=alias_index,. 278 use_raw=use_raw,. 279 ). 281 # Make df. 282 df = pd.DataFrame(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2540
https://github.com/scverse/scanpy/issues/2540:1149,modifiability,pac,packages,1149,"hat this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When trying to produce a violin plot using `pl.rank_genes_groups_violin()` of the ranked genes and passing `gene_symbols=""gene_symbols""` I get a `KeyError`. However `pl.rank_genes_groups()` works fine with the option `gene_symbols=""gene_symbols""` and the same `adata` object. ### Minimal code sample. ```python. sc.pl.rank_genes_groups_violin(. adata, groups=cell_types, n_genes=15, strip=False, gene_symbols=""gene_symbols"". ). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[258], line 1. ----> 1 sc.pl.rank_genes_groups_violin(. 2 adata, groups=cell_types, n_genes=15, strip=False, gene_symbols=""gene_symbols"". 3 ). File ~/.conda/envs/scirpy/lib/python3.10/site-packages/scanpy/plotting/_tools/__init__.py:1157, in rank_genes_groups_violin(adata, groups, n_genes, gene_names, gene_symbols, use_raw, key, split, scale, strip, jitter, size, ax, show, save). 1155 if isinstance(_gene_names, np.ndarray):. 1156 _gene_names = _gene_names.tolist(). -> 1157 df = obs_df(adata, _gene_names, use_raw=use_raw, gene_symbols=gene_symbols). 1158 new_gene_names = df.columns. 1159 df['hue'] = adata.obs[groups_key].astype(str).values. File ~/.conda/envs/scirpy/lib/python3.10/site-packages/scanpy/get/get.py:272, in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 269 else:. 270 alias_index = None. --> 272 obs_cols, var_idx_keys, var_symbols = _check_indices(. 273 adata.obs,. 274 var.index,. 275 ""obs"",. 276 keys,. 277 alias_index=alias_index,. 278 use_raw=use_raw,. 279 ). 281 # Make df. 282 df = pd.DataFrame(index=adata.obs_names). File ~/.conda/envs/scirpy/lib/python3.10/site-packages/scanpy/get/get.py:167, in _check_indices(dim_df, alt_index, dim, keys, ali",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2540
https://github.com/scverse/scanpy/issues/2540:1298,modifiability,scal,scale,1298,"med this bug exists on the master branch of scanpy. ### What happened? When trying to produce a violin plot using `pl.rank_genes_groups_violin()` of the ranked genes and passing `gene_symbols=""gene_symbols""` I get a `KeyError`. However `pl.rank_genes_groups()` works fine with the option `gene_symbols=""gene_symbols""` and the same `adata` object. ### Minimal code sample. ```python. sc.pl.rank_genes_groups_violin(. adata, groups=cell_types, n_genes=15, strip=False, gene_symbols=""gene_symbols"". ). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[258], line 1. ----> 1 sc.pl.rank_genes_groups_violin(. 2 adata, groups=cell_types, n_genes=15, strip=False, gene_symbols=""gene_symbols"". 3 ). File ~/.conda/envs/scirpy/lib/python3.10/site-packages/scanpy/plotting/_tools/__init__.py:1157, in rank_genes_groups_violin(adata, groups, n_genes, gene_names, gene_symbols, use_raw, key, split, scale, strip, jitter, size, ax, show, save). 1155 if isinstance(_gene_names, np.ndarray):. 1156 _gene_names = _gene_names.tolist(). -> 1157 df = obs_df(adata, _gene_names, use_raw=use_raw, gene_symbols=gene_symbols). 1158 new_gene_names = df.columns. 1159 df['hue'] = adata.obs[groups_key].astype(str).values. File ~/.conda/envs/scirpy/lib/python3.10/site-packages/scanpy/get/get.py:272, in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 269 else:. 270 alias_index = None. --> 272 obs_cols, var_idx_keys, var_symbols = _check_indices(. 273 adata.obs,. 274 var.index,. 275 ""obs"",. 276 keys,. 277 alias_index=alias_index,. 278 use_raw=use_raw,. 279 ). 281 # Make df. 282 df = pd.DataFrame(index=adata.obs_names). File ~/.conda/envs/scirpy/lib/python3.10/site-packages/scanpy/get/get.py:167, in _check_indices(dim_df, alt_index, dim, keys, alias_index, use_raw). 165 not_found.append(key). 166 if len(not_found) > 0:. --> 167 raise KeyError(. 168 f""Could not find keys '{not_found}' in colum",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2540
https://github.com/scverse/scanpy/issues/2540:1654,modifiability,pac,packages,1654,"l code sample. ```python. sc.pl.rank_genes_groups_violin(. adata, groups=cell_types, n_genes=15, strip=False, gene_symbols=""gene_symbols"". ). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[258], line 1. ----> 1 sc.pl.rank_genes_groups_violin(. 2 adata, groups=cell_types, n_genes=15, strip=False, gene_symbols=""gene_symbols"". 3 ). File ~/.conda/envs/scirpy/lib/python3.10/site-packages/scanpy/plotting/_tools/__init__.py:1157, in rank_genes_groups_violin(adata, groups, n_genes, gene_names, gene_symbols, use_raw, key, split, scale, strip, jitter, size, ax, show, save). 1155 if isinstance(_gene_names, np.ndarray):. 1156 _gene_names = _gene_names.tolist(). -> 1157 df = obs_df(adata, _gene_names, use_raw=use_raw, gene_symbols=gene_symbols). 1158 new_gene_names = df.columns. 1159 df['hue'] = adata.obs[groups_key].astype(str).values. File ~/.conda/envs/scirpy/lib/python3.10/site-packages/scanpy/get/get.py:272, in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 269 else:. 270 alias_index = None. --> 272 obs_cols, var_idx_keys, var_symbols = _check_indices(. 273 adata.obs,. 274 var.index,. 275 ""obs"",. 276 keys,. 277 alias_index=alias_index,. 278 use_raw=use_raw,. 279 ). 281 # Make df. 282 df = pd.DataFrame(index=adata.obs_names). File ~/.conda/envs/scirpy/lib/python3.10/site-packages/scanpy/get/get.py:167, in _check_indices(dim_df, alt_index, dim, keys, alias_index, use_raw). 165 not_found.append(key). 166 if len(not_found) > 0:. --> 167 raise KeyError(. 168 f""Could not find keys '{not_found}' in columns of `adata.{dim}` or in"". 169 f"" {alt_repr}.{alt_search_repr}."". 170 ). 172 return col_keys, index_keys, index_aliases. KeyError: ""Could not find keys '['ENSMUSG00000003420', 'ENSMUSG00000022584', 'ENSMUSG00000024910', 'ENSMUSG00000028495', 'ENSMUSG00000032446', 'ENSMUSG00000035042', 'ENSMUSG00000045777', 'ENSMUSG00000051354', 'ENSMUSG00000054206', 'ENS",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2540
https://github.com/scverse/scanpy/issues/2540:1720,modifiability,layer,layer,1720," groups=cell_types, n_genes=15, strip=False, gene_symbols=""gene_symbols"". ). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[258], line 1. ----> 1 sc.pl.rank_genes_groups_violin(. 2 adata, groups=cell_types, n_genes=15, strip=False, gene_symbols=""gene_symbols"". 3 ). File ~/.conda/envs/scirpy/lib/python3.10/site-packages/scanpy/plotting/_tools/__init__.py:1157, in rank_genes_groups_violin(adata, groups, n_genes, gene_names, gene_symbols, use_raw, key, split, scale, strip, jitter, size, ax, show, save). 1155 if isinstance(_gene_names, np.ndarray):. 1156 _gene_names = _gene_names.tolist(). -> 1157 df = obs_df(adata, _gene_names, use_raw=use_raw, gene_symbols=gene_symbols). 1158 new_gene_names = df.columns. 1159 df['hue'] = adata.obs[groups_key].astype(str).values. File ~/.conda/envs/scirpy/lib/python3.10/site-packages/scanpy/get/get.py:272, in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 269 else:. 270 alias_index = None. --> 272 obs_cols, var_idx_keys, var_symbols = _check_indices(. 273 adata.obs,. 274 var.index,. 275 ""obs"",. 276 keys,. 277 alias_index=alias_index,. 278 use_raw=use_raw,. 279 ). 281 # Make df. 282 df = pd.DataFrame(index=adata.obs_names). File ~/.conda/envs/scirpy/lib/python3.10/site-packages/scanpy/get/get.py:167, in _check_indices(dim_df, alt_index, dim, keys, alias_index, use_raw). 165 not_found.append(key). 166 if len(not_found) > 0:. --> 167 raise KeyError(. 168 f""Could not find keys '{not_found}' in columns of `adata.{dim}` or in"". 169 f"" {alt_repr}.{alt_search_repr}."". 170 ). 172 return col_keys, index_keys, index_aliases. KeyError: ""Could not find keys '['ENSMUSG00000003420', 'ENSMUSG00000022584', 'ENSMUSG00000024910', 'ENSMUSG00000028495', 'ENSMUSG00000032446', 'ENSMUSG00000035042', 'ENSMUSG00000045777', 'ENSMUSG00000051354', 'ENSMUSG00000054206', 'ENSMUSG00000056209', 'ENSMUSG00000056290', 'ENSMUSG00000057113', 'EN",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2540
https://github.com/scverse/scanpy/issues/2540:2070,modifiability,pac,packages,2070,"symbols=""gene_symbols"". 3 ). File ~/.conda/envs/scirpy/lib/python3.10/site-packages/scanpy/plotting/_tools/__init__.py:1157, in rank_genes_groups_violin(adata, groups, n_genes, gene_names, gene_symbols, use_raw, key, split, scale, strip, jitter, size, ax, show, save). 1155 if isinstance(_gene_names, np.ndarray):. 1156 _gene_names = _gene_names.tolist(). -> 1157 df = obs_df(adata, _gene_names, use_raw=use_raw, gene_symbols=gene_symbols). 1158 new_gene_names = df.columns. 1159 df['hue'] = adata.obs[groups_key].astype(str).values. File ~/.conda/envs/scirpy/lib/python3.10/site-packages/scanpy/get/get.py:272, in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 269 else:. 270 alias_index = None. --> 272 obs_cols, var_idx_keys, var_symbols = _check_indices(. 273 adata.obs,. 274 var.index,. 275 ""obs"",. 276 keys,. 277 alias_index=alias_index,. 278 use_raw=use_raw,. 279 ). 281 # Make df. 282 df = pd.DataFrame(index=adata.obs_names). File ~/.conda/envs/scirpy/lib/python3.10/site-packages/scanpy/get/get.py:167, in _check_indices(dim_df, alt_index, dim, keys, alias_index, use_raw). 165 not_found.append(key). 166 if len(not_found) > 0:. --> 167 raise KeyError(. 168 f""Could not find keys '{not_found}' in columns of `adata.{dim}` or in"". 169 f"" {alt_repr}.{alt_search_repr}."". 170 ). 172 return col_keys, index_keys, index_aliases. KeyError: ""Could not find keys '['ENSMUSG00000003420', 'ENSMUSG00000022584', 'ENSMUSG00000024910', 'ENSMUSG00000028495', 'ENSMUSG00000032446', 'ENSMUSG00000035042', 'ENSMUSG00000045777', 'ENSMUSG00000051354', 'ENSMUSG00000054206', 'ENSMUSG00000056209', 'ENSMUSG00000056290', 'ENSMUSG00000057113', 'ENSMUSG00000057329', 'ENSMUSG00000064023', 'ENSMUSG00000076461']' in columns of `adata.obs` or in adata.raw.var['gene_symbols']."". ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. Levenshtein 0.21.0. PIL 9.5.0. adjustText NA. airr 1.4.1. asttokens NA. awkward 2.2.2. awkward_cpp NA. backcall 0.2.0. beta_ufunc NA. binom_ufun",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2540
https://github.com/scverse/scanpy/issues/2540:2861,modifiability,Version,Versions,2861,"274 var.index,. 275 ""obs"",. 276 keys,. 277 alias_index=alias_index,. 278 use_raw=use_raw,. 279 ). 281 # Make df. 282 df = pd.DataFrame(index=adata.obs_names). File ~/.conda/envs/scirpy/lib/python3.10/site-packages/scanpy/get/get.py:167, in _check_indices(dim_df, alt_index, dim, keys, alias_index, use_raw). 165 not_found.append(key). 166 if len(not_found) > 0:. --> 167 raise KeyError(. 168 f""Could not find keys '{not_found}' in columns of `adata.{dim}` or in"". 169 f"" {alt_repr}.{alt_search_repr}."". 170 ). 172 return col_keys, index_keys, index_aliases. KeyError: ""Could not find keys '['ENSMUSG00000003420', 'ENSMUSG00000022584', 'ENSMUSG00000024910', 'ENSMUSG00000028495', 'ENSMUSG00000032446', 'ENSMUSG00000035042', 'ENSMUSG00000045777', 'ENSMUSG00000051354', 'ENSMUSG00000054206', 'ENSMUSG00000056209', 'ENSMUSG00000056290', 'ENSMUSG00000057113', 'ENSMUSG00000057329', 'ENSMUSG00000064023', 'ENSMUSG00000076461']' in columns of `adata.obs` or in adata.raw.var['gene_symbols']."". ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. Levenshtein 0.21.0. PIL 9.5.0. adjustText NA. airr 1.4.1. asttokens NA. awkward 2.2.2. awkward_cpp NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. brotli NA. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.8.0. hypergeom_ufunc NA. idna 3.4. igraph 0.10.4. invgauss_ufunc NA. ipykernel 6.22.0. ipywidgets 8.0.6. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. mudata 0.2.3. muon 0.1.5. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. networkx 3.1. numba 0.56.4. numpy 1.23.5. packaging 23.1. pandas 1.5.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.2.0. pooch v1.7.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2540
https://github.com/scverse/scanpy/issues/2540:3243,modifiability,deco,decorator,3243,"or(. 168 f""Could not find keys '{not_found}' in columns of `adata.{dim}` or in"". 169 f"" {alt_repr}.{alt_search_repr}."". 170 ). 172 return col_keys, index_keys, index_aliases. KeyError: ""Could not find keys '['ENSMUSG00000003420', 'ENSMUSG00000022584', 'ENSMUSG00000024910', 'ENSMUSG00000028495', 'ENSMUSG00000032446', 'ENSMUSG00000035042', 'ENSMUSG00000045777', 'ENSMUSG00000051354', 'ENSMUSG00000054206', 'ENSMUSG00000056209', 'ENSMUSG00000056290', 'ENSMUSG00000057113', 'ENSMUSG00000057329', 'ENSMUSG00000064023', 'ENSMUSG00000076461']' in columns of `adata.obs` or in adata.raw.var['gene_symbols']."". ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. Levenshtein 0.21.0. PIL 9.5.0. adjustText NA. airr 1.4.1. asttokens NA. awkward 2.2.2. awkward_cpp NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. brotli NA. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.8.0. hypergeom_ufunc NA. idna 3.4. igraph 0.10.4. invgauss_ufunc NA. ipykernel 6.22.0. ipywidgets 8.0.6. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. mudata 0.2.3. muon 0.1.5. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. networkx 3.1. numba 0.56.4. numpy 1.23.5. packaging 23.1. pandas 1.5.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.2.0. pooch v1.7.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. rapidfuzz 2.15.1. requests 2.28.2. scipy 1.10.1. scirpy 0.13.1.dev2+g4ed908b. seaborn 0.12.2. session_info 1.0.0. setuptools 67.7.1. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. socks",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2540
https://github.com/scverse/scanpy/issues/2540:3671,modifiability,pac,packaging,3671,"'ENSMUSG00000056290', 'ENSMUSG00000057113', 'ENSMUSG00000057329', 'ENSMUSG00000064023', 'ENSMUSG00000076461']' in columns of `adata.obs` or in adata.raw.var['gene_symbols']."". ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. Levenshtein 0.21.0. PIL 9.5.0. adjustText NA. airr 1.4.1. asttokens NA. awkward 2.2.2. awkward_cpp NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. brotli NA. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.8.0. hypergeom_ufunc NA. idna 3.4. igraph 0.10.4. invgauss_ufunc NA. ipykernel 6.22.0. ipywidgets 8.0.6. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. mudata 0.2.3. muon 0.1.5. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. networkx 3.1. numba 0.56.4. numpy 1.23.5. packaging 23.1. pandas 1.5.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.2.0. pooch v1.7.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. rapidfuzz 2.15.1. requests 2.28.2. scipy 1.10.1. scirpy 0.13.1.dev2+g4ed908b. seaborn 0.12.2. session_info 1.0.0. setuptools 67.7.1. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. socks 1.7.1. squarify NA. stack_data 0.6.2. statsmodels 0.13.5. texttable 1.6.7. threadpoolctl 3.1.0. torch 1.12.1.post201. tornado 6.3. tqdm 4.65.0. tracerlib NA. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. urllib3 1.26.15. wcwidth 0.2.6. yaml 6.0. yamlordereddictloader NA. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.2. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2540
https://github.com/scverse/scanpy/issues/2540:4640,modifiability,pac,packaged,4640,"symbols']."". ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. Levenshtein 0.21.0. PIL 9.5.0. adjustText NA. airr 1.4.1. asttokens NA. awkward 2.2.2. awkward_cpp NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. brotli NA. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.8.0. hypergeom_ufunc NA. idna 3.4. igraph 0.10.4. invgauss_ufunc NA. ipykernel 6.22.0. ipywidgets 8.0.6. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. mudata 0.2.3. muon 0.1.5. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. networkx 3.1. numba 0.56.4. numpy 1.23.5. packaging 23.1. pandas 1.5.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.2.0. pooch v1.7.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. rapidfuzz 2.15.1. requests 2.28.2. scipy 1.10.1. scirpy 0.13.1.dev2+g4ed908b. seaborn 0.12.2. session_info 1.0.0. setuptools 67.7.1. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. socks 1.7.1. squarify NA. stack_data 0.6.2. statsmodels 0.13.5. texttable 1.6.7. threadpoolctl 3.1.0. torch 1.12.1.post201. tornado 6.3. tqdm 4.65.0. tracerlib NA. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. urllib3 1.26.15. wcwidth 0.2.6. yaml 6.0. yamlordereddictloader NA. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.2. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]. Linux-3.10.0-1160.66.1.el7.x86_64-x86_64-with-glibc2.17. -----. Session information updated at 2023-06-28 18:02. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2540
https://github.com/scverse/scanpy/issues/2540:809,performance,Error,Error,809,"pl.rank_genes_groups_violin() throws a KeyError when gene_symbols option is passed; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When trying to produce a violin plot using `pl.rank_genes_groups_violin()` of the ranked genes and passing `gene_symbols=""gene_symbols""` I get a `KeyError`. However `pl.rank_genes_groups()` works fine with the option `gene_symbols=""gene_symbols""` and the same `adata` object. ### Minimal code sample. ```python. sc.pl.rank_genes_groups_violin(. adata, groups=cell_types, n_genes=15, strip=False, gene_symbols=""gene_symbols"". ). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[258], line 1. ----> 1 sc.pl.rank_genes_groups_violin(. 2 adata, groups=cell_types, n_genes=15, strip=False, gene_symbols=""gene_symbols"". 3 ). File ~/.conda/envs/scirpy/lib/python3.10/site-packages/scanpy/plotting/_tools/__init__.py:1157, in rank_genes_groups_violin(adata, groups, n_genes, gene_names, gene_symbols, use_raw, key, split, scale, strip, jitter, size, ax, show, save). 1155 if isinstance(_gene_names, np.ndarray):. 1156 _gene_names = _gene_names.tolist(). -> 1157 df = obs_df(adata, _gene_names, use_raw=use_raw, gene_symbols=gene_symbols). 1158 new_gene_names = df.columns. 1159 df['hue'] = adata.obs[groups_key].astype(str).values. File ~/.conda/envs/scirpy/lib/python3.10/site-packages/scanpy/get/get.py:272, in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 269 else:. 270 alias_index = None. --> 272 obs_cols, var_idx_keys, var_symbols = _check_indices(. 273 adata.obs,. 274 var.index,. 275 ""obs"",. 276 keys,. 277 alias_index=alias_index,. 278 use_raw=use_raw,. 279 ). 281 # Make df. 282 df = pd.DataFrame(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2540
https://github.com/scverse/scanpy/issues/2540:1298,performance,scale,scale,1298,"med this bug exists on the master branch of scanpy. ### What happened? When trying to produce a violin plot using `pl.rank_genes_groups_violin()` of the ranked genes and passing `gene_symbols=""gene_symbols""` I get a `KeyError`. However `pl.rank_genes_groups()` works fine with the option `gene_symbols=""gene_symbols""` and the same `adata` object. ### Minimal code sample. ```python. sc.pl.rank_genes_groups_violin(. adata, groups=cell_types, n_genes=15, strip=False, gene_symbols=""gene_symbols"". ). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[258], line 1. ----> 1 sc.pl.rank_genes_groups_violin(. 2 adata, groups=cell_types, n_genes=15, strip=False, gene_symbols=""gene_symbols"". 3 ). File ~/.conda/envs/scirpy/lib/python3.10/site-packages/scanpy/plotting/_tools/__init__.py:1157, in rank_genes_groups_violin(adata, groups, n_genes, gene_names, gene_symbols, use_raw, key, split, scale, strip, jitter, size, ax, show, save). 1155 if isinstance(_gene_names, np.ndarray):. 1156 _gene_names = _gene_names.tolist(). -> 1157 df = obs_df(adata, _gene_names, use_raw=use_raw, gene_symbols=gene_symbols). 1158 new_gene_names = df.columns. 1159 df['hue'] = adata.obs[groups_key].astype(str).values. File ~/.conda/envs/scirpy/lib/python3.10/site-packages/scanpy/get/get.py:272, in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 269 else:. 270 alias_index = None. --> 272 obs_cols, var_idx_keys, var_symbols = _check_indices(. 273 adata.obs,. 274 var.index,. 275 ""obs"",. 276 keys,. 277 alias_index=alias_index,. 278 use_raw=use_raw,. 279 ). 281 # Make df. 282 df = pd.DataFrame(index=adata.obs_names). File ~/.conda/envs/scirpy/lib/python3.10/site-packages/scanpy/get/get.py:167, in _check_indices(dim_df, alt_index, dim, keys, alias_index, use_raw). 165 not_found.append(key). 166 if len(not_found) > 0:. --> 167 raise KeyError(. 168 f""Could not find keys '{not_found}' in colum",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2540
https://github.com/scverse/scanpy/issues/2540:3629,performance,network,networkx,3629,"ENSMUSG00000054206', 'ENSMUSG00000056209', 'ENSMUSG00000056290', 'ENSMUSG00000057113', 'ENSMUSG00000057329', 'ENSMUSG00000064023', 'ENSMUSG00000076461']' in columns of `adata.obs` or in adata.raw.var['gene_symbols']."". ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. Levenshtein 0.21.0. PIL 9.5.0. adjustText NA. airr 1.4.1. asttokens NA. awkward 2.2.2. awkward_cpp NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. brotli NA. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.8.0. hypergeom_ufunc NA. idna 3.4. igraph 0.10.4. invgauss_ufunc NA. ipykernel 6.22.0. ipywidgets 8.0.6. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. mudata 0.2.3. muon 0.1.5. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. networkx 3.1. numba 0.56.4. numpy 1.23.5. packaging 23.1. pandas 1.5.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.2.0. pooch v1.7.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. rapidfuzz 2.15.1. requests 2.28.2. scipy 1.10.1. scirpy 0.13.1.dev2+g4ed908b. seaborn 0.12.2. session_info 1.0.0. setuptools 67.7.1. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. socks 1.7.1. squarify NA. stack_data 0.6.2. statsmodels 0.13.5. texttable 1.6.7. threadpoolctl 3.1.0. torch 1.12.1.post201. tornado 6.3. tqdm 4.65.0. tracerlib NA. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. urllib3 1.26.15. wcwidth 0.2.6. yaml 6.0. yamlordereddictloader NA. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.2. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2540
https://github.com/scverse/scanpy/issues/2540:809,safety,Error,Error,809,"pl.rank_genes_groups_violin() throws a KeyError when gene_symbols option is passed; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When trying to produce a violin plot using `pl.rank_genes_groups_violin()` of the ranked genes and passing `gene_symbols=""gene_symbols""` I get a `KeyError`. However `pl.rank_genes_groups()` works fine with the option `gene_symbols=""gene_symbols""` and the same `adata` object. ### Minimal code sample. ```python. sc.pl.rank_genes_groups_violin(. adata, groups=cell_types, n_genes=15, strip=False, gene_symbols=""gene_symbols"". ). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[258], line 1. ----> 1 sc.pl.rank_genes_groups_violin(. 2 adata, groups=cell_types, n_genes=15, strip=False, gene_symbols=""gene_symbols"". 3 ). File ~/.conda/envs/scirpy/lib/python3.10/site-packages/scanpy/plotting/_tools/__init__.py:1157, in rank_genes_groups_violin(adata, groups, n_genes, gene_names, gene_symbols, use_raw, key, split, scale, strip, jitter, size, ax, show, save). 1155 if isinstance(_gene_names, np.ndarray):. 1156 _gene_names = _gene_names.tolist(). -> 1157 df = obs_df(adata, _gene_names, use_raw=use_raw, gene_symbols=gene_symbols). 1158 new_gene_names = df.columns. 1159 df['hue'] = adata.obs[groups_key].astype(str).values. File ~/.conda/envs/scirpy/lib/python3.10/site-packages/scanpy/get/get.py:272, in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 269 else:. 270 alias_index = None. --> 272 obs_cols, var_idx_keys, var_symbols = _check_indices(. 273 adata.obs,. 274 var.index,. 275 ""obs"",. 276 keys,. 277 alias_index=alias_index,. 278 use_raw=use_raw,. 279 ). 281 # Make df. 282 df = pd.DataFrame(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2540
https://github.com/scverse/scanpy/issues/2540:4794,safety,updat,updated,4794,"symbols']."". ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. Levenshtein 0.21.0. PIL 9.5.0. adjustText NA. airr 1.4.1. asttokens NA. awkward 2.2.2. awkward_cpp NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. brotli NA. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.8.0. hypergeom_ufunc NA. idna 3.4. igraph 0.10.4. invgauss_ufunc NA. ipykernel 6.22.0. ipywidgets 8.0.6. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. mudata 0.2.3. muon 0.1.5. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. networkx 3.1. numba 0.56.4. numpy 1.23.5. packaging 23.1. pandas 1.5.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.2.0. pooch v1.7.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. rapidfuzz 2.15.1. requests 2.28.2. scipy 1.10.1. scirpy 0.13.1.dev2+g4ed908b. seaborn 0.12.2. session_info 1.0.0. setuptools 67.7.1. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. socks 1.7.1. squarify NA. stack_data 0.6.2. statsmodels 0.13.5. texttable 1.6.7. threadpoolctl 3.1.0. torch 1.12.1.post201. tornado 6.3. tqdm 4.65.0. tracerlib NA. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. urllib3 1.26.15. wcwidth 0.2.6. yaml 6.0. yamlordereddictloader NA. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.2. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]. Linux-3.10.0-1160.66.1.el7.x86_64-x86_64-with-glibc2.17. -----. Session information updated at 2023-06-28 18:02. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2540
https://github.com/scverse/scanpy/issues/2540:3091,security,certif,certifi,3091,"y:167, in _check_indices(dim_df, alt_index, dim, keys, alias_index, use_raw). 165 not_found.append(key). 166 if len(not_found) > 0:. --> 167 raise KeyError(. 168 f""Could not find keys '{not_found}' in columns of `adata.{dim}` or in"". 169 f"" {alt_repr}.{alt_search_repr}."". 170 ). 172 return col_keys, index_keys, index_aliases. KeyError: ""Could not find keys '['ENSMUSG00000003420', 'ENSMUSG00000022584', 'ENSMUSG00000024910', 'ENSMUSG00000028495', 'ENSMUSG00000032446', 'ENSMUSG00000035042', 'ENSMUSG00000045777', 'ENSMUSG00000051354', 'ENSMUSG00000054206', 'ENSMUSG00000056209', 'ENSMUSG00000056290', 'ENSMUSG00000057113', 'ENSMUSG00000057329', 'ENSMUSG00000064023', 'ENSMUSG00000076461']' in columns of `adata.obs` or in adata.raw.var['gene_symbols']."". ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. Levenshtein 0.21.0. PIL 9.5.0. adjustText NA. airr 1.4.1. asttokens NA. awkward 2.2.2. awkward_cpp NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. brotli NA. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.8.0. hypergeom_ufunc NA. idna 3.4. igraph 0.10.4. invgauss_ufunc NA. ipykernel 6.22.0. ipywidgets 8.0.6. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. mudata 0.2.3. muon 0.1.5. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. networkx 3.1. numba 0.56.4. numpy 1.23.5. packaging 23.1. pandas 1.5.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.2.0. pooch v1.7.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. rapidfuzz 2.15.1. requests 2.28",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2540
https://github.com/scverse/scanpy/issues/2540:3629,security,network,networkx,3629,"ENSMUSG00000054206', 'ENSMUSG00000056209', 'ENSMUSG00000056290', 'ENSMUSG00000057113', 'ENSMUSG00000057329', 'ENSMUSG00000064023', 'ENSMUSG00000076461']' in columns of `adata.obs` or in adata.raw.var['gene_symbols']."". ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. Levenshtein 0.21.0. PIL 9.5.0. adjustText NA. airr 1.4.1. asttokens NA. awkward 2.2.2. awkward_cpp NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. brotli NA. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.8.0. hypergeom_ufunc NA. idna 3.4. igraph 0.10.4. invgauss_ufunc NA. ipykernel 6.22.0. ipywidgets 8.0.6. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. mudata 0.2.3. muon 0.1.5. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. networkx 3.1. numba 0.56.4. numpy 1.23.5. packaging 23.1. pandas 1.5.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.2.0. pooch v1.7.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. rapidfuzz 2.15.1. requests 2.28.2. scipy 1.10.1. scirpy 0.13.1.dev2+g4ed908b. seaborn 0.12.2. session_info 1.0.0. setuptools 67.7.1. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. socks 1.7.1. squarify NA. stack_data 0.6.2. statsmodels 0.13.5. texttable 1.6.7. threadpoolctl 3.1.0. torch 1.12.1.post201. tornado 6.3. tqdm 4.65.0. tracerlib NA. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. urllib3 1.26.15. wcwidth 0.2.6. yaml 6.0. yamlordereddictloader NA. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.2. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2540
https://github.com/scverse/scanpy/issues/2540:4243,security,soc,socks,4243,"symbols']."". ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. Levenshtein 0.21.0. PIL 9.5.0. adjustText NA. airr 1.4.1. asttokens NA. awkward 2.2.2. awkward_cpp NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. brotli NA. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.8.0. hypergeom_ufunc NA. idna 3.4. igraph 0.10.4. invgauss_ufunc NA. ipykernel 6.22.0. ipywidgets 8.0.6. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. mudata 0.2.3. muon 0.1.5. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. networkx 3.1. numba 0.56.4. numpy 1.23.5. packaging 23.1. pandas 1.5.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.2.0. pooch v1.7.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. rapidfuzz 2.15.1. requests 2.28.2. scipy 1.10.1. scirpy 0.13.1.dev2+g4ed908b. seaborn 0.12.2. session_info 1.0.0. setuptools 67.7.1. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. socks 1.7.1. squarify NA. stack_data 0.6.2. statsmodels 0.13.5. texttable 1.6.7. threadpoolctl 3.1.0. torch 1.12.1.post201. tornado 6.3. tqdm 4.65.0. tracerlib NA. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. urllib3 1.26.15. wcwidth 0.2.6. yaml 6.0. yamlordereddictloader NA. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.2. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]. Linux-3.10.0-1160.66.1.el7.x86_64-x86_64-with-glibc2.17. -----. Session information updated at 2023-06-28 18:02. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2540
https://github.com/scverse/scanpy/issues/2540:4774,security,Session,Session,4774,"symbols']."". ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. Levenshtein 0.21.0. PIL 9.5.0. adjustText NA. airr 1.4.1. asttokens NA. awkward 2.2.2. awkward_cpp NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. brotli NA. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.8.0. hypergeom_ufunc NA. idna 3.4. igraph 0.10.4. invgauss_ufunc NA. ipykernel 6.22.0. ipywidgets 8.0.6. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. mudata 0.2.3. muon 0.1.5. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. networkx 3.1. numba 0.56.4. numpy 1.23.5. packaging 23.1. pandas 1.5.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.2.0. pooch v1.7.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. rapidfuzz 2.15.1. requests 2.28.2. scipy 1.10.1. scirpy 0.13.1.dev2+g4ed908b. seaborn 0.12.2. session_info 1.0.0. setuptools 67.7.1. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. socks 1.7.1. squarify NA. stack_data 0.6.2. statsmodels 0.13.5. texttable 1.6.7. threadpoolctl 3.1.0. torch 1.12.1.post201. tornado 6.3. tqdm 4.65.0. tracerlib NA. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. urllib3 1.26.15. wcwidth 0.2.6. yaml 6.0. yamlordereddictloader NA. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.2. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]. Linux-3.10.0-1160.66.1.el7.x86_64-x86_64-with-glibc2.17. -----. Session information updated at 2023-06-28 18:02. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2540
https://github.com/scverse/scanpy/issues/2540:4794,security,updat,updated,4794,"symbols']."". ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. Levenshtein 0.21.0. PIL 9.5.0. adjustText NA. airr 1.4.1. asttokens NA. awkward 2.2.2. awkward_cpp NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. brotli NA. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.8.0. hypergeom_ufunc NA. idna 3.4. igraph 0.10.4. invgauss_ufunc NA. ipykernel 6.22.0. ipywidgets 8.0.6. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. mudata 0.2.3. muon 0.1.5. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. networkx 3.1. numba 0.56.4. numpy 1.23.5. packaging 23.1. pandas 1.5.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.2.0. pooch v1.7.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. rapidfuzz 2.15.1. requests 2.28.2. scipy 1.10.1. scirpy 0.13.1.dev2+g4ed908b. seaborn 0.12.2. session_info 1.0.0. setuptools 67.7.1. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. socks 1.7.1. squarify NA. stack_data 0.6.2. statsmodels 0.13.5. texttable 1.6.7. threadpoolctl 3.1.0. torch 1.12.1.post201. tornado 6.3. tqdm 4.65.0. tracerlib NA. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. urllib3 1.26.15. wcwidth 0.2.6. yaml 6.0. yamlordereddictloader NA. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.2. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]. Linux-3.10.0-1160.66.1.el7.x86_64-x86_64-with-glibc2.17. -----. Session information updated at 2023-06-28 18:02. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2540
https://github.com/scverse/scanpy/issues/2540:918,testability,Trace,Traceback,918,"pl.rank_genes_groups_violin() throws a KeyError when gene_symbols option is passed; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When trying to produce a violin plot using `pl.rank_genes_groups_violin()` of the ranked genes and passing `gene_symbols=""gene_symbols""` I get a `KeyError`. However `pl.rank_genes_groups()` works fine with the option `gene_symbols=""gene_symbols""` and the same `adata` object. ### Minimal code sample. ```python. sc.pl.rank_genes_groups_violin(. adata, groups=cell_types, n_genes=15, strip=False, gene_symbols=""gene_symbols"". ). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[258], line 1. ----> 1 sc.pl.rank_genes_groups_violin(. 2 adata, groups=cell_types, n_genes=15, strip=False, gene_symbols=""gene_symbols"". 3 ). File ~/.conda/envs/scirpy/lib/python3.10/site-packages/scanpy/plotting/_tools/__init__.py:1157, in rank_genes_groups_violin(adata, groups, n_genes, gene_names, gene_symbols, use_raw, key, split, scale, strip, jitter, size, ax, show, save). 1155 if isinstance(_gene_names, np.ndarray):. 1156 _gene_names = _gene_names.tolist(). -> 1157 df = obs_df(adata, _gene_names, use_raw=use_raw, gene_symbols=gene_symbols). 1158 new_gene_names = df.columns. 1159 df['hue'] = adata.obs[groups_key].astype(str).values. File ~/.conda/envs/scirpy/lib/python3.10/site-packages/scanpy/get/get.py:272, in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 269 else:. 270 alias_index = None. --> 272 obs_cols, var_idx_keys, var_symbols = _check_indices(. 273 adata.obs,. 274 var.index,. 275 ""obs"",. 276 keys,. 277 alias_index=alias_index,. 278 use_raw=use_raw,. 279 ). 281 # Make df. 282 df = pd.DataFrame(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2540
https://github.com/scverse/scanpy/issues/2540:4393,testability,trace,tracerlib,4393,"symbols']."". ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. Levenshtein 0.21.0. PIL 9.5.0. adjustText NA. airr 1.4.1. asttokens NA. awkward 2.2.2. awkward_cpp NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. brotli NA. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. executing 1.2.0. h5py 3.8.0. hypergeom_ufunc NA. idna 3.4. igraph 0.10.4. invgauss_ufunc NA. ipykernel 6.22.0. ipywidgets 8.0.6. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.7.1. matplotlib_inline 0.1.6. mpl_toolkits NA. mudata 0.2.3. muon 0.1.5. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. networkx 3.1. numba 0.56.4. numpy 1.23.5. packaging 23.1. pandas 1.5.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.2.0. pooch v1.7.0. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pytz 2023.3. rapidfuzz 2.15.1. requests 2.28.2. scipy 1.10.1. scirpy 0.13.1.dev2+g4ed908b. seaborn 0.12.2. session_info 1.0.0. setuptools 67.7.1. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. socks 1.7.1. squarify NA. stack_data 0.6.2. statsmodels 0.13.5. texttable 1.6.7. threadpoolctl 3.1.0. torch 1.12.1.post201. tornado 6.3. tqdm 4.65.0. tracerlib NA. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. urllib3 1.26.15. wcwidth 0.2.6. yaml 6.0. yamlordereddictloader NA. zmq 25.0.2. zoneinfo NA. -----. IPython 8.13.2. jupyter_client 8.2.0. jupyter_core 5.3.0. -----. Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]. Linux-3.10.0-1160.66.1.el7.x86_64-x86_64-with-glibc2.17. -----. Session information updated at 2023-06-28 18:02. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2540
https://github.com/scverse/scanpy/issues/2540:212,usability,confirm,confirmed,212,"pl.rank_genes_groups_violin() throws a KeyError when gene_symbols option is passed; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When trying to produce a violin plot using `pl.rank_genes_groups_violin()` of the ranked genes and passing `gene_symbols=""gene_symbols""` I get a `KeyError`. However `pl.rank_genes_groups()` works fine with the option `gene_symbols=""gene_symbols""` and the same `adata` object. ### Minimal code sample. ```python. sc.pl.rank_genes_groups_violin(. adata, groups=cell_types, n_genes=15, strip=False, gene_symbols=""gene_symbols"". ). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[258], line 1. ----> 1 sc.pl.rank_genes_groups_violin(. 2 adata, groups=cell_types, n_genes=15, strip=False, gene_symbols=""gene_symbols"". 3 ). File ~/.conda/envs/scirpy/lib/python3.10/site-packages/scanpy/plotting/_tools/__init__.py:1157, in rank_genes_groups_violin(adata, groups, n_genes, gene_names, gene_symbols, use_raw, key, split, scale, strip, jitter, size, ax, show, save). 1155 if isinstance(_gene_names, np.ndarray):. 1156 _gene_names = _gene_names.tolist(). -> 1157 df = obs_df(adata, _gene_names, use_raw=use_raw, gene_symbols=gene_symbols). 1158 new_gene_names = df.columns. 1159 df['hue'] = adata.obs[groups_key].astype(str).values. File ~/.conda/envs/scirpy/lib/python3.10/site-packages/scanpy/get/get.py:272, in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 269 else:. 270 alias_index = None. --> 272 obs_cols, var_idx_keys, var_symbols = _check_indices(. 273 adata.obs,. 274 var.index,. 275 ""obs"",. 276 keys,. 277 alias_index=alias_index,. 278 use_raw=use_raw,. 279 ). 281 # Make df. 282 df = pd.DataFrame(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2540
https://github.com/scverse/scanpy/issues/2540:295,usability,confirm,confirmed,295,"pl.rank_genes_groups_violin() throws a KeyError when gene_symbols option is passed; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When trying to produce a violin plot using `pl.rank_genes_groups_violin()` of the ranked genes and passing `gene_symbols=""gene_symbols""` I get a `KeyError`. However `pl.rank_genes_groups()` works fine with the option `gene_symbols=""gene_symbols""` and the same `adata` object. ### Minimal code sample. ```python. sc.pl.rank_genes_groups_violin(. adata, groups=cell_types, n_genes=15, strip=False, gene_symbols=""gene_symbols"". ). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[258], line 1. ----> 1 sc.pl.rank_genes_groups_violin(. 2 adata, groups=cell_types, n_genes=15, strip=False, gene_symbols=""gene_symbols"". 3 ). File ~/.conda/envs/scirpy/lib/python3.10/site-packages/scanpy/plotting/_tools/__init__.py:1157, in rank_genes_groups_violin(adata, groups, n_genes, gene_names, gene_symbols, use_raw, key, split, scale, strip, jitter, size, ax, show, save). 1155 if isinstance(_gene_names, np.ndarray):. 1156 _gene_names = _gene_names.tolist(). -> 1157 df = obs_df(adata, _gene_names, use_raw=use_raw, gene_symbols=gene_symbols). 1158 new_gene_names = df.columns. 1159 df['hue'] = adata.obs[groups_key].astype(str).values. File ~/.conda/envs/scirpy/lib/python3.10/site-packages/scanpy/get/get.py:272, in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 269 else:. 270 alias_index = None. --> 272 obs_cols, var_idx_keys, var_symbols = _check_indices(. 273 adata.obs,. 274 var.index,. 275 ""obs"",. 276 keys,. 277 alias_index=alias_index,. 278 use_raw=use_raw,. 279 ). 281 # Make df. 282 df = pd.DataFrame(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2540
https://github.com/scverse/scanpy/issues/2540:652,usability,Minim,Minimal,652,"pl.rank_genes_groups_violin() throws a KeyError when gene_symbols option is passed; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When trying to produce a violin plot using `pl.rank_genes_groups_violin()` of the ranked genes and passing `gene_symbols=""gene_symbols""` I get a `KeyError`. However `pl.rank_genes_groups()` works fine with the option `gene_symbols=""gene_symbols""` and the same `adata` object. ### Minimal code sample. ```python. sc.pl.rank_genes_groups_violin(. adata, groups=cell_types, n_genes=15, strip=False, gene_symbols=""gene_symbols"". ). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[258], line 1. ----> 1 sc.pl.rank_genes_groups_violin(. 2 adata, groups=cell_types, n_genes=15, strip=False, gene_symbols=""gene_symbols"". 3 ). File ~/.conda/envs/scirpy/lib/python3.10/site-packages/scanpy/plotting/_tools/__init__.py:1157, in rank_genes_groups_violin(adata, groups, n_genes, gene_names, gene_symbols, use_raw, key, split, scale, strip, jitter, size, ax, show, save). 1155 if isinstance(_gene_names, np.ndarray):. 1156 _gene_names = _gene_names.tolist(). -> 1157 df = obs_df(adata, _gene_names, use_raw=use_raw, gene_symbols=gene_symbols). 1158 new_gene_names = df.columns. 1159 df['hue'] = adata.obs[groups_key].astype(str).values. File ~/.conda/envs/scirpy/lib/python3.10/site-packages/scanpy/get/get.py:272, in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 269 else:. 270 alias_index = None. --> 272 obs_cols, var_idx_keys, var_symbols = _check_indices(. 273 adata.obs,. 274 var.index,. 275 ""obs"",. 276 keys,. 277 alias_index=alias_index,. 278 use_raw=use_raw,. 279 ). 281 # Make df. 282 df = pd.DataFrame(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2540
https://github.com/scverse/scanpy/issues/2540:809,usability,Error,Error,809,"pl.rank_genes_groups_violin() throws a KeyError when gene_symbols option is passed; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When trying to produce a violin plot using `pl.rank_genes_groups_violin()` of the ranked genes and passing `gene_symbols=""gene_symbols""` I get a `KeyError`. However `pl.rank_genes_groups()` works fine with the option `gene_symbols=""gene_symbols""` and the same `adata` object. ### Minimal code sample. ```python. sc.pl.rank_genes_groups_violin(. adata, groups=cell_types, n_genes=15, strip=False, gene_symbols=""gene_symbols"". ). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[258], line 1. ----> 1 sc.pl.rank_genes_groups_violin(. 2 adata, groups=cell_types, n_genes=15, strip=False, gene_symbols=""gene_symbols"". 3 ). File ~/.conda/envs/scirpy/lib/python3.10/site-packages/scanpy/plotting/_tools/__init__.py:1157, in rank_genes_groups_violin(adata, groups, n_genes, gene_names, gene_symbols, use_raw, key, split, scale, strip, jitter, size, ax, show, save). 1155 if isinstance(_gene_names, np.ndarray):. 1156 _gene_names = _gene_names.tolist(). -> 1157 df = obs_df(adata, _gene_names, use_raw=use_raw, gene_symbols=gene_symbols). 1158 new_gene_names = df.columns. 1159 df['hue'] = adata.obs[groups_key].astype(str).values. File ~/.conda/envs/scirpy/lib/python3.10/site-packages/scanpy/get/get.py:272, in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 269 else:. 270 alias_index = None. --> 272 obs_cols, var_idx_keys, var_symbols = _check_indices(. 273 adata.obs,. 274 var.index,. 275 ""obs"",. 276 keys,. 277 alias_index=alias_index,. 278 use_raw=use_raw,. 279 ). 281 # Make df. 282 df = pd.DataFrame(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2540
https://github.com/scverse/scanpy/issues/2542:0,availability,Error,Error,0,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:373,availability,error,error,373,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:446,availability,Error,Error,446,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:217,deployability,version,version,217,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:553,deployability,pipelin,pipeline,553,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:579,deployability,modul,module,579,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:704,deployability,modul,module,704,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:852,deployability,modul,module,852,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:1018,deployability,modul,module,1018,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:1165,deployability,modul,module,1165,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:1281,deployability,modul,module,1281,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:1304,deployability,version,version,1304,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:1380,deployability,version,version,1380,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:1406,deployability,modul,module,1406,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:1507,deployability,fail,failed,1507,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:1589,deployability,Version,Versions,1589,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:1502,energy efficiency,load,load,1502,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:217,integrability,version,version,217,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:553,integrability,pipelin,pipeline,553,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:1304,integrability,version,version,1304,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:1380,integrability,version,version,1380,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:1589,integrability,Version,Versions,1589,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:1540,interoperability,specif,specified,1540,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:217,modifiability,version,version,217,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:579,modifiability,modul,module,579,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:662,modifiability,pac,packages,662,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:704,modifiability,modul,module,704,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:802,modifiability,pac,packages,802,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:852,modifiability,modul,module,852,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:975,modifiability,pac,packages,975,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:1018,modifiability,modul,module,1018,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:1116,modifiability,pac,packages,1116,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:1165,modifiability,modul,module,1165,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:1240,modifiability,pac,packages,1240,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:1281,modifiability,modul,module,1281,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:1304,modifiability,version,version,1304,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:1366,modifiability,pac,packages,1366,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:1380,modifiability,version,version,1380,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:1406,modifiability,modul,module,1406,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:1589,modifiability,Version,Versions,1589,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:0,performance,Error,Error,0,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:373,performance,error,error,373,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:446,performance,Error,Error,446,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:1502,performance,load,load,1502,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:1507,reliability,fail,failed,1507,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:0,safety,Error,Error,0,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:373,safety,error,error,373,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:446,safety,Error,Error,446,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:579,safety,modul,module,579,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:704,safety,modul,module,704,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:852,safety,modul,module,852,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:1018,safety,modul,module,1018,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:1165,safety,modul,module,1165,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:1281,safety,modul,module,1281,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:1406,safety,modul,module,1406,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:469,testability,Trace,Traceback,469,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:0,usability,Error,Error,0,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:42,usability,tool,tools,42,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:177,usability,confirm,confirmed,177,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:260,usability,confirm,confirmed,260,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:373,usability,error,error,373,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:384,usability,Minim,Minimal,384,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:446,usability,Error,Error,446,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:514,usability,User,Users,514,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:618,usability,User,Users,618,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:758,usability,User,Users,758,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:931,usability,User,Users,931,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:1072,usability,User,Users,1072,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:1196,usability,User,Users,1196,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/issues/2542:1322,usability,User,Users,1322,"Error in importing scanpy when using scvi-tools; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I tried to import scanpy and got an error. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. Traceback (most recent call last):. File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>. import scanpy as sc. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>. from ._utils import check_versions. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>. from anndata import AnnData, __version__ as anndata_version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>. from ._core.anndata import AnnData. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>. import h5py. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>. from . import version. File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>. from . import h5 as _h5. File ""h5py\h5.pyx"", line 1, in init h5py.h5. ImportError: DLL load failed while importing defs: The specified procedure could not be found. ```. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542
https://github.com/scverse/scanpy/pull/2543:0,availability,Restor,Restore,0,Restore numpydoc-like parameter lists; I just saw that @adamgayoso accidentally removed something we want to keep from the docs in #2220. This restores it. before | after. --- | ---. ![grafik](https://github.com/scverse/scanpy/assets/291575/8a941ac6-f822-45b7-91fd-fa3201873b64) | ![grafik](https://github.com/scverse/scanpy/assets/291575/c7188bad-a5db-445c-b5e5-ed72f6e3f0a6).,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2543
https://github.com/scverse/scanpy/pull/2543:143,availability,restor,restores,143,Restore numpydoc-like parameter lists; I just saw that @adamgayoso accidentally removed something we want to keep from the docs in #2220. This restores it. before | after. --- | ---. ![grafik](https://github.com/scverse/scanpy/assets/291575/8a941ac6-f822-45b7-91fd-fa3201873b64) | ![grafik](https://github.com/scverse/scanpy/assets/291575/c7188bad-a5db-445c-b5e5-ed72f6e3f0a6).,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2543
https://github.com/scverse/scanpy/pull/2543:22,modifiability,paramet,parameter,22,Restore numpydoc-like parameter lists; I just saw that @adamgayoso accidentally removed something we want to keep from the docs in #2220. This restores it. before | after. --- | ---. ![grafik](https://github.com/scverse/scanpy/assets/291575/8a941ac6-f822-45b7-91fd-fa3201873b64) | ![grafik](https://github.com/scverse/scanpy/assets/291575/c7188bad-a5db-445c-b5e5-ed72f6e3f0a6).,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2543
https://github.com/scverse/scanpy/pull/2543:0,reliability,Restor,Restore,0,Restore numpydoc-like parameter lists; I just saw that @adamgayoso accidentally removed something we want to keep from the docs in #2220. This restores it. before | after. --- | ---. ![grafik](https://github.com/scverse/scanpy/assets/291575/8a941ac6-f822-45b7-91fd-fa3201873b64) | ![grafik](https://github.com/scverse/scanpy/assets/291575/c7188bad-a5db-445c-b5e5-ed72f6e3f0a6).,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2543
https://github.com/scverse/scanpy/pull/2543:143,reliability,restor,restores,143,Restore numpydoc-like parameter lists; I just saw that @adamgayoso accidentally removed something we want to keep from the docs in #2220. This restores it. before | after. --- | ---. ![grafik](https://github.com/scverse/scanpy/assets/291575/8a941ac6-f822-45b7-91fd-fa3201873b64) | ![grafik](https://github.com/scverse/scanpy/assets/291575/c7188bad-a5db-445c-b5e5-ed72f6e3f0a6).,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2543
https://github.com/scverse/scanpy/pull/2543:67,safety,accid,accidentally,67,Restore numpydoc-like parameter lists; I just saw that @adamgayoso accidentally removed something we want to keep from the docs in #2220. This restores it. before | after. --- | ---. ![grafik](https://github.com/scverse/scanpy/assets/291575/8a941ac6-f822-45b7-91fd-fa3201873b64) | ![grafik](https://github.com/scverse/scanpy/assets/291575/c7188bad-a5db-445c-b5e5-ed72f6e3f0a6).,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2543
https://github.com/scverse/scanpy/pull/2545:254,interoperability,specif,specific,254,"Added marker parameter to scatter plots; fixed #2122 . This [issue](https://github.com/scverse/scanpy/issues/2122) was really bothering me as I had to switch to matplotlib for custom markers and it made my code inconsistent. I added the parameter to use specific markers instead of always using `'.'`. The default is still `'.'` and easily changeable using param: `marker`. Examples:. `sc.pl.umap(adata, color='random_group', size=300)`. ![image](https://github.com/scverse/scanpy/assets/78699446/ed216d41-10ad-4b96-a990-f0e69257cc4d). `sc.pl.umap(adata, color='random_group', size=300, marker='x')`. ![image](https://github.com/scverse/scanpy/assets/78699446/5a8d87ea-7a16-40f2-b34e-6f894ccb81d2). `sc.pl.umap(adata, color=['random_group', 'second_random_group'], size=300)`. ![image](https://github.com/scverse/scanpy/assets/78699446/4e611a51-0bb0-4ba1-a5ed-2d7d8f2703de). `sc.pl.umap(adata, color=['random_group', 'second_random_group'], size=300, marker='^')`. ![image](https://github.com/scverse/scanpy/assets/78699446/604d3fd3-55cf-4954-a9a9-cf8b3b047a1b). `sc.pl.umap(adata, color=['random_group', 'second_random_group'], size=300, marker=['x', '^'])`. ![image](https://github.com/scverse/scanpy/assets/78699446/2917c40b-22a3-4bac-a10c-6794a2ee2782).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2545
https://github.com/scverse/scanpy/pull/2545:13,modifiability,paramet,parameter,13,"Added marker parameter to scatter plots; fixed #2122 . This [issue](https://github.com/scverse/scanpy/issues/2122) was really bothering me as I had to switch to matplotlib for custom markers and it made my code inconsistent. I added the parameter to use specific markers instead of always using `'.'`. The default is still `'.'` and easily changeable using param: `marker`. Examples:. `sc.pl.umap(adata, color='random_group', size=300)`. ![image](https://github.com/scverse/scanpy/assets/78699446/ed216d41-10ad-4b96-a990-f0e69257cc4d). `sc.pl.umap(adata, color='random_group', size=300, marker='x')`. ![image](https://github.com/scverse/scanpy/assets/78699446/5a8d87ea-7a16-40f2-b34e-6f894ccb81d2). `sc.pl.umap(adata, color=['random_group', 'second_random_group'], size=300)`. ![image](https://github.com/scverse/scanpy/assets/78699446/4e611a51-0bb0-4ba1-a5ed-2d7d8f2703de). `sc.pl.umap(adata, color=['random_group', 'second_random_group'], size=300, marker='^')`. ![image](https://github.com/scverse/scanpy/assets/78699446/604d3fd3-55cf-4954-a9a9-cf8b3b047a1b). `sc.pl.umap(adata, color=['random_group', 'second_random_group'], size=300, marker=['x', '^'])`. ![image](https://github.com/scverse/scanpy/assets/78699446/2917c40b-22a3-4bac-a10c-6794a2ee2782).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2545
https://github.com/scverse/scanpy/pull/2545:237,modifiability,paramet,parameter,237,"Added marker parameter to scatter plots; fixed #2122 . This [issue](https://github.com/scverse/scanpy/issues/2122) was really bothering me as I had to switch to matplotlib for custom markers and it made my code inconsistent. I added the parameter to use specific markers instead of always using `'.'`. The default is still `'.'` and easily changeable using param: `marker`. Examples:. `sc.pl.umap(adata, color='random_group', size=300)`. ![image](https://github.com/scverse/scanpy/assets/78699446/ed216d41-10ad-4b96-a990-f0e69257cc4d). `sc.pl.umap(adata, color='random_group', size=300, marker='x')`. ![image](https://github.com/scverse/scanpy/assets/78699446/5a8d87ea-7a16-40f2-b34e-6f894ccb81d2). `sc.pl.umap(adata, color=['random_group', 'second_random_group'], size=300)`. ![image](https://github.com/scverse/scanpy/assets/78699446/4e611a51-0bb0-4ba1-a5ed-2d7d8f2703de). `sc.pl.umap(adata, color=['random_group', 'second_random_group'], size=300, marker='^')`. ![image](https://github.com/scverse/scanpy/assets/78699446/604d3fd3-55cf-4954-a9a9-cf8b3b047a1b). `sc.pl.umap(adata, color=['random_group', 'second_random_group'], size=300, marker=['x', '^'])`. ![image](https://github.com/scverse/scanpy/assets/78699446/2917c40b-22a3-4bac-a10c-6794a2ee2782).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2545
https://github.com/scverse/scanpy/pull/2545:176,usability,custom,custom,176,"Added marker parameter to scatter plots; fixed #2122 . This [issue](https://github.com/scverse/scanpy/issues/2122) was really bothering me as I had to switch to matplotlib for custom markers and it made my code inconsistent. I added the parameter to use specific markers instead of always using `'.'`. The default is still `'.'` and easily changeable using param: `marker`. Examples:. `sc.pl.umap(adata, color='random_group', size=300)`. ![image](https://github.com/scverse/scanpy/assets/78699446/ed216d41-10ad-4b96-a990-f0e69257cc4d). `sc.pl.umap(adata, color='random_group', size=300, marker='x')`. ![image](https://github.com/scverse/scanpy/assets/78699446/5a8d87ea-7a16-40f2-b34e-6f894ccb81d2). `sc.pl.umap(adata, color=['random_group', 'second_random_group'], size=300)`. ![image](https://github.com/scverse/scanpy/assets/78699446/4e611a51-0bb0-4ba1-a5ed-2d7d8f2703de). `sc.pl.umap(adata, color=['random_group', 'second_random_group'], size=300, marker='^')`. ![image](https://github.com/scverse/scanpy/assets/78699446/604d3fd3-55cf-4954-a9a9-cf8b3b047a1b). `sc.pl.umap(adata, color=['random_group', 'second_random_group'], size=300, marker=['x', '^'])`. ![image](https://github.com/scverse/scanpy/assets/78699446/2917c40b-22a3-4bac-a10c-6794a2ee2782).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2545
https://github.com/scverse/scanpy/pull/2546:262,availability,error,error,262,"Fix getting log1p base; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. Fixes error if the `log1p` dict doesn't have a `base` key. Fixes https://github.com/scverse/scanpy/issues/2497, fixes https://github.com/scverse/scanpy-tutorials/issues/65, fixes https://github.com/scverse/scanpy/issues/2181",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2546
https://github.com/scverse/scanpy/pull/2546:262,performance,error,error,262,"Fix getting log1p base; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. Fixes error if the `log1p` dict doesn't have a `base` key. Fixes https://github.com/scverse/scanpy/issues/2497, fixes https://github.com/scverse/scanpy-tutorials/issues/65, fixes https://github.com/scverse/scanpy/issues/2181",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2546
https://github.com/scverse/scanpy/pull/2546:288,reliability,doe,doesn,288,"Fix getting log1p base; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. Fixes error if the `log1p` dict doesn't have a `base` key. Fixes https://github.com/scverse/scanpy/issues/2497, fixes https://github.com/scverse/scanpy-tutorials/issues/65, fixes https://github.com/scverse/scanpy/issues/2181",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2546
https://github.com/scverse/scanpy/pull/2546:243,safety,review,review,243,"Fix getting log1p base; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. Fixes error if the `log1p` dict doesn't have a `base` key. Fixes https://github.com/scverse/scanpy/issues/2497, fixes https://github.com/scverse/scanpy-tutorials/issues/65, fixes https://github.com/scverse/scanpy/issues/2181",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2546
https://github.com/scverse/scanpy/pull/2546:262,safety,error,error,262,"Fix getting log1p base; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. Fixes error if the `log1p` dict doesn't have a `base` key. Fixes https://github.com/scverse/scanpy/issues/2497, fixes https://github.com/scverse/scanpy-tutorials/issues/65, fixes https://github.com/scverse/scanpy/issues/2181",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2546
https://github.com/scverse/scanpy/pull/2546:243,testability,review,review,243,"Fix getting log1p base; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. Fixes error if the `log1p` dict doesn't have a `base` key. Fixes https://github.com/scverse/scanpy/issues/2497, fixes https://github.com/scverse/scanpy-tutorials/issues/65, fixes https://github.com/scverse/scanpy/issues/2181",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2546
https://github.com/scverse/scanpy/pull/2546:94,usability,guid,guidelines,94,"Fix getting log1p base; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. Fixes error if the `log1p` dict doesn't have a `base` key. Fixes https://github.com/scverse/scanpy/issues/2497, fixes https://github.com/scverse/scanpy-tutorials/issues/65, fixes https://github.com/scverse/scanpy/issues/2181",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2546
https://github.com/scverse/scanpy/pull/2546:125,usability,guid,guide,125,"Fix getting log1p base; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. Fixes error if the `log1p` dict doesn't have a `base` key. Fixes https://github.com/scverse/scanpy/issues/2497, fixes https://github.com/scverse/scanpy-tutorials/issues/65, fixes https://github.com/scverse/scanpy/issues/2181",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2546
https://github.com/scverse/scanpy/pull/2546:221,usability,workflow,workflow,221,"Fix getting log1p base; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. Fixes error if the `log1p` dict doesn't have a `base` key. Fixes https://github.com/scverse/scanpy/issues/2497, fixes https://github.com/scverse/scanpy-tutorials/issues/65, fixes https://github.com/scverse/scanpy/issues/2181",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2546
https://github.com/scverse/scanpy/pull/2546:262,usability,error,error,262,"Fix getting log1p base; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. Fixes error if the `log1p` dict doesn't have a `base` key. Fixes https://github.com/scverse/scanpy/issues/2497, fixes https://github.com/scverse/scanpy-tutorials/issues/65, fixes https://github.com/scverse/scanpy/issues/2181",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2546
https://github.com/scverse/scanpy/issues/2547:404,availability,error,error,404,"highly_variable_genes gets a KeyError: '[nan] not in index'; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Running the highly_variable_genes function produced an error. ### Minimal code sample. ```python. sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[101], line 2. 1 # Identify highly-variable genes and plot. ----> 2 sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:440, in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values). 428 return _highly_variable_genes_seurat_v3(. 429 adata,. 430 layer=layer,. (...). 436 inplace=inplace,. 437 ). 439 if batch_key is None:. --> 440 df = _highly_variable_genes_single_batch(. 441 adata,. 442 layer=layer,. 443 min_disp=min_disp,. 444 max_disp=max_disp,. 445 min_mean=min_mean,. 446 max_mean=max_mean,. 447 n_top_genes=n_top_genes,. 448 n_bins=n_bins,. 449 flavor=flavor,. 450 ). 451 else:. 452 sanitize_anndata(adata). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:223, in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 219 # retrieve those genes that have nan std, these are the ones where. 220 # only a single gene fell in the bin and implicitly set them to have. 221 # a normalized disperion of 1. 222 one_gene_per_bin = disp_std",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:541,availability,Error,Error,541,"highly_variable_genes gets a KeyError: '[nan] not in index'; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Running the highly_variable_genes function produced an error. ### Minimal code sample. ```python. sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[101], line 2. 1 # Identify highly-variable genes and plot. ----> 2 sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:440, in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values). 428 return _highly_variable_genes_seurat_v3(. 429 adata,. 430 layer=layer,. (...). 436 inplace=inplace,. 437 ). 439 if batch_key is None:. --> 440 df = _highly_variable_genes_single_batch(. 441 adata,. 442 layer=layer,. 443 min_disp=min_disp,. 444 max_disp=max_disp,. 445 min_mean=min_mean,. 446 max_mean=max_mean,. 447 n_top_genes=n_top_genes,. 448 n_bins=n_bins,. 449 flavor=flavor,. 450 ). 451 else:. 452 sanitize_anndata(adata). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:223, in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 219 # retrieve those genes that have nan std, these are the ones where. 220 # only a single gene fell in the bin and implicitly set them to have. 221 # a normalized disperion of 1. 222 one_gene_per_bin = disp_std",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:3401,availability,sli,slicing,3401,".py:1033, in Series.__getitem__(self, key). 1030 key = np.asarray(key, dtype=bool). 1031 return self._get_values(key). -> 1033 return self._get_with(key). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/series.py:1073, in Series._get_with(self, key). 1070 return self.iloc[key]. 1072 # handle the dup indexing case GH#4246. -> 1073 return self.loc[key]. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1103, in _LocationIndexer.__getitem__(self, key). 1100 axis = self.axis or 0. 1102 maybe_callable = com.apply_if_callable(key, self.obj). -> 1103 return self._getitem_axis(maybe_callable, axis=axis). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1332, in _LocIndexer._getitem_axis(self, key, axis). 1329 if hasattr(key, ""ndim"") and key.ndim > 1:. 1330 raise ValueError(""Cannot index with multidimensional key""). -> 1332 return self._getitem_iterable(key, axis=axis). 1334 # nested tuple slicing. 1335 if is_nested_tuple(key, labels):. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1272, in _LocIndexer._getitem_iterable(self, key, axis). 1269 self._validate_key(key, axis). 1271 # A collection of keys. -> 1272 keyarr, indexer = self._get_listlike_indexer(key, axis). 1273 return self.obj._reindex_with_indexers(. 1274 {axis: [keyarr, indexer]}, copy=True, allow_dups=True. 1275 ). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1462, in _LocIndexer._get_listlike_indexer(self, key, axis). 1459 ax = self.obj._get_axis(axis). 1460 axis_name = self.obj._get_axis_name(axis). -> 1462 keyarr, indexer = ax._get_indexer_strict(key, axis_name). 1464 return keyarr, indexer. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexes/base.py:5876, in Index._get_indexer_strict(self, key, axis_name). 5873 else:. 5874 keyarr, indexer, new_indexer = self._reindex_non_unique(keyarr). -> 5876 se",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:229,deployability,version,version,229,"highly_variable_genes gets a KeyError: '[nan] not in index'; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Running the highly_variable_genes function produced an error. ### Minimal code sample. ```python. sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[101], line 2. 1 # Identify highly-variable genes and plot. ----> 2 sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:440, in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values). 428 return _highly_variable_genes_seurat_v3(. 429 adata,. 430 layer=layer,. (...). 436 inplace=inplace,. 437 ). 439 if batch_key is None:. --> 440 df = _highly_variable_genes_single_batch(. 441 adata,. 442 layer=layer,. 443 min_disp=min_disp,. 444 max_disp=max_disp,. 445 min_mean=min_mean,. 446 max_mean=max_mean,. 447 n_top_genes=n_top_genes,. 448 n_bins=n_bins,. 449 flavor=flavor,. 450 ). 451 else:. 452 sanitize_anndata(adata). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:223, in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 219 # retrieve those genes that have nan std, these are the ones where. 220 # only a single gene fell in the bin and implicitly set them to have. 221 # a normalized disperion of 1. 222 one_gene_per_bin = disp_std",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:2134,deployability,log,logg,2134,"variable_genes_seurat_v3(. 429 adata,. 430 layer=layer,. (...). 436 inplace=inplace,. 437 ). 439 if batch_key is None:. --> 440 df = _highly_variable_genes_single_batch(. 441 adata,. 442 layer=layer,. 443 min_disp=min_disp,. 444 max_disp=max_disp,. 445 min_mean=min_mean,. 446 max_mean=max_mean,. 447 n_top_genes=n_top_genes,. 448 n_bins=n_bins,. 449 flavor=flavor,. 450 ). 451 else:. 452 sanitize_anndata(adata). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:223, in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 219 # retrieve those genes that have nan std, these are the ones where. 220 # only a single gene fell in the bin and implicitly set them to have. 221 # a normalized disperion of 1. 222 one_gene_per_bin = disp_std_bin.isnull(). --> 223 gen_indices = np.where(one_gene_per_bin[df['mean_bin'].values])[0].tolist(). 224 if len(gen_indices) > 0:. 225 logg.debug(. 226 f'Gene indices {gen_indices} fell into a single bin: their '. 227 'normalized dispersion was set to 1.\n '. 228 'Decreasing `n_bins` will likely avoid this effect.'. 229 ). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/series.py:1033, in Series.__getitem__(self, key). 1030 key = np.asarray(key, dtype=bool). 1031 return self._get_values(key). -> 1033 return self._get_with(key). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/series.py:1073, in Series._get_with(self, key). 1070 return self.iloc[key]. 1072 # handle the dup indexing case GH#4246. -> 1073 return self.loc[key]. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1103, in _LocationIndexer.__getitem__(self, key). 1100 axis = self.axis or 0. 1102 maybe_callable = com.apply_if_callable(key, self.obj). -> 1103 return self._getitem_axis(maybe_callable, axis=axis). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:4963,deployability,Version,Versions,4963,"self, key, axis). 1459 ax = self.obj._get_axis(axis). 1460 axis_name = self.obj._get_axis_name(axis). -> 1462 keyarr, indexer = ax._get_indexer_strict(key, axis_name). 1464 return keyarr, indexer. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexes/base.py:5876, in Index._get_indexer_strict(self, key, axis_name). 5873 else:. 5874 keyarr, indexer, new_indexer = self._reindex_non_unique(keyarr). -> 5876 self._raise_if_missing(keyarr, indexer, axis_name). 5878 keyarr = self.take(indexer). 5879 if isinstance(key, Index):. 5880 # GH 42790 - Preserve name from an Index. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexes/base.py:5938, in Index._raise_if_missing(self, key, indexer, axis_name). 5935 raise KeyError(f""None of [{key}] are in the [{axis_name}]""). 5937 not_found = list(ensure_index(key)[missing_mask.nonzero()[0]].unique()). -> 5938 raise KeyError(f""{not_found} not in index""). KeyError: '[nan] not in index'. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. absl NA. aiohttp 3.8.4. aiosignal 1.3.1. anyio NA. asttokens NA. async_timeout 4.0.2. attr 23.1.0. backcall 0.2.0. bs4 4.12.2. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. chex 0.1.7. click 8.1.3. colorama 0.4.6. comm 0.1.3. contextlib2 NA. croniter NA. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. deepdiff 6.3.0. defusedxml 0.7.1. dill 0.3.6. docrep 0.3.2. dot_parser NA. etils 1.3.0. executing 1.2.0. fastapi 0.88.0. flax 0.6.10. frozenlist 1.3.3. fsspec 2023.6.0. h5py 3.8.0. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.1. ipython_genutils 0.2.0. ipywidgets 8.0.6. jax 0.4.12. jaxlib 0.4.12. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. lightning 2.0.3. lightning_cloud NA. lightning_fabric 2.0.3. lightning_utilities 0.8.0. llvmlite 0.40.0. louvain 0.8.0. matplotlib 3.7.1. matplotlib_inline 0.1.6. ml_collections NA. ml_dtypes 0.2",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:7539,deployability,updat,updated,7539,"igraph 0.10.4. importlib_resources NA. ipykernel 6.23.1. ipython_genutils 0.2.0. ipywidgets 8.0.6. jax 0.4.12. jaxlib 0.4.12. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. lightning 2.0.3. lightning_cloud NA. lightning_fabric 2.0.3. lightning_utilities 0.8.0. llvmlite 0.40.0. louvain 0.8.0. matplotlib 3.7.1. matplotlib_inline 0.1.6. ml_collections NA. ml_dtypes 0.2.0. mpl_toolkits NA. mpmath 1.3.0. msgpack 1.0.5. mudata 0.2.3. multidict 6.0.4. multipart 0.0.6. multipledispatch 0.6.0. natsort 8.3.1. numba 0.57.0. numpy 1.24.3. numpyro 0.12.1. nvfuser NA. opt_einsum v3.3.0. optax 0.1.5. ordered_set 4.1.0. packaging 23.1. pandas 2.0.2. parso 0.8.3. patsy 0.5.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.3. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydantic 1.10.9. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pydot 1.4.2. pygments 2.15.1. pyparsing 3.0.9. pyro 1.8.5. pytorch_lightning 2.0.3. pytz 2023.3. requests 2.31.0. rich NA. scipy 1.10.1. scvi 1.0.0. seaborn 0.12.2. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. sklearn 1.2.2. sniffio 1.3.0. socks 1.7.1. soupsieve 2.3.2.post1. sparse 0.14.0. sphinxcontrib NA. stack_data 0.6.2. starlette 0.22.0. statsmodels 0.14.0. sympy 1.12. texttable 1.6.7. threadpoolctl 3.1.0. toml 0.10.2. toolz 0.12.0. torch 2.0.1+cu117. torchmetrics 0.11.4. tornado 6.3.2. tqdm 4.65.0. traitlets 5.9.0. tree 0.1.8. typing_extensions NA. urllib3 2.0.3. uvicorn 0.22.0. wcwidth 0.2.6. websocket 1.5.3. websockets 11.0.3. wrapt 1.15.0. xarray 2023.5.0. yaml 6.0. yarl 1.9.2. zmq 25.1.0. zoneinfo NA. -----. IPython 8.14.0. jupyter_client 8.2.0. jupyter_core 5.3.1. notebook 6.5.4. -----. Python 3.11.4 | packaged by conda-forge | (main, Jun 10 2023, 18:08:17) [GCC 12.2.0]. Linux-4.18.0-425.19.2.el8_7.x86_64-x86_64-with-glibc2.27. -----. Session information updated at 2023-07-06 03:56. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:2394,energy efficiency,core,core,2394,"n=min_mean,. 446 max_mean=max_mean,. 447 n_top_genes=n_top_genes,. 448 n_bins=n_bins,. 449 flavor=flavor,. 450 ). 451 else:. 452 sanitize_anndata(adata). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:223, in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 219 # retrieve those genes that have nan std, these are the ones where. 220 # only a single gene fell in the bin and implicitly set them to have. 221 # a normalized disperion of 1. 222 one_gene_per_bin = disp_std_bin.isnull(). --> 223 gen_indices = np.where(one_gene_per_bin[df['mean_bin'].values])[0].tolist(). 224 if len(gen_indices) > 0:. 225 logg.debug(. 226 f'Gene indices {gen_indices} fell into a single bin: their '. 227 'normalized dispersion was set to 1.\n '. 228 'Decreasing `n_bins` will likely avoid this effect.'. 229 ). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/series.py:1033, in Series.__getitem__(self, key). 1030 key = np.asarray(key, dtype=bool). 1031 return self._get_values(key). -> 1033 return self._get_with(key). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/series.py:1073, in Series._get_with(self, key). 1070 return self.iloc[key]. 1072 # handle the dup indexing case GH#4246. -> 1073 return self.loc[key]. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1103, in _LocationIndexer.__getitem__(self, key). 1100 axis = self.axis or 0. 1102 maybe_callable = com.apply_if_callable(key, self.obj). -> 1103 return self._getitem_axis(maybe_callable, axis=axis). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1332, in _LocIndexer._getitem_axis(self, key, axis). 1329 if hasattr(key, ""ndim"") and key.ndim > 1:. 1330 raise ValueError(""Cannot index with multidimensional key""). -> 1332 return self._getitem_iterable(key, axis=axis). 1334 # nested t",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:2630,energy efficiency,core,core,2630,"g/_highly_variable_genes.py:223, in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 219 # retrieve those genes that have nan std, these are the ones where. 220 # only a single gene fell in the bin and implicitly set them to have. 221 # a normalized disperion of 1. 222 one_gene_per_bin = disp_std_bin.isnull(). --> 223 gen_indices = np.where(one_gene_per_bin[df['mean_bin'].values])[0].tolist(). 224 if len(gen_indices) > 0:. 225 logg.debug(. 226 f'Gene indices {gen_indices} fell into a single bin: their '. 227 'normalized dispersion was set to 1.\n '. 228 'Decreasing `n_bins` will likely avoid this effect.'. 229 ). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/series.py:1033, in Series.__getitem__(self, key). 1030 key = np.asarray(key, dtype=bool). 1031 return self._get_values(key). -> 1033 return self._get_with(key). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/series.py:1073, in Series._get_with(self, key). 1070 return self.iloc[key]. 1072 # handle the dup indexing case GH#4246. -> 1073 return self.loc[key]. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1103, in _LocationIndexer.__getitem__(self, key). 1100 axis = self.axis or 0. 1102 maybe_callable = com.apply_if_callable(key, self.obj). -> 1103 return self._getitem_axis(maybe_callable, axis=axis). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1332, in _LocIndexer._getitem_axis(self, key, axis). 1329 if hasattr(key, ""ndim"") and key.ndim > 1:. 1330 raise ValueError(""Cannot index with multidimensional key""). -> 1332 return self._getitem_iterable(key, axis=axis). 1334 # nested tuple slicing. 1335 if is_nested_tuple(key, labels):. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1272, in _LocIndexer._getitem_iterable(self, key, axis). 1269 self._validate_key(key, axis). 127",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:2856,energy efficiency,core,core,2856,". 220 # only a single gene fell in the bin and implicitly set them to have. 221 # a normalized disperion of 1. 222 one_gene_per_bin = disp_std_bin.isnull(). --> 223 gen_indices = np.where(one_gene_per_bin[df['mean_bin'].values])[0].tolist(). 224 if len(gen_indices) > 0:. 225 logg.debug(. 226 f'Gene indices {gen_indices} fell into a single bin: their '. 227 'normalized dispersion was set to 1.\n '. 228 'Decreasing `n_bins` will likely avoid this effect.'. 229 ). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/series.py:1033, in Series.__getitem__(self, key). 1030 key = np.asarray(key, dtype=bool). 1031 return self._get_values(key). -> 1033 return self._get_with(key). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/series.py:1073, in Series._get_with(self, key). 1070 return self.iloc[key]. 1072 # handle the dup indexing case GH#4246. -> 1073 return self.loc[key]. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1103, in _LocationIndexer.__getitem__(self, key). 1100 axis = self.axis or 0. 1102 maybe_callable = com.apply_if_callable(key, self.obj). -> 1103 return self._getitem_axis(maybe_callable, axis=axis). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1332, in _LocIndexer._getitem_axis(self, key, axis). 1329 if hasattr(key, ""ndim"") and key.ndim > 1:. 1330 raise ValueError(""Cannot index with multidimensional key""). -> 1332 return self._getitem_iterable(key, axis=axis). 1334 # nested tuple slicing. 1335 if is_nested_tuple(key, labels):. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1272, in _LocIndexer._getitem_iterable(self, key, axis). 1269 self._validate_key(key, axis). 1271 # A collection of keys. -> 1272 keyarr, indexer = self._get_listlike_indexer(key, axis). 1273 return self.obj._reindex_with_indexers(. 1274 {axis: [keyarr, indexer]}, copy=True, allow_dups=True. 1275 ). File /opt/conda/envs/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:3143,energy efficiency,core,core,3143,". 226 f'Gene indices {gen_indices} fell into a single bin: their '. 227 'normalized dispersion was set to 1.\n '. 228 'Decreasing `n_bins` will likely avoid this effect.'. 229 ). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/series.py:1033, in Series.__getitem__(self, key). 1030 key = np.asarray(key, dtype=bool). 1031 return self._get_values(key). -> 1033 return self._get_with(key). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/series.py:1073, in Series._get_with(self, key). 1070 return self.iloc[key]. 1072 # handle the dup indexing case GH#4246. -> 1073 return self.loc[key]. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1103, in _LocationIndexer.__getitem__(self, key). 1100 axis = self.axis or 0. 1102 maybe_callable = com.apply_if_callable(key, self.obj). -> 1103 return self._getitem_axis(maybe_callable, axis=axis). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1332, in _LocIndexer._getitem_axis(self, key, axis). 1329 if hasattr(key, ""ndim"") and key.ndim > 1:. 1330 raise ValueError(""Cannot index with multidimensional key""). -> 1332 return self._getitem_iterable(key, axis=axis). 1334 # nested tuple slicing. 1335 if is_nested_tuple(key, labels):. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1272, in _LocIndexer._getitem_iterable(self, key, axis). 1269 self._validate_key(key, axis). 1271 # A collection of keys. -> 1272 keyarr, indexer = self._get_listlike_indexer(key, axis). 1273 return self.obj._reindex_with_indexers(. 1274 {axis: [keyarr, indexer]}, copy=True, allow_dups=True. 1275 ). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1462, in _LocIndexer._get_listlike_indexer(self, key, axis). 1459 ax = self.obj._get_axis(axis). 1460 axis_name = self.obj._get_axis_name(axis). -> 1462 keyarr, indexer = ax._get_indexer_strict(key, axis_name). 1464 retur",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:3519,energy efficiency,core,core,3519,"). -> 1033 return self._get_with(key). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/series.py:1073, in Series._get_with(self, key). 1070 return self.iloc[key]. 1072 # handle the dup indexing case GH#4246. -> 1073 return self.loc[key]. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1103, in _LocationIndexer.__getitem__(self, key). 1100 axis = self.axis or 0. 1102 maybe_callable = com.apply_if_callable(key, self.obj). -> 1103 return self._getitem_axis(maybe_callable, axis=axis). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1332, in _LocIndexer._getitem_axis(self, key, axis). 1329 if hasattr(key, ""ndim"") and key.ndim > 1:. 1330 raise ValueError(""Cannot index with multidimensional key""). -> 1332 return self._getitem_iterable(key, axis=axis). 1334 # nested tuple slicing. 1335 if is_nested_tuple(key, labels):. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1272, in _LocIndexer._getitem_iterable(self, key, axis). 1269 self._validate_key(key, axis). 1271 # A collection of keys. -> 1272 keyarr, indexer = self._get_listlike_indexer(key, axis). 1273 return self.obj._reindex_with_indexers(. 1274 {axis: [keyarr, indexer]}, copy=True, allow_dups=True. 1275 ). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1462, in _LocIndexer._get_listlike_indexer(self, key, axis). 1459 ax = self.obj._get_axis(axis). 1460 axis_name = self.obj._get_axis_name(axis). -> 1462 keyarr, indexer = ax._get_indexer_strict(key, axis_name). 1464 return keyarr, indexer. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexes/base.py:5876, in Index._get_indexer_strict(self, key, axis_name). 5873 else:. 5874 keyarr, indexer, new_indexer = self._reindex_non_unique(keyarr). -> 5876 self._raise_if_missing(keyarr, indexer, axis_name). 5878 keyarr = self.take(indexer). 5879 if isinstance(key, Index):.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:3907,energy efficiency,core,core,3907,"_(self, key). 1100 axis = self.axis or 0. 1102 maybe_callable = com.apply_if_callable(key, self.obj). -> 1103 return self._getitem_axis(maybe_callable, axis=axis). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1332, in _LocIndexer._getitem_axis(self, key, axis). 1329 if hasattr(key, ""ndim"") and key.ndim > 1:. 1330 raise ValueError(""Cannot index with multidimensional key""). -> 1332 return self._getitem_iterable(key, axis=axis). 1334 # nested tuple slicing. 1335 if is_nested_tuple(key, labels):. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1272, in _LocIndexer._getitem_iterable(self, key, axis). 1269 self._validate_key(key, axis). 1271 # A collection of keys. -> 1272 keyarr, indexer = self._get_listlike_indexer(key, axis). 1273 return self.obj._reindex_with_indexers(. 1274 {axis: [keyarr, indexer]}, copy=True, allow_dups=True. 1275 ). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1462, in _LocIndexer._get_listlike_indexer(self, key, axis). 1459 ax = self.obj._get_axis(axis). 1460 axis_name = self.obj._get_axis_name(axis). -> 1462 keyarr, indexer = ax._get_indexer_strict(key, axis_name). 1464 return keyarr, indexer. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexes/base.py:5876, in Index._get_indexer_strict(self, key, axis_name). 5873 else:. 5874 keyarr, indexer, new_indexer = self._reindex_non_unique(keyarr). -> 5876 self._raise_if_missing(keyarr, indexer, axis_name). 5878 keyarr = self.take(indexer). 5879 if isinstance(key, Index):. 5880 # GH 42790 - Preserve name from an Index. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexes/base.py:5938, in Index._raise_if_missing(self, key, indexer, axis_name). 5935 raise KeyError(f""None of [{key}] are in the [{axis_name}]""). 5937 not_found = list(ensure_index(key)[missing_mask.nonzero()[0]].unique()). -> 5938 raise KeyError(f""{not_found} not",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:4234,energy efficiency,core,core,4234,"dim"") and key.ndim > 1:. 1330 raise ValueError(""Cannot index with multidimensional key""). -> 1332 return self._getitem_iterable(key, axis=axis). 1334 # nested tuple slicing. 1335 if is_nested_tuple(key, labels):. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1272, in _LocIndexer._getitem_iterable(self, key, axis). 1269 self._validate_key(key, axis). 1271 # A collection of keys. -> 1272 keyarr, indexer = self._get_listlike_indexer(key, axis). 1273 return self.obj._reindex_with_indexers(. 1274 {axis: [keyarr, indexer]}, copy=True, allow_dups=True. 1275 ). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1462, in _LocIndexer._get_listlike_indexer(self, key, axis). 1459 ax = self.obj._get_axis(axis). 1460 axis_name = self.obj._get_axis_name(axis). -> 1462 keyarr, indexer = ax._get_indexer_strict(key, axis_name). 1464 return keyarr, indexer. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexes/base.py:5876, in Index._get_indexer_strict(self, key, axis_name). 5873 else:. 5874 keyarr, indexer, new_indexer = self._reindex_non_unique(keyarr). -> 5876 self._raise_if_missing(keyarr, indexer, axis_name). 5878 keyarr = self.take(indexer). 5879 if isinstance(key, Index):. 5880 # GH 42790 - Preserve name from an Index. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexes/base.py:5938, in Index._raise_if_missing(self, key, indexer, axis_name). 5935 raise KeyError(f""None of [{key}] are in the [{axis_name}]""). 5937 not_found = list(ensure_index(key)[missing_mask.nonzero()[0]].unique()). -> 5938 raise KeyError(f""{not_found} not in index""). KeyError: '[nan] not in index'. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. absl NA. aiohttp 3.8.4. aiosignal 1.3.1. anyio NA. asttokens NA. async_timeout 4.0.2. attr 23.1.0. backcall 0.2.0. bs4 4.12.2. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. chex 0.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:4639,energy efficiency,core,core,4639,"lection of keys. -> 1272 keyarr, indexer = self._get_listlike_indexer(key, axis). 1273 return self.obj._reindex_with_indexers(. 1274 {axis: [keyarr, indexer]}, copy=True, allow_dups=True. 1275 ). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1462, in _LocIndexer._get_listlike_indexer(self, key, axis). 1459 ax = self.obj._get_axis(axis). 1460 axis_name = self.obj._get_axis_name(axis). -> 1462 keyarr, indexer = ax._get_indexer_strict(key, axis_name). 1464 return keyarr, indexer. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexes/base.py:5876, in Index._get_indexer_strict(self, key, axis_name). 5873 else:. 5874 keyarr, indexer, new_indexer = self._reindex_non_unique(keyarr). -> 5876 self._raise_if_missing(keyarr, indexer, axis_name). 5878 keyarr = self.take(indexer). 5879 if isinstance(key, Index):. 5880 # GH 42790 - Preserve name from an Index. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexes/base.py:5938, in Index._raise_if_missing(self, key, indexer, axis_name). 5935 raise KeyError(f""None of [{key}] are in the [{axis_name}]""). 5937 not_found = list(ensure_index(key)[missing_mask.nonzero()[0]].unique()). -> 5938 raise KeyError(f""{not_found} not in index""). KeyError: '[nan] not in index'. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. absl NA. aiohttp 3.8.4. aiosignal 1.3.1. anyio NA. asttokens NA. async_timeout 4.0.2. attr 23.1.0. backcall 0.2.0. bs4 4.12.2. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. chex 0.1.7. click 8.1.3. colorama 0.4.6. comm 0.1.3. contextlib2 NA. croniter NA. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. deepdiff 6.3.0. defusedxml 0.7.1. dill 0.3.6. docrep 0.3.2. dot_parser NA. etils 1.3.0. executing 1.2.0. fastapi 0.88.0. flax 0.6.10. frozenlist 1.3.3. fsspec 2023.6.0. h5py 3.8.0. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.1. ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:229,integrability,version,version,229,"highly_variable_genes gets a KeyError: '[nan] not in index'; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Running the highly_variable_genes function produced an error. ### Minimal code sample. ```python. sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[101], line 2. 1 # Identify highly-variable genes and plot. ----> 2 sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:440, in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values). 428 return _highly_variable_genes_seurat_v3(. 429 adata,. 430 layer=layer,. (...). 436 inplace=inplace,. 437 ). 439 if batch_key is None:. --> 440 df = _highly_variable_genes_single_batch(. 441 adata,. 442 layer=layer,. 443 min_disp=min_disp,. 444 max_disp=max_disp,. 445 min_mean=min_mean,. 446 max_mean=max_mean,. 447 n_top_genes=n_top_genes,. 448 n_bins=n_bins,. 449 flavor=flavor,. 450 ). 451 else:. 452 sanitize_anndata(adata). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:223, in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 219 # retrieve those genes that have nan std, these are the ones where. 220 # only a single gene fell in the bin and implicitly set them to have. 221 # a normalized disperion of 1. 222 one_gene_per_bin = disp_std",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:1074,integrability,sub,subset,1074,"sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Running the highly_variable_genes function produced an error. ### Minimal code sample. ```python. sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[101], line 2. 1 # Identify highly-variable genes and plot. ----> 2 sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:440, in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values). 428 return _highly_variable_genes_seurat_v3(. 429 adata,. 430 layer=layer,. (...). 436 inplace=inplace,. 437 ). 439 if batch_key is None:. --> 440 df = _highly_variable_genes_single_batch(. 441 adata,. 442 layer=layer,. 443 min_disp=min_disp,. 444 max_disp=max_disp,. 445 min_mean=min_mean,. 446 max_mean=max_mean,. 447 n_top_genes=n_top_genes,. 448 n_bins=n_bins,. 449 flavor=flavor,. 450 ). 451 else:. 452 sanitize_anndata(adata). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:223, in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 219 # retrieve those genes that have nan std, these are the ones where. 220 # only a single gene fell in the bin and implicitly set them to have. 221 # a normalized disperion of 1. 222 one_gene_per_bin = disp_std_bin.isnull(). --> 223 gen_indices = np.where(one_gene_per_bin[df['mean_bin']",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:4963,integrability,Version,Versions,4963,"self, key, axis). 1459 ax = self.obj._get_axis(axis). 1460 axis_name = self.obj._get_axis_name(axis). -> 1462 keyarr, indexer = ax._get_indexer_strict(key, axis_name). 1464 return keyarr, indexer. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexes/base.py:5876, in Index._get_indexer_strict(self, key, axis_name). 5873 else:. 5874 keyarr, indexer, new_indexer = self._reindex_non_unique(keyarr). -> 5876 self._raise_if_missing(keyarr, indexer, axis_name). 5878 keyarr = self.take(indexer). 5879 if isinstance(key, Index):. 5880 # GH 42790 - Preserve name from an Index. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexes/base.py:5938, in Index._raise_if_missing(self, key, indexer, axis_name). 5935 raise KeyError(f""None of [{key}] are in the [{axis_name}]""). 5937 not_found = list(ensure_index(key)[missing_mask.nonzero()[0]].unique()). -> 5938 raise KeyError(f""{not_found} not in index""). KeyError: '[nan] not in index'. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. absl NA. aiohttp 3.8.4. aiosignal 1.3.1. anyio NA. asttokens NA. async_timeout 4.0.2. attr 23.1.0. backcall 0.2.0. bs4 4.12.2. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. chex 0.1.7. click 8.1.3. colorama 0.4.6. comm 0.1.3. contextlib2 NA. croniter NA. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. deepdiff 6.3.0. defusedxml 0.7.1. dill 0.3.6. docrep 0.3.2. dot_parser NA. etils 1.3.0. executing 1.2.0. fastapi 0.88.0. flax 0.6.10. frozenlist 1.3.3. fsspec 2023.6.0. h5py 3.8.0. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.1. ipython_genutils 0.2.0. ipywidgets 8.0.6. jax 0.4.12. jaxlib 0.4.12. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. lightning 2.0.3. lightning_cloud NA. lightning_fabric 2.0.3. lightning_utilities 0.8.0. llvmlite 0.40.0. louvain 0.8.0. matplotlib 3.7.1. matplotlib_inline 0.1.6. ml_collections NA. ml_dtypes 0.2",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:7202,integrability,wrap,wrapt,7202,"igraph 0.10.4. importlib_resources NA. ipykernel 6.23.1. ipython_genutils 0.2.0. ipywidgets 8.0.6. jax 0.4.12. jaxlib 0.4.12. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. lightning 2.0.3. lightning_cloud NA. lightning_fabric 2.0.3. lightning_utilities 0.8.0. llvmlite 0.40.0. louvain 0.8.0. matplotlib 3.7.1. matplotlib_inline 0.1.6. ml_collections NA. ml_dtypes 0.2.0. mpl_toolkits NA. mpmath 1.3.0. msgpack 1.0.5. mudata 0.2.3. multidict 6.0.4. multipart 0.0.6. multipledispatch 0.6.0. natsort 8.3.1. numba 0.57.0. numpy 1.24.3. numpyro 0.12.1. nvfuser NA. opt_einsum v3.3.0. optax 0.1.5. ordered_set 4.1.0. packaging 23.1. pandas 2.0.2. parso 0.8.3. patsy 0.5.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.3. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydantic 1.10.9. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pydot 1.4.2. pygments 2.15.1. pyparsing 3.0.9. pyro 1.8.5. pytorch_lightning 2.0.3. pytz 2023.3. requests 2.31.0. rich NA. scipy 1.10.1. scvi 1.0.0. seaborn 0.12.2. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. sklearn 1.2.2. sniffio 1.3.0. socks 1.7.1. soupsieve 2.3.2.post1. sparse 0.14.0. sphinxcontrib NA. stack_data 0.6.2. starlette 0.22.0. statsmodels 0.14.0. sympy 1.12. texttable 1.6.7. threadpoolctl 3.1.0. toml 0.10.2. toolz 0.12.0. torch 2.0.1+cu117. torchmetrics 0.11.4. tornado 6.3.2. tqdm 4.65.0. traitlets 5.9.0. tree 0.1.8. typing_extensions NA. urllib3 2.0.3. uvicorn 0.22.0. wcwidth 0.2.6. websocket 1.5.3. websockets 11.0.3. wrapt 1.15.0. xarray 2023.5.0. yaml 6.0. yarl 1.9.2. zmq 25.1.0. zoneinfo NA. -----. IPython 8.14.0. jupyter_client 8.2.0. jupyter_core 5.3.1. notebook 6.5.4. -----. Python 3.11.4 | packaged by conda-forge | (main, Jun 10 2023, 18:08:17) [GCC 12.2.0]. Linux-4.18.0-425.19.2.el8_7.x86_64-x86_64-with-glibc2.27. -----. Session information updated at 2023-07-06 03:56. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:6319,interoperability,platform,platformdirs,6319, cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. deepdiff 6.3.0. defusedxml 0.7.1. dill 0.3.6. docrep 0.3.2. dot_parser NA. etils 1.3.0. executing 1.2.0. fastapi 0.88.0. flax 0.6.10. frozenlist 1.3.3. fsspec 2023.6.0. h5py 3.8.0. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.1. ipython_genutils 0.2.0. ipywidgets 8.0.6. jax 0.4.12. jaxlib 0.4.12. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. lightning 2.0.3. lightning_cloud NA. lightning_fabric 2.0.3. lightning_utilities 0.8.0. llvmlite 0.40.0. louvain 0.8.0. matplotlib 3.7.1. matplotlib_inline 0.1.6. ml_collections NA. ml_dtypes 0.2.0. mpl_toolkits NA. mpmath 1.3.0. msgpack 1.0.5. mudata 0.2.3. multidict 6.0.4. multipart 0.0.6. multipledispatch 0.6.0. natsort 8.3.1. numba 0.57.0. numpy 1.24.3. numpyro 0.12.1. nvfuser NA. opt_einsum v3.3.0. optax 0.1.5. ordered_set 4.1.0. packaging 23.1. pandas 2.0.2. parso 0.8.3. patsy 0.5.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.3. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydantic 1.10.9. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pydot 1.4.2. pygments 2.15.1. pyparsing 3.0.9. pyro 1.8.5. pytorch_lightning 2.0.3. pytz 2023.3. requests 2.31.0. rich NA. scipy 1.10.1. scvi 1.0.0. seaborn 0.12.2. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. sklearn 1.2.2. sniffio 1.3.0. socks 1.7.1. soupsieve 2.3.2.post1. sparse 0.14.0. sphinxcontrib NA. stack_data 0.6.2. starlette 0.22.0. statsmodels 0.14.0. sympy 1.12. texttable 1.6.7. threadpoolctl 3.1.0. toml 0.10.2. toolz 0.12.0. torch 2.0.1+cu117. torchmetrics 0.11.4. tornado 6.3.2. tqdm 4.65.0. traitlets 5.9.0. tree 0.1.8. typing_extensions NA. urllib3 2.0.3. uvicorn 0.22.0. wcwidth 0.2.6. websocket 1.5.3. websockets 11.0.3. wrapt 1.15.0. xarray 2023.5.0. yaml 6.0. yarl 1.9.2. zmq 25.1.0. zoneinfo NA. -----. IPython 8.14.0. jupyter_client 8.2.0. ,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:229,modifiability,version,version,229,"highly_variable_genes gets a KeyError: '[nan] not in index'; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Running the highly_variable_genes function produced an error. ### Minimal code sample. ```python. sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[101], line 2. 1 # Identify highly-variable genes and plot. ----> 2 sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:440, in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values). 428 return _highly_variable_genes_seurat_v3(. 429 adata,. 430 layer=layer,. (...). 436 inplace=inplace,. 437 ). 439 if batch_key is None:. --> 440 df = _highly_variable_genes_single_batch(. 441 adata,. 442 layer=layer,. 443 min_disp=min_disp,. 444 max_disp=max_disp,. 445 min_mean=min_mean,. 446 max_mean=max_mean,. 447 n_top_genes=n_top_genes,. 448 n_bins=n_bins,. 449 flavor=flavor,. 450 ). 451 else:. 452 sanitize_anndata(adata). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:223, in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 219 # retrieve those genes that have nan std, these are the ones where. 220 # only a single gene fell in the bin and implicitly set them to have. 221 # a normalized disperion of 1. 222 one_gene_per_bin = disp_std",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:727,modifiability,variab,variable,727,"highly_variable_genes gets a KeyError: '[nan] not in index'; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Running the highly_variable_genes function produced an error. ### Minimal code sample. ```python. sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[101], line 2. 1 # Identify highly-variable genes and plot. ----> 2 sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:440, in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values). 428 return _highly_variable_genes_seurat_v3(. 429 adata,. 430 layer=layer,. (...). 436 inplace=inplace,. 437 ). 439 if batch_key is None:. --> 440 df = _highly_variable_genes_single_batch(. 441 adata,. 442 layer=layer,. 443 min_disp=min_disp,. 444 max_disp=max_disp,. 445 min_mean=min_mean,. 446 max_mean=max_mean,. 447 n_top_genes=n_top_genes,. 448 n_bins=n_bins,. 449 flavor=flavor,. 450 ). 451 else:. 452 sanitize_anndata(adata). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:223, in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 219 # retrieve those genes that have nan std, these are the ones where. 220 # only a single gene fell in the bin and implicitly set them to have. 221 # a normalized disperion of 1. 222 one_gene_per_bin = disp_std",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:899,modifiability,pac,packages,899,"highly_variable_genes gets a KeyError: '[nan] not in index'; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Running the highly_variable_genes function produced an error. ### Minimal code sample. ```python. sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[101], line 2. 1 # Identify highly-variable genes and plot. ----> 2 sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:440, in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values). 428 return _highly_variable_genes_seurat_v3(. 429 adata,. 430 layer=layer,. (...). 436 inplace=inplace,. 437 ). 439 if batch_key is None:. --> 440 df = _highly_variable_genes_single_batch(. 441 adata,. 442 layer=layer,. 443 min_disp=min_disp,. 444 max_disp=max_disp,. 445 min_mean=min_mean,. 446 max_mean=max_mean,. 447 n_top_genes=n_top_genes,. 448 n_bins=n_bins,. 449 flavor=flavor,. 450 ). 451 else:. 452 sanitize_anndata(adata). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:223, in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 219 # retrieve those genes that have nan std, these are the ones where. 220 # only a single gene fell in the bin and implicitly set them to have. 221 # a normalized disperion of 1. 222 one_gene_per_bin = disp_std",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:992,modifiability,layer,layer,992,"highly_variable_genes gets a KeyError: '[nan] not in index'; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Running the highly_variable_genes function produced an error. ### Minimal code sample. ```python. sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[101], line 2. 1 # Identify highly-variable genes and plot. ----> 2 sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:440, in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values). 428 return _highly_variable_genes_seurat_v3(. 429 adata,. 430 layer=layer,. (...). 436 inplace=inplace,. 437 ). 439 if batch_key is None:. --> 440 df = _highly_variable_genes_single_batch(. 441 adata,. 442 layer=layer,. 443 min_disp=min_disp,. 444 max_disp=max_disp,. 445 min_mean=min_mean,. 446 max_mean=max_mean,. 447 n_top_genes=n_top_genes,. 448 n_bins=n_bins,. 449 flavor=flavor,. 450 ). 451 else:. 452 sanitize_anndata(adata). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:223, in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 219 # retrieve those genes that have nan std, these are the ones where. 220 # only a single gene fell in the bin and implicitly set them to have. 221 # a normalized disperion of 1. 222 one_gene_per_bin = disp_std",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:1179,modifiability,layer,layer,1179,"I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Running the highly_variable_genes function produced an error. ### Minimal code sample. ```python. sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[101], line 2. 1 # Identify highly-variable genes and plot. ----> 2 sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:440, in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values). 428 return _highly_variable_genes_seurat_v3(. 429 adata,. 430 layer=layer,. (...). 436 inplace=inplace,. 437 ). 439 if batch_key is None:. --> 440 df = _highly_variable_genes_single_batch(. 441 adata,. 442 layer=layer,. 443 min_disp=min_disp,. 444 max_disp=max_disp,. 445 min_mean=min_mean,. 446 max_mean=max_mean,. 447 n_top_genes=n_top_genes,. 448 n_bins=n_bins,. 449 flavor=flavor,. 450 ). 451 else:. 452 sanitize_anndata(adata). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:223, in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 219 # retrieve those genes that have nan std, these are the ones where. 220 # only a single gene fell in the bin and implicitly set them to have. 221 # a normalized disperion of 1. 222 one_gene_per_bin = disp_std_bin.isnull(). --> 223 gen_indices = np.where(one_gene_per_bin[df['mean_bin'].values])[0].tolist(). 224 if len(gen_indices) > 0:. 225 logg.debug(. 226 f'Gene indices {gen_indices} fe",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:1185,modifiability,layer,layer,1185," confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Running the highly_variable_genes function produced an error. ### Minimal code sample. ```python. sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[101], line 2. 1 # Identify highly-variable genes and plot. ----> 2 sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:440, in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values). 428 return _highly_variable_genes_seurat_v3(. 429 adata,. 430 layer=layer,. (...). 436 inplace=inplace,. 437 ). 439 if batch_key is None:. --> 440 df = _highly_variable_genes_single_batch(. 441 adata,. 442 layer=layer,. 443 min_disp=min_disp,. 444 max_disp=max_disp,. 445 min_mean=min_mean,. 446 max_mean=max_mean,. 447 n_top_genes=n_top_genes,. 448 n_bins=n_bins,. 449 flavor=flavor,. 450 ). 451 else:. 452 sanitize_anndata(adata). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:223, in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 219 # retrieve those genes that have nan std, these are the ones where. 220 # only a single gene fell in the bin and implicitly set them to have. 221 # a normalized disperion of 1. 222 one_gene_per_bin = disp_std_bin.isnull(). --> 223 gen_indices = np.where(one_gene_per_bin[df['mean_bin'].values])[0].tolist(). 224 if len(gen_indices) > 0:. 225 logg.debug(. 226 f'Gene indices {gen_indices} fell int",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:1323,modifiability,layer,layer,1323,"py. ### What happened? Running the highly_variable_genes function produced an error. ### Minimal code sample. ```python. sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[101], line 2. 1 # Identify highly-variable genes and plot. ----> 2 sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:440, in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values). 428 return _highly_variable_genes_seurat_v3(. 429 adata,. 430 layer=layer,. (...). 436 inplace=inplace,. 437 ). 439 if batch_key is None:. --> 440 df = _highly_variable_genes_single_batch(. 441 adata,. 442 layer=layer,. 443 min_disp=min_disp,. 444 max_disp=max_disp,. 445 min_mean=min_mean,. 446 max_mean=max_mean,. 447 n_top_genes=n_top_genes,. 448 n_bins=n_bins,. 449 flavor=flavor,. 450 ). 451 else:. 452 sanitize_anndata(adata). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:223, in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 219 # retrieve those genes that have nan std, these are the ones where. 220 # only a single gene fell in the bin and implicitly set them to have. 221 # a normalized disperion of 1. 222 one_gene_per_bin = disp_std_bin.isnull(). --> 223 gen_indices = np.where(one_gene_per_bin[df['mean_bin'].values])[0].tolist(). 224 if len(gen_indices) > 0:. 225 logg.debug(. 226 f'Gene indices {gen_indices} fell into a single bin: their '. 227 'normalized dispersion was set to 1.\n '. 228 'Decreasing `n_bins` will likely avoid this effect.'. 229 ). Fi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:1329,modifiability,layer,layer,1329,"# What happened? Running the highly_variable_genes function produced an error. ### Minimal code sample. ```python. sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[101], line 2. 1 # Identify highly-variable genes and plot. ----> 2 sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:440, in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values). 428 return _highly_variable_genes_seurat_v3(. 429 adata,. 430 layer=layer,. (...). 436 inplace=inplace,. 437 ). 439 if batch_key is None:. --> 440 df = _highly_variable_genes_single_batch(. 441 adata,. 442 layer=layer,. 443 min_disp=min_disp,. 444 max_disp=max_disp,. 445 min_mean=min_mean,. 446 max_mean=max_mean,. 447 n_top_genes=n_top_genes,. 448 n_bins=n_bins,. 449 flavor=flavor,. 450 ). 451 else:. 452 sanitize_anndata(adata). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:223, in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 219 # retrieve those genes that have nan std, these are the ones where. 220 # only a single gene fell in the bin and implicitly set them to have. 221 # a normalized disperion of 1. 222 one_gene_per_bin = disp_std_bin.isnull(). --> 223 gen_indices = np.where(one_gene_per_bin[df['mean_bin'].values])[0].tolist(). 224 if len(gen_indices) > 0:. 225 logg.debug(. 226 f'Gene indices {gen_indices} fell into a single bin: their '. 227 'normalized dispersion was set to 1.\n '. 228 'Decreasing `n_bins` will likely avoid this effect.'. 229 ). File /op",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:1604,modifiability,pac,packages,1604,"-------------------------------. KeyError Traceback (most recent call last). Cell In[101], line 2. 1 # Identify highly-variable genes and plot. ----> 2 sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:440, in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values). 428 return _highly_variable_genes_seurat_v3(. 429 adata,. 430 layer=layer,. (...). 436 inplace=inplace,. 437 ). 439 if batch_key is None:. --> 440 df = _highly_variable_genes_single_batch(. 441 adata,. 442 layer=layer,. 443 min_disp=min_disp,. 444 max_disp=max_disp,. 445 min_mean=min_mean,. 446 max_mean=max_mean,. 447 n_top_genes=n_top_genes,. 448 n_bins=n_bins,. 449 flavor=flavor,. 450 ). 451 else:. 452 sanitize_anndata(adata). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:223, in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 219 # retrieve those genes that have nan std, these are the ones where. 220 # only a single gene fell in the bin and implicitly set them to have. 221 # a normalized disperion of 1. 222 one_gene_per_bin = disp_std_bin.isnull(). --> 223 gen_indices = np.where(one_gene_per_bin[df['mean_bin'].values])[0].tolist(). 224 if len(gen_indices) > 0:. 225 logg.debug(. 226 f'Gene indices {gen_indices} fell into a single bin: their '. 227 'normalized dispersion was set to 1.\n '. 228 'Decreasing `n_bins` will likely avoid this effect.'. 229 ). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/series.py:1033, in Series.__getitem__(self, key). 1030 key = np.asarray(key, dtype=bool). 1031 return self._get_values(key). -> 1033 return self._get_with(key). File /opt/conda/envs/cell2loc_env/lib/python3.11",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:1711,modifiability,layer,layer,1711,"ntify highly-variable genes and plot. ----> 2 sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:440, in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values). 428 return _highly_variable_genes_seurat_v3(. 429 adata,. 430 layer=layer,. (...). 436 inplace=inplace,. 437 ). 439 if batch_key is None:. --> 440 df = _highly_variable_genes_single_batch(. 441 adata,. 442 layer=layer,. 443 min_disp=min_disp,. 444 max_disp=max_disp,. 445 min_mean=min_mean,. 446 max_mean=max_mean,. 447 n_top_genes=n_top_genes,. 448 n_bins=n_bins,. 449 flavor=flavor,. 450 ). 451 else:. 452 sanitize_anndata(adata). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:223, in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 219 # retrieve those genes that have nan std, these are the ones where. 220 # only a single gene fell in the bin and implicitly set them to have. 221 # a normalized disperion of 1. 222 one_gene_per_bin = disp_std_bin.isnull(). --> 223 gen_indices = np.where(one_gene_per_bin[df['mean_bin'].values])[0].tolist(). 224 if len(gen_indices) > 0:. 225 logg.debug(. 226 f'Gene indices {gen_indices} fell into a single bin: their '. 227 'normalized dispersion was set to 1.\n '. 228 'Decreasing `n_bins` will likely avoid this effect.'. 229 ). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/series.py:1033, in Series.__getitem__(self, key). 1030 key = np.asarray(key, dtype=bool). 1031 return self._get_values(key). -> 1033 return self._get_with(key). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/series.py:1073, in Series._get_with(self, key). 1070 return self.iloc[key]. 107",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:2378,modifiability,pac,packages,2378,",. 445 min_mean=min_mean,. 446 max_mean=max_mean,. 447 n_top_genes=n_top_genes,. 448 n_bins=n_bins,. 449 flavor=flavor,. 450 ). 451 else:. 452 sanitize_anndata(adata). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:223, in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 219 # retrieve those genes that have nan std, these are the ones where. 220 # only a single gene fell in the bin and implicitly set them to have. 221 # a normalized disperion of 1. 222 one_gene_per_bin = disp_std_bin.isnull(). --> 223 gen_indices = np.where(one_gene_per_bin[df['mean_bin'].values])[0].tolist(). 224 if len(gen_indices) > 0:. 225 logg.debug(. 226 f'Gene indices {gen_indices} fell into a single bin: their '. 227 'normalized dispersion was set to 1.\n '. 228 'Decreasing `n_bins` will likely avoid this effect.'. 229 ). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/series.py:1033, in Series.__getitem__(self, key). 1030 key = np.asarray(key, dtype=bool). 1031 return self._get_values(key). -> 1033 return self._get_with(key). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/series.py:1073, in Series._get_with(self, key). 1070 return self.iloc[key]. 1072 # handle the dup indexing case GH#4246. -> 1073 return self.loc[key]. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1103, in _LocationIndexer.__getitem__(self, key). 1100 axis = self.axis or 0. 1102 maybe_callable = com.apply_if_callable(key, self.obj). -> 1103 return self._getitem_axis(maybe_callable, axis=axis). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1332, in _LocIndexer._getitem_axis(self, key, axis). 1329 if hasattr(key, ""ndim"") and key.ndim > 1:. 1330 raise ValueError(""Cannot index with multidimensional key""). -> 1332 return self._getitem_iterable(key, axis=axis). 1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:2614,modifiability,pac,packages,2614,"y/preprocessing/_highly_variable_genes.py:223, in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 219 # retrieve those genes that have nan std, these are the ones where. 220 # only a single gene fell in the bin and implicitly set them to have. 221 # a normalized disperion of 1. 222 one_gene_per_bin = disp_std_bin.isnull(). --> 223 gen_indices = np.where(one_gene_per_bin[df['mean_bin'].values])[0].tolist(). 224 if len(gen_indices) > 0:. 225 logg.debug(. 226 f'Gene indices {gen_indices} fell into a single bin: their '. 227 'normalized dispersion was set to 1.\n '. 228 'Decreasing `n_bins` will likely avoid this effect.'. 229 ). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/series.py:1033, in Series.__getitem__(self, key). 1030 key = np.asarray(key, dtype=bool). 1031 return self._get_values(key). -> 1033 return self._get_with(key). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/series.py:1073, in Series._get_with(self, key). 1070 return self.iloc[key]. 1072 # handle the dup indexing case GH#4246. -> 1073 return self.loc[key]. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1103, in _LocationIndexer.__getitem__(self, key). 1100 axis = self.axis or 0. 1102 maybe_callable = com.apply_if_callable(key, self.obj). -> 1103 return self._getitem_axis(maybe_callable, axis=axis). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1332, in _LocIndexer._getitem_axis(self, key, axis). 1329 if hasattr(key, ""ndim"") and key.ndim > 1:. 1330 raise ValueError(""Cannot index with multidimensional key""). -> 1332 return self._getitem_iterable(key, axis=axis). 1334 # nested tuple slicing. 1335 if is_nested_tuple(key, labels):. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1272, in _LocIndexer._getitem_iterable(self, key, axis). 1269 self._validate_key(k",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:2840,modifiability,pac,packages,2840,"the ones where. 220 # only a single gene fell in the bin and implicitly set them to have. 221 # a normalized disperion of 1. 222 one_gene_per_bin = disp_std_bin.isnull(). --> 223 gen_indices = np.where(one_gene_per_bin[df['mean_bin'].values])[0].tolist(). 224 if len(gen_indices) > 0:. 225 logg.debug(. 226 f'Gene indices {gen_indices} fell into a single bin: their '. 227 'normalized dispersion was set to 1.\n '. 228 'Decreasing `n_bins` will likely avoid this effect.'. 229 ). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/series.py:1033, in Series.__getitem__(self, key). 1030 key = np.asarray(key, dtype=bool). 1031 return self._get_values(key). -> 1033 return self._get_with(key). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/series.py:1073, in Series._get_with(self, key). 1070 return self.iloc[key]. 1072 # handle the dup indexing case GH#4246. -> 1073 return self.loc[key]. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1103, in _LocationIndexer.__getitem__(self, key). 1100 axis = self.axis or 0. 1102 maybe_callable = com.apply_if_callable(key, self.obj). -> 1103 return self._getitem_axis(maybe_callable, axis=axis). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1332, in _LocIndexer._getitem_axis(self, key, axis). 1329 if hasattr(key, ""ndim"") and key.ndim > 1:. 1330 raise ValueError(""Cannot index with multidimensional key""). -> 1332 return self._getitem_iterable(key, axis=axis). 1334 # nested tuple slicing. 1335 if is_nested_tuple(key, labels):. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1272, in _LocIndexer._getitem_iterable(self, key, axis). 1269 self._validate_key(key, axis). 1271 # A collection of keys. -> 1272 keyarr, indexer = self._get_listlike_indexer(key, axis). 1273 return self.obj._reindex_with_indexers(. 1274 {axis: [keyarr, indexer]}, copy=True, allow_dups=True. 1275 ). File /o",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:3127,modifiability,pac,packages,3127,"25 logg.debug(. 226 f'Gene indices {gen_indices} fell into a single bin: their '. 227 'normalized dispersion was set to 1.\n '. 228 'Decreasing `n_bins` will likely avoid this effect.'. 229 ). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/series.py:1033, in Series.__getitem__(self, key). 1030 key = np.asarray(key, dtype=bool). 1031 return self._get_values(key). -> 1033 return self._get_with(key). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/series.py:1073, in Series._get_with(self, key). 1070 return self.iloc[key]. 1072 # handle the dup indexing case GH#4246. -> 1073 return self.loc[key]. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1103, in _LocationIndexer.__getitem__(self, key). 1100 axis = self.axis or 0. 1102 maybe_callable = com.apply_if_callable(key, self.obj). -> 1103 return self._getitem_axis(maybe_callable, axis=axis). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1332, in _LocIndexer._getitem_axis(self, key, axis). 1329 if hasattr(key, ""ndim"") and key.ndim > 1:. 1330 raise ValueError(""Cannot index with multidimensional key""). -> 1332 return self._getitem_iterable(key, axis=axis). 1334 # nested tuple slicing. 1335 if is_nested_tuple(key, labels):. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1272, in _LocIndexer._getitem_iterable(self, key, axis). 1269 self._validate_key(key, axis). 1271 # A collection of keys. -> 1272 keyarr, indexer = self._get_listlike_indexer(key, axis). 1273 return self.obj._reindex_with_indexers(. 1274 {axis: [keyarr, indexer]}, copy=True, allow_dups=True. 1275 ). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1462, in _LocIndexer._get_listlike_indexer(self, key, axis). 1459 ax = self.obj._get_axis(axis). 1460 axis_name = self.obj._get_axis_name(axis). -> 1462 keyarr, indexer = ax._get_indexer_strict(key, axis_nam",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:3503,modifiability,pac,packages,3503,"get_values(key). -> 1033 return self._get_with(key). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/series.py:1073, in Series._get_with(self, key). 1070 return self.iloc[key]. 1072 # handle the dup indexing case GH#4246. -> 1073 return self.loc[key]. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1103, in _LocationIndexer.__getitem__(self, key). 1100 axis = self.axis or 0. 1102 maybe_callable = com.apply_if_callable(key, self.obj). -> 1103 return self._getitem_axis(maybe_callable, axis=axis). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1332, in _LocIndexer._getitem_axis(self, key, axis). 1329 if hasattr(key, ""ndim"") and key.ndim > 1:. 1330 raise ValueError(""Cannot index with multidimensional key""). -> 1332 return self._getitem_iterable(key, axis=axis). 1334 # nested tuple slicing. 1335 if is_nested_tuple(key, labels):. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1272, in _LocIndexer._getitem_iterable(self, key, axis). 1269 self._validate_key(key, axis). 1271 # A collection of keys. -> 1272 keyarr, indexer = self._get_listlike_indexer(key, axis). 1273 return self.obj._reindex_with_indexers(. 1274 {axis: [keyarr, indexer]}, copy=True, allow_dups=True. 1275 ). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1462, in _LocIndexer._get_listlike_indexer(self, key, axis). 1459 ax = self.obj._get_axis(axis). 1460 axis_name = self.obj._get_axis_name(axis). -> 1462 keyarr, indexer = ax._get_indexer_strict(key, axis_name). 1464 return keyarr, indexer. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexes/base.py:5876, in Index._get_indexer_strict(self, key, axis_name). 5873 else:. 5874 keyarr, indexer, new_indexer = self._reindex_non_unique(keyarr). -> 5876 self._raise_if_missing(keyarr, indexer, axis_name). 5878 keyarr = self.take(indexer). 5879 if isinstance",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:3891,modifiability,pac,packages,3891,"xer.__getitem__(self, key). 1100 axis = self.axis or 0. 1102 maybe_callable = com.apply_if_callable(key, self.obj). -> 1103 return self._getitem_axis(maybe_callable, axis=axis). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1332, in _LocIndexer._getitem_axis(self, key, axis). 1329 if hasattr(key, ""ndim"") and key.ndim > 1:. 1330 raise ValueError(""Cannot index with multidimensional key""). -> 1332 return self._getitem_iterable(key, axis=axis). 1334 # nested tuple slicing. 1335 if is_nested_tuple(key, labels):. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1272, in _LocIndexer._getitem_iterable(self, key, axis). 1269 self._validate_key(key, axis). 1271 # A collection of keys. -> 1272 keyarr, indexer = self._get_listlike_indexer(key, axis). 1273 return self.obj._reindex_with_indexers(. 1274 {axis: [keyarr, indexer]}, copy=True, allow_dups=True. 1275 ). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1462, in _LocIndexer._get_listlike_indexer(self, key, axis). 1459 ax = self.obj._get_axis(axis). 1460 axis_name = self.obj._get_axis_name(axis). -> 1462 keyarr, indexer = ax._get_indexer_strict(key, axis_name). 1464 return keyarr, indexer. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexes/base.py:5876, in Index._get_indexer_strict(self, key, axis_name). 5873 else:. 5874 keyarr, indexer, new_indexer = self._reindex_non_unique(keyarr). -> 5876 self._raise_if_missing(keyarr, indexer, axis_name). 5878 keyarr = self.take(indexer). 5879 if isinstance(key, Index):. 5880 # GH 42790 - Preserve name from an Index. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexes/base.py:5938, in Index._raise_if_missing(self, key, indexer, axis_name). 5935 raise KeyError(f""None of [{key}] are in the [{axis_name}]""). 5937 not_found = list(ensure_index(key)[missing_mask.nonzero()[0]].unique()). -> 5938 raise KeyError(f""{",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:4218,modifiability,pac,packages,4218,"asattr(key, ""ndim"") and key.ndim > 1:. 1330 raise ValueError(""Cannot index with multidimensional key""). -> 1332 return self._getitem_iterable(key, axis=axis). 1334 # nested tuple slicing. 1335 if is_nested_tuple(key, labels):. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1272, in _LocIndexer._getitem_iterable(self, key, axis). 1269 self._validate_key(key, axis). 1271 # A collection of keys. -> 1272 keyarr, indexer = self._get_listlike_indexer(key, axis). 1273 return self.obj._reindex_with_indexers(. 1274 {axis: [keyarr, indexer]}, copy=True, allow_dups=True. 1275 ). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1462, in _LocIndexer._get_listlike_indexer(self, key, axis). 1459 ax = self.obj._get_axis(axis). 1460 axis_name = self.obj._get_axis_name(axis). -> 1462 keyarr, indexer = ax._get_indexer_strict(key, axis_name). 1464 return keyarr, indexer. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexes/base.py:5876, in Index._get_indexer_strict(self, key, axis_name). 5873 else:. 5874 keyarr, indexer, new_indexer = self._reindex_non_unique(keyarr). -> 5876 self._raise_if_missing(keyarr, indexer, axis_name). 5878 keyarr = self.take(indexer). 5879 if isinstance(key, Index):. 5880 # GH 42790 - Preserve name from an Index. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexes/base.py:5938, in Index._raise_if_missing(self, key, indexer, axis_name). 5935 raise KeyError(f""None of [{key}] are in the [{axis_name}]""). 5937 not_found = list(ensure_index(key)[missing_mask.nonzero()[0]].unique()). -> 5938 raise KeyError(f""{not_found} not in index""). KeyError: '[nan] not in index'. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. absl NA. aiohttp 3.8.4. aiosignal 1.3.1. anyio NA. asttokens NA. async_timeout 4.0.2. attr 23.1.0. backcall 0.2.0. bs4 4.12.2. certifi 2023.05.07. cffi 1.15.1. charset_normalizer ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:4623,modifiability,pac,packages,4623,". 1271 # A collection of keys. -> 1272 keyarr, indexer = self._get_listlike_indexer(key, axis). 1273 return self.obj._reindex_with_indexers(. 1274 {axis: [keyarr, indexer]}, copy=True, allow_dups=True. 1275 ). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1462, in _LocIndexer._get_listlike_indexer(self, key, axis). 1459 ax = self.obj._get_axis(axis). 1460 axis_name = self.obj._get_axis_name(axis). -> 1462 keyarr, indexer = ax._get_indexer_strict(key, axis_name). 1464 return keyarr, indexer. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexes/base.py:5876, in Index._get_indexer_strict(self, key, axis_name). 5873 else:. 5874 keyarr, indexer, new_indexer = self._reindex_non_unique(keyarr). -> 5876 self._raise_if_missing(keyarr, indexer, axis_name). 5878 keyarr = self.take(indexer). 5879 if isinstance(key, Index):. 5880 # GH 42790 - Preserve name from an Index. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexes/base.py:5938, in Index._raise_if_missing(self, key, indexer, axis_name). 5935 raise KeyError(f""None of [{key}] are in the [{axis_name}]""). 5937 not_found = list(ensure_index(key)[missing_mask.nonzero()[0]].unique()). -> 5938 raise KeyError(f""{not_found} not in index""). KeyError: '[nan] not in index'. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. absl NA. aiohttp 3.8.4. aiosignal 1.3.1. anyio NA. asttokens NA. async_timeout 4.0.2. attr 23.1.0. backcall 0.2.0. bs4 4.12.2. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. chex 0.1.7. click 8.1.3. colorama 0.4.6. comm 0.1.3. contextlib2 NA. croniter NA. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. deepdiff 6.3.0. defusedxml 0.7.1. dill 0.3.6. docrep 0.3.2. dot_parser NA. etils 1.3.0. executing 1.2.0. fastapi 0.88.0. flax 0.6.10. frozenlist 1.3.3. fsspec 2023.6.0. h5py 3.8.0. idna 3.4. igraph 0.10.4. importlib_resources NA. ipyk",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:4963,modifiability,Version,Versions,4963,"self, key, axis). 1459 ax = self.obj._get_axis(axis). 1460 axis_name = self.obj._get_axis_name(axis). -> 1462 keyarr, indexer = ax._get_indexer_strict(key, axis_name). 1464 return keyarr, indexer. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexes/base.py:5876, in Index._get_indexer_strict(self, key, axis_name). 5873 else:. 5874 keyarr, indexer, new_indexer = self._reindex_non_unique(keyarr). -> 5876 self._raise_if_missing(keyarr, indexer, axis_name). 5878 keyarr = self.take(indexer). 5879 if isinstance(key, Index):. 5880 # GH 42790 - Preserve name from an Index. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexes/base.py:5938, in Index._raise_if_missing(self, key, indexer, axis_name). 5935 raise KeyError(f""None of [{key}] are in the [{axis_name}]""). 5937 not_found = list(ensure_index(key)[missing_mask.nonzero()[0]].unique()). -> 5938 raise KeyError(f""{not_found} not in index""). KeyError: '[nan] not in index'. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. absl NA. aiohttp 3.8.4. aiosignal 1.3.1. anyio NA. asttokens NA. async_timeout 4.0.2. attr 23.1.0. backcall 0.2.0. bs4 4.12.2. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. chex 0.1.7. click 8.1.3. colorama 0.4.6. comm 0.1.3. contextlib2 NA. croniter NA. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. deepdiff 6.3.0. defusedxml 0.7.1. dill 0.3.6. docrep 0.3.2. dot_parser NA. etils 1.3.0. executing 1.2.0. fastapi 0.88.0. flax 0.6.10. frozenlist 1.3.3. fsspec 2023.6.0. h5py 3.8.0. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.1. ipython_genutils 0.2.0. ipywidgets 8.0.6. jax 0.4.12. jaxlib 0.4.12. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. lightning 2.0.3. lightning_cloud NA. lightning_fabric 2.0.3. lightning_utilities 0.8.0. llvmlite 0.40.0. louvain 0.8.0. matplotlib 3.7.1. matplotlib_inline 0.1.6. ml_collections NA. ml_dtypes 0.2",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:5376,modifiability,deco,decorator,5376,"ique(keyarr). -> 5876 self._raise_if_missing(keyarr, indexer, axis_name). 5878 keyarr = self.take(indexer). 5879 if isinstance(key, Index):. 5880 # GH 42790 - Preserve name from an Index. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexes/base.py:5938, in Index._raise_if_missing(self, key, indexer, axis_name). 5935 raise KeyError(f""None of [{key}] are in the [{axis_name}]""). 5937 not_found = list(ensure_index(key)[missing_mask.nonzero()[0]].unique()). -> 5938 raise KeyError(f""{not_found} not in index""). KeyError: '[nan] not in index'. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. absl NA. aiohttp 3.8.4. aiosignal 1.3.1. anyio NA. asttokens NA. async_timeout 4.0.2. attr 23.1.0. backcall 0.2.0. bs4 4.12.2. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. chex 0.1.7. click 8.1.3. colorama 0.4.6. comm 0.1.3. contextlib2 NA. croniter NA. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. deepdiff 6.3.0. defusedxml 0.7.1. dill 0.3.6. docrep 0.3.2. dot_parser NA. etils 1.3.0. executing 1.2.0. fastapi 0.88.0. flax 0.6.10. frozenlist 1.3.3. fsspec 2023.6.0. h5py 3.8.0. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.1. ipython_genutils 0.2.0. ipywidgets 8.0.6. jax 0.4.12. jaxlib 0.4.12. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. lightning 2.0.3. lightning_cloud NA. lightning_fabric 2.0.3. lightning_utilities 0.8.0. llvmlite 0.40.0. louvain 0.8.0. matplotlib 3.7.1. matplotlib_inline 0.1.6. ml_collections NA. ml_dtypes 0.2.0. mpl_toolkits NA. mpmath 1.3.0. msgpack 1.0.5. mudata 0.2.3. multidict 6.0.4. multipart 0.0.6. multipledispatch 0.6.0. natsort 8.3.1. numba 0.57.0. numpy 1.24.3. numpyro 0.12.1. nvfuser NA. opt_einsum v3.3.0. optax 0.1.5. ordered_set 4.1.0. packaging 23.1. pandas 2.0.2. parso 0.8.3. patsy 0.5.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.3. prompt_toolkit 3.0.38. psutil 5.9.5. ptypr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:6211,modifiability,pac,packaging,6211,lizer 3.1.0. chex 0.1.7. click 8.1.3. colorama 0.4.6. comm 0.1.3. contextlib2 NA. croniter NA. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. deepdiff 6.3.0. defusedxml 0.7.1. dill 0.3.6. docrep 0.3.2. dot_parser NA. etils 1.3.0. executing 1.2.0. fastapi 0.88.0. flax 0.6.10. frozenlist 1.3.3. fsspec 2023.6.0. h5py 3.8.0. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.1. ipython_genutils 0.2.0. ipywidgets 8.0.6. jax 0.4.12. jaxlib 0.4.12. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. lightning 2.0.3. lightning_cloud NA. lightning_fabric 2.0.3. lightning_utilities 0.8.0. llvmlite 0.40.0. louvain 0.8.0. matplotlib 3.7.1. matplotlib_inline 0.1.6. ml_collections NA. ml_dtypes 0.2.0. mpl_toolkits NA. mpmath 1.3.0. msgpack 1.0.5. mudata 0.2.3. multidict 6.0.4. multipart 0.0.6. multipledispatch 0.6.0. natsort 8.3.1. numba 0.57.0. numpy 1.24.3. numpyro 0.12.1. nvfuser NA. opt_einsum v3.3.0. optax 0.1.5. ordered_set 4.1.0. packaging 23.1. pandas 2.0.2. parso 0.8.3. patsy 0.5.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.3. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydantic 1.10.9. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pydot 1.4.2. pygments 2.15.1. pyparsing 3.0.9. pyro 1.8.5. pytorch_lightning 2.0.3. pytz 2023.3. requests 2.31.0. rich NA. scipy 1.10.1. scvi 1.0.0. seaborn 0.12.2. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. sklearn 1.2.2. sniffio 1.3.0. socks 1.7.1. soupsieve 2.3.2.post1. sparse 0.14.0. sphinxcontrib NA. stack_data 0.6.2. starlette 0.22.0. statsmodels 0.14.0. sympy 1.12. texttable 1.6.7. threadpoolctl 3.1.0. toml 0.10.2. toolz 0.12.0. torch 2.0.1+cu117. torchmetrics 0.11.4. tornado 6.3.2. tqdm 4.65.0. traitlets 5.9.0. tree 0.1.8. typing_extensions NA. urllib3 2.0.3. uvicorn 0.22.0. wcwidth 0.2.6. websocket 1.5.3. websockets 11.0.3. wrapt 1.15.0. ,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:7384,modifiability,pac,packaged,7384,"igraph 0.10.4. importlib_resources NA. ipykernel 6.23.1. ipython_genutils 0.2.0. ipywidgets 8.0.6. jax 0.4.12. jaxlib 0.4.12. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. lightning 2.0.3. lightning_cloud NA. lightning_fabric 2.0.3. lightning_utilities 0.8.0. llvmlite 0.40.0. louvain 0.8.0. matplotlib 3.7.1. matplotlib_inline 0.1.6. ml_collections NA. ml_dtypes 0.2.0. mpl_toolkits NA. mpmath 1.3.0. msgpack 1.0.5. mudata 0.2.3. multidict 6.0.4. multipart 0.0.6. multipledispatch 0.6.0. natsort 8.3.1. numba 0.57.0. numpy 1.24.3. numpyro 0.12.1. nvfuser NA. opt_einsum v3.3.0. optax 0.1.5. ordered_set 4.1.0. packaging 23.1. pandas 2.0.2. parso 0.8.3. patsy 0.5.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.3. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydantic 1.10.9. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pydot 1.4.2. pygments 2.15.1. pyparsing 3.0.9. pyro 1.8.5. pytorch_lightning 2.0.3. pytz 2023.3. requests 2.31.0. rich NA. scipy 1.10.1. scvi 1.0.0. seaborn 0.12.2. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. sklearn 1.2.2. sniffio 1.3.0. socks 1.7.1. soupsieve 2.3.2.post1. sparse 0.14.0. sphinxcontrib NA. stack_data 0.6.2. starlette 0.22.0. statsmodels 0.14.0. sympy 1.12. texttable 1.6.7. threadpoolctl 3.1.0. toml 0.10.2. toolz 0.12.0. torch 2.0.1+cu117. torchmetrics 0.11.4. tornado 6.3.2. tqdm 4.65.0. traitlets 5.9.0. tree 0.1.8. typing_extensions NA. urllib3 2.0.3. uvicorn 0.22.0. wcwidth 0.2.6. websocket 1.5.3. websockets 11.0.3. wrapt 1.15.0. xarray 2023.5.0. yaml 6.0. yarl 1.9.2. zmq 25.1.0. zoneinfo NA. -----. IPython 8.14.0. jupyter_client 8.2.0. jupyter_core 5.3.1. notebook 6.5.4. -----. Python 3.11.4 | packaged by conda-forge | (main, Jun 10 2023, 18:08:17) [GCC 12.2.0]. Linux-4.18.0-425.19.2.el8_7.x86_64-x86_64-with-glibc2.27. -----. Session information updated at 2023-07-06 03:56. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:404,performance,error,error,404,"highly_variable_genes gets a KeyError: '[nan] not in index'; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Running the highly_variable_genes function produced an error. ### Minimal code sample. ```python. sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[101], line 2. 1 # Identify highly-variable genes and plot. ----> 2 sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:440, in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values). 428 return _highly_variable_genes_seurat_v3(. 429 adata,. 430 layer=layer,. (...). 436 inplace=inplace,. 437 ). 439 if batch_key is None:. --> 440 df = _highly_variable_genes_single_batch(. 441 adata,. 442 layer=layer,. 443 min_disp=min_disp,. 444 max_disp=max_disp,. 445 min_mean=min_mean,. 446 max_mean=max_mean,. 447 n_top_genes=n_top_genes,. 448 n_bins=n_bins,. 449 flavor=flavor,. 450 ). 451 else:. 452 sanitize_anndata(adata). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:223, in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 219 # retrieve those genes that have nan std, these are the ones where. 220 # only a single gene fell in the bin and implicitly set them to have. 221 # a normalized disperion of 1. 222 one_gene_per_bin = disp_std",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:541,performance,Error,Error,541,"highly_variable_genes gets a KeyError: '[nan] not in index'; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Running the highly_variable_genes function produced an error. ### Minimal code sample. ```python. sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[101], line 2. 1 # Identify highly-variable genes and plot. ----> 2 sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:440, in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values). 428 return _highly_variable_genes_seurat_v3(. 429 adata,. 430 layer=layer,. (...). 436 inplace=inplace,. 437 ). 439 if batch_key is None:. --> 440 df = _highly_variable_genes_single_batch(. 441 adata,. 442 layer=layer,. 443 min_disp=min_disp,. 444 max_disp=max_disp,. 445 min_mean=min_mean,. 446 max_mean=max_mean,. 447 n_top_genes=n_top_genes,. 448 n_bins=n_bins,. 449 flavor=flavor,. 450 ). 451 else:. 452 sanitize_anndata(adata). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:223, in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 219 # retrieve those genes that have nan std, these are the ones where. 220 # only a single gene fell in the bin and implicitly set them to have. 221 # a normalized disperion of 1. 222 one_gene_per_bin = disp_std",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:3401,reliability,sli,slicing,3401,".py:1033, in Series.__getitem__(self, key). 1030 key = np.asarray(key, dtype=bool). 1031 return self._get_values(key). -> 1033 return self._get_with(key). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/series.py:1073, in Series._get_with(self, key). 1070 return self.iloc[key]. 1072 # handle the dup indexing case GH#4246. -> 1073 return self.loc[key]. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1103, in _LocationIndexer.__getitem__(self, key). 1100 axis = self.axis or 0. 1102 maybe_callable = com.apply_if_callable(key, self.obj). -> 1103 return self._getitem_axis(maybe_callable, axis=axis). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1332, in _LocIndexer._getitem_axis(self, key, axis). 1329 if hasattr(key, ""ndim"") and key.ndim > 1:. 1330 raise ValueError(""Cannot index with multidimensional key""). -> 1332 return self._getitem_iterable(key, axis=axis). 1334 # nested tuple slicing. 1335 if is_nested_tuple(key, labels):. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1272, in _LocIndexer._getitem_iterable(self, key, axis). 1269 self._validate_key(key, axis). 1271 # A collection of keys. -> 1272 keyarr, indexer = self._get_listlike_indexer(key, axis). 1273 return self.obj._reindex_with_indexers(. 1274 {axis: [keyarr, indexer]}, copy=True, allow_dups=True. 1275 ). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1462, in _LocIndexer._get_listlike_indexer(self, key, axis). 1459 ax = self.obj._get_axis(axis). 1460 axis_name = self.obj._get_axis_name(axis). -> 1462 keyarr, indexer = ax._get_indexer_strict(key, axis_name). 1464 return keyarr, indexer. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexes/base.py:5876, in Index._get_indexer_strict(self, key, axis_name). 5873 else:. 5874 keyarr, indexer, new_indexer = self._reindex_non_unique(keyarr). -> 5876 se",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:404,safety,error,error,404,"highly_variable_genes gets a KeyError: '[nan] not in index'; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Running the highly_variable_genes function produced an error. ### Minimal code sample. ```python. sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[101], line 2. 1 # Identify highly-variable genes and plot. ----> 2 sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:440, in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values). 428 return _highly_variable_genes_seurat_v3(. 429 adata,. 430 layer=layer,. (...). 436 inplace=inplace,. 437 ). 439 if batch_key is None:. --> 440 df = _highly_variable_genes_single_batch(. 441 adata,. 442 layer=layer,. 443 min_disp=min_disp,. 444 max_disp=max_disp,. 445 min_mean=min_mean,. 446 max_mean=max_mean,. 447 n_top_genes=n_top_genes,. 448 n_bins=n_bins,. 449 flavor=flavor,. 450 ). 451 else:. 452 sanitize_anndata(adata). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:223, in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 219 # retrieve those genes that have nan std, these are the ones where. 220 # only a single gene fell in the bin and implicitly set them to have. 221 # a normalized disperion of 1. 222 one_gene_per_bin = disp_std",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:541,safety,Error,Error,541,"highly_variable_genes gets a KeyError: '[nan] not in index'; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Running the highly_variable_genes function produced an error. ### Minimal code sample. ```python. sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[101], line 2. 1 # Identify highly-variable genes and plot. ----> 2 sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:440, in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values). 428 return _highly_variable_genes_seurat_v3(. 429 adata,. 430 layer=layer,. (...). 436 inplace=inplace,. 437 ). 439 if batch_key is None:. --> 440 df = _highly_variable_genes_single_batch(. 441 adata,. 442 layer=layer,. 443 min_disp=min_disp,. 444 max_disp=max_disp,. 445 min_mean=min_mean,. 446 max_mean=max_mean,. 447 n_top_genes=n_top_genes,. 448 n_bins=n_bins,. 449 flavor=flavor,. 450 ). 451 else:. 452 sanitize_anndata(adata). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:223, in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 219 # retrieve those genes that have nan std, these are the ones where. 220 # only a single gene fell in the bin and implicitly set them to have. 221 # a normalized disperion of 1. 222 one_gene_per_bin = disp_std",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:2134,safety,log,logg,2134,"variable_genes_seurat_v3(. 429 adata,. 430 layer=layer,. (...). 436 inplace=inplace,. 437 ). 439 if batch_key is None:. --> 440 df = _highly_variable_genes_single_batch(. 441 adata,. 442 layer=layer,. 443 min_disp=min_disp,. 444 max_disp=max_disp,. 445 min_mean=min_mean,. 446 max_mean=max_mean,. 447 n_top_genes=n_top_genes,. 448 n_bins=n_bins,. 449 flavor=flavor,. 450 ). 451 else:. 452 sanitize_anndata(adata). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:223, in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 219 # retrieve those genes that have nan std, these are the ones where. 220 # only a single gene fell in the bin and implicitly set them to have. 221 # a normalized disperion of 1. 222 one_gene_per_bin = disp_std_bin.isnull(). --> 223 gen_indices = np.where(one_gene_per_bin[df['mean_bin'].values])[0].tolist(). 224 if len(gen_indices) > 0:. 225 logg.debug(. 226 f'Gene indices {gen_indices} fell into a single bin: their '. 227 'normalized dispersion was set to 1.\n '. 228 'Decreasing `n_bins` will likely avoid this effect.'. 229 ). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/series.py:1033, in Series.__getitem__(self, key). 1030 key = np.asarray(key, dtype=bool). 1031 return self._get_values(key). -> 1033 return self._get_with(key). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/series.py:1073, in Series._get_with(self, key). 1070 return self.iloc[key]. 1072 # handle the dup indexing case GH#4246. -> 1073 return self.loc[key]. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1103, in _LocationIndexer.__getitem__(self, key). 1100 axis = self.axis or 0. 1102 maybe_callable = com.apply_if_callable(key, self.obj). -> 1103 return self._getitem_axis(maybe_callable, axis=axis). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:2296,safety,avoid,avoid,2296,"batch(. 441 adata,. 442 layer=layer,. 443 min_disp=min_disp,. 444 max_disp=max_disp,. 445 min_mean=min_mean,. 446 max_mean=max_mean,. 447 n_top_genes=n_top_genes,. 448 n_bins=n_bins,. 449 flavor=flavor,. 450 ). 451 else:. 452 sanitize_anndata(adata). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:223, in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 219 # retrieve those genes that have nan std, these are the ones where. 220 # only a single gene fell in the bin and implicitly set them to have. 221 # a normalized disperion of 1. 222 one_gene_per_bin = disp_std_bin.isnull(). --> 223 gen_indices = np.where(one_gene_per_bin[df['mean_bin'].values])[0].tolist(). 224 if len(gen_indices) > 0:. 225 logg.debug(. 226 f'Gene indices {gen_indices} fell into a single bin: their '. 227 'normalized dispersion was set to 1.\n '. 228 'Decreasing `n_bins` will likely avoid this effect.'. 229 ). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/series.py:1033, in Series.__getitem__(self, key). 1030 key = np.asarray(key, dtype=bool). 1031 return self._get_values(key). -> 1033 return self._get_with(key). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/series.py:1073, in Series._get_with(self, key). 1070 return self.iloc[key]. 1072 # handle the dup indexing case GH#4246. -> 1073 return self.loc[key]. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1103, in _LocationIndexer.__getitem__(self, key). 1100 axis = self.axis or 0. 1102 maybe_callable = com.apply_if_callable(key, self.obj). -> 1103 return self._getitem_axis(maybe_callable, axis=axis). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1332, in _LocIndexer._getitem_axis(self, key, axis). 1329 if hasattr(key, ""ndim"") and key.ndim > 1:. 1330 raise ValueError(""Cannot index wi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:7539,safety,updat,updated,7539,"igraph 0.10.4. importlib_resources NA. ipykernel 6.23.1. ipython_genutils 0.2.0. ipywidgets 8.0.6. jax 0.4.12. jaxlib 0.4.12. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. lightning 2.0.3. lightning_cloud NA. lightning_fabric 2.0.3. lightning_utilities 0.8.0. llvmlite 0.40.0. louvain 0.8.0. matplotlib 3.7.1. matplotlib_inline 0.1.6. ml_collections NA. ml_dtypes 0.2.0. mpl_toolkits NA. mpmath 1.3.0. msgpack 1.0.5. mudata 0.2.3. multidict 6.0.4. multipart 0.0.6. multipledispatch 0.6.0. natsort 8.3.1. numba 0.57.0. numpy 1.24.3. numpyro 0.12.1. nvfuser NA. opt_einsum v3.3.0. optax 0.1.5. ordered_set 4.1.0. packaging 23.1. pandas 2.0.2. parso 0.8.3. patsy 0.5.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.3. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydantic 1.10.9. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pydot 1.4.2. pygments 2.15.1. pyparsing 3.0.9. pyro 1.8.5. pytorch_lightning 2.0.3. pytz 2023.3. requests 2.31.0. rich NA. scipy 1.10.1. scvi 1.0.0. seaborn 0.12.2. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. sklearn 1.2.2. sniffio 1.3.0. socks 1.7.1. soupsieve 2.3.2.post1. sparse 0.14.0. sphinxcontrib NA. stack_data 0.6.2. starlette 0.22.0. statsmodels 0.14.0. sympy 1.12. texttable 1.6.7. threadpoolctl 3.1.0. toml 0.10.2. toolz 0.12.0. torch 2.0.1+cu117. torchmetrics 0.11.4. tornado 6.3.2. tqdm 4.65.0. traitlets 5.9.0. tree 0.1.8. typing_extensions NA. urllib3 2.0.3. uvicorn 0.22.0. wcwidth 0.2.6. websocket 1.5.3. websockets 11.0.3. wrapt 1.15.0. xarray 2023.5.0. yaml 6.0. yarl 1.9.2. zmq 25.1.0. zoneinfo NA. -----. IPython 8.14.0. jupyter_client 8.2.0. jupyter_core 5.3.1. notebook 6.5.4. -----. Python 3.11.4 | packaged by conda-forge | (main, Jun 10 2023, 18:08:17) [GCC 12.2.0]. Linux-4.18.0-425.19.2.el8_7.x86_64-x86_64-with-glibc2.27. -----. Session information updated at 2023-07-06 03:56. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:711,security,Ident,Identify,711,"highly_variable_genes gets a KeyError: '[nan] not in index'; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Running the highly_variable_genes function produced an error. ### Minimal code sample. ```python. sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[101], line 2. 1 # Identify highly-variable genes and plot. ----> 2 sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:440, in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values). 428 return _highly_variable_genes_seurat_v3(. 429 adata,. 430 layer=layer,. (...). 436 inplace=inplace,. 437 ). 439 if batch_key is None:. --> 440 df = _highly_variable_genes_single_batch(. 441 adata,. 442 layer=layer,. 443 min_disp=min_disp,. 444 max_disp=max_disp,. 445 min_mean=min_mean,. 446 max_mean=max_mean,. 447 n_top_genes=n_top_genes,. 448 n_bins=n_bins,. 449 flavor=flavor,. 450 ). 451 else:. 452 sanitize_anndata(adata). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:223, in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 219 # retrieve those genes that have nan std, these are the ones where. 220 # only a single gene fell in the bin and implicitly set them to have. 221 # a normalized disperion of 1. 222 one_gene_per_bin = disp_std",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:2134,security,log,logg,2134,"variable_genes_seurat_v3(. 429 adata,. 430 layer=layer,. (...). 436 inplace=inplace,. 437 ). 439 if batch_key is None:. --> 440 df = _highly_variable_genes_single_batch(. 441 adata,. 442 layer=layer,. 443 min_disp=min_disp,. 444 max_disp=max_disp,. 445 min_mean=min_mean,. 446 max_mean=max_mean,. 447 n_top_genes=n_top_genes,. 448 n_bins=n_bins,. 449 flavor=flavor,. 450 ). 451 else:. 452 sanitize_anndata(adata). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:223, in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 219 # retrieve those genes that have nan std, these are the ones where. 220 # only a single gene fell in the bin and implicitly set them to have. 221 # a normalized disperion of 1. 222 one_gene_per_bin = disp_std_bin.isnull(). --> 223 gen_indices = np.where(one_gene_per_bin[df['mean_bin'].values])[0].tolist(). 224 if len(gen_indices) > 0:. 225 logg.debug(. 226 f'Gene indices {gen_indices} fell into a single bin: their '. 227 'normalized dispersion was set to 1.\n '. 228 'Decreasing `n_bins` will likely avoid this effect.'. 229 ). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/series.py:1033, in Series.__getitem__(self, key). 1030 key = np.asarray(key, dtype=bool). 1031 return self._get_values(key). -> 1033 return self._get_with(key). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/series.py:1073, in Series._get_with(self, key). 1070 return self.iloc[key]. 1072 # handle the dup indexing case GH#4246. -> 1073 return self.loc[key]. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1103, in _LocationIndexer.__getitem__(self, key). 1100 axis = self.axis or 0. 1102 maybe_callable = com.apply_if_callable(key, self.obj). -> 1103 return self._getitem_axis(maybe_callable, axis=axis). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:5170,security,certif,certifi,5170,"conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexes/base.py:5876, in Index._get_indexer_strict(self, key, axis_name). 5873 else:. 5874 keyarr, indexer, new_indexer = self._reindex_non_unique(keyarr). -> 5876 self._raise_if_missing(keyarr, indexer, axis_name). 5878 keyarr = self.take(indexer). 5879 if isinstance(key, Index):. 5880 # GH 42790 - Preserve name from an Index. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexes/base.py:5938, in Index._raise_if_missing(self, key, indexer, axis_name). 5935 raise KeyError(f""None of [{key}] are in the [{axis_name}]""). 5937 not_found = list(ensure_index(key)[missing_mask.nonzero()[0]].unique()). -> 5938 raise KeyError(f""{not_found} not in index""). KeyError: '[nan] not in index'. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. absl NA. aiohttp 3.8.4. aiosignal 1.3.1. anyio NA. asttokens NA. async_timeout 4.0.2. attr 23.1.0. backcall 0.2.0. bs4 4.12.2. certifi 2023.05.07. cffi 1.15.1. charset_normalizer 3.1.0. chex 0.1.7. click 8.1.3. colorama 0.4.6. comm 0.1.3. contextlib2 NA. croniter NA. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. deepdiff 6.3.0. defusedxml 0.7.1. dill 0.3.6. docrep 0.3.2. dot_parser NA. etils 1.3.0. executing 1.2.0. fastapi 0.88.0. flax 0.6.10. frozenlist 1.3.3. fsspec 2023.6.0. h5py 3.8.0. idna 3.4. igraph 0.10.4. importlib_resources NA. ipykernel 6.23.1. ipython_genutils 0.2.0. ipywidgets 8.0.6. jax 0.4.12. jaxlib 0.4.12. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. lightning 2.0.3. lightning_cloud NA. lightning_fabric 2.0.3. lightning_utilities 0.8.0. llvmlite 0.40.0. louvain 0.8.0. matplotlib 3.7.1. matplotlib_inline 0.1.6. ml_collections NA. ml_dtypes 0.2.0. mpl_toolkits NA. mpmath 1.3.0. msgpack 1.0.5. mudata 0.2.3. multidict 6.0.4. multipart 0.0.6. multipledispatch 0.6.0. natsort 8.3.1. numba 0.57.0. numpy 1.24.3. numpyro 0.12.1. nvfuser NA. opt_einsum v3.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:6799,security,soc,socks,6799,"igraph 0.10.4. importlib_resources NA. ipykernel 6.23.1. ipython_genutils 0.2.0. ipywidgets 8.0.6. jax 0.4.12. jaxlib 0.4.12. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. lightning 2.0.3. lightning_cloud NA. lightning_fabric 2.0.3. lightning_utilities 0.8.0. llvmlite 0.40.0. louvain 0.8.0. matplotlib 3.7.1. matplotlib_inline 0.1.6. ml_collections NA. ml_dtypes 0.2.0. mpl_toolkits NA. mpmath 1.3.0. msgpack 1.0.5. mudata 0.2.3. multidict 6.0.4. multipart 0.0.6. multipledispatch 0.6.0. natsort 8.3.1. numba 0.57.0. numpy 1.24.3. numpyro 0.12.1. nvfuser NA. opt_einsum v3.3.0. optax 0.1.5. ordered_set 4.1.0. packaging 23.1. pandas 2.0.2. parso 0.8.3. patsy 0.5.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.3. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydantic 1.10.9. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pydot 1.4.2. pygments 2.15.1. pyparsing 3.0.9. pyro 1.8.5. pytorch_lightning 2.0.3. pytz 2023.3. requests 2.31.0. rich NA. scipy 1.10.1. scvi 1.0.0. seaborn 0.12.2. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. sklearn 1.2.2. sniffio 1.3.0. socks 1.7.1. soupsieve 2.3.2.post1. sparse 0.14.0. sphinxcontrib NA. stack_data 0.6.2. starlette 0.22.0. statsmodels 0.14.0. sympy 1.12. texttable 1.6.7. threadpoolctl 3.1.0. toml 0.10.2. toolz 0.12.0. torch 2.0.1+cu117. torchmetrics 0.11.4. tornado 6.3.2. tqdm 4.65.0. traitlets 5.9.0. tree 0.1.8. typing_extensions NA. urllib3 2.0.3. uvicorn 0.22.0. wcwidth 0.2.6. websocket 1.5.3. websockets 11.0.3. wrapt 1.15.0. xarray 2023.5.0. yaml 6.0. yarl 1.9.2. zmq 25.1.0. zoneinfo NA. -----. IPython 8.14.0. jupyter_client 8.2.0. jupyter_core 5.3.1. notebook 6.5.4. -----. Python 3.11.4 | packaged by conda-forge | (main, Jun 10 2023, 18:08:17) [GCC 12.2.0]. Linux-4.18.0-425.19.2.el8_7.x86_64-x86_64-with-glibc2.27. -----. Session information updated at 2023-07-06 03:56. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:7519,security,Session,Session,7519,"igraph 0.10.4. importlib_resources NA. ipykernel 6.23.1. ipython_genutils 0.2.0. ipywidgets 8.0.6. jax 0.4.12. jaxlib 0.4.12. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. lightning 2.0.3. lightning_cloud NA. lightning_fabric 2.0.3. lightning_utilities 0.8.0. llvmlite 0.40.0. louvain 0.8.0. matplotlib 3.7.1. matplotlib_inline 0.1.6. ml_collections NA. ml_dtypes 0.2.0. mpl_toolkits NA. mpmath 1.3.0. msgpack 1.0.5. mudata 0.2.3. multidict 6.0.4. multipart 0.0.6. multipledispatch 0.6.0. natsort 8.3.1. numba 0.57.0. numpy 1.24.3. numpyro 0.12.1. nvfuser NA. opt_einsum v3.3.0. optax 0.1.5. ordered_set 4.1.0. packaging 23.1. pandas 2.0.2. parso 0.8.3. patsy 0.5.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.3. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydantic 1.10.9. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pydot 1.4.2. pygments 2.15.1. pyparsing 3.0.9. pyro 1.8.5. pytorch_lightning 2.0.3. pytz 2023.3. requests 2.31.0. rich NA. scipy 1.10.1. scvi 1.0.0. seaborn 0.12.2. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. sklearn 1.2.2. sniffio 1.3.0. socks 1.7.1. soupsieve 2.3.2.post1. sparse 0.14.0. sphinxcontrib NA. stack_data 0.6.2. starlette 0.22.0. statsmodels 0.14.0. sympy 1.12. texttable 1.6.7. threadpoolctl 3.1.0. toml 0.10.2. toolz 0.12.0. torch 2.0.1+cu117. torchmetrics 0.11.4. tornado 6.3.2. tqdm 4.65.0. traitlets 5.9.0. tree 0.1.8. typing_extensions NA. urllib3 2.0.3. uvicorn 0.22.0. wcwidth 0.2.6. websocket 1.5.3. websockets 11.0.3. wrapt 1.15.0. xarray 2023.5.0. yaml 6.0. yarl 1.9.2. zmq 25.1.0. zoneinfo NA. -----. IPython 8.14.0. jupyter_client 8.2.0. jupyter_core 5.3.1. notebook 6.5.4. -----. Python 3.11.4 | packaged by conda-forge | (main, Jun 10 2023, 18:08:17) [GCC 12.2.0]. Linux-4.18.0-425.19.2.el8_7.x86_64-x86_64-with-glibc2.27. -----. Session information updated at 2023-07-06 03:56. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:7539,security,updat,updated,7539,"igraph 0.10.4. importlib_resources NA. ipykernel 6.23.1. ipython_genutils 0.2.0. ipywidgets 8.0.6. jax 0.4.12. jaxlib 0.4.12. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. lightning 2.0.3. lightning_cloud NA. lightning_fabric 2.0.3. lightning_utilities 0.8.0. llvmlite 0.40.0. louvain 0.8.0. matplotlib 3.7.1. matplotlib_inline 0.1.6. ml_collections NA. ml_dtypes 0.2.0. mpl_toolkits NA. mpmath 1.3.0. msgpack 1.0.5. mudata 0.2.3. multidict 6.0.4. multipart 0.0.6. multipledispatch 0.6.0. natsort 8.3.1. numba 0.57.0. numpy 1.24.3. numpyro 0.12.1. nvfuser NA. opt_einsum v3.3.0. optax 0.1.5. ordered_set 4.1.0. packaging 23.1. pandas 2.0.2. parso 0.8.3. patsy 0.5.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.3. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydantic 1.10.9. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pydot 1.4.2. pygments 2.15.1. pyparsing 3.0.9. pyro 1.8.5. pytorch_lightning 2.0.3. pytz 2023.3. requests 2.31.0. rich NA. scipy 1.10.1. scvi 1.0.0. seaborn 0.12.2. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. sklearn 1.2.2. sniffio 1.3.0. socks 1.7.1. soupsieve 2.3.2.post1. sparse 0.14.0. sphinxcontrib NA. stack_data 0.6.2. starlette 0.22.0. statsmodels 0.14.0. sympy 1.12. texttable 1.6.7. threadpoolctl 3.1.0. toml 0.10.2. toolz 0.12.0. torch 2.0.1+cu117. torchmetrics 0.11.4. tornado 6.3.2. tqdm 4.65.0. traitlets 5.9.0. tree 0.1.8. typing_extensions NA. urllib3 2.0.3. uvicorn 0.22.0. wcwidth 0.2.6. websocket 1.5.3. websockets 11.0.3. wrapt 1.15.0. xarray 2023.5.0. yaml 6.0. yarl 1.9.2. zmq 25.1.0. zoneinfo NA. -----. IPython 8.14.0. jupyter_client 8.2.0. jupyter_core 5.3.1. notebook 6.5.4. -----. Python 3.11.4 | packaged by conda-forge | (main, Jun 10 2023, 18:08:17) [GCC 12.2.0]. Linux-4.18.0-425.19.2.el8_7.x86_64-x86_64-with-glibc2.27. -----. Session information updated at 2023-07-06 03:56. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:650,testability,Trace,Traceback,650,"highly_variable_genes gets a KeyError: '[nan] not in index'; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Running the highly_variable_genes function produced an error. ### Minimal code sample. ```python. sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[101], line 2. 1 # Identify highly-variable genes and plot. ----> 2 sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:440, in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values). 428 return _highly_variable_genes_seurat_v3(. 429 adata,. 430 layer=layer,. (...). 436 inplace=inplace,. 437 ). 439 if batch_key is None:. --> 440 df = _highly_variable_genes_single_batch(. 441 adata,. 442 layer=layer,. 443 min_disp=min_disp,. 444 max_disp=max_disp,. 445 min_mean=min_mean,. 446 max_mean=max_mean,. 447 n_top_genes=n_top_genes,. 448 n_bins=n_bins,. 449 flavor=flavor,. 450 ). 451 else:. 452 sanitize_anndata(adata). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:223, in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 219 # retrieve those genes that have nan std, these are the ones where. 220 # only a single gene fell in the bin and implicitly set them to have. 221 # a normalized disperion of 1. 222 one_gene_per_bin = disp_std",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:2134,testability,log,logg,2134,"variable_genes_seurat_v3(. 429 adata,. 430 layer=layer,. (...). 436 inplace=inplace,. 437 ). 439 if batch_key is None:. --> 440 df = _highly_variable_genes_single_batch(. 441 adata,. 442 layer=layer,. 443 min_disp=min_disp,. 444 max_disp=max_disp,. 445 min_mean=min_mean,. 446 max_mean=max_mean,. 447 n_top_genes=n_top_genes,. 448 n_bins=n_bins,. 449 flavor=flavor,. 450 ). 451 else:. 452 sanitize_anndata(adata). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:223, in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 219 # retrieve those genes that have nan std, these are the ones where. 220 # only a single gene fell in the bin and implicitly set them to have. 221 # a normalized disperion of 1. 222 one_gene_per_bin = disp_std_bin.isnull(). --> 223 gen_indices = np.where(one_gene_per_bin[df['mean_bin'].values])[0].tolist(). 224 if len(gen_indices) > 0:. 225 logg.debug(. 226 f'Gene indices {gen_indices} fell into a single bin: their '. 227 'normalized dispersion was set to 1.\n '. 228 'Decreasing `n_bins` will likely avoid this effect.'. 229 ). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/series.py:1033, in Series.__getitem__(self, key). 1030 key = np.asarray(key, dtype=bool). 1031 return self._get_values(key). -> 1033 return self._get_with(key). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/series.py:1073, in Series._get_with(self, key). 1070 return self.iloc[key]. 1072 # handle the dup indexing case GH#4246. -> 1073 return self.loc[key]. File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/pandas/core/indexing.py:1103, in _LocationIndexer.__getitem__(self, key). 1100 axis = self.axis or 0. 1102 maybe_callable = com.apply_if_callable(key, self.obj). -> 1103 return self._getitem_axis(maybe_callable, axis=axis). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:189,usability,confirm,confirmed,189,"highly_variable_genes gets a KeyError: '[nan] not in index'; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Running the highly_variable_genes function produced an error. ### Minimal code sample. ```python. sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[101], line 2. 1 # Identify highly-variable genes and plot. ----> 2 sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:440, in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values). 428 return _highly_variable_genes_seurat_v3(. 429 adata,. 430 layer=layer,. (...). 436 inplace=inplace,. 437 ). 439 if batch_key is None:. --> 440 df = _highly_variable_genes_single_batch(. 441 adata,. 442 layer=layer,. 443 min_disp=min_disp,. 444 max_disp=max_disp,. 445 min_mean=min_mean,. 446 max_mean=max_mean,. 447 n_top_genes=n_top_genes,. 448 n_bins=n_bins,. 449 flavor=flavor,. 450 ). 451 else:. 452 sanitize_anndata(adata). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:223, in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 219 # retrieve those genes that have nan std, these are the ones where. 220 # only a single gene fell in the bin and implicitly set them to have. 221 # a normalized disperion of 1. 222 one_gene_per_bin = disp_std",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:272,usability,confirm,confirmed,272,"highly_variable_genes gets a KeyError: '[nan] not in index'; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Running the highly_variable_genes function produced an error. ### Minimal code sample. ```python. sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[101], line 2. 1 # Identify highly-variable genes and plot. ----> 2 sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:440, in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values). 428 return _highly_variable_genes_seurat_v3(. 429 adata,. 430 layer=layer,. (...). 436 inplace=inplace,. 437 ). 439 if batch_key is None:. --> 440 df = _highly_variable_genes_single_batch(. 441 adata,. 442 layer=layer,. 443 min_disp=min_disp,. 444 max_disp=max_disp,. 445 min_mean=min_mean,. 446 max_mean=max_mean,. 447 n_top_genes=n_top_genes,. 448 n_bins=n_bins,. 449 flavor=flavor,. 450 ). 451 else:. 452 sanitize_anndata(adata). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:223, in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 219 # retrieve those genes that have nan std, these are the ones where. 220 # only a single gene fell in the bin and implicitly set them to have. 221 # a normalized disperion of 1. 222 one_gene_per_bin = disp_std",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:404,usability,error,error,404,"highly_variable_genes gets a KeyError: '[nan] not in index'; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Running the highly_variable_genes function produced an error. ### Minimal code sample. ```python. sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[101], line 2. 1 # Identify highly-variable genes and plot. ----> 2 sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:440, in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values). 428 return _highly_variable_genes_seurat_v3(. 429 adata,. 430 layer=layer,. (...). 436 inplace=inplace,. 437 ). 439 if batch_key is None:. --> 440 df = _highly_variable_genes_single_batch(. 441 adata,. 442 layer=layer,. 443 min_disp=min_disp,. 444 max_disp=max_disp,. 445 min_mean=min_mean,. 446 max_mean=max_mean,. 447 n_top_genes=n_top_genes,. 448 n_bins=n_bins,. 449 flavor=flavor,. 450 ). 451 else:. 452 sanitize_anndata(adata). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:223, in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 219 # retrieve those genes that have nan std, these are the ones where. 220 # only a single gene fell in the bin and implicitly set them to have. 221 # a normalized disperion of 1. 222 one_gene_per_bin = disp_std",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:415,usability,Minim,Minimal,415,"highly_variable_genes gets a KeyError: '[nan] not in index'; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Running the highly_variable_genes function produced an error. ### Minimal code sample. ```python. sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[101], line 2. 1 # Identify highly-variable genes and plot. ----> 2 sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:440, in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values). 428 return _highly_variable_genes_seurat_v3(. 429 adata,. 430 layer=layer,. (...). 436 inplace=inplace,. 437 ). 439 if batch_key is None:. --> 440 df = _highly_variable_genes_single_batch(. 441 adata,. 442 layer=layer,. 443 min_disp=min_disp,. 444 max_disp=max_disp,. 445 min_mean=min_mean,. 446 max_mean=max_mean,. 447 n_top_genes=n_top_genes,. 448 n_bins=n_bins,. 449 flavor=flavor,. 450 ). 451 else:. 452 sanitize_anndata(adata). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:223, in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 219 # retrieve those genes that have nan std, these are the ones where. 220 # only a single gene fell in the bin and implicitly set them to have. 221 # a normalized disperion of 1. 222 one_gene_per_bin = disp_std",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:541,usability,Error,Error,541,"highly_variable_genes gets a KeyError: '[nan] not in index'; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Running the highly_variable_genes function produced an error. ### Minimal code sample. ```python. sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[101], line 2. 1 # Identify highly-variable genes and plot. ----> 2 sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:440, in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values). 428 return _highly_variable_genes_seurat_v3(. 429 adata,. 430 layer=layer,. (...). 436 inplace=inplace,. 437 ). 439 if batch_key is None:. --> 440 df = _highly_variable_genes_single_batch(. 441 adata,. 442 layer=layer,. 443 min_disp=min_disp,. 444 max_disp=max_disp,. 445 min_mean=min_mean,. 446 max_mean=max_mean,. 447 n_top_genes=n_top_genes,. 448 n_bins=n_bins,. 449 flavor=flavor,. 450 ). 451 else:. 452 sanitize_anndata(adata). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:223, in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 219 # retrieve those genes that have nan std, these are the ones where. 220 # only a single gene fell in the bin and implicitly set them to have. 221 # a normalized disperion of 1. 222 one_gene_per_bin = disp_std",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/issues/2547:6987,usability,tool,toolz,6987,"igraph 0.10.4. importlib_resources NA. ipykernel 6.23.1. ipython_genutils 0.2.0. ipywidgets 8.0.6. jax 0.4.12. jaxlib 0.4.12. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. leidenalg 0.9.1. lightning 2.0.3. lightning_cloud NA. lightning_fabric 2.0.3. lightning_utilities 0.8.0. llvmlite 0.40.0. louvain 0.8.0. matplotlib 3.7.1. matplotlib_inline 0.1.6. ml_collections NA. ml_dtypes 0.2.0. mpl_toolkits NA. mpmath 1.3.0. msgpack 1.0.5. mudata 0.2.3. multidict 6.0.4. multipart 0.0.6. multipledispatch 0.6.0. natsort 8.3.1. numba 0.57.0. numpy 1.24.3. numpyro 0.12.1. nvfuser NA. opt_einsum v3.3.0. optax 0.1.5. ordered_set 4.1.0. packaging 23.1. pandas 2.0.2. parso 0.8.3. patsy 0.5.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.3. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pycparser 2.21. pydantic 1.10.9. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pydot 1.4.2. pygments 2.15.1. pyparsing 3.0.9. pyro 1.8.5. pytorch_lightning 2.0.3. pytz 2023.3. requests 2.31.0. rich NA. scipy 1.10.1. scvi 1.0.0. seaborn 0.12.2. session_info 1.0.0. setuptools 67.7.2. six 1.16.0. sklearn 1.2.2. sniffio 1.3.0. socks 1.7.1. soupsieve 2.3.2.post1. sparse 0.14.0. sphinxcontrib NA. stack_data 0.6.2. starlette 0.22.0. statsmodels 0.14.0. sympy 1.12. texttable 1.6.7. threadpoolctl 3.1.0. toml 0.10.2. toolz 0.12.0. torch 2.0.1+cu117. torchmetrics 0.11.4. tornado 6.3.2. tqdm 4.65.0. traitlets 5.9.0. tree 0.1.8. typing_extensions NA. urllib3 2.0.3. uvicorn 0.22.0. wcwidth 0.2.6. websocket 1.5.3. websockets 11.0.3. wrapt 1.15.0. xarray 2023.5.0. yaml 6.0. yarl 1.9.2. zmq 25.1.0. zoneinfo NA. -----. IPython 8.14.0. jupyter_client 8.2.0. jupyter_core 5.3.1. notebook 6.5.4. -----. Python 3.11.4 | packaged by conda-forge | (main, Jun 10 2023, 18:08:17) [GCC 12.2.0]. Linux-4.18.0-425.19.2.el8_7.x86_64-x86_64-with-glibc2.27. -----. Session information updated at 2023-07-06 03:56. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547
https://github.com/scverse/scanpy/pull/2548:254,safety,review,review,254,Auto backport of pr 2546 on 1 9 x; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2548
https://github.com/scverse/scanpy/pull/2548:254,testability,review,review,254,Auto backport of pr 2546 on 1 9 x; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2548
https://github.com/scverse/scanpy/pull/2548:105,usability,guid,guidelines,105,Auto backport of pr 2546 on 1 9 x; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2548
https://github.com/scverse/scanpy/pull/2548:136,usability,guid,guide,136,Auto backport of pr 2546 on 1 9 x; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2548
https://github.com/scverse/scanpy/pull/2548:232,usability,workflow,workflow,232,Auto backport of pr 2546 on 1 9 x; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2548
https://github.com/scverse/scanpy/pull/2549:262,safety,review,review,262,Backport PR #2546: Fix getting log1p base; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2549
https://github.com/scverse/scanpy/pull/2549:262,testability,review,review,262,Backport PR #2546: Fix getting log1p base; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2549
https://github.com/scverse/scanpy/pull/2549:113,usability,guid,guidelines,113,Backport PR #2546: Fix getting log1p base; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2549
https://github.com/scverse/scanpy/pull/2549:144,usability,guid,guide,144,Backport PR #2546: Fix getting log1p base; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2549
https://github.com/scverse/scanpy/pull/2549:240,usability,workflow,workflow,240,Backport PR #2546: Fix getting log1p base; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2549
https://github.com/scverse/scanpy/issues/2550:448,deployability,log,logfoldchanges,448,"sc.pl.rank_genes_groups with filtering by condition; ### What kind of feature would you like to request? Additional function parameters . ### Please describe your wishes. Recently I ran `sc.tl.rank_genes_groups(adata_t, 'leiden_res1', method='wilcoxon', pts=True)` on my datasets, then I plot `sc.pl.rank_genes_groups(adata_t, n_genes=25, sharey=False, ncols=3, )`. . I found ribo genes rank top in some groups. Then I want to filtering results by logfoldchanges, pvals_adj, like Seurat's `FindAllMarkers` did, so I ran `sc.get.rank_genes_groups_df(adata_t, group=None, pval_cutoff=0.01, log2fc_min=1)`, and the ribo genes are filtered successfully. Can function `sc.pl.rank_genes_groups` add filtering params lilke pvals_adj, pct_nt_group, pct_nz_reference which are really useful to filter meaningless genes?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2550
https://github.com/scverse/scanpy/issues/2550:29,integrability,filter,filtering,29,"sc.pl.rank_genes_groups with filtering by condition; ### What kind of feature would you like to request? Additional function parameters . ### Please describe your wishes. Recently I ran `sc.tl.rank_genes_groups(adata_t, 'leiden_res1', method='wilcoxon', pts=True)` on my datasets, then I plot `sc.pl.rank_genes_groups(adata_t, n_genes=25, sharey=False, ncols=3, )`. . I found ribo genes rank top in some groups. Then I want to filtering results by logfoldchanges, pvals_adj, like Seurat's `FindAllMarkers` did, so I ran `sc.get.rank_genes_groups_df(adata_t, group=None, pval_cutoff=0.01, log2fc_min=1)`, and the ribo genes are filtered successfully. Can function `sc.pl.rank_genes_groups` add filtering params lilke pvals_adj, pct_nt_group, pct_nz_reference which are really useful to filter meaningless genes?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2550
https://github.com/scverse/scanpy/issues/2550:427,integrability,filter,filtering,427,"sc.pl.rank_genes_groups with filtering by condition; ### What kind of feature would you like to request? Additional function parameters . ### Please describe your wishes. Recently I ran `sc.tl.rank_genes_groups(adata_t, 'leiden_res1', method='wilcoxon', pts=True)` on my datasets, then I plot `sc.pl.rank_genes_groups(adata_t, n_genes=25, sharey=False, ncols=3, )`. . I found ribo genes rank top in some groups. Then I want to filtering results by logfoldchanges, pvals_adj, like Seurat's `FindAllMarkers` did, so I ran `sc.get.rank_genes_groups_df(adata_t, group=None, pval_cutoff=0.01, log2fc_min=1)`, and the ribo genes are filtered successfully. Can function `sc.pl.rank_genes_groups` add filtering params lilke pvals_adj, pct_nt_group, pct_nz_reference which are really useful to filter meaningless genes?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2550
https://github.com/scverse/scanpy/issues/2550:627,integrability,filter,filtered,627,"sc.pl.rank_genes_groups with filtering by condition; ### What kind of feature would you like to request? Additional function parameters . ### Please describe your wishes. Recently I ran `sc.tl.rank_genes_groups(adata_t, 'leiden_res1', method='wilcoxon', pts=True)` on my datasets, then I plot `sc.pl.rank_genes_groups(adata_t, n_genes=25, sharey=False, ncols=3, )`. . I found ribo genes rank top in some groups. Then I want to filtering results by logfoldchanges, pvals_adj, like Seurat's `FindAllMarkers` did, so I ran `sc.get.rank_genes_groups_df(adata_t, group=None, pval_cutoff=0.01, log2fc_min=1)`, and the ribo genes are filtered successfully. Can function `sc.pl.rank_genes_groups` add filtering params lilke pvals_adj, pct_nt_group, pct_nz_reference which are really useful to filter meaningless genes?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2550
https://github.com/scverse/scanpy/issues/2550:693,integrability,filter,filtering,693,"sc.pl.rank_genes_groups with filtering by condition; ### What kind of feature would you like to request? Additional function parameters . ### Please describe your wishes. Recently I ran `sc.tl.rank_genes_groups(adata_t, 'leiden_res1', method='wilcoxon', pts=True)` on my datasets, then I plot `sc.pl.rank_genes_groups(adata_t, n_genes=25, sharey=False, ncols=3, )`. . I found ribo genes rank top in some groups. Then I want to filtering results by logfoldchanges, pvals_adj, like Seurat's `FindAllMarkers` did, so I ran `sc.get.rank_genes_groups_df(adata_t, group=None, pval_cutoff=0.01, log2fc_min=1)`, and the ribo genes are filtered successfully. Can function `sc.pl.rank_genes_groups` add filtering params lilke pvals_adj, pct_nt_group, pct_nz_reference which are really useful to filter meaningless genes?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2550
https://github.com/scverse/scanpy/issues/2550:785,integrability,filter,filter,785,"sc.pl.rank_genes_groups with filtering by condition; ### What kind of feature would you like to request? Additional function parameters . ### Please describe your wishes. Recently I ran `sc.tl.rank_genes_groups(adata_t, 'leiden_res1', method='wilcoxon', pts=True)` on my datasets, then I plot `sc.pl.rank_genes_groups(adata_t, n_genes=25, sharey=False, ncols=3, )`. . I found ribo genes rank top in some groups. Then I want to filtering results by logfoldchanges, pvals_adj, like Seurat's `FindAllMarkers` did, so I ran `sc.get.rank_genes_groups_df(adata_t, group=None, pval_cutoff=0.01, log2fc_min=1)`, and the ribo genes are filtered successfully. Can function `sc.pl.rank_genes_groups` add filtering params lilke pvals_adj, pct_nt_group, pct_nz_reference which are really useful to filter meaningless genes?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2550
https://github.com/scverse/scanpy/issues/2550:339,interoperability,share,sharey,339,"sc.pl.rank_genes_groups with filtering by condition; ### What kind of feature would you like to request? Additional function parameters . ### Please describe your wishes. Recently I ran `sc.tl.rank_genes_groups(adata_t, 'leiden_res1', method='wilcoxon', pts=True)` on my datasets, then I plot `sc.pl.rank_genes_groups(adata_t, n_genes=25, sharey=False, ncols=3, )`. . I found ribo genes rank top in some groups. Then I want to filtering results by logfoldchanges, pvals_adj, like Seurat's `FindAllMarkers` did, so I ran `sc.get.rank_genes_groups_df(adata_t, group=None, pval_cutoff=0.01, log2fc_min=1)`, and the ribo genes are filtered successfully. Can function `sc.pl.rank_genes_groups` add filtering params lilke pvals_adj, pct_nt_group, pct_nz_reference which are really useful to filter meaningless genes?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2550
https://github.com/scverse/scanpy/issues/2550:125,modifiability,paramet,parameters,125,"sc.pl.rank_genes_groups with filtering by condition; ### What kind of feature would you like to request? Additional function parameters . ### Please describe your wishes. Recently I ran `sc.tl.rank_genes_groups(adata_t, 'leiden_res1', method='wilcoxon', pts=True)` on my datasets, then I plot `sc.pl.rank_genes_groups(adata_t, n_genes=25, sharey=False, ncols=3, )`. . I found ribo genes rank top in some groups. Then I want to filtering results by logfoldchanges, pvals_adj, like Seurat's `FindAllMarkers` did, so I ran `sc.get.rank_genes_groups_df(adata_t, group=None, pval_cutoff=0.01, log2fc_min=1)`, and the ribo genes are filtered successfully. Can function `sc.pl.rank_genes_groups` add filtering params lilke pvals_adj, pct_nt_group, pct_nz_reference which are really useful to filter meaningless genes?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2550
https://github.com/scverse/scanpy/issues/2550:448,safety,log,logfoldchanges,448,"sc.pl.rank_genes_groups with filtering by condition; ### What kind of feature would you like to request? Additional function parameters . ### Please describe your wishes. Recently I ran `sc.tl.rank_genes_groups(adata_t, 'leiden_res1', method='wilcoxon', pts=True)` on my datasets, then I plot `sc.pl.rank_genes_groups(adata_t, n_genes=25, sharey=False, ncols=3, )`. . I found ribo genes rank top in some groups. Then I want to filtering results by logfoldchanges, pvals_adj, like Seurat's `FindAllMarkers` did, so I ran `sc.get.rank_genes_groups_df(adata_t, group=None, pval_cutoff=0.01, log2fc_min=1)`, and the ribo genes are filtered successfully. Can function `sc.pl.rank_genes_groups` add filtering params lilke pvals_adj, pct_nt_group, pct_nz_reference which are really useful to filter meaningless genes?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2550
https://github.com/scverse/scanpy/issues/2550:448,security,log,logfoldchanges,448,"sc.pl.rank_genes_groups with filtering by condition; ### What kind of feature would you like to request? Additional function parameters . ### Please describe your wishes. Recently I ran `sc.tl.rank_genes_groups(adata_t, 'leiden_res1', method='wilcoxon', pts=True)` on my datasets, then I plot `sc.pl.rank_genes_groups(adata_t, n_genes=25, sharey=False, ncols=3, )`. . I found ribo genes rank top in some groups. Then I want to filtering results by logfoldchanges, pvals_adj, like Seurat's `FindAllMarkers` did, so I ran `sc.get.rank_genes_groups_df(adata_t, group=None, pval_cutoff=0.01, log2fc_min=1)`, and the ribo genes are filtered successfully. Can function `sc.pl.rank_genes_groups` add filtering params lilke pvals_adj, pct_nt_group, pct_nz_reference which are really useful to filter meaningless genes?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2550
https://github.com/scverse/scanpy/issues/2550:448,testability,log,logfoldchanges,448,"sc.pl.rank_genes_groups with filtering by condition; ### What kind of feature would you like to request? Additional function parameters . ### Please describe your wishes. Recently I ran `sc.tl.rank_genes_groups(adata_t, 'leiden_res1', method='wilcoxon', pts=True)` on my datasets, then I plot `sc.pl.rank_genes_groups(adata_t, n_genes=25, sharey=False, ncols=3, )`. . I found ribo genes rank top in some groups. Then I want to filtering results by logfoldchanges, pvals_adj, like Seurat's `FindAllMarkers` did, so I ran `sc.get.rank_genes_groups_df(adata_t, group=None, pval_cutoff=0.01, log2fc_min=1)`, and the ribo genes are filtered successfully. Can function `sc.pl.rank_genes_groups` add filtering params lilke pvals_adj, pct_nt_group, pct_nz_reference which are really useful to filter meaningless genes?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2550
https://github.com/scverse/scanpy/issues/2551:7,availability,error,error,7,"Memory error and Anndata error raised while using sc.read_h5ad; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hi there , I encountered a problem while using sc.read_h5ad to read the ""ReplogleWeissman2022_K562_essential.h5ad ""datasets . At first I supposed it is an AnnDataRead error: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. . But now I believe the main reason is MemoryError : Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32 . However, I am running this on a server with about 100 GB of memory , this problem really bothers me , could you please help me with that? Thank you very much ! ### Minimal code sample. ```python. import anndata. import pandas as pd. import scanpy as sc. annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). ```. ### Error output. ```pytb. MemoryError Traceback (most recent call last). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_rea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:25,availability,error,error,25,"Memory error and Anndata error raised while using sc.read_h5ad; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hi there , I encountered a problem while using sc.read_h5ad to read the ""ReplogleWeissman2022_K562_essential.h5ad ""datasets . At first I supposed it is an AnnDataRead error: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. . But now I believe the main reason is MemoryError : Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32 . However, I am running this on a server with about 100 GB of memory , this problem really bothers me , could you please help me with that? Thank you very much ! ### Minimal code sample. ```python. import anndata. import pandas as pd. import scanpy as sc. annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). ```. ### Error output. ```pytb. MemoryError Traceback (most recent call last). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_rea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:519,availability,error,error,519,"Memory error and Anndata error raised while using sc.read_h5ad; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hi there , I encountered a problem while using sc.read_h5ad to read the ""ReplogleWeissman2022_K562_essential.h5ad ""datasets . At first I supposed it is an AnnDataRead error: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. . But now I believe the main reason is MemoryError : Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32 . However, I am running this on a server with about 100 GB of memory , this problem really bothers me , could you please help me with that? Thank you very much ! ### Minimal code sample. ```python. import anndata. import pandas as pd. import scanpy as sc. annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). ```. ### Error output. ```pytb. MemoryError Traceback (most recent call last). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_rea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:532,availability,error,error,532,"Memory error and Anndata error raised while using sc.read_h5ad; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hi there , I encountered a problem while using sc.read_h5ad to read the ""ReplogleWeissman2022_K562_essential.h5ad ""datasets . At first I supposed it is an AnnDataRead error: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. . But now I believe the main reason is MemoryError : Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32 . However, I am running this on a server with about 100 GB of memory , this problem really bothers me , could you please help me with that? Thank you very much ! ### Minimal code sample. ```python. import anndata. import pandas as pd. import scanpy as sc. annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). ```. ### Error output. ```pytb. MemoryError Traceback (most recent call last). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_rea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:1094,availability,Error,Error,1094,"ions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hi there , I encountered a problem while using sc.read_h5ad to read the ""ReplogleWeissman2022_K562_essential.h5ad ""datasets . At first I supposed it is an AnnDataRead error: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. . But now I believe the main reason is MemoryError : Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32 . However, I am running this on a server with about 100 GB of memory , this problem really bothers me , could you please help me with that? Thank you very much ! ### Minimal code sample. ```python. import anndata. import pandas as pd. import scanpy as sc. annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). ```. ### Error output. ```pytb. MemoryError Traceback (most recent call last). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_read(ZarrArray, IOSpec(""array"", ""0.2.0"")). 321 @_REGISTRY.register_read(ZarrArray, IOSpec(""string-ar",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:6487,availability,error,error,6487,"ing up backwards compat in the anndata initializer. 226 # In most cases we should be able to call `func(elen[k])` instead. --> 227 k: read_dispatched(elem[k], callback). 228 for k in elem.keys(). 229 if not k.startswith(""raw.""). 230 }. 231 ). 232 elif elem_name.startswith(""/raw.""):. 233 return None. File D:\Python3.10.9\lib\site-packages\anndata\experimental\__init__.py:58, in read_dispatched(elem, callback). 54 from anndata._io.specs import Reader, _REGISTRY. 56 reader = Reader(_REGISTRY, callback=callback). ---> 58 return reader.read_elem(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:204, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 202 return func(*args, **kwargs). 203 except Exception as e:. --> 204 re_raise_error(e, elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:188, in report_read_key_on_error.<locals>.re_raise_error(e, elem). 186 else:. 187 parent = _get_parent(elem). --> 188 raise AnnDataReadError(. 189 f""Above error raised while reading key {elem.name!r} of "". 190 f""type {type(elem)} from {parent}."". 191 ) from e. AnnDataReadError: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. ```. ### Versions. <details>. ```. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asttokens NA. astunparse 1.6.3. backcall 0.2.0. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. google NA. h5py 3.8.0. ipykernel 6.22.0. ipython_genutils 0.2.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. llvmlite 0.40.1. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.3.0. natsort 8.4.0. ntsecuritycon NA. numba 0.57.1. numpy 1.23.5. opt_einsum v3.3.0. packaging 23.1. pandas 2.0.0. parso 0.8.3. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.2.0. prompt_toolkit 3.0.38. psutil 5.9.5. pure_eval 0.2.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:6617,availability,error,error,6617," k: read_dispatched(elem[k], callback). 228 for k in elem.keys(). 229 if not k.startswith(""raw.""). 230 }. 231 ). 232 elif elem_name.startswith(""/raw.""):. 233 return None. File D:\Python3.10.9\lib\site-packages\anndata\experimental\__init__.py:58, in read_dispatched(elem, callback). 54 from anndata._io.specs import Reader, _REGISTRY. 56 reader = Reader(_REGISTRY, callback=callback). ---> 58 return reader.read_elem(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:204, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 202 return func(*args, **kwargs). 203 except Exception as e:. --> 204 re_raise_error(e, elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:188, in report_read_key_on_error.<locals>.re_raise_error(e, elem). 186 else:. 187 parent = _get_parent(elem). --> 188 raise AnnDataReadError(. 189 f""Above error raised while reading key {elem.name!r} of "". 190 f""type {type(elem)} from {parent}."". 191 ) from e. AnnDataReadError: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. ```. ### Versions. <details>. ```. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asttokens NA. astunparse 1.6.3. backcall 0.2.0. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. google NA. h5py 3.8.0. ipykernel 6.22.0. ipython_genutils 0.2.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. llvmlite 0.40.1. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.3.0. natsort 8.4.0. ntsecuritycon NA. numba 0.57.1. numpy 1.23.5. opt_einsum v3.3.0. packaging 23.1. pandas 2.0.0. parso 0.8.3. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.2.0. prompt_toolkit 3.0.38. psutil 5.9.5. pure_eval 0.2.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.0.9. pythoncom NA. pytz 2023.3. pywin32_bootstrap NA. pywin32_system32 NA. pywin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:232,deployability,version,version,232,"Memory error and Anndata error raised while using sc.read_h5ad; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hi there , I encountered a problem while using sc.read_h5ad to read the ""ReplogleWeissman2022_K562_essential.h5ad ""datasets . At first I supposed it is an AnnDataRead error: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. . But now I believe the main reason is MemoryError : Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32 . However, I am running this on a server with about 100 GB of memory , this problem really bothers me , could you please help me with that? Thank you very much ! ### Minimal code sample. ```python. import anndata. import pandas as pd. import scanpy as sc. annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). ```. ### Error output. ```pytb. MemoryError Traceback (most recent call last). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_rea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:3284,deployability,version,version,3284,"_phil.wrapper(). File D:\Python3.10.9\lib\site-packages\h5py\_hl\dataset.py:768, in Dataset.__getitem__(self, args, new_dtype). 767 try:. --> 768 return self._fast_reader.read(args). 769 except TypeError:. File h5py\_selector.pyx:368, in h5py._selector.Reader.read(). File h5py\_selector.pyx:342, in h5py._selector.Reader.make_array(). MemoryError: Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32. The above exception was the direct cause of the following exception:. AnnDataReadError Traceback (most recent call last). Cell In[2], line 4. 2 import pandas as pd. 3 import scanpy as sc. ----> 4 annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:243, in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size). 240 return read_dataframe(elem). 241 return func(elem). --> 243 adata = read_dispatched(f, callback=callback). 245 # Backwards compat (should figure out which version). 246 if ""raw.X"" in f:. File D:\Python3.10.9\lib\site-packages\anndata\experimental\__init__.py:58, in read_dispatched(elem, callback). 54 from anndata._io.specs import Reader, _REGISTRY. 56 reader = Reader(_REGISTRY, callback=callback). ---> 58 return reader.read_elem(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:204, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 202 return func(*args, **kwargs). 203 except Exception as e:. --> 204 re_raise_error(e, elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:185, in report_read_key_on_error.<locals>.re_raise_error(e, elem). 183 def re_raise_error(e, elem):. 184 if isinstance(e, AnnDataReadError):. --> 185 raise e. 186 else:. 187 parent = _get_parent(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 200 break. 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. 204 re_ra",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:6713,deployability,Version,Versions,6713,". 230 }. 231 ). 232 elif elem_name.startswith(""/raw.""):. 233 return None. File D:\Python3.10.9\lib\site-packages\anndata\experimental\__init__.py:58, in read_dispatched(elem, callback). 54 from anndata._io.specs import Reader, _REGISTRY. 56 reader = Reader(_REGISTRY, callback=callback). ---> 58 return reader.read_elem(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:204, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 202 return func(*args, **kwargs). 203 except Exception as e:. --> 204 re_raise_error(e, elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:188, in report_read_key_on_error.<locals>.re_raise_error(e, elem). 186 else:. 187 parent = _get_parent(elem). --> 188 raise AnnDataReadError(. 189 f""Above error raised while reading key {elem.name!r} of "". 190 f""type {type(elem)} from {parent}."". 191 ) from e. AnnDataReadError: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. ```. ### Versions. <details>. ```. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asttokens NA. astunparse 1.6.3. backcall 0.2.0. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. google NA. h5py 3.8.0. ipykernel 6.22.0. ipython_genutils 0.2.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. llvmlite 0.40.1. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.3.0. natsort 8.4.0. ntsecuritycon NA. numba 0.57.1. numpy 1.23.5. opt_einsum v3.3.0. packaging 23.1. pandas 2.0.0. parso 0.8.3. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.2.0. prompt_toolkit 3.0.38. psutil 5.9.5. pure_eval 0.2.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.0.9. pythoncom NA. pytz 2023.3. pywin32_bootstrap NA. pywin32_system32 NA. pywintypes NA. scipy 1.10.1. session_info 1.0.0. setuptools 65.5.0. six 1.16.0. sklearn 1.2.1. stack_d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:682,energy efficiency,alloc,allocate,682,"Memory error and Anndata error raised while using sc.read_h5ad; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hi there , I encountered a problem while using sc.read_h5ad to read the ""ReplogleWeissman2022_K562_essential.h5ad ""datasets . At first I supposed it is an AnnDataRead error: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. . But now I believe the main reason is MemoryError : Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32 . However, I am running this on a server with about 100 GB of memory , this problem really bothers me , could you please help me with that? Thank you very much ! ### Minimal code sample. ```python. import anndata. import pandas as pd. import scanpy as sc. annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). ```. ### Error output. ```pytb. MemoryError Traceback (most recent call last). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_rea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:2647,energy efficiency,alloc,allocate,2647,"kages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_read(ZarrArray, IOSpec(""array"", ""0.2.0"")). 321 @_REGISTRY.register_read(ZarrArray, IOSpec(""string-array"", ""0.2.0"")). 322 def read_array(elem, _reader):. --> 323 return elem[()]. File h5py\_objects.pyx:54, in h5py._objects.with_phil.wrapper(). File h5py\_objects.pyx:55, in h5py._objects.with_phil.wrapper(). File D:\Python3.10.9\lib\site-packages\h5py\_hl\dataset.py:768, in Dataset.__getitem__(self, args, new_dtype). 767 try:. --> 768 return self._fast_reader.read(args). 769 except TypeError:. File h5py\_selector.pyx:368, in h5py._selector.Reader.read(). File h5py\_selector.pyx:342, in h5py._selector.Reader.make_array(). MemoryError: Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32. The above exception was the direct cause of the following exception:. AnnDataReadError Traceback (most recent call last). Cell In[2], line 4. 2 import pandas as pd. 3 import scanpy as sc. ----> 4 annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:243, in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size). 240 return read_dataframe(elem). 241 return func(elem). --> 243 adata = read_dispatched(f, callback=callback). 245 # Backwards compat (should figure out which version). 246 if ""raw.X"" in f:. File D:\Python3.10.9\lib\site-packages\anndata\experimental\__init__.py:58, in read_dispatched(elem, callback). 54 from anndata._io.specs import Reader, _REGISTRY. 56 reader = Reader(_REGISTRY, callback=callback). ---> 58 return reader.read_elem(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:204, in report_read_ke",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:7775,energy efficiency,cpu,cpu,7775,".func_wrapper(*args, **kwargs). 202 return func(*args, **kwargs). 203 except Exception as e:. --> 204 re_raise_error(e, elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:188, in report_read_key_on_error.<locals>.re_raise_error(e, elem). 186 else:. 187 parent = _get_parent(elem). --> 188 raise AnnDataReadError(. 189 f""Above error raised while reading key {elem.name!r} of "". 190 f""type {type(elem)} from {parent}."". 191 ) from e. AnnDataReadError: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. ```. ### Versions. <details>. ```. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asttokens NA. astunparse 1.6.3. backcall 0.2.0. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. google NA. h5py 3.8.0. ipykernel 6.22.0. ipython_genutils 0.2.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. llvmlite 0.40.1. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.3.0. natsort 8.4.0. ntsecuritycon NA. numba 0.57.1. numpy 1.23.5. opt_einsum v3.3.0. packaging 23.1. pandas 2.0.0. parso 0.8.3. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.2.0. prompt_toolkit 3.0.38. psutil 5.9.5. pure_eval 0.2.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.0.9. pythoncom NA. pytz 2023.3. pywin32_bootstrap NA. pywin32_system32 NA. pywintypes NA. scipy 1.10.1. session_info 1.0.0. setuptools 65.5.0. six 1.16.0. sklearn 1.2.1. stack_data 0.6.2. sympy 1.11.1. threadpoolctl 3.1.0. torch 2.0.0+cpu. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. wcwidth 0.2.6. win32api NA. win32com NA. win32security NA. yaml 6.0. zmq 25.0.2. -----. IPython 8.12.0. jupyter_client 8.2.0. jupyter_core 5.3.0. notebook 6.5.4. -----. Python 3.10.9 (tags/v3.10.9:1dd9be6, Dec 6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]. Windows-10-10.0.19045-SP0. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:232,integrability,version,version,232,"Memory error and Anndata error raised while using sc.read_h5ad; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hi there , I encountered a problem while using sc.read_h5ad to read the ""ReplogleWeissman2022_K562_essential.h5ad ""datasets . At first I supposed it is an AnnDataRead error: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. . But now I believe the main reason is MemoryError : Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32 . However, I am running this on a server with about 100 GB of memory , this problem really bothers me , could you please help me with that? Thank you very much ! ### Minimal code sample. ```python. import anndata. import pandas as pd. import scanpy as sc. annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). ```. ### Error output. ```pytb. MemoryError Traceback (most recent call last). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_rea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:2229,integrability,wrap,wrapper,2229,"eport_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_read(ZarrArray, IOSpec(""array"", ""0.2.0"")). 321 @_REGISTRY.register_read(ZarrArray, IOSpec(""string-array"", ""0.2.0"")). 322 def read_array(elem, _reader):. --> 323 return elem[()]. File h5py\_objects.pyx:54, in h5py._objects.with_phil.wrapper(). File h5py\_objects.pyx:55, in h5py._objects.with_phil.wrapper(). File D:\Python3.10.9\lib\site-packages\h5py\_hl\dataset.py:768, in Dataset.__getitem__(self, args, new_dtype). 767 try:. --> 768 return self._fast_reader.read(args). 769 except TypeError:. File h5py\_selector.pyx:368, in h5py._selector.Reader.read(). File h5py\_selector.pyx:342, in h5py._selector.Reader.make_array(). MemoryError: Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32. The above exception was the direct cause of the following exception:. AnnDataReadError Traceback (most recent call last). Cell In[2], line 4. 2 import pandas as pd. 3 import scanpy as sc. ----> 4 annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:243, in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size). 240 return read_dataframe(elem). 241 return func(elem). --> 243 adata = read_dispatched(f, callback=callback",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:2294,integrability,wrap,wrapper,2294,"01 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_read(ZarrArray, IOSpec(""array"", ""0.2.0"")). 321 @_REGISTRY.register_read(ZarrArray, IOSpec(""string-array"", ""0.2.0"")). 322 def read_array(elem, _reader):. --> 323 return elem[()]. File h5py\_objects.pyx:54, in h5py._objects.with_phil.wrapper(). File h5py\_objects.pyx:55, in h5py._objects.with_phil.wrapper(). File D:\Python3.10.9\lib\site-packages\h5py\_hl\dataset.py:768, in Dataset.__getitem__(self, args, new_dtype). 767 try:. --> 768 return self._fast_reader.read(args). 769 except TypeError:. File h5py\_selector.pyx:368, in h5py._selector.Reader.read(). File h5py\_selector.pyx:342, in h5py._selector.Reader.make_array(). MemoryError: Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32. The above exception was the direct cause of the following exception:. AnnDataReadError Traceback (most recent call last). Cell In[2], line 4. 2 import pandas as pd. 3 import scanpy as sc. ----> 4 annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:243, in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size). 240 return read_dataframe(elem). 241 return func(elem). --> 243 adata = read_dispatched(f, callback=callback). 245 # Backwards compat (should figure out which version). 246 ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:3284,integrability,version,version,3284,"_phil.wrapper(). File D:\Python3.10.9\lib\site-packages\h5py\_hl\dataset.py:768, in Dataset.__getitem__(self, args, new_dtype). 767 try:. --> 768 return self._fast_reader.read(args). 769 except TypeError:. File h5py\_selector.pyx:368, in h5py._selector.Reader.read(). File h5py\_selector.pyx:342, in h5py._selector.Reader.make_array(). MemoryError: Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32. The above exception was the direct cause of the following exception:. AnnDataReadError Traceback (most recent call last). Cell In[2], line 4. 2 import pandas as pd. 3 import scanpy as sc. ----> 4 annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:243, in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size). 240 return read_dataframe(elem). 241 return func(elem). --> 243 adata = read_dispatched(f, callback=callback). 245 # Backwards compat (should figure out which version). 246 if ""raw.X"" in f:. File D:\Python3.10.9\lib\site-packages\anndata\experimental\__init__.py:58, in read_dispatched(elem, callback). 54 from anndata._io.specs import Reader, _REGISTRY. 56 reader = Reader(_REGISTRY, callback=callback). ---> 58 return reader.read_elem(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:204, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 202 return func(*args, **kwargs). 203 except Exception as e:. --> 204 re_raise_error(e, elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:185, in report_read_key_on_error.<locals>.re_raise_error(e, elem). 183 def re_raise_error(e, elem):. 184 if isinstance(e, AnnDataReadError):. --> 185 raise e. 186 else:. 187 parent = _get_parent(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 200 break. 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. 204 re_ra",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:6713,integrability,Version,Versions,6713,". 230 }. 231 ). 232 elif elem_name.startswith(""/raw.""):. 233 return None. File D:\Python3.10.9\lib\site-packages\anndata\experimental\__init__.py:58, in read_dispatched(elem, callback). 54 from anndata._io.specs import Reader, _REGISTRY. 56 reader = Reader(_REGISTRY, callback=callback). ---> 58 return reader.read_elem(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:204, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 202 return func(*args, **kwargs). 203 except Exception as e:. --> 204 re_raise_error(e, elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:188, in report_read_key_on_error.<locals>.re_raise_error(e, elem). 186 else:. 187 parent = _get_parent(elem). --> 188 raise AnnDataReadError(. 189 f""Above error raised while reading key {elem.name!r} of "". 190 f""type {type(elem)} from {parent}."". 191 ) from e. AnnDataReadError: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. ```. ### Versions. <details>. ```. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asttokens NA. astunparse 1.6.3. backcall 0.2.0. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. google NA. h5py 3.8.0. ipykernel 6.22.0. ipython_genutils 0.2.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. llvmlite 0.40.1. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.3.0. natsort 8.4.0. ntsecuritycon NA. numba 0.57.1. numpy 1.23.5. opt_einsum v3.3.0. packaging 23.1. pandas 2.0.0. parso 0.8.3. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.2.0. prompt_toolkit 3.0.38. psutil 5.9.5. pure_eval 0.2.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.0.9. pythoncom NA. pytz 2023.3. pywin32_bootstrap NA. pywin32_system32 NA. pywintypes NA. scipy 1.10.1. session_info 1.0.0. setuptools 65.5.0. six 1.16.0. sklearn 1.2.1. stack_d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:1430,interoperability,registr,registry,1430,"eissman2022_K562_essential.h5ad ""datasets . At first I supposed it is an AnnDataRead error: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. . But now I believe the main reason is MemoryError : Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32 . However, I am running this on a server with about 100 GB of memory , this problem really bothers me , could you please help me with that? Thank you very much ! ### Minimal code sample. ```python. import anndata. import pandas as pd. import scanpy as sc. annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). ```. ### Error output. ```pytb. MemoryError Traceback (most recent call last). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_read(ZarrArray, IOSpec(""array"", ""0.2.0"")). 321 @_REGISTRY.register_read(ZarrArray, IOSpec(""string-array"", ""0.2.0"")). 322 def read_array(elem, _reader):. --> 323 return elem[()]. File h5py\_objects.pyx:54, in h5py._objects.with_phil.wrapper(). File h5py\_objects.pyx:55, in h5py._objects.with_phil.wrapper(). File D:\Python3.10.9\lib\site-packages\h5py\_hl\dataset.py:768, in Dataset.__getitem__(self, args, new_dtype). 767 try:. --> 768 ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:2229,interoperability,wrapper,wrapper,2229,"eport_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_read(ZarrArray, IOSpec(""array"", ""0.2.0"")). 321 @_REGISTRY.register_read(ZarrArray, IOSpec(""string-array"", ""0.2.0"")). 322 def read_array(elem, _reader):. --> 323 return elem[()]. File h5py\_objects.pyx:54, in h5py._objects.with_phil.wrapper(). File h5py\_objects.pyx:55, in h5py._objects.with_phil.wrapper(). File D:\Python3.10.9\lib\site-packages\h5py\_hl\dataset.py:768, in Dataset.__getitem__(self, args, new_dtype). 767 try:. --> 768 return self._fast_reader.read(args). 769 except TypeError:. File h5py\_selector.pyx:368, in h5py._selector.Reader.read(). File h5py\_selector.pyx:342, in h5py._selector.Reader.make_array(). MemoryError: Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32. The above exception was the direct cause of the following exception:. AnnDataReadError Traceback (most recent call last). Cell In[2], line 4. 2 import pandas as pd. 3 import scanpy as sc. ----> 4 annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:243, in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size). 240 return read_dataframe(elem). 241 return func(elem). --> 243 adata = read_dispatched(f, callback=callback",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:2294,interoperability,wrapper,wrapper,2294,"01 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_read(ZarrArray, IOSpec(""array"", ""0.2.0"")). 321 @_REGISTRY.register_read(ZarrArray, IOSpec(""string-array"", ""0.2.0"")). 322 def read_array(elem, _reader):. --> 323 return elem[()]. File h5py\_objects.pyx:54, in h5py._objects.with_phil.wrapper(). File h5py\_objects.pyx:55, in h5py._objects.with_phil.wrapper(). File D:\Python3.10.9\lib\site-packages\h5py\_hl\dataset.py:768, in Dataset.__getitem__(self, args, new_dtype). 767 try:. --> 768 return self._fast_reader.read(args). 769 except TypeError:. File h5py\_selector.pyx:368, in h5py._selector.Reader.read(). File h5py\_selector.pyx:342, in h5py._selector.Reader.make_array(). MemoryError: Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32. The above exception was the direct cause of the following exception:. AnnDataReadError Traceback (most recent call last). Cell In[2], line 4. 2 import pandas as pd. 3 import scanpy as sc. ----> 4 annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:243, in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size). 240 return read_dataframe(elem). 241 return func(elem). --> 243 adata = read_dispatched(f, callback=callback). 245 # Backwards compat (should figure out which version). 246 ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:4365,interoperability,registr,registry,4365,"mental\__init__.py:58, in read_dispatched(elem, callback). 54 from anndata._io.specs import Reader, _REGISTRY. 56 reader = Reader(_REGISTRY, callback=callback). ---> 58 return reader.read_elem(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:204, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 202 return func(*args, **kwargs). 203 except Exception as e:. --> 204 re_raise_error(e, elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:185, in report_read_key_on_error.<locals>.re_raise_error(e, elem). 183 def re_raise_error(e, elem):. 184 if isinstance(e, AnnDataReadError):. --> 185 raise e. 186 else:. 187 parent = _get_parent(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 200 break. 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. 204 re_raise_error(e, elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 233 read_func = partial(read_func, _reader=self). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. 237 return read_func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:224, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 221 def callback(func, elem_name: str, elem, iospec):. 222 if iospec.encoding_type == ""anndata"" or elem_name.endswith(""/""):. 223 return AnnData(. --> 224 **{. 225 # This is covering up backwards compat in the anndata initializer. 226 # In most cases we should be able to call `func(elen[k])` instead. 227 k: read_dispatched(elem[k], callback). 228 for k in elem.keys(). 229 if not k.startswith(""raw.""). 230 }. 231 ). 232 elif elem_name.startswith(""/raw.""):. 233 return None. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:227, in <dictcomp>(.0). 221 def callback(func, elem_name: str, elem, iospec):",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:7328,interoperability,platform,platformdirs,7328,".func_wrapper(*args, **kwargs). 202 return func(*args, **kwargs). 203 except Exception as e:. --> 204 re_raise_error(e, elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:188, in report_read_key_on_error.<locals>.re_raise_error(e, elem). 186 else:. 187 parent = _get_parent(elem). --> 188 raise AnnDataReadError(. 189 f""Above error raised while reading key {elem.name!r} of "". 190 f""type {type(elem)} from {parent}."". 191 ) from e. AnnDataReadError: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. ```. ### Versions. <details>. ```. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asttokens NA. astunparse 1.6.3. backcall 0.2.0. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. google NA. h5py 3.8.0. ipykernel 6.22.0. ipython_genutils 0.2.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. llvmlite 0.40.1. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.3.0. natsort 8.4.0. ntsecuritycon NA. numba 0.57.1. numpy 1.23.5. opt_einsum v3.3.0. packaging 23.1. pandas 2.0.0. parso 0.8.3. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.2.0. prompt_toolkit 3.0.38. psutil 5.9.5. pure_eval 0.2.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.0.9. pythoncom NA. pytz 2023.3. pywin32_bootstrap NA. pywin32_system32 NA. pywintypes NA. scipy 1.10.1. session_info 1.0.0. setuptools 65.5.0. six 1.16.0. sklearn 1.2.1. stack_data 0.6.2. sympy 1.11.1. threadpoolctl 3.1.0. torch 2.0.0+cpu. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. wcwidth 0.2.6. win32api NA. win32com NA. win32security NA. yaml 6.0. zmq 25.0.2. -----. IPython 8.12.0. jupyter_client 8.2.0. jupyter_core 5.3.0. notebook 6.5.4. -----. Python 3.10.9 (tags/v3.10.9:1dd9be6, Dec 6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]. Windows-10-10.0.19045-SP0. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:232,modifiability,version,version,232,"Memory error and Anndata error raised while using sc.read_h5ad; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hi there , I encountered a problem while using sc.read_h5ad to read the ""ReplogleWeissman2022_K562_essential.h5ad ""datasets . At first I supposed it is an AnnDataRead error: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. . But now I believe the main reason is MemoryError : Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32 . However, I am running this on a server with about 100 GB of memory , this problem really bothers me , could you please help me with that? Thank you very much ! ### Minimal code sample. ```python. import anndata. import pandas as pd. import scanpy as sc. annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). ```. ### Error output. ```pytb. MemoryError Traceback (most recent call last). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_rea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:1194,modifiability,pac,packages,1194,"med this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hi there , I encountered a problem while using sc.read_h5ad to read the ""ReplogleWeissman2022_K562_essential.h5ad ""datasets . At first I supposed it is an AnnDataRead error: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. . But now I believe the main reason is MemoryError : Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32 . However, I am running this on a server with about 100 GB of memory , this problem really bothers me , could you please help me with that? Thank you very much ! ### Minimal code sample. ```python. import anndata. import pandas as pd. import scanpy as sc. annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). ```. ### Error output. ```pytb. MemoryError Traceback (most recent call last). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_read(ZarrArray, IOSpec(""array"", ""0.2.0"")). 321 @_REGISTRY.register_read(ZarrArray, IOSpec(""string-array"", ""0.2.0"")). 322 def read_array(elem, _reader):. --> 323 return elem[()]. File h5py\_objects.pyx:",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:1403,modifiability,pac,packages,1403,"h5ad to read the ""ReplogleWeissman2022_K562_essential.h5ad ""datasets . At first I supposed it is an AnnDataRead error: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. . But now I believe the main reason is MemoryError : Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32 . However, I am running this on a server with about 100 GB of memory , this problem really bothers me , could you please help me with that? Thank you very much ! ### Minimal code sample. ```python. import anndata. import pandas as pd. import scanpy as sc. annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). ```. ### Error output. ```pytb. MemoryError Traceback (most recent call last). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_read(ZarrArray, IOSpec(""array"", ""0.2.0"")). 321 @_REGISTRY.register_read(ZarrArray, IOSpec(""string-array"", ""0.2.0"")). 322 def read_array(elem, _reader):. --> 323 return elem[()]. File h5py\_objects.pyx:54, in h5py._objects.with_phil.wrapper(). File h5py\_objects.pyx:55, in h5py._objects.with_phil.wrapper(). File D:\Python3.10.9\lib\site-packages\h5py\_hl\dataset.py:768, in Dataset.__getitem__(self, args, new",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:1648,modifiability,pac,packages,1648,"on is MemoryError : Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32 . However, I am running this on a server with about 100 GB of memory , this problem really bothers me , could you please help me with that? Thank you very much ! ### Minimal code sample. ```python. import anndata. import pandas as pd. import scanpy as sc. annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). ```. ### Error output. ```pytb. MemoryError Traceback (most recent call last). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_read(ZarrArray, IOSpec(""array"", ""0.2.0"")). 321 @_REGISTRY.register_read(ZarrArray, IOSpec(""string-array"", ""0.2.0"")). 322 def read_array(elem, _reader):. --> 323 return elem[()]. File h5py\_objects.pyx:54, in h5py._objects.with_phil.wrapper(). File h5py\_objects.pyx:55, in h5py._objects.with_phil.wrapper(). File D:\Python3.10.9\lib\site-packages\h5py\_hl\dataset.py:768, in Dataset.__getitem__(self, args, new_dtype). 767 try:. --> 768 return self._fast_reader.read(args). 769 except TypeError:. File h5py\_selector.pyx:368, in h5py._selector.Reader.read(). File h5py\_selector.pyx:342, in h5py._selector.Reader.make_array(). MemoryError: Unable to alloc",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:1835,modifiability,pac,packages,1835,"oblem really bothers me , could you please help me with that? Thank you very much ! ### Minimal code sample. ```python. import anndata. import pandas as pd. import scanpy as sc. annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). ```. ### Error output. ```pytb. MemoryError Traceback (most recent call last). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_read(ZarrArray, IOSpec(""array"", ""0.2.0"")). 321 @_REGISTRY.register_read(ZarrArray, IOSpec(""string-array"", ""0.2.0"")). 322 def read_array(elem, _reader):. --> 323 return elem[()]. File h5py\_objects.pyx:54, in h5py._objects.with_phil.wrapper(). File h5py\_objects.pyx:55, in h5py._objects.with_phil.wrapper(). File D:\Python3.10.9\lib\site-packages\h5py\_hl\dataset.py:768, in Dataset.__getitem__(self, args, new_dtype). 767 try:. --> 768 return self._fast_reader.read(args). 769 except TypeError:. File h5py\_selector.pyx:368, in h5py._selector.Reader.read(). File h5py\_selector.pyx:342, in h5py._selector.Reader.make_array(). MemoryError: Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32. The above exception was the direct cause of the following exception:. AnnDataReadError Traceback (most recent ca",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:2335,modifiability,pac,packages,2335,"rgs). 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_read(ZarrArray, IOSpec(""array"", ""0.2.0"")). 321 @_REGISTRY.register_read(ZarrArray, IOSpec(""string-array"", ""0.2.0"")). 322 def read_array(elem, _reader):. --> 323 return elem[()]. File h5py\_objects.pyx:54, in h5py._objects.with_phil.wrapper(). File h5py\_objects.pyx:55, in h5py._objects.with_phil.wrapper(). File D:\Python3.10.9\lib\site-packages\h5py\_hl\dataset.py:768, in Dataset.__getitem__(self, args, new_dtype). 767 try:. --> 768 return self._fast_reader.read(args). 769 except TypeError:. File h5py\_selector.pyx:368, in h5py._selector.Reader.read(). File h5py\_selector.pyx:342, in h5py._selector.Reader.make_array(). MemoryError: Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32. The above exception was the direct cause of the following exception:. AnnDataReadError Traceback (most recent call last). Cell In[2], line 4. 2 import pandas as pd. 3 import scanpy as sc. ----> 4 annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:243, in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size). 240 return read_dataframe(elem). 241 return func(elem). --> 243 adata = read_dispatched(f, callback=callback). 245 # Backwards compat (should figure out which version). 246 if ""raw.X"" in f:. File D:\Python3.10.9\li",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:3021,modifiability,pac,packages,3021,"y"", ""0.2.0"")). 321 @_REGISTRY.register_read(ZarrArray, IOSpec(""string-array"", ""0.2.0"")). 322 def read_array(elem, _reader):. --> 323 return elem[()]. File h5py\_objects.pyx:54, in h5py._objects.with_phil.wrapper(). File h5py\_objects.pyx:55, in h5py._objects.with_phil.wrapper(). File D:\Python3.10.9\lib\site-packages\h5py\_hl\dataset.py:768, in Dataset.__getitem__(self, args, new_dtype). 767 try:. --> 768 return self._fast_reader.read(args). 769 except TypeError:. File h5py\_selector.pyx:368, in h5py._selector.Reader.read(). File h5py\_selector.pyx:342, in h5py._selector.Reader.make_array(). MemoryError: Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32. The above exception was the direct cause of the following exception:. AnnDataReadError Traceback (most recent call last). Cell In[2], line 4. 2 import pandas as pd. 3 import scanpy as sc. ----> 4 annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:243, in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size). 240 return read_dataframe(elem). 241 return func(elem). --> 243 adata = read_dispatched(f, callback=callback). 245 # Backwards compat (should figure out which version). 246 if ""raw.X"" in f:. File D:\Python3.10.9\lib\site-packages\anndata\experimental\__init__.py:58, in read_dispatched(elem, callback). 54 from anndata._io.specs import Reader, _REGISTRY. 56 reader = Reader(_REGISTRY, callback=callback). ---> 58 return reader.read_elem(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:204, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 202 return func(*args, **kwargs). 203 except Exception as e:. --> 204 re_raise_error(e, elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:185, in report_read_key_on_error.<locals>.re_raise_error(e, elem). 183 def re_raise_error(e, elem):. 184 if isinstance(e, AnnDataReadError):. --> 185 raise e. 186 else:",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:3284,modifiability,version,version,3284,"_phil.wrapper(). File D:\Python3.10.9\lib\site-packages\h5py\_hl\dataset.py:768, in Dataset.__getitem__(self, args, new_dtype). 767 try:. --> 768 return self._fast_reader.read(args). 769 except TypeError:. File h5py\_selector.pyx:368, in h5py._selector.Reader.read(). File h5py\_selector.pyx:342, in h5py._selector.Reader.make_array(). MemoryError: Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32. The above exception was the direct cause of the following exception:. AnnDataReadError Traceback (most recent call last). Cell In[2], line 4. 2 import pandas as pd. 3 import scanpy as sc. ----> 4 annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:243, in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size). 240 return read_dataframe(elem). 241 return func(elem). --> 243 adata = read_dispatched(f, callback=callback). 245 # Backwards compat (should figure out which version). 246 if ""raw.X"" in f:. File D:\Python3.10.9\lib\site-packages\anndata\experimental\__init__.py:58, in read_dispatched(elem, callback). 54 from anndata._io.specs import Reader, _REGISTRY. 56 reader = Reader(_REGISTRY, callback=callback). ---> 58 return reader.read_elem(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:204, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 202 return func(*args, **kwargs). 203 except Exception as e:. --> 204 re_raise_error(e, elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:185, in report_read_key_on_error.<locals>.re_raise_error(e, elem). 183 def re_raise_error(e, elem):. 184 if isinstance(e, AnnDataReadError):. --> 185 raise e. 186 else:. 187 parent = _get_parent(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 200 break. 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. 204 re_ra",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:3346,modifiability,pac,packages,3346,"hl\dataset.py:768, in Dataset.__getitem__(self, args, new_dtype). 767 try:. --> 768 return self._fast_reader.read(args). 769 except TypeError:. File h5py\_selector.pyx:368, in h5py._selector.Reader.read(). File h5py\_selector.pyx:342, in h5py._selector.Reader.make_array(). MemoryError: Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32. The above exception was the direct cause of the following exception:. AnnDataReadError Traceback (most recent call last). Cell In[2], line 4. 2 import pandas as pd. 3 import scanpy as sc. ----> 4 annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:243, in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size). 240 return read_dataframe(elem). 241 return func(elem). --> 243 adata = read_dispatched(f, callback=callback). 245 # Backwards compat (should figure out which version). 246 if ""raw.X"" in f:. File D:\Python3.10.9\lib\site-packages\anndata\experimental\__init__.py:58, in read_dispatched(elem, callback). 54 from anndata._io.specs import Reader, _REGISTRY. 56 reader = Reader(_REGISTRY, callback=callback). ---> 58 return reader.read_elem(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:204, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 202 return func(*args, **kwargs). 203 except Exception as e:. --> 204 re_raise_error(e, elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:185, in report_read_key_on_error.<locals>.re_raise_error(e, elem). 183 def re_raise_error(e, elem):. 184 if isinstance(e, AnnDataReadError):. --> 185 raise e. 186 else:. 187 parent = _get_parent(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 200 break. 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. 204 re_raise_error(e, elem). File D:\Python3.10.9\lib\site-packages\ann",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:3599,modifiability,pac,packages,3599,"Reader.make_array(). MemoryError: Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32. The above exception was the direct cause of the following exception:. AnnDataReadError Traceback (most recent call last). Cell In[2], line 4. 2 import pandas as pd. 3 import scanpy as sc. ----> 4 annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:243, in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size). 240 return read_dataframe(elem). 241 return func(elem). --> 243 adata = read_dispatched(f, callback=callback). 245 # Backwards compat (should figure out which version). 246 if ""raw.X"" in f:. File D:\Python3.10.9\lib\site-packages\anndata\experimental\__init__.py:58, in read_dispatched(elem, callback). 54 from anndata._io.specs import Reader, _REGISTRY. 56 reader = Reader(_REGISTRY, callback=callback). ---> 58 return reader.read_elem(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:204, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 202 return func(*args, **kwargs). 203 except Exception as e:. --> 204 re_raise_error(e, elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:185, in report_read_key_on_error.<locals>.re_raise_error(e, elem). 183 def re_raise_error(e, elem):. 184 if isinstance(e, AnnDataReadError):. --> 185 raise e. 186 else:. 187 parent = _get_parent(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 200 break. 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. 204 re_raise_error(e, elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 233 read_func = partial(read_func, _reader=self). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:3827,modifiability,pac,packages,3827,"ecent call last). Cell In[2], line 4. 2 import pandas as pd. 3 import scanpy as sc. ----> 4 annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:243, in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size). 240 return read_dataframe(elem). 241 return func(elem). --> 243 adata = read_dispatched(f, callback=callback). 245 # Backwards compat (should figure out which version). 246 if ""raw.X"" in f:. File D:\Python3.10.9\lib\site-packages\anndata\experimental\__init__.py:58, in read_dispatched(elem, callback). 54 from anndata._io.specs import Reader, _REGISTRY. 56 reader = Reader(_REGISTRY, callback=callback). ---> 58 return reader.read_elem(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:204, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 202 return func(*args, **kwargs). 203 except Exception as e:. --> 204 re_raise_error(e, elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:185, in report_read_key_on_error.<locals>.re_raise_error(e, elem). 183 def re_raise_error(e, elem):. 184 if isinstance(e, AnnDataReadError):. --> 185 raise e. 186 else:. 187 parent = _get_parent(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 200 break. 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. 204 re_raise_error(e, elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 233 read_func = partial(read_func, _reader=self). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. 237 return read_func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:224, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 221 def callback(func, elem_name: str, elem, iospec):. 222 if iospec.enco",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:4089,modifiability,pac,packages,4089,"se, as_sparse_fmt, chunk_size). 240 return read_dataframe(elem). 241 return func(elem). --> 243 adata = read_dispatched(f, callback=callback). 245 # Backwards compat (should figure out which version). 246 if ""raw.X"" in f:. File D:\Python3.10.9\lib\site-packages\anndata\experimental\__init__.py:58, in read_dispatched(elem, callback). 54 from anndata._io.specs import Reader, _REGISTRY. 56 reader = Reader(_REGISTRY, callback=callback). ---> 58 return reader.read_elem(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:204, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 202 return func(*args, **kwargs). 203 except Exception as e:. --> 204 re_raise_error(e, elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:185, in report_read_key_on_error.<locals>.re_raise_error(e, elem). 183 def re_raise_error(e, elem):. 184 if isinstance(e, AnnDataReadError):. --> 185 raise e. 186 else:. 187 parent = _get_parent(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 200 break. 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. 204 re_raise_error(e, elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 233 read_func = partial(read_func, _reader=self). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. 237 return read_func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:224, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 221 def callback(func, elem_name: str, elem, iospec):. 222 if iospec.encoding_type == ""anndata"" or elem_name.endswith(""/""):. 223 return AnnData(. --> 224 **{. 225 # This is covering up backwards compat in the anndata initializer. 226 # In most cases we should be able to call `func(elen[k])` instead. 227 k: read_dispatched(elem[k], ca",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:4338,modifiability,pac,packages,4338,"ite-packages\anndata\experimental\__init__.py:58, in read_dispatched(elem, callback). 54 from anndata._io.specs import Reader, _REGISTRY. 56 reader = Reader(_REGISTRY, callback=callback). ---> 58 return reader.read_elem(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:204, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 202 return func(*args, **kwargs). 203 except Exception as e:. --> 204 re_raise_error(e, elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:185, in report_read_key_on_error.<locals>.re_raise_error(e, elem). 183 def re_raise_error(e, elem):. 184 if isinstance(e, AnnDataReadError):. --> 185 raise e. 186 else:. 187 parent = _get_parent(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 200 break. 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. 204 re_raise_error(e, elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 233 read_func = partial(read_func, _reader=self). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. 237 return read_func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:224, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 221 def callback(func, elem_name: str, elem, iospec):. 222 if iospec.encoding_type == ""anndata"" or elem_name.endswith(""/""):. 223 return AnnData(. --> 224 **{. 225 # This is covering up backwards compat in the anndata initializer. 226 # In most cases we should be able to call `func(elen[k])` instead. 227 k: read_dispatched(elem[k], callback). 228 for k in elem.keys(). 229 if not k.startswith(""raw.""). 230 }. 231 ). 232 elif elem_name.startswith(""/raw.""):. 233 return None. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:227, in <dictcomp>(.0). 221 def callback(func, ele",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:4661,modifiability,pac,packages,4661,"cals>.func_wrapper(*args, **kwargs). 202 return func(*args, **kwargs). 203 except Exception as e:. --> 204 re_raise_error(e, elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:185, in report_read_key_on_error.<locals>.re_raise_error(e, elem). 183 def re_raise_error(e, elem):. 184 if isinstance(e, AnnDataReadError):. --> 185 raise e. 186 else:. 187 parent = _get_parent(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 200 break. 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. 204 re_raise_error(e, elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 233 read_func = partial(read_func, _reader=self). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. 237 return read_func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:224, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 221 def callback(func, elem_name: str, elem, iospec):. 222 if iospec.encoding_type == ""anndata"" or elem_name.endswith(""/""):. 223 return AnnData(. --> 224 **{. 225 # This is covering up backwards compat in the anndata initializer. 226 # In most cases we should be able to call `func(elen[k])` instead. 227 k: read_dispatched(elem[k], callback). 228 for k in elem.keys(). 229 if not k.startswith(""raw.""). 230 }. 231 ). 232 elif elem_name.startswith(""/raw.""):. 233 return None. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:227, in <dictcomp>(.0). 221 def callback(func, elem_name: str, elem, iospec):. 222 if iospec.encoding_type == ""anndata"" or elem_name.endswith(""/""):. 223 return AnnData(. 224 **{. 225 # This is covering up backwards compat in the anndata initializer. 226 # In most cases we should be able to call `func(elen[k])` instead. --> 227 k: read_dispatched(elem[k], callback). 228 f",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:5263,modifiability,pac,packages,5263,"tion as e:. 204 re_raise_error(e, elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 233 read_func = partial(read_func, _reader=self). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. 237 return read_func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:224, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 221 def callback(func, elem_name: str, elem, iospec):. 222 if iospec.encoding_type == ""anndata"" or elem_name.endswith(""/""):. 223 return AnnData(. --> 224 **{. 225 # This is covering up backwards compat in the anndata initializer. 226 # In most cases we should be able to call `func(elen[k])` instead. 227 k: read_dispatched(elem[k], callback). 228 for k in elem.keys(). 229 if not k.startswith(""raw.""). 230 }. 231 ). 232 elif elem_name.startswith(""/raw.""):. 233 return None. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:227, in <dictcomp>(.0). 221 def callback(func, elem_name: str, elem, iospec):. 222 if iospec.encoding_type == ""anndata"" or elem_name.endswith(""/""):. 223 return AnnData(. 224 **{. 225 # This is covering up backwards compat in the anndata initializer. 226 # In most cases we should be able to call `func(elen[k])` instead. --> 227 k: read_dispatched(elem[k], callback). 228 for k in elem.keys(). 229 if not k.startswith(""raw.""). 230 }. 231 ). 232 elif elem_name.startswith(""/raw.""):. 233 return None. File D:\Python3.10.9\lib\site-packages\anndata\experimental\__init__.py:58, in read_dispatched(elem, callback). 54 from anndata._io.specs import Reader, _REGISTRY. 56 reader = Reader(_REGISTRY, callback=callback). ---> 58 return reader.read_elem(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:204, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 202 return func(*args, **kwargs). 203 except Exception as e:. --> 204 re_raise_error(e, el",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:5821,modifiability,pac,packages,5821,"c.encoding_type == ""anndata"" or elem_name.endswith(""/""):. 223 return AnnData(. --> 224 **{. 225 # This is covering up backwards compat in the anndata initializer. 226 # In most cases we should be able to call `func(elen[k])` instead. 227 k: read_dispatched(elem[k], callback). 228 for k in elem.keys(). 229 if not k.startswith(""raw.""). 230 }. 231 ). 232 elif elem_name.startswith(""/raw.""):. 233 return None. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:227, in <dictcomp>(.0). 221 def callback(func, elem_name: str, elem, iospec):. 222 if iospec.encoding_type == ""anndata"" or elem_name.endswith(""/""):. 223 return AnnData(. 224 **{. 225 # This is covering up backwards compat in the anndata initializer. 226 # In most cases we should be able to call `func(elen[k])` instead. --> 227 k: read_dispatched(elem[k], callback). 228 for k in elem.keys(). 229 if not k.startswith(""raw.""). 230 }. 231 ). 232 elif elem_name.startswith(""/raw.""):. 233 return None. File D:\Python3.10.9\lib\site-packages\anndata\experimental\__init__.py:58, in read_dispatched(elem, callback). 54 from anndata._io.specs import Reader, _REGISTRY. 56 reader = Reader(_REGISTRY, callback=callback). ---> 58 return reader.read_elem(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:204, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 202 return func(*args, **kwargs). 203 except Exception as e:. --> 204 re_raise_error(e, elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:188, in report_read_key_on_error.<locals>.re_raise_error(e, elem). 186 else:. 187 parent = _get_parent(elem). --> 188 raise AnnDataReadError(. 189 f""Above error raised while reading key {elem.name!r} of "". 190 f""type {type(elem)} from {parent}."". 191 ) from e. AnnDataReadError: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. ```. ### Versions. <details>. ```. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asttokens NA. astunparse 1.6.3. backcal",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:6074,modifiability,pac,packages,6074,"hed(elem[k], callback). 228 for k in elem.keys(). 229 if not k.startswith(""raw.""). 230 }. 231 ). 232 elif elem_name.startswith(""/raw.""):. 233 return None. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:227, in <dictcomp>(.0). 221 def callback(func, elem_name: str, elem, iospec):. 222 if iospec.encoding_type == ""anndata"" or elem_name.endswith(""/""):. 223 return AnnData(. 224 **{. 225 # This is covering up backwards compat in the anndata initializer. 226 # In most cases we should be able to call `func(elen[k])` instead. --> 227 k: read_dispatched(elem[k], callback). 228 for k in elem.keys(). 229 if not k.startswith(""raw.""). 230 }. 231 ). 232 elif elem_name.startswith(""/raw.""):. 233 return None. File D:\Python3.10.9\lib\site-packages\anndata\experimental\__init__.py:58, in read_dispatched(elem, callback). 54 from anndata._io.specs import Reader, _REGISTRY. 56 reader = Reader(_REGISTRY, callback=callback). ---> 58 return reader.read_elem(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:204, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 202 return func(*args, **kwargs). 203 except Exception as e:. --> 204 re_raise_error(e, elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:188, in report_read_key_on_error.<locals>.re_raise_error(e, elem). 186 else:. 187 parent = _get_parent(elem). --> 188 raise AnnDataReadError(. 189 f""Above error raised while reading key {elem.name!r} of "". 190 f""type {type(elem)} from {parent}."". 191 ) from e. AnnDataReadError: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. ```. ### Versions. <details>. ```. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asttokens NA. astunparse 1.6.3. backcall 0.2.0. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. google NA. h5py 3.8.0. ipykernel 6.22.0. ipython_genutils 0.2.0. jedi 0.18.2. joblib 1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:6302,modifiability,pac,packages,6302,"omp>(.0). 221 def callback(func, elem_name: str, elem, iospec):. 222 if iospec.encoding_type == ""anndata"" or elem_name.endswith(""/""):. 223 return AnnData(. 224 **{. 225 # This is covering up backwards compat in the anndata initializer. 226 # In most cases we should be able to call `func(elen[k])` instead. --> 227 k: read_dispatched(elem[k], callback). 228 for k in elem.keys(). 229 if not k.startswith(""raw.""). 230 }. 231 ). 232 elif elem_name.startswith(""/raw.""):. 233 return None. File D:\Python3.10.9\lib\site-packages\anndata\experimental\__init__.py:58, in read_dispatched(elem, callback). 54 from anndata._io.specs import Reader, _REGISTRY. 56 reader = Reader(_REGISTRY, callback=callback). ---> 58 return reader.read_elem(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:204, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 202 return func(*args, **kwargs). 203 except Exception as e:. --> 204 re_raise_error(e, elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:188, in report_read_key_on_error.<locals>.re_raise_error(e, elem). 186 else:. 187 parent = _get_parent(elem). --> 188 raise AnnDataReadError(. 189 f""Above error raised while reading key {elem.name!r} of "". 190 f""type {type(elem)} from {parent}."". 191 ) from e. AnnDataReadError: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. ```. ### Versions. <details>. ```. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asttokens NA. astunparse 1.6.3. backcall 0.2.0. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. google NA. h5py 3.8.0. ipykernel 6.22.0. ipython_genutils 0.2.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. llvmlite 0.40.1. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.3.0. natsort 8.4.0. ntsecuritycon NA. numba 0.57.1. numpy 1.23.5. opt_einsum v3.3.0. packaging 23.1. pandas 2.0.0. parso 0.8.3. pickleshare 0.7",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:6713,modifiability,Version,Versions,6713,". 230 }. 231 ). 232 elif elem_name.startswith(""/raw.""):. 233 return None. File D:\Python3.10.9\lib\site-packages\anndata\experimental\__init__.py:58, in read_dispatched(elem, callback). 54 from anndata._io.specs import Reader, _REGISTRY. 56 reader = Reader(_REGISTRY, callback=callback). ---> 58 return reader.read_elem(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:204, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 202 return func(*args, **kwargs). 203 except Exception as e:. --> 204 re_raise_error(e, elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:188, in report_read_key_on_error.<locals>.re_raise_error(e, elem). 186 else:. 187 parent = _get_parent(elem). --> 188 raise AnnDataReadError(. 189 f""Above error raised while reading key {elem.name!r} of "". 190 f""type {type(elem)} from {parent}."". 191 ) from e. AnnDataReadError: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. ```. ### Versions. <details>. ```. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asttokens NA. astunparse 1.6.3. backcall 0.2.0. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. google NA. h5py 3.8.0. ipykernel 6.22.0. ipython_genutils 0.2.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. llvmlite 0.40.1. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.3.0. natsort 8.4.0. ntsecuritycon NA. numba 0.57.1. numpy 1.23.5. opt_einsum v3.3.0. packaging 23.1. pandas 2.0.0. parso 0.8.3. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.2.0. prompt_toolkit 3.0.38. psutil 5.9.5. pure_eval 0.2.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.0.9. pythoncom NA. pytz 2023.3. pywin32_bootstrap NA. pywin32_system32 NA. pywintypes NA. scipy 1.10.1. session_info 1.0.0. setuptools 65.5.0. six 1.16.0. sklearn 1.2.1. stack_d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:6940,modifiability,deco,decorator,6940,"REGISTRY. 56 reader = Reader(_REGISTRY, callback=callback). ---> 58 return reader.read_elem(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:204, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 202 return func(*args, **kwargs). 203 except Exception as e:. --> 204 re_raise_error(e, elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:188, in report_read_key_on_error.<locals>.re_raise_error(e, elem). 186 else:. 187 parent = _get_parent(elem). --> 188 raise AnnDataReadError(. 189 f""Above error raised while reading key {elem.name!r} of "". 190 f""type {type(elem)} from {parent}."". 191 ) from e. AnnDataReadError: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. ```. ### Versions. <details>. ```. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asttokens NA. astunparse 1.6.3. backcall 0.2.0. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. google NA. h5py 3.8.0. ipykernel 6.22.0. ipython_genutils 0.2.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. llvmlite 0.40.1. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.3.0. natsort 8.4.0. ntsecuritycon NA. numba 0.57.1. numpy 1.23.5. opt_einsum v3.3.0. packaging 23.1. pandas 2.0.0. parso 0.8.3. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.2.0. prompt_toolkit 3.0.38. psutil 5.9.5. pure_eval 0.2.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.0.9. pythoncom NA. pytz 2023.3. pywin32_bootstrap NA. pywin32_system32 NA. pywintypes NA. scipy 1.10.1. session_info 1.0.0. setuptools 65.5.0. six 1.16.0. sklearn 1.2.1. stack_data 0.6.2. sympy 1.11.1. threadpoolctl 3.1.0. torch 2.0.0+cpu. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. wcwidth 0.2.6. win32api NA. win32com NA. win32security NA. yaml 6.0. zmq 25.0.2. -----. IPython 8.12",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:7248,modifiability,pac,packaging,7248,".func_wrapper(*args, **kwargs). 202 return func(*args, **kwargs). 203 except Exception as e:. --> 204 re_raise_error(e, elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:188, in report_read_key_on_error.<locals>.re_raise_error(e, elem). 186 else:. 187 parent = _get_parent(elem). --> 188 raise AnnDataReadError(. 189 f""Above error raised while reading key {elem.name!r} of "". 190 f""type {type(elem)} from {parent}."". 191 ) from e. AnnDataReadError: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. ```. ### Versions. <details>. ```. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asttokens NA. astunparse 1.6.3. backcall 0.2.0. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. google NA. h5py 3.8.0. ipykernel 6.22.0. ipython_genutils 0.2.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. llvmlite 0.40.1. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.3.0. natsort 8.4.0. ntsecuritycon NA. numba 0.57.1. numpy 1.23.5. opt_einsum v3.3.0. packaging 23.1. pandas 2.0.0. parso 0.8.3. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.2.0. prompt_toolkit 3.0.38. psutil 5.9.5. pure_eval 0.2.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.0.9. pythoncom NA. pytz 2023.3. pywin32_bootstrap NA. pywin32_system32 NA. pywintypes NA. scipy 1.10.1. session_info 1.0.0. setuptools 65.5.0. six 1.16.0. sklearn 1.2.1. stack_data 0.6.2. sympy 1.11.1. threadpoolctl 3.1.0. torch 2.0.0+cpu. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. wcwidth 0.2.6. win32api NA. win32com NA. win32security NA. yaml 6.0. zmq 25.0.2. -----. IPython 8.12.0. jupyter_client 8.2.0. jupyter_core 5.3.0. notebook 6.5.4. -----. Python 3.10.9 (tags/v3.10.9:1dd9be6, Dec 6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]. Windows-10-10.0.19045-SP0. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:0,performance,Memor,Memory,0,"Memory error and Anndata error raised while using sc.read_h5ad; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hi there , I encountered a problem while using sc.read_h5ad to read the ""ReplogleWeissman2022_K562_essential.h5ad ""datasets . At first I supposed it is an AnnDataRead error: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. . But now I believe the main reason is MemoryError : Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32 . However, I am running this on a server with about 100 GB of memory , this problem really bothers me , could you please help me with that? Thank you very much ! ### Minimal code sample. ```python. import anndata. import pandas as pd. import scanpy as sc. annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). ```. ### Error output. ```pytb. MemoryError Traceback (most recent call last). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_rea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:7,performance,error,error,7,"Memory error and Anndata error raised while using sc.read_h5ad; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hi there , I encountered a problem while using sc.read_h5ad to read the ""ReplogleWeissman2022_K562_essential.h5ad ""datasets . At first I supposed it is an AnnDataRead error: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. . But now I believe the main reason is MemoryError : Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32 . However, I am running this on a server with about 100 GB of memory , this problem really bothers me , could you please help me with that? Thank you very much ! ### Minimal code sample. ```python. import anndata. import pandas as pd. import scanpy as sc. annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). ```. ### Error output. ```pytb. MemoryError Traceback (most recent call last). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_rea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:25,performance,error,error,25,"Memory error and Anndata error raised while using sc.read_h5ad; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hi there , I encountered a problem while using sc.read_h5ad to read the ""ReplogleWeissman2022_K562_essential.h5ad ""datasets . At first I supposed it is an AnnDataRead error: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. . But now I believe the main reason is MemoryError : Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32 . However, I am running this on a server with about 100 GB of memory , this problem really bothers me , could you please help me with that? Thank you very much ! ### Minimal code sample. ```python. import anndata. import pandas as pd. import scanpy as sc. annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). ```. ### Error output. ```pytb. MemoryError Traceback (most recent call last). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_rea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:519,performance,error,error,519,"Memory error and Anndata error raised while using sc.read_h5ad; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hi there , I encountered a problem while using sc.read_h5ad to read the ""ReplogleWeissman2022_K562_essential.h5ad ""datasets . At first I supposed it is an AnnDataRead error: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. . But now I believe the main reason is MemoryError : Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32 . However, I am running this on a server with about 100 GB of memory , this problem really bothers me , could you please help me with that? Thank you very much ! ### Minimal code sample. ```python. import anndata. import pandas as pd. import scanpy as sc. annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). ```. ### Error output. ```pytb. MemoryError Traceback (most recent call last). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_rea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:532,performance,error,error,532,"Memory error and Anndata error raised while using sc.read_h5ad; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hi there , I encountered a problem while using sc.read_h5ad to read the ""ReplogleWeissman2022_K562_essential.h5ad ""datasets . At first I supposed it is an AnnDataRead error: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. . But now I believe the main reason is MemoryError : Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32 . However, I am running this on a server with about 100 GB of memory , this problem really bothers me , could you please help me with that? Thank you very much ! ### Minimal code sample. ```python. import anndata. import pandas as pd. import scanpy as sc. annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). ```. ### Error output. ```pytb. MemoryError Traceback (most recent call last). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_rea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:658,performance,Memor,MemoryError,658,"Memory error and Anndata error raised while using sc.read_h5ad; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hi there , I encountered a problem while using sc.read_h5ad to read the ""ReplogleWeissman2022_K562_essential.h5ad ""datasets . At first I supposed it is an AnnDataRead error: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. . But now I believe the main reason is MemoryError : Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32 . However, I am running this on a server with about 100 GB of memory , this problem really bothers me , could you please help me with that? Thank you very much ! ### Minimal code sample. ```python. import anndata. import pandas as pd. import scanpy as sc. annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). ```. ### Error output. ```pytb. MemoryError Traceback (most recent call last). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_rea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:823,performance,memor,memory,823,"Memory error and Anndata error raised while using sc.read_h5ad; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hi there , I encountered a problem while using sc.read_h5ad to read the ""ReplogleWeissman2022_K562_essential.h5ad ""datasets . At first I supposed it is an AnnDataRead error: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. . But now I believe the main reason is MemoryError : Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32 . However, I am running this on a server with about 100 GB of memory , this problem really bothers me , could you please help me with that? Thank you very much ! ### Minimal code sample. ```python. import anndata. import pandas as pd. import scanpy as sc. annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). ```. ### Error output. ```pytb. MemoryError Traceback (most recent call last). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_rea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:1094,performance,Error,Error,1094,"ions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hi there , I encountered a problem while using sc.read_h5ad to read the ""ReplogleWeissman2022_K562_essential.h5ad ""datasets . At first I supposed it is an AnnDataRead error: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. . But now I believe the main reason is MemoryError : Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32 . However, I am running this on a server with about 100 GB of memory , this problem really bothers me , could you please help me with that? Thank you very much ! ### Minimal code sample. ```python. import anndata. import pandas as pd. import scanpy as sc. annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). ```. ### Error output. ```pytb. MemoryError Traceback (most recent call last). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_read(ZarrArray, IOSpec(""array"", ""0.2.0"")). 321 @_REGISTRY.register_read(ZarrArray, IOSpec(""string-ar",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:1117,performance,Memor,MemoryError,1117," checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hi there , I encountered a problem while using sc.read_h5ad to read the ""ReplogleWeissman2022_K562_essential.h5ad ""datasets . At first I supposed it is an AnnDataRead error: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. . But now I believe the main reason is MemoryError : Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32 . However, I am running this on a server with about 100 GB of memory , this problem really bothers me , could you please help me with that? Thank you very much ! ### Minimal code sample. ```python. import anndata. import pandas as pd. import scanpy as sc. annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). ```. ### Error output. ```pytb. MemoryError Traceback (most recent call last). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_read(ZarrArray, IOSpec(""array"", ""0.2.0"")). 321 @_REGISTRY.register_read(ZarrArray, IOSpec(""string-array"", ""0.2.0"")). 322 def r",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:2624,performance,Memor,MemoryError,2624,"on3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_read(ZarrArray, IOSpec(""array"", ""0.2.0"")). 321 @_REGISTRY.register_read(ZarrArray, IOSpec(""string-array"", ""0.2.0"")). 322 def read_array(elem, _reader):. --> 323 return elem[()]. File h5py\_objects.pyx:54, in h5py._objects.with_phil.wrapper(). File h5py\_objects.pyx:55, in h5py._objects.with_phil.wrapper(). File D:\Python3.10.9\lib\site-packages\h5py\_hl\dataset.py:768, in Dataset.__getitem__(self, args, new_dtype). 767 try:. --> 768 return self._fast_reader.read(args). 769 except TypeError:. File h5py\_selector.pyx:368, in h5py._selector.Reader.read(). File h5py\_selector.pyx:342, in h5py._selector.Reader.make_array(). MemoryError: Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32. The above exception was the direct cause of the following exception:. AnnDataReadError Traceback (most recent call last). Cell In[2], line 4. 2 import pandas as pd. 3 import scanpy as sc. ----> 4 annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:243, in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size). 240 return read_dataframe(elem). 241 return func(elem). --> 243 adata = read_dispatched(f, callback=callback). 245 # Backwards compat (should figure out which version). 246 if ""raw.X"" in f:. File D:\Python3.10.9\lib\site-packages\anndata\experimental\__init__.py:58, in read_dispatched(elem, callback). 54 from anndata._io.specs import Reader, _REGISTRY. 56 reader = Reader(_REGISTRY, callback=callback). ---> 58 return reader.read_elem(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:2",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:6487,performance,error,error,6487,"ing up backwards compat in the anndata initializer. 226 # In most cases we should be able to call `func(elen[k])` instead. --> 227 k: read_dispatched(elem[k], callback). 228 for k in elem.keys(). 229 if not k.startswith(""raw.""). 230 }. 231 ). 232 elif elem_name.startswith(""/raw.""):. 233 return None. File D:\Python3.10.9\lib\site-packages\anndata\experimental\__init__.py:58, in read_dispatched(elem, callback). 54 from anndata._io.specs import Reader, _REGISTRY. 56 reader = Reader(_REGISTRY, callback=callback). ---> 58 return reader.read_elem(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:204, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 202 return func(*args, **kwargs). 203 except Exception as e:. --> 204 re_raise_error(e, elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:188, in report_read_key_on_error.<locals>.re_raise_error(e, elem). 186 else:. 187 parent = _get_parent(elem). --> 188 raise AnnDataReadError(. 189 f""Above error raised while reading key {elem.name!r} of "". 190 f""type {type(elem)} from {parent}."". 191 ) from e. AnnDataReadError: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. ```. ### Versions. <details>. ```. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asttokens NA. astunparse 1.6.3. backcall 0.2.0. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. google NA. h5py 3.8.0. ipykernel 6.22.0. ipython_genutils 0.2.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. llvmlite 0.40.1. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.3.0. natsort 8.4.0. ntsecuritycon NA. numba 0.57.1. numpy 1.23.5. opt_einsum v3.3.0. packaging 23.1. pandas 2.0.0. parso 0.8.3. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.2.0. prompt_toolkit 3.0.38. psutil 5.9.5. pure_eval 0.2.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:6617,performance,error,error,6617," k: read_dispatched(elem[k], callback). 228 for k in elem.keys(). 229 if not k.startswith(""raw.""). 230 }. 231 ). 232 elif elem_name.startswith(""/raw.""):. 233 return None. File D:\Python3.10.9\lib\site-packages\anndata\experimental\__init__.py:58, in read_dispatched(elem, callback). 54 from anndata._io.specs import Reader, _REGISTRY. 56 reader = Reader(_REGISTRY, callback=callback). ---> 58 return reader.read_elem(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:204, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 202 return func(*args, **kwargs). 203 except Exception as e:. --> 204 re_raise_error(e, elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:188, in report_read_key_on_error.<locals>.re_raise_error(e, elem). 186 else:. 187 parent = _get_parent(elem). --> 188 raise AnnDataReadError(. 189 f""Above error raised while reading key {elem.name!r} of "". 190 f""type {type(elem)} from {parent}."". 191 ) from e. AnnDataReadError: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. ```. ### Versions. <details>. ```. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asttokens NA. astunparse 1.6.3. backcall 0.2.0. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. google NA. h5py 3.8.0. ipykernel 6.22.0. ipython_genutils 0.2.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. llvmlite 0.40.1. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.3.0. natsort 8.4.0. ntsecuritycon NA. numba 0.57.1. numpy 1.23.5. opt_einsum v3.3.0. packaging 23.1. pandas 2.0.0. parso 0.8.3. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.2.0. prompt_toolkit 3.0.38. psutil 5.9.5. pure_eval 0.2.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.0.9. pythoncom NA. pytz 2023.3. pywin32_bootstrap NA. pywin32_system32 NA. pywin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:7775,performance,cpu,cpu,7775,".func_wrapper(*args, **kwargs). 202 return func(*args, **kwargs). 203 except Exception as e:. --> 204 re_raise_error(e, elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:188, in report_read_key_on_error.<locals>.re_raise_error(e, elem). 186 else:. 187 parent = _get_parent(elem). --> 188 raise AnnDataReadError(. 189 f""Above error raised while reading key {elem.name!r} of "". 190 f""type {type(elem)} from {parent}."". 191 ) from e. AnnDataReadError: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. ```. ### Versions. <details>. ```. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asttokens NA. astunparse 1.6.3. backcall 0.2.0. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. google NA. h5py 3.8.0. ipykernel 6.22.0. ipython_genutils 0.2.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. llvmlite 0.40.1. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.3.0. natsort 8.4.0. ntsecuritycon NA. numba 0.57.1. numpy 1.23.5. opt_einsum v3.3.0. packaging 23.1. pandas 2.0.0. parso 0.8.3. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.2.0. prompt_toolkit 3.0.38. psutil 5.9.5. pure_eval 0.2.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.0.9. pythoncom NA. pytz 2023.3. pywin32_bootstrap NA. pywin32_system32 NA. pywintypes NA. scipy 1.10.1. session_info 1.0.0. setuptools 65.5.0. six 1.16.0. sklearn 1.2.1. stack_data 0.6.2. sympy 1.11.1. threadpoolctl 3.1.0. torch 2.0.0+cpu. tornado 6.3. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. wcwidth 0.2.6. win32api NA. win32com NA. win32security NA. yaml 6.0. zmq 25.0.2. -----. IPython 8.12.0. jupyter_client 8.2.0. jupyter_core 5.3.0. notebook 6.5.4. -----. Python 3.10.9 (tags/v3.10.9:1dd9be6, Dec 6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]. Windows-10-10.0.19045-SP0. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:7,safety,error,error,7,"Memory error and Anndata error raised while using sc.read_h5ad; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hi there , I encountered a problem while using sc.read_h5ad to read the ""ReplogleWeissman2022_K562_essential.h5ad ""datasets . At first I supposed it is an AnnDataRead error: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. . But now I believe the main reason is MemoryError : Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32 . However, I am running this on a server with about 100 GB of memory , this problem really bothers me , could you please help me with that? Thank you very much ! ### Minimal code sample. ```python. import anndata. import pandas as pd. import scanpy as sc. annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). ```. ### Error output. ```pytb. MemoryError Traceback (most recent call last). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_rea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:25,safety,error,error,25,"Memory error and Anndata error raised while using sc.read_h5ad; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hi there , I encountered a problem while using sc.read_h5ad to read the ""ReplogleWeissman2022_K562_essential.h5ad ""datasets . At first I supposed it is an AnnDataRead error: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. . But now I believe the main reason is MemoryError : Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32 . However, I am running this on a server with about 100 GB of memory , this problem really bothers me , could you please help me with that? Thank you very much ! ### Minimal code sample. ```python. import anndata. import pandas as pd. import scanpy as sc. annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). ```. ### Error output. ```pytb. MemoryError Traceback (most recent call last). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_rea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:519,safety,error,error,519,"Memory error and Anndata error raised while using sc.read_h5ad; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hi there , I encountered a problem while using sc.read_h5ad to read the ""ReplogleWeissman2022_K562_essential.h5ad ""datasets . At first I supposed it is an AnnDataRead error: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. . But now I believe the main reason is MemoryError : Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32 . However, I am running this on a server with about 100 GB of memory , this problem really bothers me , could you please help me with that? Thank you very much ! ### Minimal code sample. ```python. import anndata. import pandas as pd. import scanpy as sc. annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). ```. ### Error output. ```pytb. MemoryError Traceback (most recent call last). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_rea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:532,safety,error,error,532,"Memory error and Anndata error raised while using sc.read_h5ad; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hi there , I encountered a problem while using sc.read_h5ad to read the ""ReplogleWeissman2022_K562_essential.h5ad ""datasets . At first I supposed it is an AnnDataRead error: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. . But now I believe the main reason is MemoryError : Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32 . However, I am running this on a server with about 100 GB of memory , this problem really bothers me , could you please help me with that? Thank you very much ! ### Minimal code sample. ```python. import anndata. import pandas as pd. import scanpy as sc. annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). ```. ### Error output. ```pytb. MemoryError Traceback (most recent call last). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_rea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:1094,safety,Error,Error,1094,"ions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hi there , I encountered a problem while using sc.read_h5ad to read the ""ReplogleWeissman2022_K562_essential.h5ad ""datasets . At first I supposed it is an AnnDataRead error: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. . But now I believe the main reason is MemoryError : Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32 . However, I am running this on a server with about 100 GB of memory , this problem really bothers me , could you please help me with that? Thank you very much ! ### Minimal code sample. ```python. import anndata. import pandas as pd. import scanpy as sc. annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). ```. ### Error output. ```pytb. MemoryError Traceback (most recent call last). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_read(ZarrArray, IOSpec(""array"", ""0.2.0"")). 321 @_REGISTRY.register_read(ZarrArray, IOSpec(""string-ar",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:1349,safety,except,except,1349,"Hi there , I encountered a problem while using sc.read_h5ad to read the ""ReplogleWeissman2022_K562_essential.h5ad ""datasets . At first I supposed it is an AnnDataRead error: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. . But now I believe the main reason is MemoryError : Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32 . However, I am running this on a server with about 100 GB of memory , this problem really bothers me , could you please help me with that? Thank you very much ! ### Minimal code sample. ```python. import anndata. import pandas as pd. import scanpy as sc. annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). ```. ### Error output. ```pytb. MemoryError Traceback (most recent call last). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_read(ZarrArray, IOSpec(""array"", ""0.2.0"")). 321 @_REGISTRY.register_read(ZarrArray, IOSpec(""string-array"", ""0.2.0"")). 322 def read_array(elem, _reader):. --> 323 return elem[()]. File h5py\_objects.pyx:54, in h5py._objects.with_phil.wrapper(). File h5py\_objects.pyx:55, in h5py._objects.with_phil.wrapper(). File D:\Python3.10.9\lib\site-packages\h5py\_hl",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:1356,safety,Except,Exception,1356,", I encountered a problem while using sc.read_h5ad to read the ""ReplogleWeissman2022_K562_essential.h5ad ""datasets . At first I supposed it is an AnnDataRead error: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. . But now I believe the main reason is MemoryError : Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32 . However, I am running this on a server with about 100 GB of memory , this problem really bothers me , could you please help me with that? Thank you very much ! ### Minimal code sample. ```python. import anndata. import pandas as pd. import scanpy as sc. annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). ```. ### Error output. ```pytb. MemoryError Traceback (most recent call last). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_read(ZarrArray, IOSpec(""array"", ""0.2.0"")). 321 @_REGISTRY.register_read(ZarrArray, IOSpec(""string-array"", ""0.2.0"")). 322 def read_array(elem, _reader):. --> 323 return elem[()]. File h5py\_objects.pyx:54, in h5py._objects.with_phil.wrapper(). File h5py\_objects.pyx:55, in h5py._objects.with_phil.wrapper(). File D:\Python3.10.9\lib\site-packages\h5py\_hl\dataset.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:2475,safety,except,except,2475," modifiers). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_read(ZarrArray, IOSpec(""array"", ""0.2.0"")). 321 @_REGISTRY.register_read(ZarrArray, IOSpec(""string-array"", ""0.2.0"")). 322 def read_array(elem, _reader):. --> 323 return elem[()]. File h5py\_objects.pyx:54, in h5py._objects.with_phil.wrapper(). File h5py\_objects.pyx:55, in h5py._objects.with_phil.wrapper(). File D:\Python3.10.9\lib\site-packages\h5py\_hl\dataset.py:768, in Dataset.__getitem__(self, args, new_dtype). 767 try:. --> 768 return self._fast_reader.read(args). 769 except TypeError:. File h5py\_selector.pyx:368, in h5py._selector.Reader.read(). File h5py\_selector.pyx:342, in h5py._selector.Reader.make_array(). MemoryError: Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32. The above exception was the direct cause of the following exception:. AnnDataReadError Traceback (most recent call last). Cell In[2], line 4. 2 import pandas as pd. 3 import scanpy as sc. ----> 4 annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:243, in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size). 240 return read_dataframe(elem). 241 return func(elem). --> 243 adata = read_dispatched(f, callback=callback). 245 # Backwards compat (should figure out which version). 246 if ""raw.X"" in f:. File D:\Python3.10.9\lib\site-packages\anndata\experimental\__init__.py:58, in read_dispatched(elem, callback). 54 from anndata._io.specs import Reader, _REGISTRY",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:2737,safety,except,exception,2737,"). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_read(ZarrArray, IOSpec(""array"", ""0.2.0"")). 321 @_REGISTRY.register_read(ZarrArray, IOSpec(""string-array"", ""0.2.0"")). 322 def read_array(elem, _reader):. --> 323 return elem[()]. File h5py\_objects.pyx:54, in h5py._objects.with_phil.wrapper(). File h5py\_objects.pyx:55, in h5py._objects.with_phil.wrapper(). File D:\Python3.10.9\lib\site-packages\h5py\_hl\dataset.py:768, in Dataset.__getitem__(self, args, new_dtype). 767 try:. --> 768 return self._fast_reader.read(args). 769 except TypeError:. File h5py\_selector.pyx:368, in h5py._selector.Reader.read(). File h5py\_selector.pyx:342, in h5py._selector.Reader.make_array(). MemoryError: Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32. The above exception was the direct cause of the following exception:. AnnDataReadError Traceback (most recent call last). Cell In[2], line 4. 2 import pandas as pd. 3 import scanpy as sc. ----> 4 annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:243, in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size). 240 return read_dataframe(elem). 241 return func(elem). --> 243 adata = read_dispatched(f, callback=callback). 245 # Backwards compat (should figure out which version). 246 if ""raw.X"" in f:. File D:\Python3.10.9\lib\site-packages\anndata\experimental\__init__.py:58, in read_dispatched(elem, callback). 54 from anndata._io.specs import Reader, _REGISTRY. 56 reader = Reader(_REGISTRY, callback=callback). ---> 58 return reader.read_elem(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:204, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 202 return func(*args, **kwargs). 203 ex",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:2785,safety,except,exception,2785,"rn func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_read(ZarrArray, IOSpec(""array"", ""0.2.0"")). 321 @_REGISTRY.register_read(ZarrArray, IOSpec(""string-array"", ""0.2.0"")). 322 def read_array(elem, _reader):. --> 323 return elem[()]. File h5py\_objects.pyx:54, in h5py._objects.with_phil.wrapper(). File h5py\_objects.pyx:55, in h5py._objects.with_phil.wrapper(). File D:\Python3.10.9\lib\site-packages\h5py\_hl\dataset.py:768, in Dataset.__getitem__(self, args, new_dtype). 767 try:. --> 768 return self._fast_reader.read(args). 769 except TypeError:. File h5py\_selector.pyx:368, in h5py._selector.Reader.read(). File h5py\_selector.pyx:342, in h5py._selector.Reader.make_array(). MemoryError: Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32. The above exception was the direct cause of the following exception:. AnnDataReadError Traceback (most recent call last). Cell In[2], line 4. 2 import pandas as pd. 3 import scanpy as sc. ----> 4 annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:243, in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size). 240 return read_dataframe(elem). 241 return func(elem). --> 243 adata = read_dispatched(f, callback=callback). 245 # Backwards compat (should figure out which version). 246 if ""raw.X"" in f:. File D:\Python3.10.9\lib\site-packages\anndata\experimental\__init__.py:58, in read_dispatched(elem, callback). 54 from anndata._io.specs import Reader, _REGISTRY. 56 reader = Reader(_REGISTRY, callback=callback). ---> 58 return reader.read_elem(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:204, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 202 return func(*args, **kwargs). 203 except Exception as e:. --> 204 re_raise_error(e, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:3740,safety,except,except,3740,"ion was the direct cause of the following exception:. AnnDataReadError Traceback (most recent call last). Cell In[2], line 4. 2 import pandas as pd. 3 import scanpy as sc. ----> 4 annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:243, in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size). 240 return read_dataframe(elem). 241 return func(elem). --> 243 adata = read_dispatched(f, callback=callback). 245 # Backwards compat (should figure out which version). 246 if ""raw.X"" in f:. File D:\Python3.10.9\lib\site-packages\anndata\experimental\__init__.py:58, in read_dispatched(elem, callback). 54 from anndata._io.specs import Reader, _REGISTRY. 56 reader = Reader(_REGISTRY, callback=callback). ---> 58 return reader.read_elem(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:204, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 202 return func(*args, **kwargs). 203 except Exception as e:. --> 204 re_raise_error(e, elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:185, in report_read_key_on_error.<locals>.re_raise_error(e, elem). 183 def re_raise_error(e, elem):. 184 if isinstance(e, AnnDataReadError):. --> 185 raise e. 186 else:. 187 parent = _get_parent(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 200 break. 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. 204 re_raise_error(e, elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 233 read_func = partial(read_func, _reader=self). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. 237 return read_func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:224, in read_h5ad.<locals>.callback(func, elem_name, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:3747,safety,Except,Exception,3747,"he direct cause of the following exception:. AnnDataReadError Traceback (most recent call last). Cell In[2], line 4. 2 import pandas as pd. 3 import scanpy as sc. ----> 4 annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:243, in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size). 240 return read_dataframe(elem). 241 return func(elem). --> 243 adata = read_dispatched(f, callback=callback). 245 # Backwards compat (should figure out which version). 246 if ""raw.X"" in f:. File D:\Python3.10.9\lib\site-packages\anndata\experimental\__init__.py:58, in read_dispatched(elem, callback). 54 from anndata._io.specs import Reader, _REGISTRY. 56 reader = Reader(_REGISTRY, callback=callback). ---> 58 return reader.read_elem(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:204, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 202 return func(*args, **kwargs). 203 except Exception as e:. --> 204 re_raise_error(e, elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:185, in report_read_key_on_error.<locals>.re_raise_error(e, elem). 183 def re_raise_error(e, elem):. 184 if isinstance(e, AnnDataReadError):. --> 185 raise e. 186 else:. 187 parent = _get_parent(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 200 break. 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. 204 re_raise_error(e, elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 233 read_func = partial(read_func, _reader=self). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. 237 return read_func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:224, in read_h5ad.<locals>.callback(func, elem_name, elem, ios",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:4255,safety,except,except,4255," (should figure out which version). 246 if ""raw.X"" in f:. File D:\Python3.10.9\lib\site-packages\anndata\experimental\__init__.py:58, in read_dispatched(elem, callback). 54 from anndata._io.specs import Reader, _REGISTRY. 56 reader = Reader(_REGISTRY, callback=callback). ---> 58 return reader.read_elem(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:204, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 202 return func(*args, **kwargs). 203 except Exception as e:. --> 204 re_raise_error(e, elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:185, in report_read_key_on_error.<locals>.re_raise_error(e, elem). 183 def re_raise_error(e, elem):. 184 if isinstance(e, AnnDataReadError):. --> 185 raise e. 186 else:. 187 parent = _get_parent(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 200 break. 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. 204 re_raise_error(e, elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 233 read_func = partial(read_func, _reader=self). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. 237 return read_func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:224, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 221 def callback(func, elem_name: str, elem, iospec):. 222 if iospec.encoding_type == ""anndata"" or elem_name.endswith(""/""):. 223 return AnnData(. --> 224 **{. 225 # This is covering up backwards compat in the anndata initializer. 226 # In most cases we should be able to call `func(elen[k])` instead. 227 k: read_dispatched(elem[k], callback). 228 for k in elem.keys(). 229 if not k.startswith(""raw.""). 230 }. 231 ). 232 elif elem_name.startswith(""/raw.""):. 233 return None. File D:\Python3.10.9\lib\",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:4262,safety,Except,Exception,4262,"figure out which version). 246 if ""raw.X"" in f:. File D:\Python3.10.9\lib\site-packages\anndata\experimental\__init__.py:58, in read_dispatched(elem, callback). 54 from anndata._io.specs import Reader, _REGISTRY. 56 reader = Reader(_REGISTRY, callback=callback). ---> 58 return reader.read_elem(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:204, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 202 return func(*args, **kwargs). 203 except Exception as e:. --> 204 re_raise_error(e, elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:185, in report_read_key_on_error.<locals>.re_raise_error(e, elem). 183 def re_raise_error(e, elem):. 184 if isinstance(e, AnnDataReadError):. --> 185 raise e. 186 else:. 187 parent = _get_parent(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 200 break. 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. 204 re_raise_error(e, elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 233 read_func = partial(read_func, _reader=self). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. 237 return read_func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:224, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 221 def callback(func, elem_name: str, elem, iospec):. 222 if iospec.encoding_type == ""anndata"" or elem_name.endswith(""/""):. 223 return AnnData(. --> 224 **{. 225 # This is covering up backwards compat in the anndata initializer. 226 # In most cases we should be able to call `func(elen[k])` instead. 227 k: read_dispatched(elem[k], callback). 228 for k in elem.keys(). 229 if not k.startswith(""raw.""). 230 }. 231 ). 232 elif elem_name.startswith(""/raw.""):. 233 return None. File D:\Python3.10.9\lib\site-pack",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:6215,safety,except,except,6215,"3 return None. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:227, in <dictcomp>(.0). 221 def callback(func, elem_name: str, elem, iospec):. 222 if iospec.encoding_type == ""anndata"" or elem_name.endswith(""/""):. 223 return AnnData(. 224 **{. 225 # This is covering up backwards compat in the anndata initializer. 226 # In most cases we should be able to call `func(elen[k])` instead. --> 227 k: read_dispatched(elem[k], callback). 228 for k in elem.keys(). 229 if not k.startswith(""raw.""). 230 }. 231 ). 232 elif elem_name.startswith(""/raw.""):. 233 return None. File D:\Python3.10.9\lib\site-packages\anndata\experimental\__init__.py:58, in read_dispatched(elem, callback). 54 from anndata._io.specs import Reader, _REGISTRY. 56 reader = Reader(_REGISTRY, callback=callback). ---> 58 return reader.read_elem(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:204, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 202 return func(*args, **kwargs). 203 except Exception as e:. --> 204 re_raise_error(e, elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:188, in report_read_key_on_error.<locals>.re_raise_error(e, elem). 186 else:. 187 parent = _get_parent(elem). --> 188 raise AnnDataReadError(. 189 f""Above error raised while reading key {elem.name!r} of "". 190 f""type {type(elem)} from {parent}."". 191 ) from e. AnnDataReadError: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. ```. ### Versions. <details>. ```. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asttokens NA. astunparse 1.6.3. backcall 0.2.0. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. google NA. h5py 3.8.0. ipykernel 6.22.0. ipython_genutils 0.2.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. llvmlite 0.40.1. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.3.0. natsort 8.4.0. ntsecuritycon NA. numba 0.57.1. num",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:6222,safety,Except,Exception,6222,"None. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:227, in <dictcomp>(.0). 221 def callback(func, elem_name: str, elem, iospec):. 222 if iospec.encoding_type == ""anndata"" or elem_name.endswith(""/""):. 223 return AnnData(. 224 **{. 225 # This is covering up backwards compat in the anndata initializer. 226 # In most cases we should be able to call `func(elen[k])` instead. --> 227 k: read_dispatched(elem[k], callback). 228 for k in elem.keys(). 229 if not k.startswith(""raw.""). 230 }. 231 ). 232 elif elem_name.startswith(""/raw.""):. 233 return None. File D:\Python3.10.9\lib\site-packages\anndata\experimental\__init__.py:58, in read_dispatched(elem, callback). 54 from anndata._io.specs import Reader, _REGISTRY. 56 reader = Reader(_REGISTRY, callback=callback). ---> 58 return reader.read_elem(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:204, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 202 return func(*args, **kwargs). 203 except Exception as e:. --> 204 re_raise_error(e, elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:188, in report_read_key_on_error.<locals>.re_raise_error(e, elem). 186 else:. 187 parent = _get_parent(elem). --> 188 raise AnnDataReadError(. 189 f""Above error raised while reading key {elem.name!r} of "". 190 f""type {type(elem)} from {parent}."". 191 ) from e. AnnDataReadError: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. ```. ### Versions. <details>. ```. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asttokens NA. astunparse 1.6.3. backcall 0.2.0. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. google NA. h5py 3.8.0. ipykernel 6.22.0. ipython_genutils 0.2.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. llvmlite 0.40.1. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.3.0. natsort 8.4.0. ntsecuritycon NA. numba 0.57.1. numpy 1.23.5",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:6487,safety,error,error,6487,"ing up backwards compat in the anndata initializer. 226 # In most cases we should be able to call `func(elen[k])` instead. --> 227 k: read_dispatched(elem[k], callback). 228 for k in elem.keys(). 229 if not k.startswith(""raw.""). 230 }. 231 ). 232 elif elem_name.startswith(""/raw.""):. 233 return None. File D:\Python3.10.9\lib\site-packages\anndata\experimental\__init__.py:58, in read_dispatched(elem, callback). 54 from anndata._io.specs import Reader, _REGISTRY. 56 reader = Reader(_REGISTRY, callback=callback). ---> 58 return reader.read_elem(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:204, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 202 return func(*args, **kwargs). 203 except Exception as e:. --> 204 re_raise_error(e, elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:188, in report_read_key_on_error.<locals>.re_raise_error(e, elem). 186 else:. 187 parent = _get_parent(elem). --> 188 raise AnnDataReadError(. 189 f""Above error raised while reading key {elem.name!r} of "". 190 f""type {type(elem)} from {parent}."". 191 ) from e. AnnDataReadError: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. ```. ### Versions. <details>. ```. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asttokens NA. astunparse 1.6.3. backcall 0.2.0. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. google NA. h5py 3.8.0. ipykernel 6.22.0. ipython_genutils 0.2.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. llvmlite 0.40.1. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.3.0. natsort 8.4.0. ntsecuritycon NA. numba 0.57.1. numpy 1.23.5. opt_einsum v3.3.0. packaging 23.1. pandas 2.0.0. parso 0.8.3. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.2.0. prompt_toolkit 3.0.38. psutil 5.9.5. pure_eval 0.2.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:6617,safety,error,error,6617," k: read_dispatched(elem[k], callback). 228 for k in elem.keys(). 229 if not k.startswith(""raw.""). 230 }. 231 ). 232 elif elem_name.startswith(""/raw.""):. 233 return None. File D:\Python3.10.9\lib\site-packages\anndata\experimental\__init__.py:58, in read_dispatched(elem, callback). 54 from anndata._io.specs import Reader, _REGISTRY. 56 reader = Reader(_REGISTRY, callback=callback). ---> 58 return reader.read_elem(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:204, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 202 return func(*args, **kwargs). 203 except Exception as e:. --> 204 re_raise_error(e, elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:188, in report_read_key_on_error.<locals>.re_raise_error(e, elem). 186 else:. 187 parent = _get_parent(elem). --> 188 raise AnnDataReadError(. 189 f""Above error raised while reading key {elem.name!r} of "". 190 f""type {type(elem)} from {parent}."". 191 ) from e. AnnDataReadError: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. ```. ### Versions. <details>. ```. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asttokens NA. astunparse 1.6.3. backcall 0.2.0. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. google NA. h5py 3.8.0. ipykernel 6.22.0. ipython_genutils 0.2.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. llvmlite 0.40.1. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.3.0. natsort 8.4.0. ntsecuritycon NA. numba 0.57.1. numpy 1.23.5. opt_einsum v3.3.0. packaging 23.1. pandas 2.0.0. parso 0.8.3. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.2.0. prompt_toolkit 3.0.38. psutil 5.9.5. pure_eval 0.2.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.0.9. pythoncom NA. pytz 2023.3. pywin32_bootstrap NA. pywin32_system32 NA. pywin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:1479,security,modif,modifiers,1479,"st I supposed it is an AnnDataRead error: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. . But now I believe the main reason is MemoryError : Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32 . However, I am running this on a server with about 100 GB of memory , this problem really bothers me , could you please help me with that? Thank you very much ! ### Minimal code sample. ```python. import anndata. import pandas as pd. import scanpy as sc. annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). ```. ### Error output. ```pytb. MemoryError Traceback (most recent call last). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_read(ZarrArray, IOSpec(""array"", ""0.2.0"")). 321 @_REGISTRY.register_read(ZarrArray, IOSpec(""string-array"", ""0.2.0"")). 322 def read_array(elem, _reader):. --> 323 return elem[()]. File h5py\_objects.pyx:54, in h5py._objects.with_phil.wrapper(). File h5py\_objects.pyx:55, in h5py._objects.with_phil.wrapper(). File D:\Python3.10.9\lib\site-packages\h5py\_hl\dataset.py:768, in Dataset.__getitem__(self, args, new_dtype). 767 try:. --> 768 return self._fast_reader.read(args). 769 except Ty",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:4414,security,modif,modifiers,4414,"llback). 54 from anndata._io.specs import Reader, _REGISTRY. 56 reader = Reader(_REGISTRY, callback=callback). ---> 58 return reader.read_elem(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:204, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 202 return func(*args, **kwargs). 203 except Exception as e:. --> 204 re_raise_error(e, elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:185, in report_read_key_on_error.<locals>.re_raise_error(e, elem). 183 def re_raise_error(e, elem):. 184 if isinstance(e, AnnDataReadError):. --> 185 raise e. 186 else:. 187 parent = _get_parent(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 200 break. 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. 204 re_raise_error(e, elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 233 read_func = partial(read_func, _reader=self). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. 237 return read_func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:224, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 221 def callback(func, elem_name: str, elem, iospec):. 222 if iospec.encoding_type == ""anndata"" or elem_name.endswith(""/""):. 223 return AnnData(. --> 224 **{. 225 # This is covering up backwards compat in the anndata initializer. 226 # In most cases we should be able to call `func(elen[k])` instead. 227 k: read_dispatched(elem[k], callback). 228 for k in elem.keys(). 229 if not k.startswith(""raw.""). 230 }. 231 ). 232 elif elem_name.startswith(""/raw.""):. 233 return None. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:227, in <dictcomp>(.0). 221 def callback(func, elem_name: str, elem, iospec):. 222 if iospec.encoding_type == ""anndata"" or elem",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:1129,testability,Trace,Traceback,1129,"at this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hi there , I encountered a problem while using sc.read_h5ad to read the ""ReplogleWeissman2022_K562_essential.h5ad ""datasets . At first I supposed it is an AnnDataRead error: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. . But now I believe the main reason is MemoryError : Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32 . However, I am running this on a server with about 100 GB of memory , this problem really bothers me , could you please help me with that? Thank you very much ! ### Minimal code sample. ```python. import anndata. import pandas as pd. import scanpy as sc. annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). ```. ### Error output. ```pytb. MemoryError Traceback (most recent call last). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_read(ZarrArray, IOSpec(""array"", ""0.2.0"")). 321 @_REGISTRY.register_read(ZarrArray, IOSpec(""string-array"", ""0.2.0"")). 322 def read_array(e",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:2814,testability,Trace,Traceback,2814,"3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_read(ZarrArray, IOSpec(""array"", ""0.2.0"")). 321 @_REGISTRY.register_read(ZarrArray, IOSpec(""string-array"", ""0.2.0"")). 322 def read_array(elem, _reader):. --> 323 return elem[()]. File h5py\_objects.pyx:54, in h5py._objects.with_phil.wrapper(). File h5py\_objects.pyx:55, in h5py._objects.with_phil.wrapper(). File D:\Python3.10.9\lib\site-packages\h5py\_hl\dataset.py:768, in Dataset.__getitem__(self, args, new_dtype). 767 try:. --> 768 return self._fast_reader.read(args). 769 except TypeError:. File h5py\_selector.pyx:368, in h5py._selector.Reader.read(). File h5py\_selector.pyx:342, in h5py._selector.Reader.make_array(). MemoryError: Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32. The above exception was the direct cause of the following exception:. AnnDataReadError Traceback (most recent call last). Cell In[2], line 4. 2 import pandas as pd. 3 import scanpy as sc. ----> 4 annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:243, in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size). 240 return read_dataframe(elem). 241 return func(elem). --> 243 adata = read_dispatched(f, callback=callback). 245 # Backwards compat (should figure out which version). 246 if ""raw.X"" in f:. File D:\Python3.10.9\lib\site-packages\anndata\experimental\__init__.py:58, in read_dispatched(elem, callback). 54 from anndata._io.specs import Reader, _REGISTRY. 56 reader = Reader(_REGISTRY, callback=callback). ---> 58 return reader.read_elem(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:204, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 202 return func(*args, **kwargs). 203 except Exception as e:. --> 204 re_raise_error(e, elem). File D:\Python3.10.9\l",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:0,usability,Memor,Memory,0,"Memory error and Anndata error raised while using sc.read_h5ad; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hi there , I encountered a problem while using sc.read_h5ad to read the ""ReplogleWeissman2022_K562_essential.h5ad ""datasets . At first I supposed it is an AnnDataRead error: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. . But now I believe the main reason is MemoryError : Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32 . However, I am running this on a server with about 100 GB of memory , this problem really bothers me , could you please help me with that? Thank you very much ! ### Minimal code sample. ```python. import anndata. import pandas as pd. import scanpy as sc. annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). ```. ### Error output. ```pytb. MemoryError Traceback (most recent call last). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_rea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:7,usability,error,error,7,"Memory error and Anndata error raised while using sc.read_h5ad; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hi there , I encountered a problem while using sc.read_h5ad to read the ""ReplogleWeissman2022_K562_essential.h5ad ""datasets . At first I supposed it is an AnnDataRead error: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. . But now I believe the main reason is MemoryError : Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32 . However, I am running this on a server with about 100 GB of memory , this problem really bothers me , could you please help me with that? Thank you very much ! ### Minimal code sample. ```python. import anndata. import pandas as pd. import scanpy as sc. annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). ```. ### Error output. ```pytb. MemoryError Traceback (most recent call last). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_rea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:25,usability,error,error,25,"Memory error and Anndata error raised while using sc.read_h5ad; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hi there , I encountered a problem while using sc.read_h5ad to read the ""ReplogleWeissman2022_K562_essential.h5ad ""datasets . At first I supposed it is an AnnDataRead error: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. . But now I believe the main reason is MemoryError : Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32 . However, I am running this on a server with about 100 GB of memory , this problem really bothers me , could you please help me with that? Thank you very much ! ### Minimal code sample. ```python. import anndata. import pandas as pd. import scanpy as sc. annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). ```. ### Error output. ```pytb. MemoryError Traceback (most recent call last). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_rea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:192,usability,confirm,confirmed,192,"Memory error and Anndata error raised while using sc.read_h5ad; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hi there , I encountered a problem while using sc.read_h5ad to read the ""ReplogleWeissman2022_K562_essential.h5ad ""datasets . At first I supposed it is an AnnDataRead error: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. . But now I believe the main reason is MemoryError : Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32 . However, I am running this on a server with about 100 GB of memory , this problem really bothers me , could you please help me with that? Thank you very much ! ### Minimal code sample. ```python. import anndata. import pandas as pd. import scanpy as sc. annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). ```. ### Error output. ```pytb. MemoryError Traceback (most recent call last). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_rea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:275,usability,confirm,confirmed,275,"Memory error and Anndata error raised while using sc.read_h5ad; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hi there , I encountered a problem while using sc.read_h5ad to read the ""ReplogleWeissman2022_K562_essential.h5ad ""datasets . At first I supposed it is an AnnDataRead error: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. . But now I believe the main reason is MemoryError : Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32 . However, I am running this on a server with about 100 GB of memory , this problem really bothers me , could you please help me with that? Thank you very much ! ### Minimal code sample. ```python. import anndata. import pandas as pd. import scanpy as sc. annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). ```. ### Error output. ```pytb. MemoryError Traceback (most recent call last). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_rea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:519,usability,error,error,519,"Memory error and Anndata error raised while using sc.read_h5ad; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hi there , I encountered a problem while using sc.read_h5ad to read the ""ReplogleWeissman2022_K562_essential.h5ad ""datasets . At first I supposed it is an AnnDataRead error: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. . But now I believe the main reason is MemoryError : Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32 . However, I am running this on a server with about 100 GB of memory , this problem really bothers me , could you please help me with that? Thank you very much ! ### Minimal code sample. ```python. import anndata. import pandas as pd. import scanpy as sc. annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). ```. ### Error output. ```pytb. MemoryError Traceback (most recent call last). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_rea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:532,usability,error,error,532,"Memory error and Anndata error raised while using sc.read_h5ad; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hi there , I encountered a problem while using sc.read_h5ad to read the ""ReplogleWeissman2022_K562_essential.h5ad ""datasets . At first I supposed it is an AnnDataRead error: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. . But now I believe the main reason is MemoryError : Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32 . However, I am running this on a server with about 100 GB of memory , this problem really bothers me , could you please help me with that? Thank you very much ! ### Minimal code sample. ```python. import anndata. import pandas as pd. import scanpy as sc. annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). ```. ### Error output. ```pytb. MemoryError Traceback (most recent call last). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_rea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:658,usability,Memor,MemoryError,658,"Memory error and Anndata error raised while using sc.read_h5ad; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hi there , I encountered a problem while using sc.read_h5ad to read the ""ReplogleWeissman2022_K562_essential.h5ad ""datasets . At first I supposed it is an AnnDataRead error: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. . But now I believe the main reason is MemoryError : Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32 . However, I am running this on a server with about 100 GB of memory , this problem really bothers me , could you please help me with that? Thank you very much ! ### Minimal code sample. ```python. import anndata. import pandas as pd. import scanpy as sc. annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). ```. ### Error output. ```pytb. MemoryError Traceback (most recent call last). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_rea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:823,usability,memor,memory,823,"Memory error and Anndata error raised while using sc.read_h5ad; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hi there , I encountered a problem while using sc.read_h5ad to read the ""ReplogleWeissman2022_K562_essential.h5ad ""datasets . At first I supposed it is an AnnDataRead error: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. . But now I believe the main reason is MemoryError : Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32 . However, I am running this on a server with about 100 GB of memory , this problem really bothers me , could you please help me with that? Thank you very much ! ### Minimal code sample. ```python. import anndata. import pandas as pd. import scanpy as sc. annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). ```. ### Error output. ```pytb. MemoryError Traceback (most recent call last). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_rea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:882,usability,help,help,882,"Memory error and Anndata error raised while using sc.read_h5ad; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hi there , I encountered a problem while using sc.read_h5ad to read the ""ReplogleWeissman2022_K562_essential.h5ad ""datasets . At first I supposed it is an AnnDataRead error: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. . But now I believe the main reason is MemoryError : Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32 . However, I am running this on a server with about 100 GB of memory , this problem really bothers me , could you please help me with that? Thank you very much ! ### Minimal code sample. ```python. import anndata. import pandas as pd. import scanpy as sc. annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). ```. ### Error output. ```pytb. MemoryError Traceback (most recent call last). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_rea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:927,usability,Minim,Minimal,927,"Memory error and Anndata error raised while using sc.read_h5ad; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hi there , I encountered a problem while using sc.read_h5ad to read the ""ReplogleWeissman2022_K562_essential.h5ad ""datasets . At first I supposed it is an AnnDataRead error: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. . But now I believe the main reason is MemoryError : Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32 . However, I am running this on a server with about 100 GB of memory , this problem really bothers me , could you please help me with that? Thank you very much ! ### Minimal code sample. ```python. import anndata. import pandas as pd. import scanpy as sc. annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). ```. ### Error output. ```pytb. MemoryError Traceback (most recent call last). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_rea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:1094,usability,Error,Error,1094,"ions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hi there , I encountered a problem while using sc.read_h5ad to read the ""ReplogleWeissman2022_K562_essential.h5ad ""datasets . At first I supposed it is an AnnDataRead error: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. . But now I believe the main reason is MemoryError : Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32 . However, I am running this on a server with about 100 GB of memory , this problem really bothers me , could you please help me with that? Thank you very much ! ### Minimal code sample. ```python. import anndata. import pandas as pd. import scanpy as sc. annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). ```. ### Error output. ```pytb. MemoryError Traceback (most recent call last). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_read(ZarrArray, IOSpec(""array"", ""0.2.0"")). 321 @_REGISTRY.register_read(ZarrArray, IOSpec(""string-ar",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:1117,usability,Memor,MemoryError,1117," checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hi there , I encountered a problem while using sc.read_h5ad to read the ""ReplogleWeissman2022_K562_essential.h5ad ""datasets . At first I supposed it is an AnnDataRead error: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. . But now I believe the main reason is MemoryError : Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32 . However, I am running this on a server with about 100 GB of memory , this problem really bothers me , could you please help me with that? Thank you very much ! ### Minimal code sample. ```python. import anndata. import pandas as pd. import scanpy as sc. annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). ```. ### Error output. ```pytb. MemoryError Traceback (most recent call last). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 201 try:. --> 202 return func(*args, **kwargs). 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers). 234 if self.callback is not None:. --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)). 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_read(ZarrArray, IOSpec(""array"", ""0.2.0"")). 321 @_REGISTRY.register_read(ZarrArray, IOSpec(""string-array"", ""0.2.0"")). 322 def r",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:2624,usability,Memor,MemoryError,2624,"on3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec). 240 return read_dataframe(elem). --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader). 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")). 320 @_REGISTRY.register_read(ZarrArray, IOSpec(""array"", ""0.2.0"")). 321 @_REGISTRY.register_read(ZarrArray, IOSpec(""string-array"", ""0.2.0"")). 322 def read_array(elem, _reader):. --> 323 return elem[()]. File h5py\_objects.pyx:54, in h5py._objects.with_phil.wrapper(). File h5py\_objects.pyx:55, in h5py._objects.with_phil.wrapper(). File D:\Python3.10.9\lib\site-packages\h5py\_hl\dataset.py:768, in Dataset.__getitem__(self, args, new_dtype). 767 try:. --> 768 return self._fast_reader.read(args). 769 except TypeError:. File h5py\_selector.pyx:368, in h5py._selector.Reader.read(). File h5py\_selector.pyx:342, in h5py._selector.Reader.make_array(). MemoryError: Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32. The above exception was the direct cause of the following exception:. AnnDataReadError Traceback (most recent call last). Cell In[2], line 4. 2 import pandas as pd. 3 import scanpy as sc. ----> 4 annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:243, in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size). 240 return read_dataframe(elem). 241 return func(elem). --> 243 adata = read_dispatched(f, callback=callback). 245 # Backwards compat (should figure out which version). 246 if ""raw.X"" in f:. File D:\Python3.10.9\lib\site-packages\anndata\experimental\__init__.py:58, in read_dispatched(elem, callback). 54 from anndata._io.specs import Reader, _REGISTRY. 56 reader = Reader(_REGISTRY, callback=callback). ---> 58 return reader.read_elem(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:2",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:6487,usability,error,error,6487,"ing up backwards compat in the anndata initializer. 226 # In most cases we should be able to call `func(elen[k])` instead. --> 227 k: read_dispatched(elem[k], callback). 228 for k in elem.keys(). 229 if not k.startswith(""raw.""). 230 }. 231 ). 232 elif elem_name.startswith(""/raw.""):. 233 return None. File D:\Python3.10.9\lib\site-packages\anndata\experimental\__init__.py:58, in read_dispatched(elem, callback). 54 from anndata._io.specs import Reader, _REGISTRY. 56 reader = Reader(_REGISTRY, callback=callback). ---> 58 return reader.read_elem(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:204, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 202 return func(*args, **kwargs). 203 except Exception as e:. --> 204 re_raise_error(e, elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:188, in report_read_key_on_error.<locals>.re_raise_error(e, elem). 186 else:. 187 parent = _get_parent(elem). --> 188 raise AnnDataReadError(. 189 f""Above error raised while reading key {elem.name!r} of "". 190 f""type {type(elem)} from {parent}."". 191 ) from e. AnnDataReadError: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. ```. ### Versions. <details>. ```. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asttokens NA. astunparse 1.6.3. backcall 0.2.0. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. google NA. h5py 3.8.0. ipykernel 6.22.0. ipython_genutils 0.2.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. llvmlite 0.40.1. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.3.0. natsort 8.4.0. ntsecuritycon NA. numba 0.57.1. numpy 1.23.5. opt_einsum v3.3.0. packaging 23.1. pandas 2.0.0. parso 0.8.3. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.2.0. prompt_toolkit 3.0.38. psutil 5.9.5. pure_eval 0.2.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2551:6617,usability,error,error,6617," k: read_dispatched(elem[k], callback). 228 for k in elem.keys(). 229 if not k.startswith(""raw.""). 230 }. 231 ). 232 elif elem_name.startswith(""/raw.""):. 233 return None. File D:\Python3.10.9\lib\site-packages\anndata\experimental\__init__.py:58, in read_dispatched(elem, callback). 54 from anndata._io.specs import Reader, _REGISTRY. 56 reader = Reader(_REGISTRY, callback=callback). ---> 58 return reader.read_elem(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:204, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs). 202 return func(*args, **kwargs). 203 except Exception as e:. --> 204 re_raise_error(e, elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:188, in report_read_key_on_error.<locals>.re_raise_error(e, elem). 186 else:. 187 parent = _get_parent(elem). --> 188 raise AnnDataReadError(. 189 f""Above error raised while reading key {elem.name!r} of "". 190 f""type {type(elem)} from {parent}."". 191 ) from e. AnnDataReadError: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. ```. ### Versions. <details>. ```. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.5.0. asttokens NA. astunparse 1.6.3. backcall 0.2.0. cffi 1.15.1. colorama 0.4.6. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. google NA. h5py 3.8.0. ipykernel 6.22.0. ipython_genutils 0.2.0. jedi 0.18.2. joblib 1.2.0. kiwisolver 1.4.4. llvmlite 0.40.1. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.3.0. natsort 8.4.0. ntsecuritycon NA. numba 0.57.1. numpy 1.23.5. opt_einsum v3.3.0. packaging 23.1. pandas 2.0.0. parso 0.8.3. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.2.0. prompt_toolkit 3.0.38. psutil 5.9.5. pure_eval 0.2.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.0.9. pythoncom NA. pytz 2023.3. pywin32_bootstrap NA. pywin32_system32 NA. pywin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551
https://github.com/scverse/scanpy/issues/2555:148,performance,cach,cache,148,Stop tests from writing files everywhere; ### What kind of feature would you like to request? Other? ### Please describe your wishes. - maybe use [`cache.mkdir()`](https://docs.pytest.org/en/7.1.x/reference/reference.html#std-fixture-cache) for persistent files (data). - set directory to read only in CI during test run,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2555
https://github.com/scverse/scanpy/issues/2555:234,performance,cach,cache,234,Stop tests from writing files everywhere; ### What kind of feature would you like to request? Other? ### Please describe your wishes. - maybe use [`cache.mkdir()`](https://docs.pytest.org/en/7.1.x/reference/reference.html#std-fixture-cache) for persistent files (data). - set directory to read only in CI during test run,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2555
https://github.com/scverse/scanpy/issues/2555:5,safety,test,tests,5,Stop tests from writing files everywhere; ### What kind of feature would you like to request? Other? ### Please describe your wishes. - maybe use [`cache.mkdir()`](https://docs.pytest.org/en/7.1.x/reference/reference.html#std-fixture-cache) for persistent files (data). - set directory to read only in CI during test run,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2555
https://github.com/scverse/scanpy/issues/2555:312,safety,test,test,312,Stop tests from writing files everywhere; ### What kind of feature would you like to request? Other? ### Please describe your wishes. - maybe use [`cache.mkdir()`](https://docs.pytest.org/en/7.1.x/reference/reference.html#std-fixture-cache) for persistent files (data). - set directory to read only in CI during test run,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2555
https://github.com/scverse/scanpy/issues/2555:5,testability,test,tests,5,Stop tests from writing files everywhere; ### What kind of feature would you like to request? Other? ### Please describe your wishes. - maybe use [`cache.mkdir()`](https://docs.pytest.org/en/7.1.x/reference/reference.html#std-fixture-cache) for persistent files (data). - set directory to read only in CI during test run,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2555
https://github.com/scverse/scanpy/issues/2555:312,testability,test,test,312,Stop tests from writing files everywhere; ### What kind of feature would you like to request? Other? ### Please describe your wishes. - maybe use [`cache.mkdir()`](https://docs.pytest.org/en/7.1.x/reference/reference.html#std-fixture-cache) for persistent files (data). - set directory to read only in CI during test run,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2555
https://github.com/scverse/scanpy/issues/2555:0,usability,Stop,Stop,0,Stop tests from writing files everywhere; ### What kind of feature would you like to request? Other? ### Please describe your wishes. - maybe use [`cache.mkdir()`](https://docs.pytest.org/en/7.1.x/reference/reference.html#std-fixture-cache) for persistent files (data). - set directory to read only in CI during test run,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2555
https://github.com/scverse/scanpy/issues/2556:1078,availability,Error,Error,1078,"t; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hi,. I have two different datasets, both with raw counts. After using the function `sc.pp.normalize_total` and `sc.pp.log1p` and plotting the data with UMAP coordinates, there is no gene expression in cells coming from one of the datasets. I did the analysis separately (without concatenating) and the same happens. . I thought that maybe is a problem with the data type, but when I checked this, both anndatas.X were np.float32 and sparse.csr_matrixes ( #1612 ). Also, I made sure the anndata matrix related to the problematic dataset has acceptable values and they are not zeros. Any idea about this problem? ### Minimal code sample. ```python. sc.pp.normalize_total(adata). sc.pp.log1p(adata). ```. ### Error output. _No response_. ### Versions. <details>. ```. anndata 0.9.1. scanpy 1.8.1. sinfo 0.3.4. -----. PIL 9.1.0. anyio NA. astunparse 1.6.3. attr 21.2.0. babel 2.9.1. backcall 0.2.0. brotli NA. certifi 2022.12.07. cffi 1.15.0. charset_normalizer 2.0.12. cloudpickle 2.0.0. cycler 0.10.0. cython_runtime NA. dask 2022.8.1. dateutil 2.8.2. debugpy 1.6.0. decorator 5.0.9. defusedxml 0.7.1. entrypoints 0.4. fastjsonschema NA. fsspec 2022.7.1. google NA. h5py 3.4.0. idna 3.3. igraph 0.9.6. ipykernel 6.4.0. ipython_genutils 0.2.0. jedi 0.18.1. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 3.2.0. jupyter_server 1.11.0. jupyterlab_server 2.8.1. kiwisolver 1.3.2. leidenalg 0.8.7. llvmlite 0.38.0. louvain 0.7.0. markupsafe 2.1.1. matplotlib 3.6.0. mpl_toolkits NA. mpmath 1.3.0. natsort 7.1.1. nbclassic NA. nbformat 5.1.3. numba 0.55.1. numexpr 2.7.3. numpy 1.21.6. nvfuser NA. opt_einsum v3.3.0. packaging 23.1. pandas 1.5.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. pro",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2556
https://github.com/scverse/scanpy/issues/2556:252,deployability,version,version,252,"After sc.pp.normalize_total() and log1p() there is no gene expression in UMAP plot; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hi,. I have two different datasets, both with raw counts. After using the function `sc.pp.normalize_total` and `sc.pp.log1p` and plotting the data with UMAP coordinates, there is no gene expression in cells coming from one of the datasets. I did the analysis separately (without concatenating) and the same happens. . I thought that maybe is a problem with the data type, but when I checked this, both anndatas.X were np.float32 and sparse.csr_matrixes ( #1612 ). Also, I made sure the anndata matrix related to the problematic dataset has acceptable values and they are not zeros. Any idea about this problem? ### Minimal code sample. ```python. sc.pp.normalize_total(adata). sc.pp.log1p(adata). ```. ### Error output. _No response_. ### Versions. <details>. ```. anndata 0.9.1. scanpy 1.8.1. sinfo 0.3.4. -----. PIL 9.1.0. anyio NA. astunparse 1.6.3. attr 21.2.0. babel 2.9.1. backcall 0.2.0. brotli NA. certifi 2022.12.07. cffi 1.15.0. charset_normalizer 2.0.12. cloudpickle 2.0.0. cycler 0.10.0. cython_runtime NA. dask 2022.8.1. dateutil 2.8.2. debugpy 1.6.0. decorator 5.0.9. defusedxml 0.7.1. entrypoints 0.4. fastjsonschema NA. fsspec 2022.7.1. google NA. h5py 3.4.0. idna 3.3. igraph 0.9.6. ipykernel 6.4.0. ipython_genutils 0.2.0. jedi 0.18.1. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 3.2.0. jupyter_server 1.11.0. jupyterlab_server 2.8.1. kiwisolver 1.3.2. leidenalg 0.8.7. llvmlite 0.38.0. louvain 0.7.0. markupsafe 2.1.1. matplotlib 3.6.0. mpl_toolkits NA. mpmath 1.3.0. natsort 7.1.1. nbclassic NA. nbformat 5.1.3. numba 0.55.1. numexpr 2.7.3. numpy 1.21.6. nvfuser NA. opt_einsum v3.3.0. packaging 23.1. p",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2556
https://github.com/scverse/scanpy/issues/2556:1111,deployability,Version,Versions,1111,"itions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hi,. I have two different datasets, both with raw counts. After using the function `sc.pp.normalize_total` and `sc.pp.log1p` and plotting the data with UMAP coordinates, there is no gene expression in cells coming from one of the datasets. I did the analysis separately (without concatenating) and the same happens. . I thought that maybe is a problem with the data type, but when I checked this, both anndatas.X were np.float32 and sparse.csr_matrixes ( #1612 ). Also, I made sure the anndata matrix related to the problematic dataset has acceptable values and they are not zeros. Any idea about this problem? ### Minimal code sample. ```python. sc.pp.normalize_total(adata). sc.pp.log1p(adata). ```. ### Error output. _No response_. ### Versions. <details>. ```. anndata 0.9.1. scanpy 1.8.1. sinfo 0.3.4. -----. PIL 9.1.0. anyio NA. astunparse 1.6.3. attr 21.2.0. babel 2.9.1. backcall 0.2.0. brotli NA. certifi 2022.12.07. cffi 1.15.0. charset_normalizer 2.0.12. cloudpickle 2.0.0. cycler 0.10.0. cython_runtime NA. dask 2022.8.1. dateutil 2.8.2. debugpy 1.6.0. decorator 5.0.9. defusedxml 0.7.1. entrypoints 0.4. fastjsonschema NA. fsspec 2022.7.1. google NA. h5py 3.4.0. idna 3.3. igraph 0.9.6. ipykernel 6.4.0. ipython_genutils 0.2.0. jedi 0.18.1. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 3.2.0. jupyter_server 1.11.0. jupyterlab_server 2.8.1. kiwisolver 1.3.2. leidenalg 0.8.7. llvmlite 0.38.0. louvain 0.7.0. markupsafe 2.1.1. matplotlib 3.6.0. mpl_toolkits NA. mpmath 1.3.0. natsort 7.1.1. nbclassic NA. nbformat 5.1.3. numba 0.55.1. numexpr 2.7.3. numpy 1.21.6. nvfuser NA. opt_einsum v3.3.0. packaging 23.1. pandas 1.5.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2556
https://github.com/scverse/scanpy/issues/2556:1338,energy efficiency,cloud,cloudpickle,1338," scanpy. ### What happened? Hi,. I have two different datasets, both with raw counts. After using the function `sc.pp.normalize_total` and `sc.pp.log1p` and plotting the data with UMAP coordinates, there is no gene expression in cells coming from one of the datasets. I did the analysis separately (without concatenating) and the same happens. . I thought that maybe is a problem with the data type, but when I checked this, both anndatas.X were np.float32 and sparse.csr_matrixes ( #1612 ). Also, I made sure the anndata matrix related to the problematic dataset has acceptable values and they are not zeros. Any idea about this problem? ### Minimal code sample. ```python. sc.pp.normalize_total(adata). sc.pp.log1p(adata). ```. ### Error output. _No response_. ### Versions. <details>. ```. anndata 0.9.1. scanpy 1.8.1. sinfo 0.3.4. -----. PIL 9.1.0. anyio NA. astunparse 1.6.3. attr 21.2.0. babel 2.9.1. backcall 0.2.0. brotli NA. certifi 2022.12.07. cffi 1.15.0. charset_normalizer 2.0.12. cloudpickle 2.0.0. cycler 0.10.0. cython_runtime NA. dask 2022.8.1. dateutil 2.8.2. debugpy 1.6.0. decorator 5.0.9. defusedxml 0.7.1. entrypoints 0.4. fastjsonschema NA. fsspec 2022.7.1. google NA. h5py 3.4.0. idna 3.3. igraph 0.9.6. ipykernel 6.4.0. ipython_genutils 0.2.0. jedi 0.18.1. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 3.2.0. jupyter_server 1.11.0. jupyterlab_server 2.8.1. kiwisolver 1.3.2. leidenalg 0.8.7. llvmlite 0.38.0. louvain 0.7.0. markupsafe 2.1.1. matplotlib 3.6.0. mpl_toolkits NA. mpmath 1.3.0. natsort 7.1.1. nbclassic NA. nbformat 5.1.3. numba 0.55.1. numexpr 2.7.3. numpy 1.21.6. nvfuser NA. opt_einsum v3.3.0. packaging 23.1. pandas 1.5.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.31. psutil 5.9.0. ptyprocess 0.7.0. pvectorc NA. pydev_ipython NA. pydevconsole NA. pydevd 2.8.0. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 2.4.7. pyrsistent NA. pytz 2022.2.1. req",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2556
https://github.com/scverse/scanpy/issues/2556:252,integrability,version,version,252,"After sc.pp.normalize_total() and log1p() there is no gene expression in UMAP plot; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hi,. I have two different datasets, both with raw counts. After using the function `sc.pp.normalize_total` and `sc.pp.log1p` and plotting the data with UMAP coordinates, there is no gene expression in cells coming from one of the datasets. I did the analysis separately (without concatenating) and the same happens. . I thought that maybe is a problem with the data type, but when I checked this, both anndatas.X were np.float32 and sparse.csr_matrixes ( #1612 ). Also, I made sure the anndata matrix related to the problematic dataset has acceptable values and they are not zeros. Any idea about this problem? ### Minimal code sample. ```python. sc.pp.normalize_total(adata). sc.pp.log1p(adata). ```. ### Error output. _No response_. ### Versions. <details>. ```. anndata 0.9.1. scanpy 1.8.1. sinfo 0.3.4. -----. PIL 9.1.0. anyio NA. astunparse 1.6.3. attr 21.2.0. babel 2.9.1. backcall 0.2.0. brotli NA. certifi 2022.12.07. cffi 1.15.0. charset_normalizer 2.0.12. cloudpickle 2.0.0. cycler 0.10.0. cython_runtime NA. dask 2022.8.1. dateutil 2.8.2. debugpy 1.6.0. decorator 5.0.9. defusedxml 0.7.1. entrypoints 0.4. fastjsonschema NA. fsspec 2022.7.1. google NA. h5py 3.4.0. idna 3.3. igraph 0.9.6. ipykernel 6.4.0. ipython_genutils 0.2.0. jedi 0.18.1. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 3.2.0. jupyter_server 1.11.0. jupyterlab_server 2.8.1. kiwisolver 1.3.2. leidenalg 0.8.7. llvmlite 0.38.0. louvain 0.7.0. markupsafe 2.1.1. matplotlib 3.6.0. mpl_toolkits NA. mpmath 1.3.0. natsort 7.1.1. nbclassic NA. nbformat 5.1.3. numba 0.55.1. numexpr 2.7.3. numpy 1.21.6. nvfuser NA. opt_einsum v3.3.0. packaging 23.1. p",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2556
https://github.com/scverse/scanpy/issues/2556:1111,integrability,Version,Versions,1111,"itions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hi,. I have two different datasets, both with raw counts. After using the function `sc.pp.normalize_total` and `sc.pp.log1p` and plotting the data with UMAP coordinates, there is no gene expression in cells coming from one of the datasets. I did the analysis separately (without concatenating) and the same happens. . I thought that maybe is a problem with the data type, but when I checked this, both anndatas.X were np.float32 and sparse.csr_matrixes ( #1612 ). Also, I made sure the anndata matrix related to the problematic dataset has acceptable values and they are not zeros. Any idea about this problem? ### Minimal code sample. ```python. sc.pp.normalize_total(adata). sc.pp.log1p(adata). ```. ### Error output. _No response_. ### Versions. <details>. ```. anndata 0.9.1. scanpy 1.8.1. sinfo 0.3.4. -----. PIL 9.1.0. anyio NA. astunparse 1.6.3. attr 21.2.0. babel 2.9.1. backcall 0.2.0. brotli NA. certifi 2022.12.07. cffi 1.15.0. charset_normalizer 2.0.12. cloudpickle 2.0.0. cycler 0.10.0. cython_runtime NA. dask 2022.8.1. dateutil 2.8.2. debugpy 1.6.0. decorator 5.0.9. defusedxml 0.7.1. entrypoints 0.4. fastjsonschema NA. fsspec 2022.7.1. google NA. h5py 3.4.0. idna 3.3. igraph 0.9.6. ipykernel 6.4.0. ipython_genutils 0.2.0. jedi 0.18.1. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 3.2.0. jupyter_server 1.11.0. jupyterlab_server 2.8.1. kiwisolver 1.3.2. leidenalg 0.8.7. llvmlite 0.38.0. louvain 0.7.0. markupsafe 2.1.1. matplotlib 3.6.0. mpl_toolkits NA. mpmath 1.3.0. natsort 7.1.1. nbclassic NA. nbformat 5.1.3. numba 0.55.1. numexpr 2.7.3. numpy 1.21.6. nvfuser NA. opt_einsum v3.3.0. packaging 23.1. pandas 1.5.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2556
https://github.com/scverse/scanpy/issues/2556:529,interoperability,coordinat,coordinates,529,"After sc.pp.normalize_total() and log1p() there is no gene expression in UMAP plot; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hi,. I have two different datasets, both with raw counts. After using the function `sc.pp.normalize_total` and `sc.pp.log1p` and plotting the data with UMAP coordinates, there is no gene expression in cells coming from one of the datasets. I did the analysis separately (without concatenating) and the same happens. . I thought that maybe is a problem with the data type, but when I checked this, both anndatas.X were np.float32 and sparse.csr_matrixes ( #1612 ). Also, I made sure the anndata matrix related to the problematic dataset has acceptable values and they are not zeros. Any idea about this problem? ### Minimal code sample. ```python. sc.pp.normalize_total(adata). sc.pp.log1p(adata). ```. ### Error output. _No response_. ### Versions. <details>. ```. anndata 0.9.1. scanpy 1.8.1. sinfo 0.3.4. -----. PIL 9.1.0. anyio NA. astunparse 1.6.3. attr 21.2.0. babel 2.9.1. backcall 0.2.0. brotli NA. certifi 2022.12.07. cffi 1.15.0. charset_normalizer 2.0.12. cloudpickle 2.0.0. cycler 0.10.0. cython_runtime NA. dask 2022.8.1. dateutil 2.8.2. debugpy 1.6.0. decorator 5.0.9. defusedxml 0.7.1. entrypoints 0.4. fastjsonschema NA. fsspec 2022.7.1. google NA. h5py 3.4.0. idna 3.3. igraph 0.9.6. ipykernel 6.4.0. ipython_genutils 0.2.0. jedi 0.18.1. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 3.2.0. jupyter_server 1.11.0. jupyterlab_server 2.8.1. kiwisolver 1.3.2. leidenalg 0.8.7. llvmlite 0.38.0. louvain 0.7.0. markupsafe 2.1.1. matplotlib 3.6.0. mpl_toolkits NA. mpmath 1.3.0. natsort 7.1.1. nbclassic NA. nbformat 5.1.3. numba 0.55.1. numexpr 2.7.3. numpy 1.21.6. nvfuser NA. opt_einsum v3.3.0. packaging 23.1. p",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2556
https://github.com/scverse/scanpy/issues/2556:252,modifiability,version,version,252,"After sc.pp.normalize_total() and log1p() there is no gene expression in UMAP plot; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hi,. I have two different datasets, both with raw counts. After using the function `sc.pp.normalize_total` and `sc.pp.log1p` and plotting the data with UMAP coordinates, there is no gene expression in cells coming from one of the datasets. I did the analysis separately (without concatenating) and the same happens. . I thought that maybe is a problem with the data type, but when I checked this, both anndatas.X were np.float32 and sparse.csr_matrixes ( #1612 ). Also, I made sure the anndata matrix related to the problematic dataset has acceptable values and they are not zeros. Any idea about this problem? ### Minimal code sample. ```python. sc.pp.normalize_total(adata). sc.pp.log1p(adata). ```. ### Error output. _No response_. ### Versions. <details>. ```. anndata 0.9.1. scanpy 1.8.1. sinfo 0.3.4. -----. PIL 9.1.0. anyio NA. astunparse 1.6.3. attr 21.2.0. babel 2.9.1. backcall 0.2.0. brotli NA. certifi 2022.12.07. cffi 1.15.0. charset_normalizer 2.0.12. cloudpickle 2.0.0. cycler 0.10.0. cython_runtime NA. dask 2022.8.1. dateutil 2.8.2. debugpy 1.6.0. decorator 5.0.9. defusedxml 0.7.1. entrypoints 0.4. fastjsonschema NA. fsspec 2022.7.1. google NA. h5py 3.4.0. idna 3.3. igraph 0.9.6. ipykernel 6.4.0. ipython_genutils 0.2.0. jedi 0.18.1. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 3.2.0. jupyter_server 1.11.0. jupyterlab_server 2.8.1. kiwisolver 1.3.2. leidenalg 0.8.7. llvmlite 0.38.0. louvain 0.7.0. markupsafe 2.1.1. matplotlib 3.6.0. mpl_toolkits NA. mpmath 1.3.0. natsort 7.1.1. nbclassic NA. nbformat 5.1.3. numba 0.55.1. numexpr 2.7.3. numpy 1.21.6. nvfuser NA. opt_einsum v3.3.0. packaging 23.1. p",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2556
https://github.com/scverse/scanpy/issues/2556:1111,modifiability,Version,Versions,1111,"itions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hi,. I have two different datasets, both with raw counts. After using the function `sc.pp.normalize_total` and `sc.pp.log1p` and plotting the data with UMAP coordinates, there is no gene expression in cells coming from one of the datasets. I did the analysis separately (without concatenating) and the same happens. . I thought that maybe is a problem with the data type, but when I checked this, both anndatas.X were np.float32 and sparse.csr_matrixes ( #1612 ). Also, I made sure the anndata matrix related to the problematic dataset has acceptable values and they are not zeros. Any idea about this problem? ### Minimal code sample. ```python. sc.pp.normalize_total(adata). sc.pp.log1p(adata). ```. ### Error output. _No response_. ### Versions. <details>. ```. anndata 0.9.1. scanpy 1.8.1. sinfo 0.3.4. -----. PIL 9.1.0. anyio NA. astunparse 1.6.3. attr 21.2.0. babel 2.9.1. backcall 0.2.0. brotli NA. certifi 2022.12.07. cffi 1.15.0. charset_normalizer 2.0.12. cloudpickle 2.0.0. cycler 0.10.0. cython_runtime NA. dask 2022.8.1. dateutil 2.8.2. debugpy 1.6.0. decorator 5.0.9. defusedxml 0.7.1. entrypoints 0.4. fastjsonschema NA. fsspec 2022.7.1. google NA. h5py 3.4.0. idna 3.3. igraph 0.9.6. ipykernel 6.4.0. ipython_genutils 0.2.0. jedi 0.18.1. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 3.2.0. jupyter_server 1.11.0. jupyterlab_server 2.8.1. kiwisolver 1.3.2. leidenalg 0.8.7. llvmlite 0.38.0. louvain 0.7.0. markupsafe 2.1.1. matplotlib 3.6.0. mpl_toolkits NA. mpmath 1.3.0. natsort 7.1.1. nbclassic NA. nbformat 5.1.3. numba 0.55.1. numexpr 2.7.3. numpy 1.21.6. nvfuser NA. opt_einsum v3.3.0. packaging 23.1. pandas 1.5.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2556
https://github.com/scverse/scanpy/issues/2556:1437,modifiability,deco,decorator,1437,"the function `sc.pp.normalize_total` and `sc.pp.log1p` and plotting the data with UMAP coordinates, there is no gene expression in cells coming from one of the datasets. I did the analysis separately (without concatenating) and the same happens. . I thought that maybe is a problem with the data type, but when I checked this, both anndatas.X were np.float32 and sparse.csr_matrixes ( #1612 ). Also, I made sure the anndata matrix related to the problematic dataset has acceptable values and they are not zeros. Any idea about this problem? ### Minimal code sample. ```python. sc.pp.normalize_total(adata). sc.pp.log1p(adata). ```. ### Error output. _No response_. ### Versions. <details>. ```. anndata 0.9.1. scanpy 1.8.1. sinfo 0.3.4. -----. PIL 9.1.0. anyio NA. astunparse 1.6.3. attr 21.2.0. babel 2.9.1. backcall 0.2.0. brotli NA. certifi 2022.12.07. cffi 1.15.0. charset_normalizer 2.0.12. cloudpickle 2.0.0. cycler 0.10.0. cython_runtime NA. dask 2022.8.1. dateutil 2.8.2. debugpy 1.6.0. decorator 5.0.9. defusedxml 0.7.1. entrypoints 0.4. fastjsonschema NA. fsspec 2022.7.1. google NA. h5py 3.4.0. idna 3.3. igraph 0.9.6. ipykernel 6.4.0. ipython_genutils 0.2.0. jedi 0.18.1. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 3.2.0. jupyter_server 1.11.0. jupyterlab_server 2.8.1. kiwisolver 1.3.2. leidenalg 0.8.7. llvmlite 0.38.0. louvain 0.7.0. markupsafe 2.1.1. matplotlib 3.6.0. mpl_toolkits NA. mpmath 1.3.0. natsort 7.1.1. nbclassic NA. nbformat 5.1.3. numba 0.55.1. numexpr 2.7.3. numpy 1.21.6. nvfuser NA. opt_einsum v3.3.0. packaging 23.1. pandas 1.5.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.31. psutil 5.9.0. ptyprocess 0.7.0. pvectorc NA. pydev_ipython NA. pydevconsole NA. pydevd 2.8.0. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 2.4.7. pyrsistent NA. pytz 2022.2.1. requests 2.28.1. scipy 1.10.1. send2trash NA. setuptools 67.8.0. simplejson 3.17.6. six 1.16.0. sklea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2556
https://github.com/scverse/scanpy/issues/2556:1983,modifiability,pac,packaging,1983,inimal code sample. ```python. sc.pp.normalize_total(adata). sc.pp.log1p(adata). ```. ### Error output. _No response_. ### Versions. <details>. ```. anndata 0.9.1. scanpy 1.8.1. sinfo 0.3.4. -----. PIL 9.1.0. anyio NA. astunparse 1.6.3. attr 21.2.0. babel 2.9.1. backcall 0.2.0. brotli NA. certifi 2022.12.07. cffi 1.15.0. charset_normalizer 2.0.12. cloudpickle 2.0.0. cycler 0.10.0. cython_runtime NA. dask 2022.8.1. dateutil 2.8.2. debugpy 1.6.0. decorator 5.0.9. defusedxml 0.7.1. entrypoints 0.4. fastjsonschema NA. fsspec 2022.7.1. google NA. h5py 3.4.0. idna 3.3. igraph 0.9.6. ipykernel 6.4.0. ipython_genutils 0.2.0. jedi 0.18.1. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 3.2.0. jupyter_server 1.11.0. jupyterlab_server 2.8.1. kiwisolver 1.3.2. leidenalg 0.8.7. llvmlite 0.38.0. louvain 0.7.0. markupsafe 2.1.1. matplotlib 3.6.0. mpl_toolkits NA. mpmath 1.3.0. natsort 7.1.1. nbclassic NA. nbformat 5.1.3. numba 0.55.1. numexpr 2.7.3. numpy 1.21.6. nvfuser NA. opt_einsum v3.3.0. packaging 23.1. pandas 1.5.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.31. psutil 5.9.0. ptyprocess 0.7.0. pvectorc NA. pydev_ipython NA. pydevconsole NA. pydevd 2.8.0. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 2.4.7. pyrsistent NA. pytz 2022.2.1. requests 2.28.1. scipy 1.10.1. send2trash NA. setuptools 67.8.0. simplejson 3.17.6. six 1.16.0. sklearn 1.2.2. sniffio 1.2.0. socks 1.7.1. sparse 0.14.0. storemagic NA. sympy 1.12. tables 3.6.1. tblib 1.7.0. terminado 0.12.1. texttable 1.6.4. threadpoolctl 2.2.0. tlz 0.12.0. toolz 0.12.0. torch 2.0.1+cu117. tornado 6.1. tqdm 4.62.2. traitlets 5.1.0. typing_extensions NA. unicodedata2 NA. urllib3 1.26.9. wcwidth 0.2.5. websocket 1.3.2. yaml 6.0. zipp NA. zmq 22.3.0. zstandard 0.18.0. -----. IPython 7.34.0. jupyter_client 7.3.0. jupyter_core 4.10.0. jupyterlab 3.1.11. notebook 6.4.11. -----. Python 3.8.13 | packaged by conda-forge . ```. </d,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2556
https://github.com/scverse/scanpy/issues/2556:2954,modifiability,pac,packaged,2954,ode sample. ```python. sc.pp.normalize_total(adata). sc.pp.log1p(adata). ```. ### Error output. _No response_. ### Versions. <details>. ```. anndata 0.9.1. scanpy 1.8.1. sinfo 0.3.4. -----. PIL 9.1.0. anyio NA. astunparse 1.6.3. attr 21.2.0. babel 2.9.1. backcall 0.2.0. brotli NA. certifi 2022.12.07. cffi 1.15.0. charset_normalizer 2.0.12. cloudpickle 2.0.0. cycler 0.10.0. cython_runtime NA. dask 2022.8.1. dateutil 2.8.2. debugpy 1.6.0. decorator 5.0.9. defusedxml 0.7.1. entrypoints 0.4. fastjsonschema NA. fsspec 2022.7.1. google NA. h5py 3.4.0. idna 3.3. igraph 0.9.6. ipykernel 6.4.0. ipython_genutils 0.2.0. jedi 0.18.1. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 3.2.0. jupyter_server 1.11.0. jupyterlab_server 2.8.1. kiwisolver 1.3.2. leidenalg 0.8.7. llvmlite 0.38.0. louvain 0.7.0. markupsafe 2.1.1. matplotlib 3.6.0. mpl_toolkits NA. mpmath 1.3.0. natsort 7.1.1. nbclassic NA. nbformat 5.1.3. numba 0.55.1. numexpr 2.7.3. numpy 1.21.6. nvfuser NA. opt_einsum v3.3.0. packaging 23.1. pandas 1.5.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.31. psutil 5.9.0. ptyprocess 0.7.0. pvectorc NA. pydev_ipython NA. pydevconsole NA. pydevd 2.8.0. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 2.4.7. pyrsistent NA. pytz 2022.2.1. requests 2.28.1. scipy 1.10.1. send2trash NA. setuptools 67.8.0. simplejson 3.17.6. six 1.16.0. sklearn 1.2.2. sniffio 1.2.0. socks 1.7.1. sparse 0.14.0. storemagic NA. sympy 1.12. tables 3.6.1. tblib 1.7.0. terminado 0.12.1. texttable 1.6.4. threadpoolctl 2.2.0. tlz 0.12.0. toolz 0.12.0. torch 2.0.1+cu117. tornado 6.1. tqdm 4.62.2. traitlets 5.1.0. typing_extensions NA. unicodedata2 NA. urllib3 1.26.9. wcwidth 0.2.5. websocket 1.3.2. yaml 6.0. zipp NA. zmq 22.3.0. zstandard 0.18.0. -----. IPython 7.34.0. jupyter_client 7.3.0. jupyter_core 4.10.0. jupyterlab 3.1.11. notebook 6.4.11. -----. Python 3.8.13 | packaged by conda-forge . ```. </details>.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2556
https://github.com/scverse/scanpy/issues/2556:1078,performance,Error,Error,1078,"t; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hi,. I have two different datasets, both with raw counts. After using the function `sc.pp.normalize_total` and `sc.pp.log1p` and plotting the data with UMAP coordinates, there is no gene expression in cells coming from one of the datasets. I did the analysis separately (without concatenating) and the same happens. . I thought that maybe is a problem with the data type, but when I checked this, both anndatas.X were np.float32 and sparse.csr_matrixes ( #1612 ). Also, I made sure the anndata matrix related to the problematic dataset has acceptable values and they are not zeros. Any idea about this problem? ### Minimal code sample. ```python. sc.pp.normalize_total(adata). sc.pp.log1p(adata). ```. ### Error output. _No response_. ### Versions. <details>. ```. anndata 0.9.1. scanpy 1.8.1. sinfo 0.3.4. -----. PIL 9.1.0. anyio NA. astunparse 1.6.3. attr 21.2.0. babel 2.9.1. backcall 0.2.0. brotli NA. certifi 2022.12.07. cffi 1.15.0. charset_normalizer 2.0.12. cloudpickle 2.0.0. cycler 0.10.0. cython_runtime NA. dask 2022.8.1. dateutil 2.8.2. debugpy 1.6.0. decorator 5.0.9. defusedxml 0.7.1. entrypoints 0.4. fastjsonschema NA. fsspec 2022.7.1. google NA. h5py 3.4.0. idna 3.3. igraph 0.9.6. ipykernel 6.4.0. ipython_genutils 0.2.0. jedi 0.18.1. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 3.2.0. jupyter_server 1.11.0. jupyterlab_server 2.8.1. kiwisolver 1.3.2. leidenalg 0.8.7. llvmlite 0.38.0. louvain 0.7.0. markupsafe 2.1.1. matplotlib 3.6.0. mpl_toolkits NA. mpmath 1.3.0. natsort 7.1.1. nbclassic NA. nbformat 5.1.3. numba 0.55.1. numexpr 2.7.3. numpy 1.21.6. nvfuser NA. opt_einsum v3.3.0. packaging 23.1. pandas 1.5.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. pro",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2556
https://github.com/scverse/scanpy/issues/2556:1078,safety,Error,Error,1078,"t; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hi,. I have two different datasets, both with raw counts. After using the function `sc.pp.normalize_total` and `sc.pp.log1p` and plotting the data with UMAP coordinates, there is no gene expression in cells coming from one of the datasets. I did the analysis separately (without concatenating) and the same happens. . I thought that maybe is a problem with the data type, but when I checked this, both anndatas.X were np.float32 and sparse.csr_matrixes ( #1612 ). Also, I made sure the anndata matrix related to the problematic dataset has acceptable values and they are not zeros. Any idea about this problem? ### Minimal code sample. ```python. sc.pp.normalize_total(adata). sc.pp.log1p(adata). ```. ### Error output. _No response_. ### Versions. <details>. ```. anndata 0.9.1. scanpy 1.8.1. sinfo 0.3.4. -----. PIL 9.1.0. anyio NA. astunparse 1.6.3. attr 21.2.0. babel 2.9.1. backcall 0.2.0. brotli NA. certifi 2022.12.07. cffi 1.15.0. charset_normalizer 2.0.12. cloudpickle 2.0.0. cycler 0.10.0. cython_runtime NA. dask 2022.8.1. dateutil 2.8.2. debugpy 1.6.0. decorator 5.0.9. defusedxml 0.7.1. entrypoints 0.4. fastjsonschema NA. fsspec 2022.7.1. google NA. h5py 3.4.0. idna 3.3. igraph 0.9.6. ipykernel 6.4.0. ipython_genutils 0.2.0. jedi 0.18.1. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 3.2.0. jupyter_server 1.11.0. jupyterlab_server 2.8.1. kiwisolver 1.3.2. leidenalg 0.8.7. llvmlite 0.38.0. louvain 0.7.0. markupsafe 2.1.1. matplotlib 3.6.0. mpl_toolkits NA. mpmath 1.3.0. natsort 7.1.1. nbclassic NA. nbformat 5.1.3. numba 0.55.1. numexpr 2.7.3. numpy 1.21.6. nvfuser NA. opt_einsum v3.3.0. packaging 23.1. pandas 1.5.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. pro",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2556
https://github.com/scverse/scanpy/issues/2556:1278,security,certif,certifi,1278,"onal) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hi,. I have two different datasets, both with raw counts. After using the function `sc.pp.normalize_total` and `sc.pp.log1p` and plotting the data with UMAP coordinates, there is no gene expression in cells coming from one of the datasets. I did the analysis separately (without concatenating) and the same happens. . I thought that maybe is a problem with the data type, but when I checked this, both anndatas.X were np.float32 and sparse.csr_matrixes ( #1612 ). Also, I made sure the anndata matrix related to the problematic dataset has acceptable values and they are not zeros. Any idea about this problem? ### Minimal code sample. ```python. sc.pp.normalize_total(adata). sc.pp.log1p(adata). ```. ### Error output. _No response_. ### Versions. <details>. ```. anndata 0.9.1. scanpy 1.8.1. sinfo 0.3.4. -----. PIL 9.1.0. anyio NA. astunparse 1.6.3. attr 21.2.0. babel 2.9.1. backcall 0.2.0. brotli NA. certifi 2022.12.07. cffi 1.15.0. charset_normalizer 2.0.12. cloudpickle 2.0.0. cycler 0.10.0. cython_runtime NA. dask 2022.8.1. dateutil 2.8.2. debugpy 1.6.0. decorator 5.0.9. defusedxml 0.7.1. entrypoints 0.4. fastjsonschema NA. fsspec 2022.7.1. google NA. h5py 3.4.0. idna 3.3. igraph 0.9.6. ipykernel 6.4.0. ipython_genutils 0.2.0. jedi 0.18.1. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 3.2.0. jupyter_server 1.11.0. jupyterlab_server 2.8.1. kiwisolver 1.3.2. leidenalg 0.8.7. llvmlite 0.38.0. louvain 0.7.0. markupsafe 2.1.1. matplotlib 3.6.0. mpl_toolkits NA. mpmath 1.3.0. natsort 7.1.1. nbclassic NA. nbformat 5.1.3. numba 0.55.1. numexpr 2.7.3. numpy 1.21.6. nvfuser NA. opt_einsum v3.3.0. packaging 23.1. pandas 1.5.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.31. psutil 5.9.0. ptyprocess 0.7.0. pvectorc NA. pydev_ipython NA. pydevconsole NA. pydevd 2.8.0. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygme",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2556
https://github.com/scverse/scanpy/issues/2556:2467,security,soc,socks,2467,ode sample. ```python. sc.pp.normalize_total(adata). sc.pp.log1p(adata). ```. ### Error output. _No response_. ### Versions. <details>. ```. anndata 0.9.1. scanpy 1.8.1. sinfo 0.3.4. -----. PIL 9.1.0. anyio NA. astunparse 1.6.3. attr 21.2.0. babel 2.9.1. backcall 0.2.0. brotli NA. certifi 2022.12.07. cffi 1.15.0. charset_normalizer 2.0.12. cloudpickle 2.0.0. cycler 0.10.0. cython_runtime NA. dask 2022.8.1. dateutil 2.8.2. debugpy 1.6.0. decorator 5.0.9. defusedxml 0.7.1. entrypoints 0.4. fastjsonschema NA. fsspec 2022.7.1. google NA. h5py 3.4.0. idna 3.3. igraph 0.9.6. ipykernel 6.4.0. ipython_genutils 0.2.0. jedi 0.18.1. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 3.2.0. jupyter_server 1.11.0. jupyterlab_server 2.8.1. kiwisolver 1.3.2. leidenalg 0.8.7. llvmlite 0.38.0. louvain 0.7.0. markupsafe 2.1.1. matplotlib 3.6.0. mpl_toolkits NA. mpmath 1.3.0. natsort 7.1.1. nbclassic NA. nbformat 5.1.3. numba 0.55.1. numexpr 2.7.3. numpy 1.21.6. nvfuser NA. opt_einsum v3.3.0. packaging 23.1. pandas 1.5.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.31. psutil 5.9.0. ptyprocess 0.7.0. pvectorc NA. pydev_ipython NA. pydevconsole NA. pydevd 2.8.0. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 2.4.7. pyrsistent NA. pytz 2022.2.1. requests 2.28.1. scipy 1.10.1. send2trash NA. setuptools 67.8.0. simplejson 3.17.6. six 1.16.0. sklearn 1.2.2. sniffio 1.2.0. socks 1.7.1. sparse 0.14.0. storemagic NA. sympy 1.12. tables 3.6.1. tblib 1.7.0. terminado 0.12.1. texttable 1.6.4. threadpoolctl 2.2.0. tlz 0.12.0. toolz 0.12.0. torch 2.0.1+cu117. tornado 6.1. tqdm 4.62.2. traitlets 5.1.0. typing_extensions NA. unicodedata2 NA. urllib3 1.26.9. wcwidth 0.2.5. websocket 1.3.2. yaml 6.0. zipp NA. zmq 22.3.0. zstandard 0.18.0. -----. IPython 7.34.0. jupyter_client 7.3.0. jupyter_core 4.10.0. jupyterlab 3.1.11. notebook 6.4.11. -----. Python 3.8.13 | packaged by conda-forge . ```. </details>.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2556
https://github.com/scverse/scanpy/issues/2556:2406,testability,simpl,simplejson,2406,ode sample. ```python. sc.pp.normalize_total(adata). sc.pp.log1p(adata). ```. ### Error output. _No response_. ### Versions. <details>. ```. anndata 0.9.1. scanpy 1.8.1. sinfo 0.3.4. -----. PIL 9.1.0. anyio NA. astunparse 1.6.3. attr 21.2.0. babel 2.9.1. backcall 0.2.0. brotli NA. certifi 2022.12.07. cffi 1.15.0. charset_normalizer 2.0.12. cloudpickle 2.0.0. cycler 0.10.0. cython_runtime NA. dask 2022.8.1. dateutil 2.8.2. debugpy 1.6.0. decorator 5.0.9. defusedxml 0.7.1. entrypoints 0.4. fastjsonschema NA. fsspec 2022.7.1. google NA. h5py 3.4.0. idna 3.3. igraph 0.9.6. ipykernel 6.4.0. ipython_genutils 0.2.0. jedi 0.18.1. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 3.2.0. jupyter_server 1.11.0. jupyterlab_server 2.8.1. kiwisolver 1.3.2. leidenalg 0.8.7. llvmlite 0.38.0. louvain 0.7.0. markupsafe 2.1.1. matplotlib 3.6.0. mpl_toolkits NA. mpmath 1.3.0. natsort 7.1.1. nbclassic NA. nbformat 5.1.3. numba 0.55.1. numexpr 2.7.3. numpy 1.21.6. nvfuser NA. opt_einsum v3.3.0. packaging 23.1. pandas 1.5.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.31. psutil 5.9.0. ptyprocess 0.7.0. pvectorc NA. pydev_ipython NA. pydevconsole NA. pydevd 2.8.0. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 2.4.7. pyrsistent NA. pytz 2022.2.1. requests 2.28.1. scipy 1.10.1. send2trash NA. setuptools 67.8.0. simplejson 3.17.6. six 1.16.0. sklearn 1.2.2. sniffio 1.2.0. socks 1.7.1. sparse 0.14.0. storemagic NA. sympy 1.12. tables 3.6.1. tblib 1.7.0. terminado 0.12.1. texttable 1.6.4. threadpoolctl 2.2.0. tlz 0.12.0. toolz 0.12.0. torch 2.0.1+cu117. tornado 6.1. tqdm 4.62.2. traitlets 5.1.0. typing_extensions NA. unicodedata2 NA. urllib3 1.26.9. wcwidth 0.2.5. websocket 1.3.2. yaml 6.0. zipp NA. zmq 22.3.0. zstandard 0.18.0. -----. IPython 7.34.0. jupyter_client 7.3.0. jupyter_core 4.10.0. jupyterlab 3.1.11. notebook 6.4.11. -----. Python 3.8.13 | packaged by conda-forge . ```. </details>.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2556
https://github.com/scverse/scanpy/issues/2556:212,usability,confirm,confirmed,212,"After sc.pp.normalize_total() and log1p() there is no gene expression in UMAP plot; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hi,. I have two different datasets, both with raw counts. After using the function `sc.pp.normalize_total` and `sc.pp.log1p` and plotting the data with UMAP coordinates, there is no gene expression in cells coming from one of the datasets. I did the analysis separately (without concatenating) and the same happens. . I thought that maybe is a problem with the data type, but when I checked this, both anndatas.X were np.float32 and sparse.csr_matrixes ( #1612 ). Also, I made sure the anndata matrix related to the problematic dataset has acceptable values and they are not zeros. Any idea about this problem? ### Minimal code sample. ```python. sc.pp.normalize_total(adata). sc.pp.log1p(adata). ```. ### Error output. _No response_. ### Versions. <details>. ```. anndata 0.9.1. scanpy 1.8.1. sinfo 0.3.4. -----. PIL 9.1.0. anyio NA. astunparse 1.6.3. attr 21.2.0. babel 2.9.1. backcall 0.2.0. brotli NA. certifi 2022.12.07. cffi 1.15.0. charset_normalizer 2.0.12. cloudpickle 2.0.0. cycler 0.10.0. cython_runtime NA. dask 2022.8.1. dateutil 2.8.2. debugpy 1.6.0. decorator 5.0.9. defusedxml 0.7.1. entrypoints 0.4. fastjsonschema NA. fsspec 2022.7.1. google NA. h5py 3.4.0. idna 3.3. igraph 0.9.6. ipykernel 6.4.0. ipython_genutils 0.2.0. jedi 0.18.1. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 3.2.0. jupyter_server 1.11.0. jupyterlab_server 2.8.1. kiwisolver 1.3.2. leidenalg 0.8.7. llvmlite 0.38.0. louvain 0.7.0. markupsafe 2.1.1. matplotlib 3.6.0. mpl_toolkits NA. mpmath 1.3.0. natsort 7.1.1. nbclassic NA. nbformat 5.1.3. numba 0.55.1. numexpr 2.7.3. numpy 1.21.6. nvfuser NA. opt_einsum v3.3.0. packaging 23.1. p",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2556
https://github.com/scverse/scanpy/issues/2556:295,usability,confirm,confirmed,295,"After sc.pp.normalize_total() and log1p() there is no gene expression in UMAP plot; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hi,. I have two different datasets, both with raw counts. After using the function `sc.pp.normalize_total` and `sc.pp.log1p` and plotting the data with UMAP coordinates, there is no gene expression in cells coming from one of the datasets. I did the analysis separately (without concatenating) and the same happens. . I thought that maybe is a problem with the data type, but when I checked this, both anndatas.X were np.float32 and sparse.csr_matrixes ( #1612 ). Also, I made sure the anndata matrix related to the problematic dataset has acceptable values and they are not zeros. Any idea about this problem? ### Minimal code sample. ```python. sc.pp.normalize_total(adata). sc.pp.log1p(adata). ```. ### Error output. _No response_. ### Versions. <details>. ```. anndata 0.9.1. scanpy 1.8.1. sinfo 0.3.4. -----. PIL 9.1.0. anyio NA. astunparse 1.6.3. attr 21.2.0. babel 2.9.1. backcall 0.2.0. brotli NA. certifi 2022.12.07. cffi 1.15.0. charset_normalizer 2.0.12. cloudpickle 2.0.0. cycler 0.10.0. cython_runtime NA. dask 2022.8.1. dateutil 2.8.2. debugpy 1.6.0. decorator 5.0.9. defusedxml 0.7.1. entrypoints 0.4. fastjsonschema NA. fsspec 2022.7.1. google NA. h5py 3.4.0. idna 3.3. igraph 0.9.6. ipykernel 6.4.0. ipython_genutils 0.2.0. jedi 0.18.1. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 3.2.0. jupyter_server 1.11.0. jupyterlab_server 2.8.1. kiwisolver 1.3.2. leidenalg 0.8.7. llvmlite 0.38.0. louvain 0.7.0. markupsafe 2.1.1. matplotlib 3.6.0. mpl_toolkits NA. mpmath 1.3.0. natsort 7.1.1. nbclassic NA. nbformat 5.1.3. numba 0.55.1. numexpr 2.7.3. numpy 1.21.6. nvfuser NA. opt_einsum v3.3.0. packaging 23.1. p",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2556
https://github.com/scverse/scanpy/issues/2556:987,usability,Minim,Minimal,987,"After sc.pp.normalize_total() and log1p() there is no gene expression in UMAP plot; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hi,. I have two different datasets, both with raw counts. After using the function `sc.pp.normalize_total` and `sc.pp.log1p` and plotting the data with UMAP coordinates, there is no gene expression in cells coming from one of the datasets. I did the analysis separately (without concatenating) and the same happens. . I thought that maybe is a problem with the data type, but when I checked this, both anndatas.X were np.float32 and sparse.csr_matrixes ( #1612 ). Also, I made sure the anndata matrix related to the problematic dataset has acceptable values and they are not zeros. Any idea about this problem? ### Minimal code sample. ```python. sc.pp.normalize_total(adata). sc.pp.log1p(adata). ```. ### Error output. _No response_. ### Versions. <details>. ```. anndata 0.9.1. scanpy 1.8.1. sinfo 0.3.4. -----. PIL 9.1.0. anyio NA. astunparse 1.6.3. attr 21.2.0. babel 2.9.1. backcall 0.2.0. brotli NA. certifi 2022.12.07. cffi 1.15.0. charset_normalizer 2.0.12. cloudpickle 2.0.0. cycler 0.10.0. cython_runtime NA. dask 2022.8.1. dateutil 2.8.2. debugpy 1.6.0. decorator 5.0.9. defusedxml 0.7.1. entrypoints 0.4. fastjsonschema NA. fsspec 2022.7.1. google NA. h5py 3.4.0. idna 3.3. igraph 0.9.6. ipykernel 6.4.0. ipython_genutils 0.2.0. jedi 0.18.1. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 3.2.0. jupyter_server 1.11.0. jupyterlab_server 2.8.1. kiwisolver 1.3.2. leidenalg 0.8.7. llvmlite 0.38.0. louvain 0.7.0. markupsafe 2.1.1. matplotlib 3.6.0. mpl_toolkits NA. mpmath 1.3.0. natsort 7.1.1. nbclassic NA. nbformat 5.1.3. numba 0.55.1. numexpr 2.7.3. numpy 1.21.6. nvfuser NA. opt_einsum v3.3.0. packaging 23.1. p",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2556
https://github.com/scverse/scanpy/issues/2556:1078,usability,Error,Error,1078,"t; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hi,. I have two different datasets, both with raw counts. After using the function `sc.pp.normalize_total` and `sc.pp.log1p` and plotting the data with UMAP coordinates, there is no gene expression in cells coming from one of the datasets. I did the analysis separately (without concatenating) and the same happens. . I thought that maybe is a problem with the data type, but when I checked this, both anndatas.X were np.float32 and sparse.csr_matrixes ( #1612 ). Also, I made sure the anndata matrix related to the problematic dataset has acceptable values and they are not zeros. Any idea about this problem? ### Minimal code sample. ```python. sc.pp.normalize_total(adata). sc.pp.log1p(adata). ```. ### Error output. _No response_. ### Versions. <details>. ```. anndata 0.9.1. scanpy 1.8.1. sinfo 0.3.4. -----. PIL 9.1.0. anyio NA. astunparse 1.6.3. attr 21.2.0. babel 2.9.1. backcall 0.2.0. brotli NA. certifi 2022.12.07. cffi 1.15.0. charset_normalizer 2.0.12. cloudpickle 2.0.0. cycler 0.10.0. cython_runtime NA. dask 2022.8.1. dateutil 2.8.2. debugpy 1.6.0. decorator 5.0.9. defusedxml 0.7.1. entrypoints 0.4. fastjsonschema NA. fsspec 2022.7.1. google NA. h5py 3.4.0. idna 3.3. igraph 0.9.6. ipykernel 6.4.0. ipython_genutils 0.2.0. jedi 0.18.1. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 3.2.0. jupyter_server 1.11.0. jupyterlab_server 2.8.1. kiwisolver 1.3.2. leidenalg 0.8.7. llvmlite 0.38.0. louvain 0.7.0. markupsafe 2.1.1. matplotlib 3.6.0. mpl_toolkits NA. mpmath 1.3.0. natsort 7.1.1. nbclassic NA. nbformat 5.1.3. numba 0.55.1. numexpr 2.7.3. numpy 1.21.6. nvfuser NA. opt_einsum v3.3.0. packaging 23.1. pandas 1.5.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. pro",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2556
https://github.com/scverse/scanpy/issues/2556:2406,usability,simpl,simplejson,2406,ode sample. ```python. sc.pp.normalize_total(adata). sc.pp.log1p(adata). ```. ### Error output. _No response_. ### Versions. <details>. ```. anndata 0.9.1. scanpy 1.8.1. sinfo 0.3.4. -----. PIL 9.1.0. anyio NA. astunparse 1.6.3. attr 21.2.0. babel 2.9.1. backcall 0.2.0. brotli NA. certifi 2022.12.07. cffi 1.15.0. charset_normalizer 2.0.12. cloudpickle 2.0.0. cycler 0.10.0. cython_runtime NA. dask 2022.8.1. dateutil 2.8.2. debugpy 1.6.0. decorator 5.0.9. defusedxml 0.7.1. entrypoints 0.4. fastjsonschema NA. fsspec 2022.7.1. google NA. h5py 3.4.0. idna 3.3. igraph 0.9.6. ipykernel 6.4.0. ipython_genutils 0.2.0. jedi 0.18.1. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 3.2.0. jupyter_server 1.11.0. jupyterlab_server 2.8.1. kiwisolver 1.3.2. leidenalg 0.8.7. llvmlite 0.38.0. louvain 0.7.0. markupsafe 2.1.1. matplotlib 3.6.0. mpl_toolkits NA. mpmath 1.3.0. natsort 7.1.1. nbclassic NA. nbformat 5.1.3. numba 0.55.1. numexpr 2.7.3. numpy 1.21.6. nvfuser NA. opt_einsum v3.3.0. packaging 23.1. pandas 1.5.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.31. psutil 5.9.0. ptyprocess 0.7.0. pvectorc NA. pydev_ipython NA. pydevconsole NA. pydevd 2.8.0. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 2.4.7. pyrsistent NA. pytz 2022.2.1. requests 2.28.1. scipy 1.10.1. send2trash NA. setuptools 67.8.0. simplejson 3.17.6. six 1.16.0. sklearn 1.2.2. sniffio 1.2.0. socks 1.7.1. sparse 0.14.0. storemagic NA. sympy 1.12. tables 3.6.1. tblib 1.7.0. terminado 0.12.1. texttable 1.6.4. threadpoolctl 2.2.0. tlz 0.12.0. toolz 0.12.0. torch 2.0.1+cu117. tornado 6.1. tqdm 4.62.2. traitlets 5.1.0. typing_extensions NA. unicodedata2 NA. urllib3 1.26.9. wcwidth 0.2.5. websocket 1.3.2. yaml 6.0. zipp NA. zmq 22.3.0. zstandard 0.18.0. -----. IPython 7.34.0. jupyter_client 7.3.0. jupyter_core 4.10.0. jupyterlab 3.1.11. notebook 6.4.11. -----. Python 3.8.13 | packaged by conda-forge . ```. </details>.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2556
https://github.com/scverse/scanpy/issues/2556:2617,usability,tool,toolz,2617,ode sample. ```python. sc.pp.normalize_total(adata). sc.pp.log1p(adata). ```. ### Error output. _No response_. ### Versions. <details>. ```. anndata 0.9.1. scanpy 1.8.1. sinfo 0.3.4. -----. PIL 9.1.0. anyio NA. astunparse 1.6.3. attr 21.2.0. babel 2.9.1. backcall 0.2.0. brotli NA. certifi 2022.12.07. cffi 1.15.0. charset_normalizer 2.0.12. cloudpickle 2.0.0. cycler 0.10.0. cython_runtime NA. dask 2022.8.1. dateutil 2.8.2. debugpy 1.6.0. decorator 5.0.9. defusedxml 0.7.1. entrypoints 0.4. fastjsonschema NA. fsspec 2022.7.1. google NA. h5py 3.4.0. idna 3.3. igraph 0.9.6. ipykernel 6.4.0. ipython_genutils 0.2.0. jedi 0.18.1. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonschema 3.2.0. jupyter_server 1.11.0. jupyterlab_server 2.8.1. kiwisolver 1.3.2. leidenalg 0.8.7. llvmlite 0.38.0. louvain 0.7.0. markupsafe 2.1.1. matplotlib 3.6.0. mpl_toolkits NA. mpmath 1.3.0. natsort 7.1.1. nbclassic NA. nbformat 5.1.3. numba 0.55.1. numexpr 2.7.3. numpy 1.21.6. nvfuser NA. opt_einsum v3.3.0. packaging 23.1. pandas 1.5.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.31. psutil 5.9.0. ptyprocess 0.7.0. pvectorc NA. pydev_ipython NA. pydevconsole NA. pydevd 2.8.0. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 2.4.7. pyrsistent NA. pytz 2022.2.1. requests 2.28.1. scipy 1.10.1. send2trash NA. setuptools 67.8.0. simplejson 3.17.6. six 1.16.0. sklearn 1.2.2. sniffio 1.2.0. socks 1.7.1. sparse 0.14.0. storemagic NA. sympy 1.12. tables 3.6.1. tblib 1.7.0. terminado 0.12.1. texttable 1.6.4. threadpoolctl 2.2.0. tlz 0.12.0. toolz 0.12.0. torch 2.0.1+cu117. tornado 6.1. tqdm 4.62.2. traitlets 5.1.0. typing_extensions NA. unicodedata2 NA. urllib3 1.26.9. wcwidth 0.2.5. websocket 1.3.2. yaml 6.0. zipp NA. zmq 22.3.0. zstandard 0.18.0. -----. IPython 7.34.0. jupyter_client 7.3.0. jupyter_core 4.10.0. jupyterlab 3.1.11. notebook 6.4.11. -----. Python 3.8.13 | packaged by conda-forge . ```. </details>.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2556
https://github.com/scverse/scanpy/issues/2557:249,integrability,pub,pubulic,249,"How to import GEO dataset?; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Thank you for your excellent software, now I need to analyse pubulic data from GEO, but I do not know how to import them. Can you give me some suggestion? Thanks! ![image](https://github.com/scverse/scanpy/assets/39158528/74e8b4ed-7593-42fa-8f07-6459d33ace10).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2557
https://github.com/scverse/scanpy/issues/2557:100,modifiability,paramet,parameters,100,"How to import GEO dataset?; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Thank you for your excellent software, now I need to analyse pubulic data from GEO, but I do not know how to import them. Can you give me some suggestion? Thanks! ![image](https://github.com/scverse/scanpy/assets/39158528/74e8b4ed-7593-42fa-8f07-6459d33ace10).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2557
https://github.com/scverse/scanpy/issues/2558:102,modifiability,paramet,parameters,102,Why do the image have a red?; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. ![image](https://github.com/scverse/scanpy/assets/39158528/44931b7b-ae5a-4483-9ec9-c76ed7206d72).,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2558
https://github.com/scverse/scanpy/pull/2559:88,deployability,version,version,88,"Restrict files written by tests; Fixes #2555. This one is less important than AnnDatas version of this PR (https://github.com/scverse/anndata/pull/1061), since anndata is worse in doing this.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2559
https://github.com/scverse/scanpy/pull/2559:88,integrability,version,version,88,"Restrict files written by tests; Fixes #2555. This one is less important than AnnDatas version of this PR (https://github.com/scverse/anndata/pull/1061), since anndata is worse in doing this.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2559
https://github.com/scverse/scanpy/pull/2559:88,modifiability,version,version,88,"Restrict files written by tests; Fixes #2555. This one is less important than AnnDatas version of this PR (https://github.com/scverse/anndata/pull/1061), since anndata is worse in doing this.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2559
https://github.com/scverse/scanpy/pull/2559:26,safety,test,tests,26,"Restrict files written by tests; Fixes #2555. This one is less important than AnnDatas version of this PR (https://github.com/scverse/anndata/pull/1061), since anndata is worse in doing this.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2559
https://github.com/scverse/scanpy/pull/2559:26,testability,test,tests,26,"Restrict files written by tests; Fixes #2555. This one is less important than AnnDatas version of this PR (https://github.com/scverse/anndata/pull/1061), since anndata is worse in doing this.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2559
https://github.com/scverse/scanpy/pull/2561:167,deployability,build,build,167,"Back to scanpydoc; TODO:. - [x] check if that git_ref extension is even necessary. *yes*, see e.g. the source button on https://sphinx-book-theme--728.org.readthedocs.build/en/728/. - [x] check if `rtd_github_links` breaks doc caching. *no*, all is good. ### [Rendered](https://icb-scanpy--2561.com.readthedocs.build/en/2561/)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2561
https://github.com/scverse/scanpy/pull/2561:311,deployability,build,build,311,"Back to scanpydoc; TODO:. - [x] check if that git_ref extension is even necessary. *yes*, see e.g. the source button on https://sphinx-book-theme--728.org.readthedocs.build/en/728/. - [x] check if `rtd_github_links` breaks doc caching. *no*, all is good. ### [Rendered](https://icb-scanpy--2561.com.readthedocs.build/en/2561/)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2561
https://github.com/scverse/scanpy/pull/2561:54,modifiability,extens,extension,54,"Back to scanpydoc; TODO:. - [x] check if that git_ref extension is even necessary. *yes*, see e.g. the source button on https://sphinx-book-theme--728.org.readthedocs.build/en/728/. - [x] check if `rtd_github_links` breaks doc caching. *no*, all is good. ### [Rendered](https://icb-scanpy--2561.com.readthedocs.build/en/2561/)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2561
https://github.com/scverse/scanpy/pull/2561:227,performance,cach,caching,227,"Back to scanpydoc; TODO:. - [x] check if that git_ref extension is even necessary. *yes*, see e.g. the source button on https://sphinx-book-theme--728.org.readthedocs.build/en/728/. - [x] check if `rtd_github_links` breaks doc caching. *no*, all is good. ### [Rendered](https://icb-scanpy--2561.com.readthedocs.build/en/2561/)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2561
https://github.com/scverse/scanpy/issues/2562:847,availability,Error,Error,847,"pyproject.toml should refer to `igraph` and not `python-igraph`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've noticed that `pyproject.toml` refers to the `python-igraph` package in the PyPI repository. This name is deprecated; the `igraph` package is currently called [`igraph`](https://pypi.org/project/igraph). The old package name currently works as a redirect (i.e. it brings in `igraph` as its own sub-dependency), but it will not be maintained in the future. Please switch to referring to `igraph` in `pyproject.toml` and not `python-igraph`. ### Minimal code sample. ```python. N/A. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 10.0.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. h5py 3.9.0. joblib 1.3.1. kiwisolver 1.4.4. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. pyparsing 3.0.9. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. sitecustomize NA. six 1.16.0. sklearn 1.3.0. threadpoolctl 3.2.0. -----. Python 3.11.4 (main, Jun 20 2023, 17:23:00) [Clang 14.0.3 (clang-1403.0.22.14.1)]. macOS-13.2.1-arm64-arm-64bit. -----. Session information updated at 2023-07-19 13:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2562
https://github.com/scverse/scanpy/issues/2562:233,deployability,version,version,233,"pyproject.toml should refer to `igraph` and not `python-igraph`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've noticed that `pyproject.toml` refers to the `python-igraph` package in the PyPI repository. This name is deprecated; the `igraph` package is currently called [`igraph`](https://pypi.org/project/igraph). The old package name currently works as a redirect (i.e. it brings in `igraph` as its own sub-dependency), but it will not be maintained in the future. Please switch to referring to `igraph` in `pyproject.toml` and not `python-igraph`. ### Minimal code sample. ```python. N/A. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 10.0.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. h5py 3.9.0. joblib 1.3.1. kiwisolver 1.4.4. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. pyparsing 3.0.9. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. sitecustomize NA. six 1.16.0. sklearn 1.3.0. threadpoolctl 3.2.0. -----. Python 3.11.4 (main, Jun 20 2023, 17:23:00) [Clang 14.0.3 (clang-1403.0.22.14.1)]. macOS-13.2.1-arm64-arm-64bit. -----. Session information updated at 2023-07-19 13:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2562
https://github.com/scverse/scanpy/issues/2562:655,deployability,depend,dependency,655,"pyproject.toml should refer to `igraph` and not `python-igraph`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've noticed that `pyproject.toml` refers to the `python-igraph` package in the PyPI repository. This name is deprecated; the `igraph` package is currently called [`igraph`](https://pypi.org/project/igraph). The old package name currently works as a redirect (i.e. it brings in `igraph` as its own sub-dependency), but it will not be maintained in the future. Please switch to referring to `igraph` in `pyproject.toml` and not `python-igraph`. ### Minimal code sample. ```python. N/A. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 10.0.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. h5py 3.9.0. joblib 1.3.1. kiwisolver 1.4.4. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. pyparsing 3.0.9. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. sitecustomize NA. six 1.16.0. sklearn 1.3.0. threadpoolctl 3.2.0. -----. Python 3.11.4 (main, Jun 20 2023, 17:23:00) [Clang 14.0.3 (clang-1403.0.22.14.1)]. macOS-13.2.1-arm64-arm-64bit. -----. Session information updated at 2023-07-19 13:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2562
https://github.com/scverse/scanpy/issues/2562:880,deployability,Version,Versions,880,"pyproject.toml should refer to `igraph` and not `python-igraph`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've noticed that `pyproject.toml` refers to the `python-igraph` package in the PyPI repository. This name is deprecated; the `igraph` package is currently called [`igraph`](https://pypi.org/project/igraph). The old package name currently works as a redirect (i.e. it brings in `igraph` as its own sub-dependency), but it will not be maintained in the future. Please switch to referring to `igraph` in `pyproject.toml` and not `python-igraph`. ### Minimal code sample. ```python. N/A. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 10.0.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. h5py 3.9.0. joblib 1.3.1. kiwisolver 1.4.4. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. pyparsing 3.0.9. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. sitecustomize NA. six 1.16.0. sklearn 1.3.0. threadpoolctl 3.2.0. -----. Python 3.11.4 (main, Jun 20 2023, 17:23:00) [Clang 14.0.3 (clang-1403.0.22.14.1)]. macOS-13.2.1-arm64-arm-64bit. -----. Session information updated at 2023-07-19 13:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2562
https://github.com/scverse/scanpy/issues/2562:1457,deployability,updat,updated,1457,"pyproject.toml should refer to `igraph` and not `python-igraph`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've noticed that `pyproject.toml` refers to the `python-igraph` package in the PyPI repository. This name is deprecated; the `igraph` package is currently called [`igraph`](https://pypi.org/project/igraph). The old package name currently works as a redirect (i.e. it brings in `igraph` as its own sub-dependency), but it will not be maintained in the future. Please switch to referring to `igraph` in `pyproject.toml` and not `python-igraph`. ### Minimal code sample. ```python. N/A. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 10.0.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. h5py 3.9.0. joblib 1.3.1. kiwisolver 1.4.4. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. pyparsing 3.0.9. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. sitecustomize NA. six 1.16.0. sklearn 1.3.0. threadpoolctl 3.2.0. -----. Python 3.11.4 (main, Jun 20 2023, 17:23:00) [Clang 14.0.3 (clang-1403.0.22.14.1)]. macOS-13.2.1-arm64-arm-64bit. -----. Session information updated at 2023-07-19 13:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2562
https://github.com/scverse/scanpy/issues/2562:499,energy efficiency,current,currently,499,"pyproject.toml should refer to `igraph` and not `python-igraph`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've noticed that `pyproject.toml` refers to the `python-igraph` package in the PyPI repository. This name is deprecated; the `igraph` package is currently called [`igraph`](https://pypi.org/project/igraph). The old package name currently works as a redirect (i.e. it brings in `igraph` as its own sub-dependency), but it will not be maintained in the future. Please switch to referring to `igraph` in `pyproject.toml` and not `python-igraph`. ### Minimal code sample. ```python. N/A. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 10.0.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. h5py 3.9.0. joblib 1.3.1. kiwisolver 1.4.4. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. pyparsing 3.0.9. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. sitecustomize NA. six 1.16.0. sklearn 1.3.0. threadpoolctl 3.2.0. -----. Python 3.11.4 (main, Jun 20 2023, 17:23:00) [Clang 14.0.3 (clang-1403.0.22.14.1)]. macOS-13.2.1-arm64-arm-64bit. -----. Session information updated at 2023-07-19 13:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2562
https://github.com/scverse/scanpy/issues/2562:582,energy efficiency,current,currently,582,"pyproject.toml should refer to `igraph` and not `python-igraph`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've noticed that `pyproject.toml` refers to the `python-igraph` package in the PyPI repository. This name is deprecated; the `igraph` package is currently called [`igraph`](https://pypi.org/project/igraph). The old package name currently works as a redirect (i.e. it brings in `igraph` as its own sub-dependency), but it will not be maintained in the future. Please switch to referring to `igraph` in `pyproject.toml` and not `python-igraph`. ### Minimal code sample. ```python. N/A. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 10.0.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. h5py 3.9.0. joblib 1.3.1. kiwisolver 1.4.4. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. pyparsing 3.0.9. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. sitecustomize NA. six 1.16.0. sklearn 1.3.0. threadpoolctl 3.2.0. -----. Python 3.11.4 (main, Jun 20 2023, 17:23:00) [Clang 14.0.3 (clang-1403.0.22.14.1)]. macOS-13.2.1-arm64-arm-64bit. -----. Session information updated at 2023-07-19 13:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2562
https://github.com/scverse/scanpy/issues/2562:233,integrability,version,version,233,"pyproject.toml should refer to `igraph` and not `python-igraph`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've noticed that `pyproject.toml` refers to the `python-igraph` package in the PyPI repository. This name is deprecated; the `igraph` package is currently called [`igraph`](https://pypi.org/project/igraph). The old package name currently works as a redirect (i.e. it brings in `igraph` as its own sub-dependency), but it will not be maintained in the future. Please switch to referring to `igraph` in `pyproject.toml` and not `python-igraph`. ### Minimal code sample. ```python. N/A. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 10.0.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. h5py 3.9.0. joblib 1.3.1. kiwisolver 1.4.4. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. pyparsing 3.0.9. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. sitecustomize NA. six 1.16.0. sklearn 1.3.0. threadpoolctl 3.2.0. -----. Python 3.11.4 (main, Jun 20 2023, 17:23:00) [Clang 14.0.3 (clang-1403.0.22.14.1)]. macOS-13.2.1-arm64-arm-64bit. -----. Session information updated at 2023-07-19 13:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2562
https://github.com/scverse/scanpy/issues/2562:438,integrability,repositor,repository,438,"pyproject.toml should refer to `igraph` and not `python-igraph`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've noticed that `pyproject.toml` refers to the `python-igraph` package in the PyPI repository. This name is deprecated; the `igraph` package is currently called [`igraph`](https://pypi.org/project/igraph). The old package name currently works as a redirect (i.e. it brings in `igraph` as its own sub-dependency), but it will not be maintained in the future. Please switch to referring to `igraph` in `pyproject.toml` and not `python-igraph`. ### Minimal code sample. ```python. N/A. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 10.0.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. h5py 3.9.0. joblib 1.3.1. kiwisolver 1.4.4. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. pyparsing 3.0.9. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. sitecustomize NA. six 1.16.0. sklearn 1.3.0. threadpoolctl 3.2.0. -----. Python 3.11.4 (main, Jun 20 2023, 17:23:00) [Clang 14.0.3 (clang-1403.0.22.14.1)]. macOS-13.2.1-arm64-arm-64bit. -----. Session information updated at 2023-07-19 13:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2562
https://github.com/scverse/scanpy/issues/2562:651,integrability,sub,sub-dependency,651,"pyproject.toml should refer to `igraph` and not `python-igraph`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've noticed that `pyproject.toml` refers to the `python-igraph` package in the PyPI repository. This name is deprecated; the `igraph` package is currently called [`igraph`](https://pypi.org/project/igraph). The old package name currently works as a redirect (i.e. it brings in `igraph` as its own sub-dependency), but it will not be maintained in the future. Please switch to referring to `igraph` in `pyproject.toml` and not `python-igraph`. ### Minimal code sample. ```python. N/A. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 10.0.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. h5py 3.9.0. joblib 1.3.1. kiwisolver 1.4.4. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. pyparsing 3.0.9. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. sitecustomize NA. six 1.16.0. sklearn 1.3.0. threadpoolctl 3.2.0. -----. Python 3.11.4 (main, Jun 20 2023, 17:23:00) [Clang 14.0.3 (clang-1403.0.22.14.1)]. macOS-13.2.1-arm64-arm-64bit. -----. Session information updated at 2023-07-19 13:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2562
https://github.com/scverse/scanpy/issues/2562:880,integrability,Version,Versions,880,"pyproject.toml should refer to `igraph` and not `python-igraph`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've noticed that `pyproject.toml` refers to the `python-igraph` package in the PyPI repository. This name is deprecated; the `igraph` package is currently called [`igraph`](https://pypi.org/project/igraph). The old package name currently works as a redirect (i.e. it brings in `igraph` as its own sub-dependency), but it will not be maintained in the future. Please switch to referring to `igraph` in `pyproject.toml` and not `python-igraph`. ### Minimal code sample. ```python. N/A. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 10.0.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. h5py 3.9.0. joblib 1.3.1. kiwisolver 1.4.4. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. pyparsing 3.0.9. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. sitecustomize NA. six 1.16.0. sklearn 1.3.0. threadpoolctl 3.2.0. -----. Python 3.11.4 (main, Jun 20 2023, 17:23:00) [Clang 14.0.3 (clang-1403.0.22.14.1)]. macOS-13.2.1-arm64-arm-64bit. -----. Session information updated at 2023-07-19 13:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2562
https://github.com/scverse/scanpy/issues/2562:438,interoperability,repositor,repository,438,"pyproject.toml should refer to `igraph` and not `python-igraph`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've noticed that `pyproject.toml` refers to the `python-igraph` package in the PyPI repository. This name is deprecated; the `igraph` package is currently called [`igraph`](https://pypi.org/project/igraph). The old package name currently works as a redirect (i.e. it brings in `igraph` as its own sub-dependency), but it will not be maintained in the future. Please switch to referring to `igraph` in `pyproject.toml` and not `python-igraph`. ### Minimal code sample. ```python. N/A. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 10.0.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. h5py 3.9.0. joblib 1.3.1. kiwisolver 1.4.4. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. pyparsing 3.0.9. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. sitecustomize NA. six 1.16.0. sklearn 1.3.0. threadpoolctl 3.2.0. -----. Python 3.11.4 (main, Jun 20 2023, 17:23:00) [Clang 14.0.3 (clang-1403.0.22.14.1)]. macOS-13.2.1-arm64-arm-64bit. -----. Session information updated at 2023-07-19 13:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2562
https://github.com/scverse/scanpy/issues/2562:233,modifiability,version,version,233,"pyproject.toml should refer to `igraph` and not `python-igraph`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've noticed that `pyproject.toml` refers to the `python-igraph` package in the PyPI repository. This name is deprecated; the `igraph` package is currently called [`igraph`](https://pypi.org/project/igraph). The old package name currently works as a redirect (i.e. it brings in `igraph` as its own sub-dependency), but it will not be maintained in the future. Please switch to referring to `igraph` in `pyproject.toml` and not `python-igraph`. ### Minimal code sample. ```python. N/A. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 10.0.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. h5py 3.9.0. joblib 1.3.1. kiwisolver 1.4.4. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. pyparsing 3.0.9. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. sitecustomize NA. six 1.16.0. sklearn 1.3.0. threadpoolctl 3.2.0. -----. Python 3.11.4 (main, Jun 20 2023, 17:23:00) [Clang 14.0.3 (clang-1403.0.22.14.1)]. macOS-13.2.1-arm64-arm-64bit. -----. Session information updated at 2023-07-19 13:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2562
https://github.com/scverse/scanpy/issues/2562:418,modifiability,pac,package,418,"pyproject.toml should refer to `igraph` and not `python-igraph`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've noticed that `pyproject.toml` refers to the `python-igraph` package in the PyPI repository. This name is deprecated; the `igraph` package is currently called [`igraph`](https://pypi.org/project/igraph). The old package name currently works as a redirect (i.e. it brings in `igraph` as its own sub-dependency), but it will not be maintained in the future. Please switch to referring to `igraph` in `pyproject.toml` and not `python-igraph`. ### Minimal code sample. ```python. N/A. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 10.0.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. h5py 3.9.0. joblib 1.3.1. kiwisolver 1.4.4. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. pyparsing 3.0.9. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. sitecustomize NA. six 1.16.0. sklearn 1.3.0. threadpoolctl 3.2.0. -----. Python 3.11.4 (main, Jun 20 2023, 17:23:00) [Clang 14.0.3 (clang-1403.0.22.14.1)]. macOS-13.2.1-arm64-arm-64bit. -----. Session information updated at 2023-07-19 13:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2562
https://github.com/scverse/scanpy/issues/2562:488,modifiability,pac,package,488,"pyproject.toml should refer to `igraph` and not `python-igraph`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've noticed that `pyproject.toml` refers to the `python-igraph` package in the PyPI repository. This name is deprecated; the `igraph` package is currently called [`igraph`](https://pypi.org/project/igraph). The old package name currently works as a redirect (i.e. it brings in `igraph` as its own sub-dependency), but it will not be maintained in the future. Please switch to referring to `igraph` in `pyproject.toml` and not `python-igraph`. ### Minimal code sample. ```python. N/A. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 10.0.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. h5py 3.9.0. joblib 1.3.1. kiwisolver 1.4.4. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. pyparsing 3.0.9. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. sitecustomize NA. six 1.16.0. sklearn 1.3.0. threadpoolctl 3.2.0. -----. Python 3.11.4 (main, Jun 20 2023, 17:23:00) [Clang 14.0.3 (clang-1403.0.22.14.1)]. macOS-13.2.1-arm64-arm-64bit. -----. Session information updated at 2023-07-19 13:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2562
https://github.com/scverse/scanpy/issues/2562:569,modifiability,pac,package,569,"pyproject.toml should refer to `igraph` and not `python-igraph`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've noticed that `pyproject.toml` refers to the `python-igraph` package in the PyPI repository. This name is deprecated; the `igraph` package is currently called [`igraph`](https://pypi.org/project/igraph). The old package name currently works as a redirect (i.e. it brings in `igraph` as its own sub-dependency), but it will not be maintained in the future. Please switch to referring to `igraph` in `pyproject.toml` and not `python-igraph`. ### Minimal code sample. ```python. N/A. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 10.0.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. h5py 3.9.0. joblib 1.3.1. kiwisolver 1.4.4. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. pyparsing 3.0.9. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. sitecustomize NA. six 1.16.0. sklearn 1.3.0. threadpoolctl 3.2.0. -----. Python 3.11.4 (main, Jun 20 2023, 17:23:00) [Clang 14.0.3 (clang-1403.0.22.14.1)]. macOS-13.2.1-arm64-arm-64bit. -----. Session information updated at 2023-07-19 13:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2562
https://github.com/scverse/scanpy/issues/2562:655,modifiability,depend,dependency,655,"pyproject.toml should refer to `igraph` and not `python-igraph`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've noticed that `pyproject.toml` refers to the `python-igraph` package in the PyPI repository. This name is deprecated; the `igraph` package is currently called [`igraph`](https://pypi.org/project/igraph). The old package name currently works as a redirect (i.e. it brings in `igraph` as its own sub-dependency), but it will not be maintained in the future. Please switch to referring to `igraph` in `pyproject.toml` and not `python-igraph`. ### Minimal code sample. ```python. N/A. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 10.0.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. h5py 3.9.0. joblib 1.3.1. kiwisolver 1.4.4. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. pyparsing 3.0.9. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. sitecustomize NA. six 1.16.0. sklearn 1.3.0. threadpoolctl 3.2.0. -----. Python 3.11.4 (main, Jun 20 2023, 17:23:00) [Clang 14.0.3 (clang-1403.0.22.14.1)]. macOS-13.2.1-arm64-arm-64bit. -----. Session information updated at 2023-07-19 13:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2562
https://github.com/scverse/scanpy/issues/2562:687,modifiability,maintain,maintained,687,"pyproject.toml should refer to `igraph` and not `python-igraph`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've noticed that `pyproject.toml` refers to the `python-igraph` package in the PyPI repository. This name is deprecated; the `igraph` package is currently called [`igraph`](https://pypi.org/project/igraph). The old package name currently works as a redirect (i.e. it brings in `igraph` as its own sub-dependency), but it will not be maintained in the future. Please switch to referring to `igraph` in `pyproject.toml` and not `python-igraph`. ### Minimal code sample. ```python. N/A. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 10.0.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. h5py 3.9.0. joblib 1.3.1. kiwisolver 1.4.4. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. pyparsing 3.0.9. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. sitecustomize NA. six 1.16.0. sklearn 1.3.0. threadpoolctl 3.2.0. -----. Python 3.11.4 (main, Jun 20 2023, 17:23:00) [Clang 14.0.3 (clang-1403.0.22.14.1)]. macOS-13.2.1-arm64-arm-64bit. -----. Session information updated at 2023-07-19 13:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2562
https://github.com/scverse/scanpy/issues/2562:880,modifiability,Version,Versions,880,"pyproject.toml should refer to `igraph` and not `python-igraph`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've noticed that `pyproject.toml` refers to the `python-igraph` package in the PyPI repository. This name is deprecated; the `igraph` package is currently called [`igraph`](https://pypi.org/project/igraph). The old package name currently works as a redirect (i.e. it brings in `igraph` as its own sub-dependency), but it will not be maintained in the future. Please switch to referring to `igraph` in `pyproject.toml` and not `python-igraph`. ### Minimal code sample. ```python. N/A. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 10.0.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. h5py 3.9.0. joblib 1.3.1. kiwisolver 1.4.4. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. pyparsing 3.0.9. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. sitecustomize NA. six 1.16.0. sklearn 1.3.0. threadpoolctl 3.2.0. -----. Python 3.11.4 (main, Jun 20 2023, 17:23:00) [Clang 14.0.3 (clang-1403.0.22.14.1)]. macOS-13.2.1-arm64-arm-64bit. -----. Session information updated at 2023-07-19 13:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2562
https://github.com/scverse/scanpy/issues/2562:1150,modifiability,pac,packaging,1150,"pyproject.toml should refer to `igraph` and not `python-igraph`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've noticed that `pyproject.toml` refers to the `python-igraph` package in the PyPI repository. This name is deprecated; the `igraph` package is currently called [`igraph`](https://pypi.org/project/igraph). The old package name currently works as a redirect (i.e. it brings in `igraph` as its own sub-dependency), but it will not be maintained in the future. Please switch to referring to `igraph` in `pyproject.toml` and not `python-igraph`. ### Minimal code sample. ```python. N/A. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 10.0.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. h5py 3.9.0. joblib 1.3.1. kiwisolver 1.4.4. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. pyparsing 3.0.9. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. sitecustomize NA. six 1.16.0. sklearn 1.3.0. threadpoolctl 3.2.0. -----. Python 3.11.4 (main, Jun 20 2023, 17:23:00) [Clang 14.0.3 (clang-1403.0.22.14.1)]. macOS-13.2.1-arm64-arm-64bit. -----. Session information updated at 2023-07-19 13:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2562
https://github.com/scverse/scanpy/issues/2562:847,performance,Error,Error,847,"pyproject.toml should refer to `igraph` and not `python-igraph`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've noticed that `pyproject.toml` refers to the `python-igraph` package in the PyPI repository. This name is deprecated; the `igraph` package is currently called [`igraph`](https://pypi.org/project/igraph). The old package name currently works as a redirect (i.e. it brings in `igraph` as its own sub-dependency), but it will not be maintained in the future. Please switch to referring to `igraph` in `pyproject.toml` and not `python-igraph`. ### Minimal code sample. ```python. N/A. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 10.0.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. h5py 3.9.0. joblib 1.3.1. kiwisolver 1.4.4. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. pyparsing 3.0.9. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. sitecustomize NA. six 1.16.0. sklearn 1.3.0. threadpoolctl 3.2.0. -----. Python 3.11.4 (main, Jun 20 2023, 17:23:00) [Clang 14.0.3 (clang-1403.0.22.14.1)]. macOS-13.2.1-arm64-arm-64bit. -----. Session information updated at 2023-07-19 13:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2562
https://github.com/scverse/scanpy/issues/2562:655,safety,depend,dependency,655,"pyproject.toml should refer to `igraph` and not `python-igraph`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've noticed that `pyproject.toml` refers to the `python-igraph` package in the PyPI repository. This name is deprecated; the `igraph` package is currently called [`igraph`](https://pypi.org/project/igraph). The old package name currently works as a redirect (i.e. it brings in `igraph` as its own sub-dependency), but it will not be maintained in the future. Please switch to referring to `igraph` in `pyproject.toml` and not `python-igraph`. ### Minimal code sample. ```python. N/A. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 10.0.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. h5py 3.9.0. joblib 1.3.1. kiwisolver 1.4.4. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. pyparsing 3.0.9. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. sitecustomize NA. six 1.16.0. sklearn 1.3.0. threadpoolctl 3.2.0. -----. Python 3.11.4 (main, Jun 20 2023, 17:23:00) [Clang 14.0.3 (clang-1403.0.22.14.1)]. macOS-13.2.1-arm64-arm-64bit. -----. Session information updated at 2023-07-19 13:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2562
https://github.com/scverse/scanpy/issues/2562:687,safety,maintain,maintained,687,"pyproject.toml should refer to `igraph` and not `python-igraph`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've noticed that `pyproject.toml` refers to the `python-igraph` package in the PyPI repository. This name is deprecated; the `igraph` package is currently called [`igraph`](https://pypi.org/project/igraph). The old package name currently works as a redirect (i.e. it brings in `igraph` as its own sub-dependency), but it will not be maintained in the future. Please switch to referring to `igraph` in `pyproject.toml` and not `python-igraph`. ### Minimal code sample. ```python. N/A. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 10.0.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. h5py 3.9.0. joblib 1.3.1. kiwisolver 1.4.4. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. pyparsing 3.0.9. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. sitecustomize NA. six 1.16.0. sklearn 1.3.0. threadpoolctl 3.2.0. -----. Python 3.11.4 (main, Jun 20 2023, 17:23:00) [Clang 14.0.3 (clang-1403.0.22.14.1)]. macOS-13.2.1-arm64-arm-64bit. -----. Session information updated at 2023-07-19 13:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2562
https://github.com/scverse/scanpy/issues/2562:847,safety,Error,Error,847,"pyproject.toml should refer to `igraph` and not `python-igraph`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've noticed that `pyproject.toml` refers to the `python-igraph` package in the PyPI repository. This name is deprecated; the `igraph` package is currently called [`igraph`](https://pypi.org/project/igraph). The old package name currently works as a redirect (i.e. it brings in `igraph` as its own sub-dependency), but it will not be maintained in the future. Please switch to referring to `igraph` in `pyproject.toml` and not `python-igraph`. ### Minimal code sample. ```python. N/A. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 10.0.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. h5py 3.9.0. joblib 1.3.1. kiwisolver 1.4.4. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. pyparsing 3.0.9. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. sitecustomize NA. six 1.16.0. sklearn 1.3.0. threadpoolctl 3.2.0. -----. Python 3.11.4 (main, Jun 20 2023, 17:23:00) [Clang 14.0.3 (clang-1403.0.22.14.1)]. macOS-13.2.1-arm64-arm-64bit. -----. Session information updated at 2023-07-19 13:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2562
https://github.com/scverse/scanpy/issues/2562:1457,safety,updat,updated,1457,"pyproject.toml should refer to `igraph` and not `python-igraph`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've noticed that `pyproject.toml` refers to the `python-igraph` package in the PyPI repository. This name is deprecated; the `igraph` package is currently called [`igraph`](https://pypi.org/project/igraph). The old package name currently works as a redirect (i.e. it brings in `igraph` as its own sub-dependency), but it will not be maintained in the future. Please switch to referring to `igraph` in `pyproject.toml` and not `python-igraph`. ### Minimal code sample. ```python. N/A. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 10.0.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. h5py 3.9.0. joblib 1.3.1. kiwisolver 1.4.4. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. pyparsing 3.0.9. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. sitecustomize NA. six 1.16.0. sklearn 1.3.0. threadpoolctl 3.2.0. -----. Python 3.11.4 (main, Jun 20 2023, 17:23:00) [Clang 14.0.3 (clang-1403.0.22.14.1)]. macOS-13.2.1-arm64-arm-64bit. -----. Session information updated at 2023-07-19 13:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2562
https://github.com/scverse/scanpy/issues/2562:1437,security,Session,Session,1437,"pyproject.toml should refer to `igraph` and not `python-igraph`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've noticed that `pyproject.toml` refers to the `python-igraph` package in the PyPI repository. This name is deprecated; the `igraph` package is currently called [`igraph`](https://pypi.org/project/igraph). The old package name currently works as a redirect (i.e. it brings in `igraph` as its own sub-dependency), but it will not be maintained in the future. Please switch to referring to `igraph` in `pyproject.toml` and not `python-igraph`. ### Minimal code sample. ```python. N/A. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 10.0.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. h5py 3.9.0. joblib 1.3.1. kiwisolver 1.4.4. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. pyparsing 3.0.9. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. sitecustomize NA. six 1.16.0. sklearn 1.3.0. threadpoolctl 3.2.0. -----. Python 3.11.4 (main, Jun 20 2023, 17:23:00) [Clang 14.0.3 (clang-1403.0.22.14.1)]. macOS-13.2.1-arm64-arm-64bit. -----. Session information updated at 2023-07-19 13:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2562
https://github.com/scverse/scanpy/issues/2562:1457,security,updat,updated,1457,"pyproject.toml should refer to `igraph` and not `python-igraph`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've noticed that `pyproject.toml` refers to the `python-igraph` package in the PyPI repository. This name is deprecated; the `igraph` package is currently called [`igraph`](https://pypi.org/project/igraph). The old package name currently works as a redirect (i.e. it brings in `igraph` as its own sub-dependency), but it will not be maintained in the future. Please switch to referring to `igraph` in `pyproject.toml` and not `python-igraph`. ### Minimal code sample. ```python. N/A. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 10.0.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. h5py 3.9.0. joblib 1.3.1. kiwisolver 1.4.4. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. pyparsing 3.0.9. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. sitecustomize NA. six 1.16.0. sklearn 1.3.0. threadpoolctl 3.2.0. -----. Python 3.11.4 (main, Jun 20 2023, 17:23:00) [Clang 14.0.3 (clang-1403.0.22.14.1)]. macOS-13.2.1-arm64-arm-64bit. -----. Session information updated at 2023-07-19 13:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2562
https://github.com/scverse/scanpy/issues/2562:655,testability,depend,dependency,655,"pyproject.toml should refer to `igraph` and not `python-igraph`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've noticed that `pyproject.toml` refers to the `python-igraph` package in the PyPI repository. This name is deprecated; the `igraph` package is currently called [`igraph`](https://pypi.org/project/igraph). The old package name currently works as a redirect (i.e. it brings in `igraph` as its own sub-dependency), but it will not be maintained in the future. Please switch to referring to `igraph` in `pyproject.toml` and not `python-igraph`. ### Minimal code sample. ```python. N/A. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 10.0.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. h5py 3.9.0. joblib 1.3.1. kiwisolver 1.4.4. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. pyparsing 3.0.9. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. sitecustomize NA. six 1.16.0. sklearn 1.3.0. threadpoolctl 3.2.0. -----. Python 3.11.4 (main, Jun 20 2023, 17:23:00) [Clang 14.0.3 (clang-1403.0.22.14.1)]. macOS-13.2.1-arm64-arm-64bit. -----. Session information updated at 2023-07-19 13:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2562
https://github.com/scverse/scanpy/issues/2562:193,usability,confirm,confirmed,193,"pyproject.toml should refer to `igraph` and not `python-igraph`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've noticed that `pyproject.toml` refers to the `python-igraph` package in the PyPI repository. This name is deprecated; the `igraph` package is currently called [`igraph`](https://pypi.org/project/igraph). The old package name currently works as a redirect (i.e. it brings in `igraph` as its own sub-dependency), but it will not be maintained in the future. Please switch to referring to `igraph` in `pyproject.toml` and not `python-igraph`. ### Minimal code sample. ```python. N/A. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 10.0.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. h5py 3.9.0. joblib 1.3.1. kiwisolver 1.4.4. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. pyparsing 3.0.9. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. sitecustomize NA. six 1.16.0. sklearn 1.3.0. threadpoolctl 3.2.0. -----. Python 3.11.4 (main, Jun 20 2023, 17:23:00) [Clang 14.0.3 (clang-1403.0.22.14.1)]. macOS-13.2.1-arm64-arm-64bit. -----. Session information updated at 2023-07-19 13:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2562
https://github.com/scverse/scanpy/issues/2562:276,usability,confirm,confirmed,276,"pyproject.toml should refer to `igraph` and not `python-igraph`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've noticed that `pyproject.toml` refers to the `python-igraph` package in the PyPI repository. This name is deprecated; the `igraph` package is currently called [`igraph`](https://pypi.org/project/igraph). The old package name currently works as a redirect (i.e. it brings in `igraph` as its own sub-dependency), but it will not be maintained in the future. Please switch to referring to `igraph` in `pyproject.toml` and not `python-igraph`. ### Minimal code sample. ```python. N/A. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 10.0.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. h5py 3.9.0. joblib 1.3.1. kiwisolver 1.4.4. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. pyparsing 3.0.9. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. sitecustomize NA. six 1.16.0. sklearn 1.3.0. threadpoolctl 3.2.0. -----. Python 3.11.4 (main, Jun 20 2023, 17:23:00) [Clang 14.0.3 (clang-1403.0.22.14.1)]. macOS-13.2.1-arm64-arm-64bit. -----. Session information updated at 2023-07-19 13:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2562
https://github.com/scverse/scanpy/issues/2562:801,usability,Minim,Minimal,801,"pyproject.toml should refer to `igraph` and not `python-igraph`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've noticed that `pyproject.toml` refers to the `python-igraph` package in the PyPI repository. This name is deprecated; the `igraph` package is currently called [`igraph`](https://pypi.org/project/igraph). The old package name currently works as a redirect (i.e. it brings in `igraph` as its own sub-dependency), but it will not be maintained in the future. Please switch to referring to `igraph` in `pyproject.toml` and not `python-igraph`. ### Minimal code sample. ```python. N/A. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 10.0.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. h5py 3.9.0. joblib 1.3.1. kiwisolver 1.4.4. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. pyparsing 3.0.9. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. sitecustomize NA. six 1.16.0. sklearn 1.3.0. threadpoolctl 3.2.0. -----. Python 3.11.4 (main, Jun 20 2023, 17:23:00) [Clang 14.0.3 (clang-1403.0.22.14.1)]. macOS-13.2.1-arm64-arm-64bit. -----. Session information updated at 2023-07-19 13:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2562
https://github.com/scverse/scanpy/issues/2562:847,usability,Error,Error,847,"pyproject.toml should refer to `igraph` and not `python-igraph`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've noticed that `pyproject.toml` refers to the `python-igraph` package in the PyPI repository. This name is deprecated; the `igraph` package is currently called [`igraph`](https://pypi.org/project/igraph). The old package name currently works as a redirect (i.e. it brings in `igraph` as its own sub-dependency), but it will not be maintained in the future. Please switch to referring to `igraph` in `pyproject.toml` and not `python-igraph`. ### Minimal code sample. ```python. N/A. ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 10.0.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. h5py 3.9.0. joblib 1.3.1. kiwisolver 1.4.4. llvmlite 0.40.1. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numpy 1.24.4. packaging 23.1. pandas 2.0.3. pyparsing 3.0.9. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. sitecustomize NA. six 1.16.0. sklearn 1.3.0. threadpoolctl 3.2.0. -----. Python 3.11.4 (main, Jun 20 2023, 17:23:00) [Clang 14.0.3 (clang-1403.0.22.14.1)]. macOS-13.2.1-arm64-arm-64bit. -----. Session information updated at 2023-07-19 13:34. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2562
https://github.com/scverse/scanpy/pull/2563:219,availability,Ping,Pinging,219,"Dask PCA support; Hi,. Just wanted to start the PR. Passes the tests except one. Also need to deal with solver names since they don't correspond to anything dask uses. Also refactored where the DaskArray mock class is. Pinging @ivirshup",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2563
https://github.com/scverse/scanpy/pull/2563:173,modifiability,refact,refactored,173,"Dask PCA support; Hi,. Just wanted to start the PR. Passes the tests except one. Also need to deal with solver names since they don't correspond to anything dask uses. Also refactored where the DaskArray mock class is. Pinging @ivirshup",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2563
https://github.com/scverse/scanpy/pull/2563:173,performance,refactor,refactored,173,"Dask PCA support; Hi,. Just wanted to start the PR. Passes the tests except one. Also need to deal with solver names since they don't correspond to anything dask uses. Also refactored where the DaskArray mock class is. Pinging @ivirshup",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2563
https://github.com/scverse/scanpy/pull/2563:63,safety,test,tests,63,"Dask PCA support; Hi,. Just wanted to start the PR. Passes the tests except one. Also need to deal with solver names since they don't correspond to anything dask uses. Also refactored where the DaskArray mock class is. Pinging @ivirshup",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2563
https://github.com/scverse/scanpy/pull/2563:69,safety,except,except,69,"Dask PCA support; Hi,. Just wanted to start the PR. Passes the tests except one. Also need to deal with solver names since they don't correspond to anything dask uses. Also refactored where the DaskArray mock class is. Pinging @ivirshup",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2563
https://github.com/scverse/scanpy/pull/2563:63,testability,test,tests,63,"Dask PCA support; Hi,. Just wanted to start the PR. Passes the tests except one. Also need to deal with solver names since they don't correspond to anything dask uses. Also refactored where the DaskArray mock class is. Pinging @ivirshup",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2563
https://github.com/scverse/scanpy/pull/2563:204,testability,mock,mock,204,"Dask PCA support; Hi,. Just wanted to start the PR. Passes the tests except one. Also need to deal with solver names since they don't correspond to anything dask uses. Also refactored where the DaskArray mock class is. Pinging @ivirshup",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2563
https://github.com/scverse/scanpy/pull/2563:9,usability,support,support,9,"Dask PCA support; Hi,. Just wanted to start the PR. Passes the tests except one. Also need to deal with solver names since they don't correspond to anything dask uses. Also refactored where the DaskArray mock class is. Pinging @ivirshup",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2563
https://github.com/scverse/scanpy/issues/2564:479,availability,error,error,479,"import fails due to deprecated is_categorical method it pandas; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I installed scanpy as the first package in a brand new conda environment. This worked fine but when importing scanpy I get the error below. It sounds like this is because as of pandas 2.0 `is_categorical` is deprecated: https://stackoverflow.com/questions/76234312/importerror-cannot-import-name-is-categorical-from-pandas-api-types. This issue is also fixed when I downgrade pandas to 1.5.3. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). Untitled-1.ipynb Cell 20 in <cell line: 1>(). ----> 1 import scanpy as sc. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/__init__.py:18, in <module>. 16 from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:718,availability,down,downgrade,718,"import fails due to deprecated is_categorical method it pandas; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I installed scanpy as the first package in a brand new conda environment. This worked fine but when importing scanpy I get the error below. It sounds like this is because as of pandas 2.0 `is_categorical` is deprecated: https://stackoverflow.com/questions/76234312/importerror-cannot-import-name-is-categorical-from-pandas-api-types. This issue is also fixed when I downgrade pandas to 1.5.3. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). Untitled-1.ipynb Cell 20 in <cell line: 1>(). ----> 1 import scanpy as sc. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/__init__.py:18, in <module>. 16 from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:811,availability,Error,Error,811,"import fails due to deprecated is_categorical method it pandas; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I installed scanpy as the first package in a brand new conda environment. This worked fine but when importing scanpy I get the error below. It sounds like this is because as of pandas 2.0 `is_categorical` is deprecated: https://stackoverflow.com/questions/76234312/importerror-cannot-import-name-is-categorical-from-pandas-api-types. This issue is also fixed when I downgrade pandas to 1.5.3. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). Untitled-1.ipynb Cell 20 in <cell line: 1>(). ----> 1 import scanpy as sc. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/__init__.py:18, in <module>. 16 from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:7,deployability,fail,fails,7,"import fails due to deprecated is_categorical method it pandas; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I installed scanpy as the first package in a brand new conda environment. This worked fine but when importing scanpy I get the error below. It sounds like this is because as of pandas 2.0 `is_categorical` is deprecated: https://stackoverflow.com/questions/76234312/importerror-cannot-import-name-is-categorical-from-pandas-api-types. This issue is also fixed when I downgrade pandas to 1.5.3. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). Untitled-1.ipynb Cell 20 in <cell line: 1>(). ----> 1 import scanpy as sc. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/__init__.py:18, in <module>. 16 from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:232,deployability,version,version,232,"import fails due to deprecated is_categorical method it pandas; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I installed scanpy as the first package in a brand new conda environment. This worked fine but when importing scanpy I get the error below. It sounds like this is because as of pandas 2.0 `is_categorical` is deprecated: https://stackoverflow.com/questions/76234312/importerror-cannot-import-name-is-categorical-from-pandas-api-types. This issue is also fixed when I downgrade pandas to 1.5.3. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). Untitled-1.ipynb Cell 20 in <cell line: 1>(). ----> 1 import scanpy as sc. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/__init__.py:18, in <module>. 16 from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:354,deployability,instal,installed,354,"import fails due to deprecated is_categorical method it pandas; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I installed scanpy as the first package in a brand new conda environment. This worked fine but when importing scanpy I get the error below. It sounds like this is because as of pandas 2.0 `is_categorical` is deprecated: https://stackoverflow.com/questions/76234312/importerror-cannot-import-name-is-categorical-from-pandas-api-types. This issue is also fixed when I downgrade pandas to 1.5.3. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). Untitled-1.ipynb Cell 20 in <cell line: 1>(). ----> 1 import scanpy as sc. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/__init__.py:18, in <module>. 16 from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:580,deployability,stack,stackoverflow,580,"import fails due to deprecated is_categorical method it pandas; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I installed scanpy as the first package in a brand new conda environment. This worked fine but when importing scanpy I get the error below. It sounds like this is because as of pandas 2.0 `is_categorical` is deprecated: https://stackoverflow.com/questions/76234312/importerror-cannot-import-name-is-categorical-from-pandas-api-types. This issue is also fixed when I downgrade pandas to 1.5.3. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). Untitled-1.ipynb Cell 20 in <cell line: 1>(). ----> 1 import scanpy as sc. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/__init__.py:18, in <module>. 16 from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:675,deployability,api,api-types,675,"import fails due to deprecated is_categorical method it pandas; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I installed scanpy as the first package in a brand new conda environment. This worked fine but when importing scanpy I get the error below. It sounds like this is because as of pandas 2.0 `is_categorical` is deprecated: https://stackoverflow.com/questions/76234312/importerror-cannot-import-name-is-categorical-from-pandas-api-types. This issue is also fixed when I downgrade pandas to 1.5.3. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). Untitled-1.ipynb Cell 20 in <cell line: 1>(). ----> 1 import scanpy as sc. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/__init__.py:18, in <module>. 16 from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:1126,deployability,modul,module,1126,"ed that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I installed scanpy as the first package in a brand new conda environment. This worked fine but when importing scanpy I get the error below. It sounds like this is because as of pandas 2.0 `is_categorical` is deprecated: https://stackoverflow.com/questions/76234312/importerror-cannot-import-name-is-categorical-from-pandas-api-types. This issue is also fixed when I downgrade pandas to 1.5.3. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). Untitled-1.ipynb Cell 20 in <cell line: 1>(). ----> 1 import scanpy as sc. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/__init__.py:18, in <module>. 16 from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from 'pandas.api.types' (/Users/michael/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/pandas/api/types/__init__.py). ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:1238,deployability,log,logging,1238," scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I installed scanpy as the first package in a brand new conda environment. This worked fine but when importing scanpy I get the error below. It sounds like this is because as of pandas 2.0 `is_categorical` is deprecated: https://stackoverflow.com/questions/76234312/importerror-cannot-import-name-is-categorical-from-pandas-api-types. This issue is also fixed when I downgrade pandas to 1.5.3. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). Untitled-1.ipynb Cell 20 in <cell line: 1>(). ----> 1 import scanpy as sc. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/__init__.py:18, in <module>. 16 from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from 'pandas.api.types' (/Users/michael/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/pandas/api/types/__init__.py). ```. ### Versions. <details>. ```. conda list. # packages in environment at /Users/michael/mambaforge/envs/basic_comp",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:1500,deployability,modul,module,1500,"ike this is because as of pandas 2.0 `is_categorical` is deprecated: https://stackoverflow.com/questions/76234312/importerror-cannot-import-name-is-categorical-from-pandas-api-types. This issue is also fixed when I downgrade pandas to 1.5.3. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). Untitled-1.ipynb Cell 20 in <cell line: 1>(). ----> 1 import scanpy as sc. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/__init__.py:18, in <module>. 16 from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from 'pandas.api.types' (/Users/michael/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/pandas/api/types/__init__.py). ```. ### Versions. <details>. ```. conda list. # packages in environment at /Users/michael/mambaforge/envs/basic_comp_bio:. #. # Name Version Build Channel. anndata 0.9.1 pyhd8ed1ab_0 conda-forge. appnope 0.1.2 py39hca03da5_1001 anaconda. asttokens 2.0.5 pyhd3eb1b0_0 anaconda. backcall 0.2.0 pyhd3eb1b0_0 anaconda. blosc 1.21.4 hc338f07_0 conda-forge. brotli 1.0.9 h1a8c8d9_9 c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:1724,deployability,modul,module,1724," pandas to 1.5.3. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). Untitled-1.ipynb Cell 20 in <cell line: 1>(). ----> 1 import scanpy as sc. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/__init__.py:18, in <module>. 16 from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from 'pandas.api.types' (/Users/michael/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/pandas/api/types/__init__.py). ```. ### Versions. <details>. ```. conda list. # packages in environment at /Users/michael/mambaforge/envs/basic_comp_bio:. #. # Name Version Build Channel. anndata 0.9.1 pyhd8ed1ab_0 conda-forge. appnope 0.1.2 py39hca03da5_1001 anaconda. asttokens 2.0.5 pyhd3eb1b0_0 anaconda. backcall 0.2.0 pyhd3eb1b0_0 anaconda. blosc 1.21.4 hc338f07_0 conda-forge. brotli 1.0.9 h1a8c8d9_9 conda-forge. brotli-bin 1.0.9 h1a8c8d9_9 conda-forge. brotli-python 1.0.9 py39h23fbdae_9 conda-forge. bzip2 1.0.8 h3422bc3_4 conda-forge. c-ares 1.19.1 hb547adb_0 conda-forge. c-blosc2 2.10.0 h068da5f_0 conda-forge. ca-certif",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:1821,deployability,api,api,1821,". ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). Untitled-1.ipynb Cell 20 in <cell line: 1>(). ----> 1 import scanpy as sc. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/__init__.py:18, in <module>. 16 from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from 'pandas.api.types' (/Users/michael/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/pandas/api/types/__init__.py). ```. ### Versions. <details>. ```. conda list. # packages in environment at /Users/michael/mambaforge/envs/basic_comp_bio:. #. # Name Version Build Channel. anndata 0.9.1 pyhd8ed1ab_0 conda-forge. appnope 0.1.2 py39hca03da5_1001 anaconda. asttokens 2.0.5 pyhd3eb1b0_0 anaconda. backcall 0.2.0 pyhd3eb1b0_0 anaconda. blosc 1.21.4 hc338f07_0 conda-forge. brotli 1.0.9 h1a8c8d9_9 conda-forge. brotli-bin 1.0.9 h1a8c8d9_9 conda-forge. brotli-python 1.0.9 py39h23fbdae_9 conda-forge. bzip2 1.0.8 h3422bc3_4 conda-forge. c-ares 1.19.1 hb547adb_0 conda-forge. c-blosc2 2.10.0 h068da5f_0 conda-forge. ca-certificates 2022.4.26 hca03da5_0 anaconda. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_prope",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:2008,deployability,api,api,2008,"1 import scanpy as sc. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/__init__.py:18, in <module>. 16 from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from 'pandas.api.types' (/Users/michael/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/pandas/api/types/__init__.py). ```. ### Versions. <details>. ```. conda list. # packages in environment at /Users/michael/mambaforge/envs/basic_comp_bio:. #. # Name Version Build Channel. anndata 0.9.1 pyhd8ed1ab_0 conda-forge. appnope 0.1.2 py39hca03da5_1001 anaconda. asttokens 2.0.5 pyhd3eb1b0_0 anaconda. backcall 0.2.0 pyhd3eb1b0_0 anaconda. blosc 1.21.4 hc338f07_0 conda-forge. brotli 1.0.9 h1a8c8d9_9 conda-forge. brotli-bin 1.0.9 h1a8c8d9_9 conda-forge. brotli-python 1.0.9 py39h23fbdae_9 conda-forge. bzip2 1.0.8 h3422bc3_4 conda-forge. c-ares 1.19.1 hb547adb_0 conda-forge. c-blosc2 2.10.0 h068da5f_0 conda-forge. ca-certificates 2022.4.26 hca03da5_0 anaconda. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. certifi 2022.6.15 py39hca03da5_0 anaconda. charset-normalizer 3.2.0 pyhd8ed1ab_0 conda-forge. colorama 0.4.6 pyhd8ed1ab_0 conda-forge. curl 8.1.2 hc52a",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:2101,deployability,api,api,2101,"py/__init__.py:18, in <module>. 16 from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from 'pandas.api.types' (/Users/michael/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/pandas/api/types/__init__.py). ```. ### Versions. <details>. ```. conda list. # packages in environment at /Users/michael/mambaforge/envs/basic_comp_bio:. #. # Name Version Build Channel. anndata 0.9.1 pyhd8ed1ab_0 conda-forge. appnope 0.1.2 py39hca03da5_1001 anaconda. asttokens 2.0.5 pyhd3eb1b0_0 anaconda. backcall 0.2.0 pyhd3eb1b0_0 anaconda. blosc 1.21.4 hc338f07_0 conda-forge. brotli 1.0.9 h1a8c8d9_9 conda-forge. brotli-bin 1.0.9 h1a8c8d9_9 conda-forge. brotli-python 1.0.9 py39h23fbdae_9 conda-forge. bzip2 1.0.8 h3422bc3_4 conda-forge. c-ares 1.19.1 hb547adb_0 conda-forge. c-blosc2 2.10.0 h068da5f_0 conda-forge. ca-certificates 2022.4.26 hca03da5_0 anaconda. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. certifi 2022.6.15 py39hca03da5_0 anaconda. charset-normalizer 3.2.0 pyhd8ed1ab_0 conda-forge. colorama 0.4.6 pyhd8ed1ab_0 conda-forge. curl 8.1.2 hc52a3a8_1 conda-forge. cycler 0.11.0 pyhd8ed1ab_0 conda-forge. debugpy 1.5.1 py39hc377ac9_0 anaco",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:2134,deployability,Version,Versions,2134,"from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from 'pandas.api.types' (/Users/michael/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/pandas/api/types/__init__.py). ```. ### Versions. <details>. ```. conda list. # packages in environment at /Users/michael/mambaforge/envs/basic_comp_bio:. #. # Name Version Build Channel. anndata 0.9.1 pyhd8ed1ab_0 conda-forge. appnope 0.1.2 py39hca03da5_1001 anaconda. asttokens 2.0.5 pyhd3eb1b0_0 anaconda. backcall 0.2.0 pyhd3eb1b0_0 anaconda. blosc 1.21.4 hc338f07_0 conda-forge. brotli 1.0.9 h1a8c8d9_9 conda-forge. brotli-bin 1.0.9 h1a8c8d9_9 conda-forge. brotli-python 1.0.9 py39h23fbdae_9 conda-forge. bzip2 1.0.8 h3422bc3_4 conda-forge. c-ares 1.19.1 hb547adb_0 conda-forge. c-blosc2 2.10.0 h068da5f_0 conda-forge. ca-certificates 2022.4.26 hca03da5_0 anaconda. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. certifi 2022.6.15 py39hca03da5_0 anaconda. charset-normalizer 3.2.0 pyhd8ed1ab_0 conda-forge. colorama 0.4.6 pyhd8ed1ab_0 conda-forge. curl 8.1.2 hc52a3a8_1 conda-forge. cycler 0.11.0 pyhd8ed1ab_0 conda-forge. debugpy 1.5.1 py39hc377ac9_0 anaconda. decorator 5.1.1 pyhd3eb1b0_0 a",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:2259,deployability,Version,Version,2259,"l, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from 'pandas.api.types' (/Users/michael/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/pandas/api/types/__init__.py). ```. ### Versions. <details>. ```. conda list. # packages in environment at /Users/michael/mambaforge/envs/basic_comp_bio:. #. # Name Version Build Channel. anndata 0.9.1 pyhd8ed1ab_0 conda-forge. appnope 0.1.2 py39hca03da5_1001 anaconda. asttokens 2.0.5 pyhd3eb1b0_0 anaconda. backcall 0.2.0 pyhd3eb1b0_0 anaconda. blosc 1.21.4 hc338f07_0 conda-forge. brotli 1.0.9 h1a8c8d9_9 conda-forge. brotli-bin 1.0.9 h1a8c8d9_9 conda-forge. brotli-python 1.0.9 py39h23fbdae_9 conda-forge. bzip2 1.0.8 h3422bc3_4 conda-forge. c-ares 1.19.1 hb547adb_0 conda-forge. c-blosc2 2.10.0 h068da5f_0 conda-forge. ca-certificates 2022.4.26 hca03da5_0 anaconda. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. certifi 2022.6.15 py39hca03da5_0 anaconda. charset-normalizer 3.2.0 pyhd8ed1ab_0 conda-forge. colorama 0.4.6 pyhd8ed1ab_0 conda-forge. curl 8.1.2 hc52a3a8_1 conda-forge. cycler 0.11.0 pyhd8ed1ab_0 conda-forge. debugpy 1.5.1 py39hc377ac9_0 anaconda. decorator 5.1.1 pyhd3eb1b0_0 anaconda. dunamai 1.18.0 pyhd8ed1ab_0 conda-forge. entrypoints 0.4 py39hca03da5_0 anaconda. executing 0.8.3 pyhd3eb1b0_0 anaco",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:2267,deployability,Build,Build,2267," 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from 'pandas.api.types' (/Users/michael/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/pandas/api/types/__init__.py). ```. ### Versions. <details>. ```. conda list. # packages in environment at /Users/michael/mambaforge/envs/basic_comp_bio:. #. # Name Version Build Channel. anndata 0.9.1 pyhd8ed1ab_0 conda-forge. appnope 0.1.2 py39hca03da5_1001 anaconda. asttokens 2.0.5 pyhd3eb1b0_0 anaconda. backcall 0.2.0 pyhd3eb1b0_0 anaconda. blosc 1.21.4 hc338f07_0 conda-forge. brotli 1.0.9 h1a8c8d9_9 conda-forge. brotli-bin 1.0.9 h1a8c8d9_9 conda-forge. brotli-python 1.0.9 py39h23fbdae_9 conda-forge. bzip2 1.0.8 h3422bc3_4 conda-forge. c-ares 1.19.1 hb547adb_0 conda-forge. c-blosc2 2.10.0 h068da5f_0 conda-forge. ca-certificates 2022.4.26 hca03da5_0 anaconda. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. certifi 2022.6.15 py39hca03da5_0 anaconda. charset-normalizer 3.2.0 pyhd8ed1ab_0 conda-forge. colorama 0.4.6 pyhd8ed1ab_0 conda-forge. curl 8.1.2 hc52a3a8_1 conda-forge. cycler 0.11.0 pyhd8ed1ab_0 conda-forge. debugpy 1.5.1 py39hc377ac9_0 anaconda. decorator 5.1.1 pyhd3eb1b0_0 anaconda. dunamai 1.18.0 pyhd8ed1ab_0 conda-forge. entrypoints 0.4 py39hca03da5_0 anaconda. executing 0.8.3 pyhd3eb1b0_0 anaconda. fo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:4073,deployability,api,api-wrap,4073,1.5.1 py39hc377ac9_0 anaconda. decorator 5.1.1 pyhd3eb1b0_0 anaconda. dunamai 1.18.0 pyhd8ed1ab_0 conda-forge. entrypoints 0.4 py39hca03da5_0 anaconda. executing 0.8.3 pyhd3eb1b0_0 anaconda. fonttools 4.41.0 py39h0f82c59_0 conda-forge. freetype 2.12.1 hd633e50_1 conda-forge. get_version 3.5.4 pyhd8ed1ab_0 conda-forge. gettext 0.21.1 h0186832_0 conda-forge. git 2.41.0 pl5321h46e2b6d_0 conda-forge. h5py 3.9.0 nompi_py39he9c2634_101 conda-forge. hdf5 1.14.1 nompi_h3aba7b3_100 conda-forge. idna 3.4 pyhd8ed1ab_0 conda-forge. importlib-metadata 6.8.0 pyha770c72_0 conda-forge. importlib_metadata 6.8.0 hd8ed1ab_0 conda-forge. ipykernel 6.9.1 py39hca03da5_0 anaconda. ipython 8.3.0 py39hca03da5_0 anaconda. jedi 0.18.1 py39hca03da5_1 anaconda. joblib 1.3.0 pyhd8ed1ab_1 conda-forge. jupyter_client 7.2.2 py39hca03da5_0 anaconda. jupyter_core 4.10.0 py39hca03da5_0 anaconda. kiwisolver 1.4.4 py39haaf3ac1_1 conda-forge. krb5 1.21.1 h92f50d5_0 conda-forge. lcms2 2.15 hd835a16_1 conda-forge. legacy-api-wrap 1.2 py_0 conda-forge. lerc 4.0.0 h9a09cb3_0 conda-forge. libaec 1.0.6 hb7217d7_1 conda-forge. libblas 3.9.0 17_osxarm64_openblas conda-forge. libbrotlicommon 1.0.9 h1a8c8d9_9 conda-forge. libbrotlidec 1.0.9 h1a8c8d9_9 conda-forge. libbrotlienc 1.0.9 h1a8c8d9_9 conda-forge. libcblas 3.9.0 17_osxarm64_openblas conda-forge. libcurl 8.1.2 hc52a3a8_1 conda-forge. libcxx 16.0.6 h4653b0c_0 conda-forge. libdeflate 1.18 h1a8c8d9_0 conda-forge. libedit 3.1.20191231 hc8eb9b7_2 conda-forge. libev 4.33 h642e427_1 conda-forge. libexpat 2.5.0 hb7217d7_1 conda-forge. libffi 3.4.2 h3422bc3_5 conda-forge. libgfortran 5.0.0 12_2_0_hd922786_31 conda-forge. libgfortran5 12.2.0 h0eea778_31 conda-forge. libiconv 1.17 he4db4b2_0 conda-forge. libjpeg-turbo 2.1.5.1 h1a8c8d9_0 conda-forge. liblapack 3.9.0 17_osxarm64_openblas conda-forge. libllvm14 14.0.6 hd1a9a77_3 conda-forge. libnghttp2 1.52.0 hae82a92_0 conda-forge. libopenblas 0.3.23 openmp_hc731615_0 conda-forge. libpng 1.6.39 h76d750c_0 conda-forge. ,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:6606,energy efficiency,cpu,cpuinfo,6606,4.0 pyhd8ed1ab_0 conda-forge. ncurses 6.4 h7ea286d_0 conda-forge. nest-asyncio 1.5.5 py39hca03da5_0 anaconda. networkx 3.1 pyhd8ed1ab_0 conda-forge. numba 0.57.1 py39he8ed757_0 conda-forge. numexpr 2.8.4 py39hd28f0be_0 conda-forge. numpy 1.24.4 py39h485cf63_0 conda-forge. openjpeg 2.5.0 hbc2ba62_2 conda-forge. openssl 3.1.1 h53f4e23_1 conda-forge. packaging 23.1 pyhd8ed1ab_0 conda-forge. pandas 2.0.3 py39h6b13a34_1 conda-forge. parso 0.8.3 pyhd3eb1b0_0 anaconda. patsy 0.5.3 pyhd8ed1ab_0 conda-forge. pcre2 10.40 hb34f9b4_0 conda-forge. perl 5.32.1 4_hf2054a2_perl5 conda-forge. pexpect 4.8.0 pyhd3eb1b0_3 anaconda. pickleshare 0.7.5 pyhd3eb1b0_1003 anaconda. pillow 10.0.0 py39h1641143_0 conda-forge. pip 23.2 pyhd8ed1ab_0 conda-forge. platformdirs 3.9.1 pyhd8ed1ab_0 conda-forge. pooch 1.7.0 pyha770c72_3 conda-forge. prompt-toolkit 3.0.20 pyhd3eb1b0_0 anaconda. pthread-stubs 0.4 h27ca646_1001 conda-forge. ptyprocess 0.7.0 pyhd3eb1b0_2 anaconda. pure_eval 0.2.2 pyhd3eb1b0_0 anaconda. py-cpuinfo 9.0.0 pyhd8ed1ab_0 conda-forge. pygments 2.11.2 pyhd3eb1b0_0 anaconda. pynndescent 0.5.10 pyh1a96a4e_0 conda-forge. pyparsing 3.1.0 pyhd8ed1ab_0 conda-forge. pysocks 1.7.1 pyha2e5f31_6 conda-forge. pytables 3.8.0 py39h0da393b_2 conda-forge. python 3.9.16 hea58f1e_0_cpython conda-forge. python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge. python-tzdata 2023.3 pyhd8ed1ab_0 conda-forge. python_abi 3.9 3_cp39 conda-forge. pytz 2023.3 pyhd8ed1ab_0 conda-forge. pyzmq 22.3.0 py39hc377ac9_2 anaconda. readline 8.2 h92ec313_1 conda-forge. requests 2.31.0 pyhd8ed1ab_0 conda-forge. scanpy 1.7.2 pyhdfd78af_0 bioconda. scikit-learn 1.3.0 py39hd5c4a62_0 conda-forge. scipy 1.11.1 py39ha6b2cbd_0 conda-forge. seaborn 0.12.2 hd8ed1ab_0 conda-forge. seaborn-base 0.12.2 pyhd8ed1ab_0 conda-forge. setuptools 68.0.0 pyhd8ed1ab_0 conda-forge. setuptools-scm 7.1.0 pyhd8ed1ab_0 conda-forge. setuptools_scm 7.1.0 hd8ed1ab_0 conda-forge. sinfo 0.3.1 py_0 conda-forge. six 1.16.0 pyh6c4a22f_0 conda-forge. snappy 1.1.1,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:232,integrability,version,version,232,"import fails due to deprecated is_categorical method it pandas; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I installed scanpy as the first package in a brand new conda environment. This worked fine but when importing scanpy I get the error below. It sounds like this is because as of pandas 2.0 `is_categorical` is deprecated: https://stackoverflow.com/questions/76234312/importerror-cannot-import-name-is-categorical-from-pandas-api-types. This issue is also fixed when I downgrade pandas to 1.5.3. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). Untitled-1.ipynb Cell 20 in <cell line: 1>(). ----> 1 import scanpy as sc. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/__init__.py:18, in <module>. 16 from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:675,integrability,api,api-types,675,"import fails due to deprecated is_categorical method it pandas; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I installed scanpy as the first package in a brand new conda environment. This worked fine but when importing scanpy I get the error below. It sounds like this is because as of pandas 2.0 `is_categorical` is deprecated: https://stackoverflow.com/questions/76234312/importerror-cannot-import-name-is-categorical-from-pandas-api-types. This issue is also fixed when I downgrade pandas to 1.5.3. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). Untitled-1.ipynb Cell 20 in <cell line: 1>(). ----> 1 import scanpy as sc. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/__init__.py:18, in <module>. 16 from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:1821,integrability,api,api,1821,". ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). Untitled-1.ipynb Cell 20 in <cell line: 1>(). ----> 1 import scanpy as sc. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/__init__.py:18, in <module>. 16 from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from 'pandas.api.types' (/Users/michael/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/pandas/api/types/__init__.py). ```. ### Versions. <details>. ```. conda list. # packages in environment at /Users/michael/mambaforge/envs/basic_comp_bio:. #. # Name Version Build Channel. anndata 0.9.1 pyhd8ed1ab_0 conda-forge. appnope 0.1.2 py39hca03da5_1001 anaconda. asttokens 2.0.5 pyhd3eb1b0_0 anaconda. backcall 0.2.0 pyhd3eb1b0_0 anaconda. blosc 1.21.4 hc338f07_0 conda-forge. brotli 1.0.9 h1a8c8d9_9 conda-forge. brotli-bin 1.0.9 h1a8c8d9_9 conda-forge. brotli-python 1.0.9 py39h23fbdae_9 conda-forge. bzip2 1.0.8 h3422bc3_4 conda-forge. c-ares 1.19.1 hb547adb_0 conda-forge. c-blosc2 2.10.0 h068da5f_0 conda-forge. ca-certificates 2022.4.26 hca03da5_0 anaconda. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_prope",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:2008,integrability,api,api,2008,"1 import scanpy as sc. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/__init__.py:18, in <module>. 16 from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from 'pandas.api.types' (/Users/michael/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/pandas/api/types/__init__.py). ```. ### Versions. <details>. ```. conda list. # packages in environment at /Users/michael/mambaforge/envs/basic_comp_bio:. #. # Name Version Build Channel. anndata 0.9.1 pyhd8ed1ab_0 conda-forge. appnope 0.1.2 py39hca03da5_1001 anaconda. asttokens 2.0.5 pyhd3eb1b0_0 anaconda. backcall 0.2.0 pyhd3eb1b0_0 anaconda. blosc 1.21.4 hc338f07_0 conda-forge. brotli 1.0.9 h1a8c8d9_9 conda-forge. brotli-bin 1.0.9 h1a8c8d9_9 conda-forge. brotli-python 1.0.9 py39h23fbdae_9 conda-forge. bzip2 1.0.8 h3422bc3_4 conda-forge. c-ares 1.19.1 hb547adb_0 conda-forge. c-blosc2 2.10.0 h068da5f_0 conda-forge. ca-certificates 2022.4.26 hca03da5_0 anaconda. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. certifi 2022.6.15 py39hca03da5_0 anaconda. charset-normalizer 3.2.0 pyhd8ed1ab_0 conda-forge. colorama 0.4.6 pyhd8ed1ab_0 conda-forge. curl 8.1.2 hc52a",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:2101,integrability,api,api,2101,"py/__init__.py:18, in <module>. 16 from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from 'pandas.api.types' (/Users/michael/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/pandas/api/types/__init__.py). ```. ### Versions. <details>. ```. conda list. # packages in environment at /Users/michael/mambaforge/envs/basic_comp_bio:. #. # Name Version Build Channel. anndata 0.9.1 pyhd8ed1ab_0 conda-forge. appnope 0.1.2 py39hca03da5_1001 anaconda. asttokens 2.0.5 pyhd3eb1b0_0 anaconda. backcall 0.2.0 pyhd3eb1b0_0 anaconda. blosc 1.21.4 hc338f07_0 conda-forge. brotli 1.0.9 h1a8c8d9_9 conda-forge. brotli-bin 1.0.9 h1a8c8d9_9 conda-forge. brotli-python 1.0.9 py39h23fbdae_9 conda-forge. bzip2 1.0.8 h3422bc3_4 conda-forge. c-ares 1.19.1 hb547adb_0 conda-forge. c-blosc2 2.10.0 h068da5f_0 conda-forge. ca-certificates 2022.4.26 hca03da5_0 anaconda. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. certifi 2022.6.15 py39hca03da5_0 anaconda. charset-normalizer 3.2.0 pyhd8ed1ab_0 conda-forge. colorama 0.4.6 pyhd8ed1ab_0 conda-forge. curl 8.1.2 hc52a3a8_1 conda-forge. cycler 0.11.0 pyhd8ed1ab_0 conda-forge. debugpy 1.5.1 py39hc377ac9_0 anaco",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:2134,integrability,Version,Versions,2134,"from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from 'pandas.api.types' (/Users/michael/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/pandas/api/types/__init__.py). ```. ### Versions. <details>. ```. conda list. # packages in environment at /Users/michael/mambaforge/envs/basic_comp_bio:. #. # Name Version Build Channel. anndata 0.9.1 pyhd8ed1ab_0 conda-forge. appnope 0.1.2 py39hca03da5_1001 anaconda. asttokens 2.0.5 pyhd3eb1b0_0 anaconda. backcall 0.2.0 pyhd3eb1b0_0 anaconda. blosc 1.21.4 hc338f07_0 conda-forge. brotli 1.0.9 h1a8c8d9_9 conda-forge. brotli-bin 1.0.9 h1a8c8d9_9 conda-forge. brotli-python 1.0.9 py39h23fbdae_9 conda-forge. bzip2 1.0.8 h3422bc3_4 conda-forge. c-ares 1.19.1 hb547adb_0 conda-forge. c-blosc2 2.10.0 h068da5f_0 conda-forge. ca-certificates 2022.4.26 hca03da5_0 anaconda. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. certifi 2022.6.15 py39hca03da5_0 anaconda. charset-normalizer 3.2.0 pyhd8ed1ab_0 conda-forge. colorama 0.4.6 pyhd8ed1ab_0 conda-forge. curl 8.1.2 hc52a3a8_1 conda-forge. cycler 0.11.0 pyhd8ed1ab_0 conda-forge. debugpy 1.5.1 py39hc377ac9_0 anaconda. decorator 5.1.1 pyhd3eb1b0_0 a",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:2259,integrability,Version,Version,2259,"l, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from 'pandas.api.types' (/Users/michael/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/pandas/api/types/__init__.py). ```. ### Versions. <details>. ```. conda list. # packages in environment at /Users/michael/mambaforge/envs/basic_comp_bio:. #. # Name Version Build Channel. anndata 0.9.1 pyhd8ed1ab_0 conda-forge. appnope 0.1.2 py39hca03da5_1001 anaconda. asttokens 2.0.5 pyhd3eb1b0_0 anaconda. backcall 0.2.0 pyhd3eb1b0_0 anaconda. blosc 1.21.4 hc338f07_0 conda-forge. brotli 1.0.9 h1a8c8d9_9 conda-forge. brotli-bin 1.0.9 h1a8c8d9_9 conda-forge. brotli-python 1.0.9 py39h23fbdae_9 conda-forge. bzip2 1.0.8 h3422bc3_4 conda-forge. c-ares 1.19.1 hb547adb_0 conda-forge. c-blosc2 2.10.0 h068da5f_0 conda-forge. ca-certificates 2022.4.26 hca03da5_0 anaconda. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. certifi 2022.6.15 py39hca03da5_0 anaconda. charset-normalizer 3.2.0 pyhd8ed1ab_0 conda-forge. colorama 0.4.6 pyhd8ed1ab_0 conda-forge. curl 8.1.2 hc52a3a8_1 conda-forge. cycler 0.11.0 pyhd8ed1ab_0 conda-forge. debugpy 1.5.1 py39hc377ac9_0 anaconda. decorator 5.1.1 pyhd3eb1b0_0 anaconda. dunamai 1.18.0 pyhd8ed1ab_0 conda-forge. entrypoints 0.4 py39hca03da5_0 anaconda. executing 0.8.3 pyhd3eb1b0_0 anaco",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:4073,integrability,api,api-wrap,4073,1.5.1 py39hc377ac9_0 anaconda. decorator 5.1.1 pyhd3eb1b0_0 anaconda. dunamai 1.18.0 pyhd8ed1ab_0 conda-forge. entrypoints 0.4 py39hca03da5_0 anaconda. executing 0.8.3 pyhd3eb1b0_0 anaconda. fonttools 4.41.0 py39h0f82c59_0 conda-forge. freetype 2.12.1 hd633e50_1 conda-forge. get_version 3.5.4 pyhd8ed1ab_0 conda-forge. gettext 0.21.1 h0186832_0 conda-forge. git 2.41.0 pl5321h46e2b6d_0 conda-forge. h5py 3.9.0 nompi_py39he9c2634_101 conda-forge. hdf5 1.14.1 nompi_h3aba7b3_100 conda-forge. idna 3.4 pyhd8ed1ab_0 conda-forge. importlib-metadata 6.8.0 pyha770c72_0 conda-forge. importlib_metadata 6.8.0 hd8ed1ab_0 conda-forge. ipykernel 6.9.1 py39hca03da5_0 anaconda. ipython 8.3.0 py39hca03da5_0 anaconda. jedi 0.18.1 py39hca03da5_1 anaconda. joblib 1.3.0 pyhd8ed1ab_1 conda-forge. jupyter_client 7.2.2 py39hca03da5_0 anaconda. jupyter_core 4.10.0 py39hca03da5_0 anaconda. kiwisolver 1.4.4 py39haaf3ac1_1 conda-forge. krb5 1.21.1 h92f50d5_0 conda-forge. lcms2 2.15 hd835a16_1 conda-forge. legacy-api-wrap 1.2 py_0 conda-forge. lerc 4.0.0 h9a09cb3_0 conda-forge. libaec 1.0.6 hb7217d7_1 conda-forge. libblas 3.9.0 17_osxarm64_openblas conda-forge. libbrotlicommon 1.0.9 h1a8c8d9_9 conda-forge. libbrotlidec 1.0.9 h1a8c8d9_9 conda-forge. libbrotlienc 1.0.9 h1a8c8d9_9 conda-forge. libcblas 3.9.0 17_osxarm64_openblas conda-forge. libcurl 8.1.2 hc52a3a8_1 conda-forge. libcxx 16.0.6 h4653b0c_0 conda-forge. libdeflate 1.18 h1a8c8d9_0 conda-forge. libedit 3.1.20191231 hc8eb9b7_2 conda-forge. libev 4.33 h642e427_1 conda-forge. libexpat 2.5.0 hb7217d7_1 conda-forge. libffi 3.4.2 h3422bc3_5 conda-forge. libgfortran 5.0.0 12_2_0_hd922786_31 conda-forge. libgfortran5 12.2.0 h0eea778_31 conda-forge. libiconv 1.17 he4db4b2_0 conda-forge. libjpeg-turbo 2.1.5.1 h1a8c8d9_0 conda-forge. liblapack 3.9.0 17_osxarm64_openblas conda-forge. libllvm14 14.0.6 hd1a9a77_3 conda-forge. libnghttp2 1.52.0 hae82a92_0 conda-forge. libopenblas 0.3.23 openmp_hc731615_0 conda-forge. libpng 1.6.39 h76d750c_0 conda-forge. ,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:675,interoperability,api,api-types,675,"import fails due to deprecated is_categorical method it pandas; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I installed scanpy as the first package in a brand new conda environment. This worked fine but when importing scanpy I get the error below. It sounds like this is because as of pandas 2.0 `is_categorical` is deprecated: https://stackoverflow.com/questions/76234312/importerror-cannot-import-name-is-categorical-from-pandas-api-types. This issue is also fixed when I downgrade pandas to 1.5.3. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). Untitled-1.ipynb Cell 20 in <cell line: 1>(). ----> 1 import scanpy as sc. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/__init__.py:18, in <module>. 16 from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:1821,interoperability,api,api,1821,". ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). Untitled-1.ipynb Cell 20 in <cell line: 1>(). ----> 1 import scanpy as sc. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/__init__.py:18, in <module>. 16 from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from 'pandas.api.types' (/Users/michael/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/pandas/api/types/__init__.py). ```. ### Versions. <details>. ```. conda list. # packages in environment at /Users/michael/mambaforge/envs/basic_comp_bio:. #. # Name Version Build Channel. anndata 0.9.1 pyhd8ed1ab_0 conda-forge. appnope 0.1.2 py39hca03da5_1001 anaconda. asttokens 2.0.5 pyhd3eb1b0_0 anaconda. backcall 0.2.0 pyhd3eb1b0_0 anaconda. blosc 1.21.4 hc338f07_0 conda-forge. brotli 1.0.9 h1a8c8d9_9 conda-forge. brotli-bin 1.0.9 h1a8c8d9_9 conda-forge. brotli-python 1.0.9 py39h23fbdae_9 conda-forge. bzip2 1.0.8 h3422bc3_4 conda-forge. c-ares 1.19.1 hb547adb_0 conda-forge. c-blosc2 2.10.0 h068da5f_0 conda-forge. ca-certificates 2022.4.26 hca03da5_0 anaconda. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_prope",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:2008,interoperability,api,api,2008,"1 import scanpy as sc. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/__init__.py:18, in <module>. 16 from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from 'pandas.api.types' (/Users/michael/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/pandas/api/types/__init__.py). ```. ### Versions. <details>. ```. conda list. # packages in environment at /Users/michael/mambaforge/envs/basic_comp_bio:. #. # Name Version Build Channel. anndata 0.9.1 pyhd8ed1ab_0 conda-forge. appnope 0.1.2 py39hca03da5_1001 anaconda. asttokens 2.0.5 pyhd3eb1b0_0 anaconda. backcall 0.2.0 pyhd3eb1b0_0 anaconda. blosc 1.21.4 hc338f07_0 conda-forge. brotli 1.0.9 h1a8c8d9_9 conda-forge. brotli-bin 1.0.9 h1a8c8d9_9 conda-forge. brotli-python 1.0.9 py39h23fbdae_9 conda-forge. bzip2 1.0.8 h3422bc3_4 conda-forge. c-ares 1.19.1 hb547adb_0 conda-forge. c-blosc2 2.10.0 h068da5f_0 conda-forge. ca-certificates 2022.4.26 hca03da5_0 anaconda. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. certifi 2022.6.15 py39hca03da5_0 anaconda. charset-normalizer 3.2.0 pyhd8ed1ab_0 conda-forge. colorama 0.4.6 pyhd8ed1ab_0 conda-forge. curl 8.1.2 hc52a",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:2101,interoperability,api,api,2101,"py/__init__.py:18, in <module>. 16 from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from 'pandas.api.types' (/Users/michael/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/pandas/api/types/__init__.py). ```. ### Versions. <details>. ```. conda list. # packages in environment at /Users/michael/mambaforge/envs/basic_comp_bio:. #. # Name Version Build Channel. anndata 0.9.1 pyhd8ed1ab_0 conda-forge. appnope 0.1.2 py39hca03da5_1001 anaconda. asttokens 2.0.5 pyhd3eb1b0_0 anaconda. backcall 0.2.0 pyhd3eb1b0_0 anaconda. blosc 1.21.4 hc338f07_0 conda-forge. brotli 1.0.9 h1a8c8d9_9 conda-forge. brotli-bin 1.0.9 h1a8c8d9_9 conda-forge. brotli-python 1.0.9 py39h23fbdae_9 conda-forge. bzip2 1.0.8 h3422bc3_4 conda-forge. c-ares 1.19.1 hb547adb_0 conda-forge. c-blosc2 2.10.0 h068da5f_0 conda-forge. ca-certificates 2022.4.26 hca03da5_0 anaconda. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. certifi 2022.6.15 py39hca03da5_0 anaconda. charset-normalizer 3.2.0 pyhd8ed1ab_0 conda-forge. colorama 0.4.6 pyhd8ed1ab_0 conda-forge. curl 8.1.2 hc52a3a8_1 conda-forge. cycler 0.11.0 pyhd8ed1ab_0 conda-forge. debugpy 1.5.1 py39hc377ac9_0 anaco",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:4073,interoperability,api,api-wrap,4073,1.5.1 py39hc377ac9_0 anaconda. decorator 5.1.1 pyhd3eb1b0_0 anaconda. dunamai 1.18.0 pyhd8ed1ab_0 conda-forge. entrypoints 0.4 py39hca03da5_0 anaconda. executing 0.8.3 pyhd3eb1b0_0 anaconda. fonttools 4.41.0 py39h0f82c59_0 conda-forge. freetype 2.12.1 hd633e50_1 conda-forge. get_version 3.5.4 pyhd8ed1ab_0 conda-forge. gettext 0.21.1 h0186832_0 conda-forge. git 2.41.0 pl5321h46e2b6d_0 conda-forge. h5py 3.9.0 nompi_py39he9c2634_101 conda-forge. hdf5 1.14.1 nompi_h3aba7b3_100 conda-forge. idna 3.4 pyhd8ed1ab_0 conda-forge. importlib-metadata 6.8.0 pyha770c72_0 conda-forge. importlib_metadata 6.8.0 hd8ed1ab_0 conda-forge. ipykernel 6.9.1 py39hca03da5_0 anaconda. ipython 8.3.0 py39hca03da5_0 anaconda. jedi 0.18.1 py39hca03da5_1 anaconda. joblib 1.3.0 pyhd8ed1ab_1 conda-forge. jupyter_client 7.2.2 py39hca03da5_0 anaconda. jupyter_core 4.10.0 py39hca03da5_0 anaconda. kiwisolver 1.4.4 py39haaf3ac1_1 conda-forge. krb5 1.21.1 h92f50d5_0 conda-forge. lcms2 2.15 hd835a16_1 conda-forge. legacy-api-wrap 1.2 py_0 conda-forge. lerc 4.0.0 h9a09cb3_0 conda-forge. libaec 1.0.6 hb7217d7_1 conda-forge. libblas 3.9.0 17_osxarm64_openblas conda-forge. libbrotlicommon 1.0.9 h1a8c8d9_9 conda-forge. libbrotlidec 1.0.9 h1a8c8d9_9 conda-forge. libbrotlienc 1.0.9 h1a8c8d9_9 conda-forge. libcblas 3.9.0 17_osxarm64_openblas conda-forge. libcurl 8.1.2 hc52a3a8_1 conda-forge. libcxx 16.0.6 h4653b0c_0 conda-forge. libdeflate 1.18 h1a8c8d9_0 conda-forge. libedit 3.1.20191231 hc8eb9b7_2 conda-forge. libev 4.33 h642e427_1 conda-forge. libexpat 2.5.0 hb7217d7_1 conda-forge. libffi 3.4.2 h3422bc3_5 conda-forge. libgfortran 5.0.0 12_2_0_hd922786_31 conda-forge. libgfortran5 12.2.0 h0eea778_31 conda-forge. libiconv 1.17 he4db4b2_0 conda-forge. libjpeg-turbo 2.1.5.1 h1a8c8d9_0 conda-forge. liblapack 3.9.0 17_osxarm64_openblas conda-forge. libllvm14 14.0.6 hd1a9a77_3 conda-forge. libnghttp2 1.52.0 hae82a92_0 conda-forge. libopenblas 0.3.23 openmp_hc731615_0 conda-forge. libpng 1.6.39 h76d750c_0 conda-forge. ,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:6351,interoperability,platform,platformdirs,6351,penmp 16.0.6 h1c12783_0 conda-forge. llvmlite 0.40.1 py39hbad4f83_0 conda-forge. lz4-c 1.9.4 hb7217d7_0 conda-forge. matplotlib-base 3.5.3 py39ha500c34_2 conda-forge. matplotlib-inline 0.1.2 pyhd3eb1b0_2 anaconda. munkres 1.0.7 py_1 bioconda. natsort 8.4.0 pyhd8ed1ab_0 conda-forge. ncurses 6.4 h7ea286d_0 conda-forge. nest-asyncio 1.5.5 py39hca03da5_0 anaconda. networkx 3.1 pyhd8ed1ab_0 conda-forge. numba 0.57.1 py39he8ed757_0 conda-forge. numexpr 2.8.4 py39hd28f0be_0 conda-forge. numpy 1.24.4 py39h485cf63_0 conda-forge. openjpeg 2.5.0 hbc2ba62_2 conda-forge. openssl 3.1.1 h53f4e23_1 conda-forge. packaging 23.1 pyhd8ed1ab_0 conda-forge. pandas 2.0.3 py39h6b13a34_1 conda-forge. parso 0.8.3 pyhd3eb1b0_0 anaconda. patsy 0.5.3 pyhd8ed1ab_0 conda-forge. pcre2 10.40 hb34f9b4_0 conda-forge. perl 5.32.1 4_hf2054a2_perl5 conda-forge. pexpect 4.8.0 pyhd3eb1b0_3 anaconda. pickleshare 0.7.5 pyhd3eb1b0_1003 anaconda. pillow 10.0.0 py39h1641143_0 conda-forge. pip 23.2 pyhd8ed1ab_0 conda-forge. platformdirs 3.9.1 pyhd8ed1ab_0 conda-forge. pooch 1.7.0 pyha770c72_3 conda-forge. prompt-toolkit 3.0.20 pyhd3eb1b0_0 anaconda. pthread-stubs 0.4 h27ca646_1001 conda-forge. ptyprocess 0.7.0 pyhd3eb1b0_2 anaconda. pure_eval 0.2.2 pyhd3eb1b0_0 anaconda. py-cpuinfo 9.0.0 pyhd8ed1ab_0 conda-forge. pygments 2.11.2 pyhd3eb1b0_0 anaconda. pynndescent 0.5.10 pyh1a96a4e_0 conda-forge. pyparsing 3.1.0 pyhd8ed1ab_0 conda-forge. pysocks 1.7.1 pyha2e5f31_6 conda-forge. pytables 3.8.0 py39h0da393b_2 conda-forge. python 3.9.16 hea58f1e_0_cpython conda-forge. python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge. python-tzdata 2023.3 pyhd8ed1ab_0 conda-forge. python_abi 3.9 3_cp39 conda-forge. pytz 2023.3 pyhd8ed1ab_0 conda-forge. pyzmq 22.3.0 py39hc377ac9_2 anaconda. readline 8.2 h92ec313_1 conda-forge. requests 2.31.0 pyhd8ed1ab_0 conda-forge. scanpy 1.7.2 pyhdfd78af_0 bioconda. scikit-learn 1.3.0 py39hd5c4a62_0 conda-forge. scipy 1.11.1 py39ha6b2cbd_0 conda-forge. seaborn 0.12.2 hd8ed1ab_0 conda-forge. seaborn-,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:6487,interoperability,stub,stubs,6487,3.5.3 py39ha500c34_2 conda-forge. matplotlib-inline 0.1.2 pyhd3eb1b0_2 anaconda. munkres 1.0.7 py_1 bioconda. natsort 8.4.0 pyhd8ed1ab_0 conda-forge. ncurses 6.4 h7ea286d_0 conda-forge. nest-asyncio 1.5.5 py39hca03da5_0 anaconda. networkx 3.1 pyhd8ed1ab_0 conda-forge. numba 0.57.1 py39he8ed757_0 conda-forge. numexpr 2.8.4 py39hd28f0be_0 conda-forge. numpy 1.24.4 py39h485cf63_0 conda-forge. openjpeg 2.5.0 hbc2ba62_2 conda-forge. openssl 3.1.1 h53f4e23_1 conda-forge. packaging 23.1 pyhd8ed1ab_0 conda-forge. pandas 2.0.3 py39h6b13a34_1 conda-forge. parso 0.8.3 pyhd3eb1b0_0 anaconda. patsy 0.5.3 pyhd8ed1ab_0 conda-forge. pcre2 10.40 hb34f9b4_0 conda-forge. perl 5.32.1 4_hf2054a2_perl5 conda-forge. pexpect 4.8.0 pyhd3eb1b0_3 anaconda. pickleshare 0.7.5 pyhd3eb1b0_1003 anaconda. pillow 10.0.0 py39h1641143_0 conda-forge. pip 23.2 pyhd8ed1ab_0 conda-forge. platformdirs 3.9.1 pyhd8ed1ab_0 conda-forge. pooch 1.7.0 pyha770c72_3 conda-forge. prompt-toolkit 3.0.20 pyhd3eb1b0_0 anaconda. pthread-stubs 0.4 h27ca646_1001 conda-forge. ptyprocess 0.7.0 pyhd3eb1b0_2 anaconda. pure_eval 0.2.2 pyhd3eb1b0_0 anaconda. py-cpuinfo 9.0.0 pyhd8ed1ab_0 conda-forge. pygments 2.11.2 pyhd3eb1b0_0 anaconda. pynndescent 0.5.10 pyh1a96a4e_0 conda-forge. pyparsing 3.1.0 pyhd8ed1ab_0 conda-forge. pysocks 1.7.1 pyha2e5f31_6 conda-forge. pytables 3.8.0 py39h0da393b_2 conda-forge. python 3.9.16 hea58f1e_0_cpython conda-forge. python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge. python-tzdata 2023.3 pyhd8ed1ab_0 conda-forge. python_abi 3.9 3_cp39 conda-forge. pytz 2023.3 pyhd8ed1ab_0 conda-forge. pyzmq 22.3.0 py39hc377ac9_2 anaconda. readline 8.2 h92ec313_1 conda-forge. requests 2.31.0 pyhd8ed1ab_0 conda-forge. scanpy 1.7.2 pyhdfd78af_0 bioconda. scikit-learn 1.3.0 py39hd5c4a62_0 conda-forge. scipy 1.11.1 py39ha6b2cbd_0 conda-forge. seaborn 0.12.2 hd8ed1ab_0 conda-forge. seaborn-base 0.12.2 pyhd8ed1ab_0 conda-forge. setuptools 68.0.0 pyhd8ed1ab_0 conda-forge. setuptools-scm 7.1.0 pyhd8ed1ab_0 conda-forge. setu,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:232,modifiability,version,version,232,"import fails due to deprecated is_categorical method it pandas; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I installed scanpy as the first package in a brand new conda environment. This worked fine but when importing scanpy I get the error below. It sounds like this is because as of pandas 2.0 `is_categorical` is deprecated: https://stackoverflow.com/questions/76234312/importerror-cannot-import-name-is-categorical-from-pandas-api-types. This issue is also fixed when I downgrade pandas to 1.5.3. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). Untitled-1.ipynb Cell 20 in <cell line: 1>(). ----> 1 import scanpy as sc. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/__init__.py:18, in <module>. 16 from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:384,modifiability,pac,package,384,"import fails due to deprecated is_categorical method it pandas; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I installed scanpy as the first package in a brand new conda environment. This worked fine but when importing scanpy I get the error below. It sounds like this is because as of pandas 2.0 `is_categorical` is deprecated: https://stackoverflow.com/questions/76234312/importerror-cannot-import-name-is-categorical-from-pandas-api-types. This issue is also fixed when I downgrade pandas to 1.5.3. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). Untitled-1.ipynb Cell 20 in <cell line: 1>(). ----> 1 import scanpy as sc. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/__init__.py:18, in <module>. 16 from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:1090,modifiability,pac,packages,1090,"ditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I installed scanpy as the first package in a brand new conda environment. This worked fine but when importing scanpy I get the error below. It sounds like this is because as of pandas 2.0 `is_categorical` is deprecated: https://stackoverflow.com/questions/76234312/importerror-cannot-import-name-is-categorical-from-pandas-api-types. This issue is also fixed when I downgrade pandas to 1.5.3. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). Untitled-1.ipynb Cell 20 in <cell line: 1>(). ----> 1 import scanpy as sc. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/__init__.py:18, in <module>. 16 from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from 'pandas.api.types' (/Users/michael/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:1126,modifiability,modul,module,1126,"ed that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I installed scanpy as the first package in a brand new conda environment. This worked fine but when importing scanpy I get the error below. It sounds like this is because as of pandas 2.0 `is_categorical` is deprecated: https://stackoverflow.com/questions/76234312/importerror-cannot-import-name-is-categorical-from-pandas-api-types. This issue is also fixed when I downgrade pandas to 1.5.3. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). Untitled-1.ipynb Cell 20 in <cell line: 1>(). ----> 1 import scanpy as sc. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/__init__.py:18, in <module>. 16 from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from 'pandas.api.types' (/Users/michael/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/pandas/api/types/__init__.py). ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:1456,modifiability,pac,packages,1456,"g scanpy I get the error below. It sounds like this is because as of pandas 2.0 `is_categorical` is deprecated: https://stackoverflow.com/questions/76234312/importerror-cannot-import-name-is-categorical-from-pandas-api-types. This issue is also fixed when I downgrade pandas to 1.5.3. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). Untitled-1.ipynb Cell 20 in <cell line: 1>(). ----> 1 import scanpy as sc. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/__init__.py:18, in <module>. 16 from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from 'pandas.api.types' (/Users/michael/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/pandas/api/types/__init__.py). ```. ### Versions. <details>. ```. conda list. # packages in environment at /Users/michael/mambaforge/envs/basic_comp_bio:. #. # Name Version Build Channel. anndata 0.9.1 pyhd8ed1ab_0 conda-forge. appnope 0.1.2 py39hca03da5_1001 anaconda. asttokens 2.0.5 pyhd3eb1b0_0 anaconda. backcall 0.2.0 pyhd3eb1b0_0 anaconda. blosc 1.21.4 hc338f",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:1500,modifiability,modul,module,1500,"ike this is because as of pandas 2.0 `is_categorical` is deprecated: https://stackoverflow.com/questions/76234312/importerror-cannot-import-name-is-categorical-from-pandas-api-types. This issue is also fixed when I downgrade pandas to 1.5.3. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). Untitled-1.ipynb Cell 20 in <cell line: 1>(). ----> 1 import scanpy as sc. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/__init__.py:18, in <module>. 16 from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from 'pandas.api.types' (/Users/michael/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/pandas/api/types/__init__.py). ```. ### Versions. <details>. ```. conda list. # packages in environment at /Users/michael/mambaforge/envs/basic_comp_bio:. #. # Name Version Build Channel. anndata 0.9.1 pyhd8ed1ab_0 conda-forge. appnope 0.1.2 py39hca03da5_1001 anaconda. asttokens 2.0.5 pyhd3eb1b0_0 anaconda. backcall 0.2.0 pyhd3eb1b0_0 anaconda. blosc 1.21.4 hc338f07_0 conda-forge. brotli 1.0.9 h1a8c8d9_9 c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:1678,modifiability,pac,packages,1678,"es. This issue is also fixed when I downgrade pandas to 1.5.3. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). Untitled-1.ipynb Cell 20 in <cell line: 1>(). ----> 1 import scanpy as sc. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/__init__.py:18, in <module>. 16 from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from 'pandas.api.types' (/Users/michael/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/pandas/api/types/__init__.py). ```. ### Versions. <details>. ```. conda list. # packages in environment at /Users/michael/mambaforge/envs/basic_comp_bio:. #. # Name Version Build Channel. anndata 0.9.1 pyhd8ed1ab_0 conda-forge. appnope 0.1.2 py39hca03da5_1001 anaconda. asttokens 2.0.5 pyhd3eb1b0_0 anaconda. backcall 0.2.0 pyhd3eb1b0_0 anaconda. blosc 1.21.4 hc338f07_0 conda-forge. brotli 1.0.9 h1a8c8d9_9 conda-forge. brotli-bin 1.0.9 h1a8c8d9_9 conda-forge. brotli-python 1.0.9 py39h23fbdae_9 conda-forge. bzip2 1.0.8 h3422bc3_4 conda-forge. c-ares 1.19.1 hb547adb_0 conda-forge. c-bl",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:1724,modifiability,modul,module,1724," pandas to 1.5.3. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). Untitled-1.ipynb Cell 20 in <cell line: 1>(). ----> 1 import scanpy as sc. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/__init__.py:18, in <module>. 16 from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from 'pandas.api.types' (/Users/michael/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/pandas/api/types/__init__.py). ```. ### Versions. <details>. ```. conda list. # packages in environment at /Users/michael/mambaforge/envs/basic_comp_bio:. #. # Name Version Build Channel. anndata 0.9.1 pyhd8ed1ab_0 conda-forge. appnope 0.1.2 py39hca03da5_1001 anaconda. asttokens 2.0.5 pyhd3eb1b0_0 anaconda. backcall 0.2.0 pyhd3eb1b0_0 anaconda. blosc 1.21.4 hc338f07_0 conda-forge. brotli 1.0.9 h1a8c8d9_9 conda-forge. brotli-bin 1.0.9 h1a8c8d9_9 conda-forge. brotli-python 1.0.9 py39h23fbdae_9 conda-forge. bzip2 1.0.8 h3422bc3_4 conda-forge. c-ares 1.19.1 hb547adb_0 conda-forge. c-blosc2 2.10.0 h068da5f_0 conda-forge. ca-certif",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:2085,modifiability,pac,packages,2085,"-packages/scanpy/__init__.py:18, in <module>. 16 from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from 'pandas.api.types' (/Users/michael/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/pandas/api/types/__init__.py). ```. ### Versions. <details>. ```. conda list. # packages in environment at /Users/michael/mambaforge/envs/basic_comp_bio:. #. # Name Version Build Channel. anndata 0.9.1 pyhd8ed1ab_0 conda-forge. appnope 0.1.2 py39hca03da5_1001 anaconda. asttokens 2.0.5 pyhd3eb1b0_0 anaconda. backcall 0.2.0 pyhd3eb1b0_0 anaconda. blosc 1.21.4 hc338f07_0 conda-forge. brotli 1.0.9 h1a8c8d9_9 conda-forge. brotli-bin 1.0.9 h1a8c8d9_9 conda-forge. brotli-python 1.0.9 py39h23fbdae_9 conda-forge. bzip2 1.0.8 h3422bc3_4 conda-forge. c-ares 1.19.1 hb547adb_0 conda-forge. c-blosc2 2.10.0 h068da5f_0 conda-forge. ca-certificates 2022.4.26 hca03da5_0 anaconda. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. certifi 2022.6.15 py39hca03da5_0 anaconda. charset-normalizer 3.2.0 pyhd8ed1ab_0 conda-forge. colorama 0.4.6 pyhd8ed1ab_0 conda-forge. curl 8.1.2 hc52a3a8_1 conda-forge. cycler 0.11.0 pyhd8ed1ab_0 conda-forge. debugpy 1.5.1 py39hc",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:2134,modifiability,Version,Versions,2134,"from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from 'pandas.api.types' (/Users/michael/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/pandas/api/types/__init__.py). ```. ### Versions. <details>. ```. conda list. # packages in environment at /Users/michael/mambaforge/envs/basic_comp_bio:. #. # Name Version Build Channel. anndata 0.9.1 pyhd8ed1ab_0 conda-forge. appnope 0.1.2 py39hca03da5_1001 anaconda. asttokens 2.0.5 pyhd3eb1b0_0 anaconda. backcall 0.2.0 pyhd3eb1b0_0 anaconda. blosc 1.21.4 hc338f07_0 conda-forge. brotli 1.0.9 h1a8c8d9_9 conda-forge. brotli-bin 1.0.9 h1a8c8d9_9 conda-forge. brotli-python 1.0.9 py39h23fbdae_9 conda-forge. bzip2 1.0.8 h3422bc3_4 conda-forge. c-ares 1.19.1 hb547adb_0 conda-forge. c-blosc2 2.10.0 h068da5f_0 conda-forge. ca-certificates 2022.4.26 hca03da5_0 anaconda. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. certifi 2022.6.15 py39hca03da5_0 anaconda. charset-normalizer 3.2.0 pyhd8ed1ab_0 conda-forge. colorama 0.4.6 pyhd8ed1ab_0 conda-forge. curl 8.1.2 hc52a3a8_1 conda-forge. cycler 0.11.0 pyhd8ed1ab_0 conda-forge. debugpy 1.5.1 py39hc377ac9_0 anaconda. decorator 5.1.1 pyhd3eb1b0_0 a",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:2174,modifiability,pac,packages,2174,"om . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from 'pandas.api.types' (/Users/michael/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/pandas/api/types/__init__.py). ```. ### Versions. <details>. ```. conda list. # packages in environment at /Users/michael/mambaforge/envs/basic_comp_bio:. #. # Name Version Build Channel. anndata 0.9.1 pyhd8ed1ab_0 conda-forge. appnope 0.1.2 py39hca03da5_1001 anaconda. asttokens 2.0.5 pyhd3eb1b0_0 anaconda. backcall 0.2.0 pyhd3eb1b0_0 anaconda. blosc 1.21.4 hc338f07_0 conda-forge. brotli 1.0.9 h1a8c8d9_9 conda-forge. brotli-bin 1.0.9 h1a8c8d9_9 conda-forge. brotli-python 1.0.9 py39h23fbdae_9 conda-forge. bzip2 1.0.8 h3422bc3_4 conda-forge. c-ares 1.19.1 hb547adb_0 conda-forge. c-blosc2 2.10.0 h068da5f_0 conda-forge. ca-certificates 2022.4.26 hca03da5_0 anaconda. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. certifi 2022.6.15 py39hca03da5_0 anaconda. charset-normalizer 3.2.0 pyhd8ed1ab_0 conda-forge. colorama 0.4.6 pyhd8ed1ab_0 conda-forge. curl 8.1.2 hc52a3a8_1 conda-forge. cycler 0.11.0 pyhd8ed1ab_0 conda-forge. debugpy 1.5.1 py39hc377ac9_0 anaconda. decorator 5.1.1 pyhd3eb1b0_0 anaconda. dunamai 1.18.0 pyhd8ed1ab_0 con",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:2259,modifiability,Version,Version,2259,"l, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from 'pandas.api.types' (/Users/michael/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/pandas/api/types/__init__.py). ```. ### Versions. <details>. ```. conda list. # packages in environment at /Users/michael/mambaforge/envs/basic_comp_bio:. #. # Name Version Build Channel. anndata 0.9.1 pyhd8ed1ab_0 conda-forge. appnope 0.1.2 py39hca03da5_1001 anaconda. asttokens 2.0.5 pyhd3eb1b0_0 anaconda. backcall 0.2.0 pyhd3eb1b0_0 anaconda. blosc 1.21.4 hc338f07_0 conda-forge. brotli 1.0.9 h1a8c8d9_9 conda-forge. brotli-bin 1.0.9 h1a8c8d9_9 conda-forge. brotli-python 1.0.9 py39h23fbdae_9 conda-forge. bzip2 1.0.8 h3422bc3_4 conda-forge. c-ares 1.19.1 hb547adb_0 conda-forge. c-blosc2 2.10.0 h068da5f_0 conda-forge. ca-certificates 2022.4.26 hca03da5_0 anaconda. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. certifi 2022.6.15 py39hca03da5_0 anaconda. charset-normalizer 3.2.0 pyhd8ed1ab_0 conda-forge. colorama 0.4.6 pyhd8ed1ab_0 conda-forge. curl 8.1.2 hc52a3a8_1 conda-forge. cycler 0.11.0 pyhd8ed1ab_0 conda-forge. debugpy 1.5.1 py39hc377ac9_0 anaconda. decorator 5.1.1 pyhd3eb1b0_0 anaconda. dunamai 1.18.0 pyhd8ed1ab_0 conda-forge. entrypoints 0.4 py39hca03da5_0 anaconda. executing 0.8.3 pyhd3eb1b0_0 anaco",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:3108,modifiability,deco,decorator,3108,init__.py). ```. ### Versions. <details>. ```. conda list. # packages in environment at /Users/michael/mambaforge/envs/basic_comp_bio:. #. # Name Version Build Channel. anndata 0.9.1 pyhd8ed1ab_0 conda-forge. appnope 0.1.2 py39hca03da5_1001 anaconda. asttokens 2.0.5 pyhd3eb1b0_0 anaconda. backcall 0.2.0 pyhd3eb1b0_0 anaconda. blosc 1.21.4 hc338f07_0 conda-forge. brotli 1.0.9 h1a8c8d9_9 conda-forge. brotli-bin 1.0.9 h1a8c8d9_9 conda-forge. brotli-python 1.0.9 py39h23fbdae_9 conda-forge. bzip2 1.0.8 h3422bc3_4 conda-forge. c-ares 1.19.1 hb547adb_0 conda-forge. c-blosc2 2.10.0 h068da5f_0 conda-forge. ca-certificates 2022.4.26 hca03da5_0 anaconda. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. certifi 2022.6.15 py39hca03da5_0 anaconda. charset-normalizer 3.2.0 pyhd8ed1ab_0 conda-forge. colorama 0.4.6 pyhd8ed1ab_0 conda-forge. curl 8.1.2 hc52a3a8_1 conda-forge. cycler 0.11.0 pyhd8ed1ab_0 conda-forge. debugpy 1.5.1 py39hc377ac9_0 anaconda. decorator 5.1.1 pyhd3eb1b0_0 anaconda. dunamai 1.18.0 pyhd8ed1ab_0 conda-forge. entrypoints 0.4 py39hca03da5_0 anaconda. executing 0.8.3 pyhd3eb1b0_0 anaconda. fonttools 4.41.0 py39h0f82c59_0 conda-forge. freetype 2.12.1 hd633e50_1 conda-forge. get_version 3.5.4 pyhd8ed1ab_0 conda-forge. gettext 0.21.1 h0186832_0 conda-forge. git 2.41.0 pl5321h46e2b6d_0 conda-forge. h5py 3.9.0 nompi_py39he9c2634_101 conda-forge. hdf5 1.14.1 nompi_h3aba7b3_100 conda-forge. idna 3.4 pyhd8ed1ab_0 conda-forge. importlib-metadata 6.8.0 pyha770c72_0 conda-forge. importlib_metadata 6.8.0 hd8ed1ab_0 conda-forge. ipykernel 6.9.1 py39hca03da5_0 anaconda. ipython 8.3.0 py39hca03da5_0 anaconda. jedi 0.18.1 py39hca03da5_1 anaconda. joblib 1.3.0 pyhd8ed1ab_1 conda-forge. jupyter_client 7.2.2 py39hca03da5_0 anaconda. jupyter_core 4.10.0 py39hca03da5_0 anaconda. kiwisolver 1.4.4 py39haaf3ac1_1 conda-forge. krb5 1.21.1 h92f50d5_0 conda-forge. lcms2 2.15 hd835a16_1 conda-forge. legacy-api-wrap 1.2 py_0 conda-forge. lerc 4.0.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:5960,modifiability,pac,packaging,5960,hae82a92_0 conda-forge. libopenblas 0.3.23 openmp_hc731615_0 conda-forge. libpng 1.6.39 h76d750c_0 conda-forge. libsodium 1.0.18 h1a28f6b_0 anaconda. libsqlite 3.42.0 hb31c410_0 conda-forge. libssh2 1.11.0 h7a5bd25_0 conda-forge. libtiff 4.5.1 h23a1a89_0 conda-forge. libwebp-base 1.3.1 hb547adb_0 conda-forge. libxcb 1.15 hf346824_0 conda-forge. libzlib 1.2.13 h53f4e23_5 conda-forge. llvm-openmp 16.0.6 h1c12783_0 conda-forge. llvmlite 0.40.1 py39hbad4f83_0 conda-forge. lz4-c 1.9.4 hb7217d7_0 conda-forge. matplotlib-base 3.5.3 py39ha500c34_2 conda-forge. matplotlib-inline 0.1.2 pyhd3eb1b0_2 anaconda. munkres 1.0.7 py_1 bioconda. natsort 8.4.0 pyhd8ed1ab_0 conda-forge. ncurses 6.4 h7ea286d_0 conda-forge. nest-asyncio 1.5.5 py39hca03da5_0 anaconda. networkx 3.1 pyhd8ed1ab_0 conda-forge. numba 0.57.1 py39he8ed757_0 conda-forge. numexpr 2.8.4 py39hd28f0be_0 conda-forge. numpy 1.24.4 py39h485cf63_0 conda-forge. openjpeg 2.5.0 hbc2ba62_2 conda-forge. openssl 3.1.1 h53f4e23_1 conda-forge. packaging 23.1 pyhd8ed1ab_0 conda-forge. pandas 2.0.3 py39h6b13a34_1 conda-forge. parso 0.8.3 pyhd3eb1b0_0 anaconda. patsy 0.5.3 pyhd8ed1ab_0 conda-forge. pcre2 10.40 hb34f9b4_0 conda-forge. perl 5.32.1 4_hf2054a2_perl5 conda-forge. pexpect 4.8.0 pyhd3eb1b0_3 anaconda. pickleshare 0.7.5 pyhd3eb1b0_1003 anaconda. pillow 10.0.0 py39h1641143_0 conda-forge. pip 23.2 pyhd8ed1ab_0 conda-forge. platformdirs 3.9.1 pyhd8ed1ab_0 conda-forge. pooch 1.7.0 pyha770c72_3 conda-forge. prompt-toolkit 3.0.20 pyhd3eb1b0_0 anaconda. pthread-stubs 0.4 h27ca646_1001 conda-forge. ptyprocess 0.7.0 pyhd3eb1b0_2 anaconda. pure_eval 0.2.2 pyhd3eb1b0_0 anaconda. py-cpuinfo 9.0.0 pyhd8ed1ab_0 conda-forge. pygments 2.11.2 pyhd3eb1b0_0 anaconda. pynndescent 0.5.10 pyh1a96a4e_0 conda-forge. pyparsing 3.1.0 pyhd8ed1ab_0 conda-forge. pysocks 1.7.1 pyha2e5f31_6 conda-forge. pytables 3.8.0 py39h0da393b_2 conda-forge. python 3.9.16 hea58f1e_0_cpython conda-forge. python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge. python-tzdata 20,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:8043,modifiability,extens,extensions,8043, pyhd3eb1b0_0 anaconda. pynndescent 0.5.10 pyh1a96a4e_0 conda-forge. pyparsing 3.1.0 pyhd8ed1ab_0 conda-forge. pysocks 1.7.1 pyha2e5f31_6 conda-forge. pytables 3.8.0 py39h0da393b_2 conda-forge. python 3.9.16 hea58f1e_0_cpython conda-forge. python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge. python-tzdata 2023.3 pyhd8ed1ab_0 conda-forge. python_abi 3.9 3_cp39 conda-forge. pytz 2023.3 pyhd8ed1ab_0 conda-forge. pyzmq 22.3.0 py39hc377ac9_2 anaconda. readline 8.2 h92ec313_1 conda-forge. requests 2.31.0 pyhd8ed1ab_0 conda-forge. scanpy 1.7.2 pyhdfd78af_0 bioconda. scikit-learn 1.3.0 py39hd5c4a62_0 conda-forge. scipy 1.11.1 py39ha6b2cbd_0 conda-forge. seaborn 0.12.2 hd8ed1ab_0 conda-forge. seaborn-base 0.12.2 pyhd8ed1ab_0 conda-forge. setuptools 68.0.0 pyhd8ed1ab_0 conda-forge. setuptools-scm 7.1.0 pyhd8ed1ab_0 conda-forge. setuptools_scm 7.1.0 hd8ed1ab_0 conda-forge. sinfo 0.3.1 py_0 conda-forge. six 1.16.0 pyh6c4a22f_0 conda-forge. snappy 1.1.10 h17c5cce_0 conda-forge. stack_data 0.2.0 pyhd3eb1b0_0 anaconda. statsmodels 0.14.0 py39h8a366b7_1 conda-forge. stdlib-list 0.8.0 pyhd8ed1ab_0 conda-forge. tbb 2021.9.0 hffc8910_0 conda-forge. threadpoolctl 3.2.0 pyha21a80b_0 conda-forge. tk 8.6.12 he1e0b03_0 conda-forge. tomli 2.0.1 pyhd8ed1ab_0 conda-forge. tornado 6.1 py39h1a28f6b_0 anaconda. tqdm 4.65.0 pyhd8ed1ab_1 conda-forge. traitlets 5.1.1 pyhd3eb1b0_0 anaconda. typing-extensions 4.7.1 hd8ed1ab_0 conda-forge. typing_extensions 4.7.1 pyha770c72_0 conda-forge. tzdata 2023c h71feb2d_0 conda-forge. umap-learn 0.5.3 py39h2804cbe_1 conda-forge. unicodedata2 15.0.0 py39h02fc5c5_0 conda-forge. urllib3 2.0.3 pyhd8ed1ab_1 conda-forge. wcwidth 0.2.5 pyhd3eb1b0_0 anaconda. wheel 0.40.0 pyhd8ed1ab_1 conda-forge. xorg-libxau 1.0.11 hb547adb_0 conda-forge. xorg-libxdmcp 1.1.3 h27ca646_0 conda-forge. xz 5.2.6 h57fd34a_0 conda-forge. zeromq 4.3.4 hc377ac9_0 anaconda. zipp 3.16.2 pyhd8ed1ab_0 conda-forge. zlib-ng 2.0.7 h1a8c8d9_0 conda-forge. zstd 1.5.2 h4f39d0f_7 conda-forge. ```. </details>.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:479,performance,error,error,479,"import fails due to deprecated is_categorical method it pandas; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I installed scanpy as the first package in a brand new conda environment. This worked fine but when importing scanpy I get the error below. It sounds like this is because as of pandas 2.0 `is_categorical` is deprecated: https://stackoverflow.com/questions/76234312/importerror-cannot-import-name-is-categorical-from-pandas-api-types. This issue is also fixed when I downgrade pandas to 1.5.3. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). Untitled-1.ipynb Cell 20 in <cell line: 1>(). ----> 1 import scanpy as sc. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/__init__.py:18, in <module>. 16 from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:811,performance,Error,Error,811,"import fails due to deprecated is_categorical method it pandas; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I installed scanpy as the first package in a brand new conda environment. This worked fine but when importing scanpy I get the error below. It sounds like this is because as of pandas 2.0 `is_categorical` is deprecated: https://stackoverflow.com/questions/76234312/importerror-cannot-import-name-is-categorical-from-pandas-api-types. This issue is also fixed when I downgrade pandas to 1.5.3. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). Untitled-1.ipynb Cell 20 in <cell line: 1>(). ----> 1 import scanpy as sc. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/__init__.py:18, in <module>. 16 from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:2765,performance,cach,cached-property,2765,rom anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from 'pandas.api.types' (/Users/michael/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/pandas/api/types/__init__.py). ```. ### Versions. <details>. ```. conda list. # packages in environment at /Users/michael/mambaforge/envs/basic_comp_bio:. #. # Name Version Build Channel. anndata 0.9.1 pyhd8ed1ab_0 conda-forge. appnope 0.1.2 py39hca03da5_1001 anaconda. asttokens 2.0.5 pyhd3eb1b0_0 anaconda. backcall 0.2.0 pyhd3eb1b0_0 anaconda. blosc 1.21.4 hc338f07_0 conda-forge. brotli 1.0.9 h1a8c8d9_9 conda-forge. brotli-bin 1.0.9 h1a8c8d9_9 conda-forge. brotli-python 1.0.9 py39h23fbdae_9 conda-forge. bzip2 1.0.8 h3422bc3_4 conda-forge. c-ares 1.19.1 hb547adb_0 conda-forge. c-blosc2 2.10.0 h068da5f_0 conda-forge. ca-certificates 2022.4.26 hca03da5_0 anaconda. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. certifi 2022.6.15 py39hca03da5_0 anaconda. charset-normalizer 3.2.0 pyhd8ed1ab_0 conda-forge. colorama 0.4.6 pyhd8ed1ab_0 conda-forge. curl 8.1.2 hc52a3a8_1 conda-forge. cycler 0.11.0 pyhd8ed1ab_0 conda-forge. debugpy 1.5.1 py39hc377ac9_0 anaconda. decorator 5.1.1 pyhd3eb1b0_0 anaconda. dunamai 1.18.0 pyhd8ed1ab_0 conda-forge. entrypoints 0.4 py39hca03da5_0 anaconda. executing 0.8.3 pyhd3eb1b0_0 anaconda. fonttools 4.41.0 py39h0f82c59_0 conda-forge. freetype 2.12.1 hd633e50_1 conda-forge. get_version 3.5.4 pyhd8ed1ab_0 conda-forge. gettext 0.21.1 h0186832_0 conda-forge. git 2.41.0 pl5321h46e2b6d_0 conda-forge. h5py 3.9.0 nompi_py39he9c2634_101 conda-forge. hdf5 1.14.1 nompi_h3aba7b3_100 conda-forge. idna 3.4 pyhd8ed1ab_0 conda-forge. importlib-metadata 6.8.0 pyha770c72_0 conda-forge. importlib_metadata 6.8.0 hd8ed1ab_0 conda-forge. ipykernel 6.9.1 py39hca03da5_0 anaconda. ipython 8.3.0 py39hca03da5_0 ,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:5720,performance,network,networkx,5720,e. libgfortran5 12.2.0 h0eea778_31 conda-forge. libiconv 1.17 he4db4b2_0 conda-forge. libjpeg-turbo 2.1.5.1 h1a8c8d9_0 conda-forge. liblapack 3.9.0 17_osxarm64_openblas conda-forge. libllvm14 14.0.6 hd1a9a77_3 conda-forge. libnghttp2 1.52.0 hae82a92_0 conda-forge. libopenblas 0.3.23 openmp_hc731615_0 conda-forge. libpng 1.6.39 h76d750c_0 conda-forge. libsodium 1.0.18 h1a28f6b_0 anaconda. libsqlite 3.42.0 hb31c410_0 conda-forge. libssh2 1.11.0 h7a5bd25_0 conda-forge. libtiff 4.5.1 h23a1a89_0 conda-forge. libwebp-base 1.3.1 hb547adb_0 conda-forge. libxcb 1.15 hf346824_0 conda-forge. libzlib 1.2.13 h53f4e23_5 conda-forge. llvm-openmp 16.0.6 h1c12783_0 conda-forge. llvmlite 0.40.1 py39hbad4f83_0 conda-forge. lz4-c 1.9.4 hb7217d7_0 conda-forge. matplotlib-base 3.5.3 py39ha500c34_2 conda-forge. matplotlib-inline 0.1.2 pyhd3eb1b0_2 anaconda. munkres 1.0.7 py_1 bioconda. natsort 8.4.0 pyhd8ed1ab_0 conda-forge. ncurses 6.4 h7ea286d_0 conda-forge. nest-asyncio 1.5.5 py39hca03da5_0 anaconda. networkx 3.1 pyhd8ed1ab_0 conda-forge. numba 0.57.1 py39he8ed757_0 conda-forge. numexpr 2.8.4 py39hd28f0be_0 conda-forge. numpy 1.24.4 py39h485cf63_0 conda-forge. openjpeg 2.5.0 hbc2ba62_2 conda-forge. openssl 3.1.1 h53f4e23_1 conda-forge. packaging 23.1 pyhd8ed1ab_0 conda-forge. pandas 2.0.3 py39h6b13a34_1 conda-forge. parso 0.8.3 pyhd3eb1b0_0 anaconda. patsy 0.5.3 pyhd8ed1ab_0 conda-forge. pcre2 10.40 hb34f9b4_0 conda-forge. perl 5.32.1 4_hf2054a2_perl5 conda-forge. pexpect 4.8.0 pyhd3eb1b0_3 anaconda. pickleshare 0.7.5 pyhd3eb1b0_1003 anaconda. pillow 10.0.0 py39h1641143_0 conda-forge. pip 23.2 pyhd8ed1ab_0 conda-forge. platformdirs 3.9.1 pyhd8ed1ab_0 conda-forge. pooch 1.7.0 pyha770c72_3 conda-forge. prompt-toolkit 3.0.20 pyhd3eb1b0_0 anaconda. pthread-stubs 0.4 h27ca646_1001 conda-forge. ptyprocess 0.7.0 pyhd3eb1b0_2 anaconda. pure_eval 0.2.2 pyhd3eb1b0_0 anaconda. py-cpuinfo 9.0.0 pyhd8ed1ab_0 conda-forge. pygments 2.11.2 pyhd3eb1b0_0 anaconda. pynndescent 0.5.10 pyh1a96a4e_0 conda-f,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:6606,performance,cpu,cpuinfo,6606,4.0 pyhd8ed1ab_0 conda-forge. ncurses 6.4 h7ea286d_0 conda-forge. nest-asyncio 1.5.5 py39hca03da5_0 anaconda. networkx 3.1 pyhd8ed1ab_0 conda-forge. numba 0.57.1 py39he8ed757_0 conda-forge. numexpr 2.8.4 py39hd28f0be_0 conda-forge. numpy 1.24.4 py39h485cf63_0 conda-forge. openjpeg 2.5.0 hbc2ba62_2 conda-forge. openssl 3.1.1 h53f4e23_1 conda-forge. packaging 23.1 pyhd8ed1ab_0 conda-forge. pandas 2.0.3 py39h6b13a34_1 conda-forge. parso 0.8.3 pyhd3eb1b0_0 anaconda. patsy 0.5.3 pyhd8ed1ab_0 conda-forge. pcre2 10.40 hb34f9b4_0 conda-forge. perl 5.32.1 4_hf2054a2_perl5 conda-forge. pexpect 4.8.0 pyhd3eb1b0_3 anaconda. pickleshare 0.7.5 pyhd3eb1b0_1003 anaconda. pillow 10.0.0 py39h1641143_0 conda-forge. pip 23.2 pyhd8ed1ab_0 conda-forge. platformdirs 3.9.1 pyhd8ed1ab_0 conda-forge. pooch 1.7.0 pyha770c72_3 conda-forge. prompt-toolkit 3.0.20 pyhd3eb1b0_0 anaconda. pthread-stubs 0.4 h27ca646_1001 conda-forge. ptyprocess 0.7.0 pyhd3eb1b0_2 anaconda. pure_eval 0.2.2 pyhd3eb1b0_0 anaconda. py-cpuinfo 9.0.0 pyhd8ed1ab_0 conda-forge. pygments 2.11.2 pyhd3eb1b0_0 anaconda. pynndescent 0.5.10 pyh1a96a4e_0 conda-forge. pyparsing 3.1.0 pyhd8ed1ab_0 conda-forge. pysocks 1.7.1 pyha2e5f31_6 conda-forge. pytables 3.8.0 py39h0da393b_2 conda-forge. python 3.9.16 hea58f1e_0_cpython conda-forge. python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge. python-tzdata 2023.3 pyhd8ed1ab_0 conda-forge. python_abi 3.9 3_cp39 conda-forge. pytz 2023.3 pyhd8ed1ab_0 conda-forge. pyzmq 22.3.0 py39hc377ac9_2 anaconda. readline 8.2 h92ec313_1 conda-forge. requests 2.31.0 pyhd8ed1ab_0 conda-forge. scanpy 1.7.2 pyhdfd78af_0 bioconda. scikit-learn 1.3.0 py39hd5c4a62_0 conda-forge. scipy 1.11.1 py39ha6b2cbd_0 conda-forge. seaborn 0.12.2 hd8ed1ab_0 conda-forge. seaborn-base 0.12.2 pyhd8ed1ab_0 conda-forge. setuptools 68.0.0 pyhd8ed1ab_0 conda-forge. setuptools-scm 7.1.0 pyhd8ed1ab_0 conda-forge. setuptools_scm 7.1.0 hd8ed1ab_0 conda-forge. sinfo 0.3.1 py_0 conda-forge. six 1.16.0 pyh6c4a22f_0 conda-forge. snappy 1.1.1,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:7,reliability,fail,fails,7,"import fails due to deprecated is_categorical method it pandas; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I installed scanpy as the first package in a brand new conda environment. This worked fine but when importing scanpy I get the error below. It sounds like this is because as of pandas 2.0 `is_categorical` is deprecated: https://stackoverflow.com/questions/76234312/importerror-cannot-import-name-is-categorical-from-pandas-api-types. This issue is also fixed when I downgrade pandas to 1.5.3. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). Untitled-1.ipynb Cell 20 in <cell line: 1>(). ----> 1 import scanpy as sc. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/__init__.py:18, in <module>. 16 from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:479,safety,error,error,479,"import fails due to deprecated is_categorical method it pandas; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I installed scanpy as the first package in a brand new conda environment. This worked fine but when importing scanpy I get the error below. It sounds like this is because as of pandas 2.0 `is_categorical` is deprecated: https://stackoverflow.com/questions/76234312/importerror-cannot-import-name-is-categorical-from-pandas-api-types. This issue is also fixed when I downgrade pandas to 1.5.3. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). Untitled-1.ipynb Cell 20 in <cell line: 1>(). ----> 1 import scanpy as sc. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/__init__.py:18, in <module>. 16 from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:811,safety,Error,Error,811,"import fails due to deprecated is_categorical method it pandas; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I installed scanpy as the first package in a brand new conda environment. This worked fine but when importing scanpy I get the error below. It sounds like this is because as of pandas 2.0 `is_categorical` is deprecated: https://stackoverflow.com/questions/76234312/importerror-cannot-import-name-is-categorical-from-pandas-api-types. This issue is also fixed when I downgrade pandas to 1.5.3. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). Untitled-1.ipynb Cell 20 in <cell line: 1>(). ----> 1 import scanpy as sc. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/__init__.py:18, in <module>. 16 from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:1126,safety,modul,module,1126,"ed that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I installed scanpy as the first package in a brand new conda environment. This worked fine but when importing scanpy I get the error below. It sounds like this is because as of pandas 2.0 `is_categorical` is deprecated: https://stackoverflow.com/questions/76234312/importerror-cannot-import-name-is-categorical-from-pandas-api-types. This issue is also fixed when I downgrade pandas to 1.5.3. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). Untitled-1.ipynb Cell 20 in <cell line: 1>(). ----> 1 import scanpy as sc. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/__init__.py:18, in <module>. 16 from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from 'pandas.api.types' (/Users/michael/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/pandas/api/types/__init__.py). ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:1238,safety,log,logging,1238," scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I installed scanpy as the first package in a brand new conda environment. This worked fine but when importing scanpy I get the error below. It sounds like this is because as of pandas 2.0 `is_categorical` is deprecated: https://stackoverflow.com/questions/76234312/importerror-cannot-import-name-is-categorical-from-pandas-api-types. This issue is also fixed when I downgrade pandas to 1.5.3. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). Untitled-1.ipynb Cell 20 in <cell line: 1>(). ----> 1 import scanpy as sc. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/__init__.py:18, in <module>. 16 from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from 'pandas.api.types' (/Users/michael/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/pandas/api/types/__init__.py). ```. ### Versions. <details>. ```. conda list. # packages in environment at /Users/michael/mambaforge/envs/basic_comp",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:1500,safety,modul,module,1500,"ike this is because as of pandas 2.0 `is_categorical` is deprecated: https://stackoverflow.com/questions/76234312/importerror-cannot-import-name-is-categorical-from-pandas-api-types. This issue is also fixed when I downgrade pandas to 1.5.3. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). Untitled-1.ipynb Cell 20 in <cell line: 1>(). ----> 1 import scanpy as sc. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/__init__.py:18, in <module>. 16 from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from 'pandas.api.types' (/Users/michael/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/pandas/api/types/__init__.py). ```. ### Versions. <details>. ```. conda list. # packages in environment at /Users/michael/mambaforge/envs/basic_comp_bio:. #. # Name Version Build Channel. anndata 0.9.1 pyhd8ed1ab_0 conda-forge. appnope 0.1.2 py39hca03da5_1001 anaconda. asttokens 2.0.5 pyhd3eb1b0_0 anaconda. backcall 0.2.0 pyhd3eb1b0_0 anaconda. blosc 1.21.4 hc338f07_0 conda-forge. brotli 1.0.9 h1a8c8d9_9 c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:1724,safety,modul,module,1724," pandas to 1.5.3. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). Untitled-1.ipynb Cell 20 in <cell line: 1>(). ----> 1 import scanpy as sc. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/__init__.py:18, in <module>. 16 from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from 'pandas.api.types' (/Users/michael/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/pandas/api/types/__init__.py). ```. ### Versions. <details>. ```. conda list. # packages in environment at /Users/michael/mambaforge/envs/basic_comp_bio:. #. # Name Version Build Channel. anndata 0.9.1 pyhd8ed1ab_0 conda-forge. appnope 0.1.2 py39hca03da5_1001 anaconda. asttokens 2.0.5 pyhd3eb1b0_0 anaconda. backcall 0.2.0 pyhd3eb1b0_0 anaconda. blosc 1.21.4 hc338f07_0 conda-forge. brotli 1.0.9 h1a8c8d9_9 conda-forge. brotli-bin 1.0.9 h1a8c8d9_9 conda-forge. brotli-python 1.0.9 py39h23fbdae_9 conda-forge. bzip2 1.0.8 h3422bc3_4 conda-forge. c-ares 1.19.1 hb547adb_0 conda-forge. c-blosc2 2.10.0 h068da5f_0 conda-forge. ca-certif",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:1238,security,log,logging,1238," scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I installed scanpy as the first package in a brand new conda environment. This worked fine but when importing scanpy I get the error below. It sounds like this is because as of pandas 2.0 `is_categorical` is deprecated: https://stackoverflow.com/questions/76234312/importerror-cannot-import-name-is-categorical-from-pandas-api-types. This issue is also fixed when I downgrade pandas to 1.5.3. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). Untitled-1.ipynb Cell 20 in <cell line: 1>(). ----> 1 import scanpy as sc. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/__init__.py:18, in <module>. 16 from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from 'pandas.api.types' (/Users/michael/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/pandas/api/types/__init__.py). ```. ### Versions. <details>. ```. conda list. # packages in environment at /Users/michael/mambaforge/envs/basic_comp",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:2721,security,certif,certificates,2721,ule>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from 'pandas.api.types' (/Users/michael/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/pandas/api/types/__init__.py). ```. ### Versions. <details>. ```. conda list. # packages in environment at /Users/michael/mambaforge/envs/basic_comp_bio:. #. # Name Version Build Channel. anndata 0.9.1 pyhd8ed1ab_0 conda-forge. appnope 0.1.2 py39hca03da5_1001 anaconda. asttokens 2.0.5 pyhd3eb1b0_0 anaconda. backcall 0.2.0 pyhd3eb1b0_0 anaconda. blosc 1.21.4 hc338f07_0 conda-forge. brotli 1.0.9 h1a8c8d9_9 conda-forge. brotli-bin 1.0.9 h1a8c8d9_9 conda-forge. brotli-python 1.0.9 py39h23fbdae_9 conda-forge. bzip2 1.0.8 h3422bc3_4 conda-forge. c-ares 1.19.1 hb547adb_0 conda-forge. c-blosc2 2.10.0 h068da5f_0 conda-forge. ca-certificates 2022.4.26 hca03da5_0 anaconda. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. certifi 2022.6.15 py39hca03da5_0 anaconda. charset-normalizer 3.2.0 pyhd8ed1ab_0 conda-forge. colorama 0.4.6 pyhd8ed1ab_0 conda-forge. curl 8.1.2 hc52a3a8_1 conda-forge. cycler 0.11.0 pyhd8ed1ab_0 conda-forge. debugpy 1.5.1 py39hc377ac9_0 anaconda. decorator 5.1.1 pyhd3eb1b0_0 anaconda. dunamai 1.18.0 pyhd8ed1ab_0 conda-forge. entrypoints 0.4 py39hca03da5_0 anaconda. executing 0.8.3 pyhd3eb1b0_0 anaconda. fonttools 4.41.0 py39h0f82c59_0 conda-forge. freetype 2.12.1 hd633e50_1 conda-forge. get_version 3.5.4 pyhd8ed1ab_0 conda-forge. gettext 0.21.1 h0186832_0 conda-forge. git 2.41.0 pl5321h46e2b6d_0 conda-forge. h5py 3.9.0 nompi_py39he9c2634_101 conda-forge. hdf5 1.14.1 nompi_h3aba7b3_100 conda-forge. idna 3.4 pyhd8ed1ab_0 conda-forge. importlib-metadata 6.8.0 pyha770c72_0 conda-forge. importlib_metadata 6.8.0 hd8ed1ab_0 conda-forge. ipykernel 6.9.1 py39hca0,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:2859,security,certif,certifi,2859,.preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from 'pandas.api.types' (/Users/michael/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/pandas/api/types/__init__.py). ```. ### Versions. <details>. ```. conda list. # packages in environment at /Users/michael/mambaforge/envs/basic_comp_bio:. #. # Name Version Build Channel. anndata 0.9.1 pyhd8ed1ab_0 conda-forge. appnope 0.1.2 py39hca03da5_1001 anaconda. asttokens 2.0.5 pyhd3eb1b0_0 anaconda. backcall 0.2.0 pyhd3eb1b0_0 anaconda. blosc 1.21.4 hc338f07_0 conda-forge. brotli 1.0.9 h1a8c8d9_9 conda-forge. brotli-bin 1.0.9 h1a8c8d9_9 conda-forge. brotli-python 1.0.9 py39h23fbdae_9 conda-forge. bzip2 1.0.8 h3422bc3_4 conda-forge. c-ares 1.19.1 hb547adb_0 conda-forge. c-blosc2 2.10.0 h068da5f_0 conda-forge. ca-certificates 2022.4.26 hca03da5_0 anaconda. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. certifi 2022.6.15 py39hca03da5_0 anaconda. charset-normalizer 3.2.0 pyhd8ed1ab_0 conda-forge. colorama 0.4.6 pyhd8ed1ab_0 conda-forge. curl 8.1.2 hc52a3a8_1 conda-forge. cycler 0.11.0 pyhd8ed1ab_0 conda-forge. debugpy 1.5.1 py39hc377ac9_0 anaconda. decorator 5.1.1 pyhd3eb1b0_0 anaconda. dunamai 1.18.0 pyhd8ed1ab_0 conda-forge. entrypoints 0.4 py39hca03da5_0 anaconda. executing 0.8.3 pyhd3eb1b0_0 anaconda. fonttools 4.41.0 py39h0f82c59_0 conda-forge. freetype 2.12.1 hd633e50_1 conda-forge. get_version 3.5.4 pyhd8ed1ab_0 conda-forge. gettext 0.21.1 h0186832_0 conda-forge. git 2.41.0 pl5321h46e2b6d_0 conda-forge. h5py 3.9.0 nompi_py39he9c2634_101 conda-forge. hdf5 1.14.1 nompi_h3aba7b3_100 conda-forge. idna 3.4 pyhd8ed1ab_0 conda-forge. importlib-metadata 6.8.0 pyha770c72_0 conda-forge. importlib_metadata 6.8.0 hd8ed1ab_0 conda-forge. ipykernel 6.9.1 py39hca03da5_0 anaconda. ipython 8.3.0 py39hca03da5_0 anaconda. jedi 0.18.1 py39hca03da5_1 anaconda. joblib 1.3.0 pyhd8ed1ab_1 conda-forge. jupy,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:5720,security,network,networkx,5720,e. libgfortran5 12.2.0 h0eea778_31 conda-forge. libiconv 1.17 he4db4b2_0 conda-forge. libjpeg-turbo 2.1.5.1 h1a8c8d9_0 conda-forge. liblapack 3.9.0 17_osxarm64_openblas conda-forge. libllvm14 14.0.6 hd1a9a77_3 conda-forge. libnghttp2 1.52.0 hae82a92_0 conda-forge. libopenblas 0.3.23 openmp_hc731615_0 conda-forge. libpng 1.6.39 h76d750c_0 conda-forge. libsodium 1.0.18 h1a28f6b_0 anaconda. libsqlite 3.42.0 hb31c410_0 conda-forge. libssh2 1.11.0 h7a5bd25_0 conda-forge. libtiff 4.5.1 h23a1a89_0 conda-forge. libwebp-base 1.3.1 hb547adb_0 conda-forge. libxcb 1.15 hf346824_0 conda-forge. libzlib 1.2.13 h53f4e23_5 conda-forge. llvm-openmp 16.0.6 h1c12783_0 conda-forge. llvmlite 0.40.1 py39hbad4f83_0 conda-forge. lz4-c 1.9.4 hb7217d7_0 conda-forge. matplotlib-base 3.5.3 py39ha500c34_2 conda-forge. matplotlib-inline 0.1.2 pyhd3eb1b0_2 anaconda. munkres 1.0.7 py_1 bioconda. natsort 8.4.0 pyhd8ed1ab_0 conda-forge. ncurses 6.4 h7ea286d_0 conda-forge. nest-asyncio 1.5.5 py39hca03da5_0 anaconda. networkx 3.1 pyhd8ed1ab_0 conda-forge. numba 0.57.1 py39he8ed757_0 conda-forge. numexpr 2.8.4 py39hd28f0be_0 conda-forge. numpy 1.24.4 py39h485cf63_0 conda-forge. openjpeg 2.5.0 hbc2ba62_2 conda-forge. openssl 3.1.1 h53f4e23_1 conda-forge. packaging 23.1 pyhd8ed1ab_0 conda-forge. pandas 2.0.3 py39h6b13a34_1 conda-forge. parso 0.8.3 pyhd3eb1b0_0 anaconda. patsy 0.5.3 pyhd8ed1ab_0 conda-forge. pcre2 10.40 hb34f9b4_0 conda-forge. perl 5.32.1 4_hf2054a2_perl5 conda-forge. pexpect 4.8.0 pyhd3eb1b0_3 anaconda. pickleshare 0.7.5 pyhd3eb1b0_1003 anaconda. pillow 10.0.0 py39h1641143_0 conda-forge. pip 23.2 pyhd8ed1ab_0 conda-forge. platformdirs 3.9.1 pyhd8ed1ab_0 conda-forge. pooch 1.7.0 pyha770c72_3 conda-forge. prompt-toolkit 3.0.20 pyhd3eb1b0_0 anaconda. pthread-stubs 0.4 h27ca646_1001 conda-forge. ptyprocess 0.7.0 pyhd3eb1b0_2 anaconda. pure_eval 0.2.2 pyhd3eb1b0_0 anaconda. py-cpuinfo 9.0.0 pyhd8ed1ab_0 conda-forge. pygments 2.11.2 pyhd3eb1b0_0 anaconda. pynndescent 0.5.10 pyh1a96a4e_0 conda-f,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:923,testability,Trace,Traceback,923,"import fails due to deprecated is_categorical method it pandas; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I installed scanpy as the first package in a brand new conda environment. This worked fine but when importing scanpy I get the error below. It sounds like this is because as of pandas 2.0 `is_categorical` is deprecated: https://stackoverflow.com/questions/76234312/importerror-cannot-import-name-is-categorical-from-pandas-api-types. This issue is also fixed when I downgrade pandas to 1.5.3. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). Untitled-1.ipynb Cell 20 in <cell line: 1>(). ----> 1 import scanpy as sc. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/__init__.py:18, in <module>. 16 from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:1238,testability,log,logging,1238," scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I installed scanpy as the first package in a brand new conda environment. This worked fine but when importing scanpy I get the error below. It sounds like this is because as of pandas 2.0 `is_categorical` is deprecated: https://stackoverflow.com/questions/76234312/importerror-cannot-import-name-is-categorical-from-pandas-api-types. This issue is also fixed when I downgrade pandas to 1.5.3. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). Untitled-1.ipynb Cell 20 in <cell line: 1>(). ----> 1 import scanpy as sc. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/__init__.py:18, in <module>. 16 from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from 'pandas.api.types' (/Users/michael/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/pandas/api/types/__init__.py). ```. ### Versions. <details>. ```. conda list. # packages in environment at /Users/michael/mambaforge/envs/basic_comp",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:6487,testability,stub,stubs,6487,3.5.3 py39ha500c34_2 conda-forge. matplotlib-inline 0.1.2 pyhd3eb1b0_2 anaconda. munkres 1.0.7 py_1 bioconda. natsort 8.4.0 pyhd8ed1ab_0 conda-forge. ncurses 6.4 h7ea286d_0 conda-forge. nest-asyncio 1.5.5 py39hca03da5_0 anaconda. networkx 3.1 pyhd8ed1ab_0 conda-forge. numba 0.57.1 py39he8ed757_0 conda-forge. numexpr 2.8.4 py39hd28f0be_0 conda-forge. numpy 1.24.4 py39h485cf63_0 conda-forge. openjpeg 2.5.0 hbc2ba62_2 conda-forge. openssl 3.1.1 h53f4e23_1 conda-forge. packaging 23.1 pyhd8ed1ab_0 conda-forge. pandas 2.0.3 py39h6b13a34_1 conda-forge. parso 0.8.3 pyhd3eb1b0_0 anaconda. patsy 0.5.3 pyhd8ed1ab_0 conda-forge. pcre2 10.40 hb34f9b4_0 conda-forge. perl 5.32.1 4_hf2054a2_perl5 conda-forge. pexpect 4.8.0 pyhd3eb1b0_3 anaconda. pickleshare 0.7.5 pyhd3eb1b0_1003 anaconda. pillow 10.0.0 py39h1641143_0 conda-forge. pip 23.2 pyhd8ed1ab_0 conda-forge. platformdirs 3.9.1 pyhd8ed1ab_0 conda-forge. pooch 1.7.0 pyha770c72_3 conda-forge. prompt-toolkit 3.0.20 pyhd3eb1b0_0 anaconda. pthread-stubs 0.4 h27ca646_1001 conda-forge. ptyprocess 0.7.0 pyhd3eb1b0_2 anaconda. pure_eval 0.2.2 pyhd3eb1b0_0 anaconda. py-cpuinfo 9.0.0 pyhd8ed1ab_0 conda-forge. pygments 2.11.2 pyhd3eb1b0_0 anaconda. pynndescent 0.5.10 pyh1a96a4e_0 conda-forge. pyparsing 3.1.0 pyhd8ed1ab_0 conda-forge. pysocks 1.7.1 pyha2e5f31_6 conda-forge. pytables 3.8.0 py39h0da393b_2 conda-forge. python 3.9.16 hea58f1e_0_cpython conda-forge. python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge. python-tzdata 2023.3 pyhd8ed1ab_0 conda-forge. python_abi 3.9 3_cp39 conda-forge. pytz 2023.3 pyhd8ed1ab_0 conda-forge. pyzmq 22.3.0 py39hc377ac9_2 anaconda. readline 8.2 h92ec313_1 conda-forge. requests 2.31.0 pyhd8ed1ab_0 conda-forge. scanpy 1.7.2 pyhdfd78af_0 bioconda. scikit-learn 1.3.0 py39hd5c4a62_0 conda-forge. scipy 1.11.1 py39ha6b2cbd_0 conda-forge. seaborn 0.12.2 hd8ed1ab_0 conda-forge. seaborn-base 0.12.2 pyhd8ed1ab_0 conda-forge. setuptools 68.0.0 pyhd8ed1ab_0 conda-forge. setuptools-scm 7.1.0 pyhd8ed1ab_0 conda-forge. setu,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:192,usability,confirm,confirmed,192,"import fails due to deprecated is_categorical method it pandas; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I installed scanpy as the first package in a brand new conda environment. This worked fine but when importing scanpy I get the error below. It sounds like this is because as of pandas 2.0 `is_categorical` is deprecated: https://stackoverflow.com/questions/76234312/importerror-cannot-import-name-is-categorical-from-pandas-api-types. This issue is also fixed when I downgrade pandas to 1.5.3. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). Untitled-1.ipynb Cell 20 in <cell line: 1>(). ----> 1 import scanpy as sc. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/__init__.py:18, in <module>. 16 from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:275,usability,confirm,confirmed,275,"import fails due to deprecated is_categorical method it pandas; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I installed scanpy as the first package in a brand new conda environment. This worked fine but when importing scanpy I get the error below. It sounds like this is because as of pandas 2.0 `is_categorical` is deprecated: https://stackoverflow.com/questions/76234312/importerror-cannot-import-name-is-categorical-from-pandas-api-types. This issue is also fixed when I downgrade pandas to 1.5.3. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). Untitled-1.ipynb Cell 20 in <cell line: 1>(). ----> 1 import scanpy as sc. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/__init__.py:18, in <module>. 16 from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:479,usability,error,error,479,"import fails due to deprecated is_categorical method it pandas; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I installed scanpy as the first package in a brand new conda environment. This worked fine but when importing scanpy I get the error below. It sounds like this is because as of pandas 2.0 `is_categorical` is deprecated: https://stackoverflow.com/questions/76234312/importerror-cannot-import-name-is-categorical-from-pandas-api-types. This issue is also fixed when I downgrade pandas to 1.5.3. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). Untitled-1.ipynb Cell 20 in <cell line: 1>(). ----> 1 import scanpy as sc. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/__init__.py:18, in <module>. 16 from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:749,usability,Minim,Minimal,749,"import fails due to deprecated is_categorical method it pandas; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I installed scanpy as the first package in a brand new conda environment. This worked fine but when importing scanpy I get the error below. It sounds like this is because as of pandas 2.0 `is_categorical` is deprecated: https://stackoverflow.com/questions/76234312/importerror-cannot-import-name-is-categorical-from-pandas-api-types. This issue is also fixed when I downgrade pandas to 1.5.3. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). Untitled-1.ipynb Cell 20 in <cell line: 1>(). ----> 1 import scanpy as sc. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/__init__.py:18, in <module>. 16 from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:811,usability,Error,Error,811,"import fails due to deprecated is_categorical method it pandas; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I installed scanpy as the first package in a brand new conda environment. This worked fine but when importing scanpy I get the error below. It sounds like this is because as of pandas 2.0 `is_categorical` is deprecated: https://stackoverflow.com/questions/76234312/importerror-cannot-import-name-is-categorical-from-pandas-api-types. This issue is also fixed when I downgrade pandas to 1.5.3. ### Minimal code sample. ```python. import scanpy as sc. ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). Untitled-1.ipynb Cell 20 in <cell line: 1>(). ----> 1 import scanpy as sc. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/__init__.py:18, in <module>. 16 from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:2021,usability,User,Users,2021,"y as sc. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/__init__.py:18, in <module>. 16 from . import preprocessing as pp. 17 from . import plotting as pl. ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from 'pandas.api.types' (/Users/michael/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/pandas/api/types/__init__.py). ```. ### Versions. <details>. ```. conda list. # packages in environment at /Users/michael/mambaforge/envs/basic_comp_bio:. #. # Name Version Build Channel. anndata 0.9.1 pyhd8ed1ab_0 conda-forge. appnope 0.1.2 py39hca03da5_1001 anaconda. asttokens 2.0.5 pyhd3eb1b0_0 anaconda. backcall 0.2.0 pyhd3eb1b0_0 anaconda. blosc 1.21.4 hc338f07_0 conda-forge. brotli 1.0.9 h1a8c8d9_9 conda-forge. brotli-bin 1.0.9 h1a8c8d9_9 conda-forge. brotli-python 1.0.9 py39h23fbdae_9 conda-forge. bzip2 1.0.8 h3422bc3_4 conda-forge. c-ares 1.19.1 hb547adb_0 conda-forge. c-blosc2 2.10.0 h068da5f_0 conda-forge. ca-certificates 2022.4.26 hca03da5_0 anaconda. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. certifi 2022.6.15 py39hca03da5_0 anaconda. charset-normalizer 3.2.0 pyhd8ed1ab_0 conda-forge. colorama 0.4.6 pyhd8ed1ab_0 conda-forge. curl 8.1.2 hc52a3a8_1 conda-fo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:2202,usability,User,Users,2202," ---> 18 from . import datasets, logging, queries, external, get. 20 from anndata import AnnData, concat. 21 from anndata import (. 22 read_h5ad,. 23 read_csv,. (...). 29 read_umi_tools,. 30 ). File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/__init__.py:4, in <module>. 2 from . import pl. 3 from . import pp. ----> 4 from . import exporting. 6 import sys. 7 from .. import _utils. File ~/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/scanpy/external/exporting.py:14, in <module>. 12 import matplotlib.pyplot as plt. 13 from anndata import AnnData. ---> 14 from pandas.api.types import is_categorical. 16 from ..preprocessing._utils import _get_mean_var. 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from 'pandas.api.types' (/Users/michael/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/pandas/api/types/__init__.py). ```. ### Versions. <details>. ```. conda list. # packages in environment at /Users/michael/mambaforge/envs/basic_comp_bio:. #. # Name Version Build Channel. anndata 0.9.1 pyhd8ed1ab_0 conda-forge. appnope 0.1.2 py39hca03da5_1001 anaconda. asttokens 2.0.5 pyhd3eb1b0_0 anaconda. backcall 0.2.0 pyhd3eb1b0_0 anaconda. blosc 1.21.4 hc338f07_0 conda-forge. brotli 1.0.9 h1a8c8d9_9 conda-forge. brotli-bin 1.0.9 h1a8c8d9_9 conda-forge. brotli-python 1.0.9 py39h23fbdae_9 conda-forge. bzip2 1.0.8 h3422bc3_4 conda-forge. c-ares 1.19.1 hb547adb_0 conda-forge. c-blosc2 2.10.0 h068da5f_0 conda-forge. ca-certificates 2022.4.26 hca03da5_0 anaconda. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. certifi 2022.6.15 py39hca03da5_0 anaconda. charset-normalizer 3.2.0 pyhd8ed1ab_0 conda-forge. colorama 0.4.6 pyhd8ed1ab_0 conda-forge. curl 8.1.2 hc52a3a8_1 conda-forge. cycler 0.11.0 pyhd8ed1ab_0 conda-forge. debugpy 1.5.1 py39hc377ac9_0 anaconda. decorator 5.1.1 pyhd3eb1b0_0 anaconda. dunamai 1.18.0 pyhd8ed1ab_0 conda-forge. entrypoints 0.4 p",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:6441,usability,tool,toolkit,6441,.9.4 hb7217d7_0 conda-forge. matplotlib-base 3.5.3 py39ha500c34_2 conda-forge. matplotlib-inline 0.1.2 pyhd3eb1b0_2 anaconda. munkres 1.0.7 py_1 bioconda. natsort 8.4.0 pyhd8ed1ab_0 conda-forge. ncurses 6.4 h7ea286d_0 conda-forge. nest-asyncio 1.5.5 py39hca03da5_0 anaconda. networkx 3.1 pyhd8ed1ab_0 conda-forge. numba 0.57.1 py39he8ed757_0 conda-forge. numexpr 2.8.4 py39hd28f0be_0 conda-forge. numpy 1.24.4 py39h485cf63_0 conda-forge. openjpeg 2.5.0 hbc2ba62_2 conda-forge. openssl 3.1.1 h53f4e23_1 conda-forge. packaging 23.1 pyhd8ed1ab_0 conda-forge. pandas 2.0.3 py39h6b13a34_1 conda-forge. parso 0.8.3 pyhd3eb1b0_0 anaconda. patsy 0.5.3 pyhd8ed1ab_0 conda-forge. pcre2 10.40 hb34f9b4_0 conda-forge. perl 5.32.1 4_hf2054a2_perl5 conda-forge. pexpect 4.8.0 pyhd3eb1b0_3 anaconda. pickleshare 0.7.5 pyhd3eb1b0_1003 anaconda. pillow 10.0.0 py39h1641143_0 conda-forge. pip 23.2 pyhd8ed1ab_0 conda-forge. platformdirs 3.9.1 pyhd8ed1ab_0 conda-forge. pooch 1.7.0 pyha770c72_3 conda-forge. prompt-toolkit 3.0.20 pyhd3eb1b0_0 anaconda. pthread-stubs 0.4 h27ca646_1001 conda-forge. ptyprocess 0.7.0 pyhd3eb1b0_2 anaconda. pure_eval 0.2.2 pyhd3eb1b0_0 anaconda. py-cpuinfo 9.0.0 pyhd8ed1ab_0 conda-forge. pygments 2.11.2 pyhd3eb1b0_0 anaconda. pynndescent 0.5.10 pyh1a96a4e_0 conda-forge. pyparsing 3.1.0 pyhd8ed1ab_0 conda-forge. pysocks 1.7.1 pyha2e5f31_6 conda-forge. pytables 3.8.0 py39h0da393b_2 conda-forge. python 3.9.16 hea58f1e_0_cpython conda-forge. python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge. python-tzdata 2023.3 pyhd8ed1ab_0 conda-forge. python_abi 3.9 3_cp39 conda-forge. pytz 2023.3 pyhd8ed1ab_0 conda-forge. pyzmq 22.3.0 py39hc377ac9_2 anaconda. readline 8.2 h92ec313_1 conda-forge. requests 2.31.0 pyhd8ed1ab_0 conda-forge. scanpy 1.7.2 pyhdfd78af_0 bioconda. scikit-learn 1.3.0 py39hd5c4a62_0 conda-forge. scipy 1.11.1 py39ha6b2cbd_0 conda-forge. seaborn 0.12.2 hd8ed1ab_0 conda-forge. seaborn-base 0.12.2 pyhd8ed1ab_0 conda-forge. setuptools 68.0.0 pyhd8ed1ab_0 conda-forge. setupt,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:7229,usability,learn,learn,7229,ckleshare 0.7.5 pyhd3eb1b0_1003 anaconda. pillow 10.0.0 py39h1641143_0 conda-forge. pip 23.2 pyhd8ed1ab_0 conda-forge. platformdirs 3.9.1 pyhd8ed1ab_0 conda-forge. pooch 1.7.0 pyha770c72_3 conda-forge. prompt-toolkit 3.0.20 pyhd3eb1b0_0 anaconda. pthread-stubs 0.4 h27ca646_1001 conda-forge. ptyprocess 0.7.0 pyhd3eb1b0_2 anaconda. pure_eval 0.2.2 pyhd3eb1b0_0 anaconda. py-cpuinfo 9.0.0 pyhd8ed1ab_0 conda-forge. pygments 2.11.2 pyhd3eb1b0_0 anaconda. pynndescent 0.5.10 pyh1a96a4e_0 conda-forge. pyparsing 3.1.0 pyhd8ed1ab_0 conda-forge. pysocks 1.7.1 pyha2e5f31_6 conda-forge. pytables 3.8.0 py39h0da393b_2 conda-forge. python 3.9.16 hea58f1e_0_cpython conda-forge. python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge. python-tzdata 2023.3 pyhd8ed1ab_0 conda-forge. python_abi 3.9 3_cp39 conda-forge. pytz 2023.3 pyhd8ed1ab_0 conda-forge. pyzmq 22.3.0 py39hc377ac9_2 anaconda. readline 8.2 h92ec313_1 conda-forge. requests 2.31.0 pyhd8ed1ab_0 conda-forge. scanpy 1.7.2 pyhdfd78af_0 bioconda. scikit-learn 1.3.0 py39hd5c4a62_0 conda-forge. scipy 1.11.1 py39ha6b2cbd_0 conda-forge. seaborn 0.12.2 hd8ed1ab_0 conda-forge. seaborn-base 0.12.2 pyhd8ed1ab_0 conda-forge. setuptools 68.0.0 pyhd8ed1ab_0 conda-forge. setuptools-scm 7.1.0 pyhd8ed1ab_0 conda-forge. setuptools_scm 7.1.0 hd8ed1ab_0 conda-forge. sinfo 0.3.1 py_0 conda-forge. six 1.16.0 pyh6c4a22f_0 conda-forge. snappy 1.1.10 h17c5cce_0 conda-forge. stack_data 0.2.0 pyhd3eb1b0_0 anaconda. statsmodels 0.14.0 py39h8a366b7_1 conda-forge. stdlib-list 0.8.0 pyhd8ed1ab_0 conda-forge. tbb 2021.9.0 hffc8910_0 conda-forge. threadpoolctl 3.2.0 pyha21a80b_0 conda-forge. tk 8.6.12 he1e0b03_0 conda-forge. tomli 2.0.1 pyhd8ed1ab_0 conda-forge. tornado 6.1 py39h1a28f6b_0 anaconda. tqdm 4.65.0 pyhd8ed1ab_1 conda-forge. traitlets 5.1.1 pyhd3eb1b0_0 anaconda. typing-extensions 4.7.1 hd8ed1ab_0 conda-forge. typing_extensions 4.7.1 pyha770c72_0 conda-forge. tzdata 2023c h71feb2d_0 conda-forge. umap-learn 0.5.3 py39h2804cbe_1 conda-forge. unicodedata2 15.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2564:8176,usability,learn,learn,8176, pyhd3eb1b0_0 anaconda. pynndescent 0.5.10 pyh1a96a4e_0 conda-forge. pyparsing 3.1.0 pyhd8ed1ab_0 conda-forge. pysocks 1.7.1 pyha2e5f31_6 conda-forge. pytables 3.8.0 py39h0da393b_2 conda-forge. python 3.9.16 hea58f1e_0_cpython conda-forge. python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge. python-tzdata 2023.3 pyhd8ed1ab_0 conda-forge. python_abi 3.9 3_cp39 conda-forge. pytz 2023.3 pyhd8ed1ab_0 conda-forge. pyzmq 22.3.0 py39hc377ac9_2 anaconda. readline 8.2 h92ec313_1 conda-forge. requests 2.31.0 pyhd8ed1ab_0 conda-forge. scanpy 1.7.2 pyhdfd78af_0 bioconda. scikit-learn 1.3.0 py39hd5c4a62_0 conda-forge. scipy 1.11.1 py39ha6b2cbd_0 conda-forge. seaborn 0.12.2 hd8ed1ab_0 conda-forge. seaborn-base 0.12.2 pyhd8ed1ab_0 conda-forge. setuptools 68.0.0 pyhd8ed1ab_0 conda-forge. setuptools-scm 7.1.0 pyhd8ed1ab_0 conda-forge. setuptools_scm 7.1.0 hd8ed1ab_0 conda-forge. sinfo 0.3.1 py_0 conda-forge. six 1.16.0 pyh6c4a22f_0 conda-forge. snappy 1.1.10 h17c5cce_0 conda-forge. stack_data 0.2.0 pyhd3eb1b0_0 anaconda. statsmodels 0.14.0 py39h8a366b7_1 conda-forge. stdlib-list 0.8.0 pyhd8ed1ab_0 conda-forge. tbb 2021.9.0 hffc8910_0 conda-forge. threadpoolctl 3.2.0 pyha21a80b_0 conda-forge. tk 8.6.12 he1e0b03_0 conda-forge. tomli 2.0.1 pyhd8ed1ab_0 conda-forge. tornado 6.1 py39h1a28f6b_0 anaconda. tqdm 4.65.0 pyhd8ed1ab_1 conda-forge. traitlets 5.1.1 pyhd3eb1b0_0 anaconda. typing-extensions 4.7.1 hd8ed1ab_0 conda-forge. typing_extensions 4.7.1 pyha770c72_0 conda-forge. tzdata 2023c h71feb2d_0 conda-forge. umap-learn 0.5.3 py39h2804cbe_1 conda-forge. unicodedata2 15.0.0 py39h02fc5c5_0 conda-forge. urllib3 2.0.3 pyhd8ed1ab_1 conda-forge. wcwidth 0.2.5 pyhd3eb1b0_0 anaconda. wheel 0.40.0 pyhd8ed1ab_1 conda-forge. xorg-libxau 1.0.11 hb547adb_0 conda-forge. xorg-libxdmcp 1.1.3 h27ca646_0 conda-forge. xz 5.2.6 h57fd34a_0 conda-forge. zeromq 4.3.4 hc377ac9_0 anaconda. zipp 3.16.2 pyhd8ed1ab_0 conda-forge. zlib-ng 2.0.7 h1a8c8d9_0 conda-forge. zstd 1.5.2 h4f39d0f_7 conda-forge. ```. </details>.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564
https://github.com/scverse/scanpy/issues/2565:663,availability,Error,Error,663,"spaceranger2.0.1 outs/spatial/tissue_positions_list.csv change; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In scanpy version 1.9.3. ### Minimal code sample. ```python. adata = sc.read_visium(. '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/',. count_file='filtered_feature_bc_matrix.h5',. source_image_path='/data_disk/ST01/User/zhanghh/project/GSE206391/GSM6252954_11-V19T12-012-V4.jpg',. ). ```. ### Error output. ```pytb. /data_disk/ST01/Software/conda_env/zhhenv/lib/python3.9/site-packages/anndata/_core/anndata.py:1832: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`. utils.warn_names_duplicates(""var""). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/zhanghaihao/.local/lib/python3.9/site-packages/scanpy/readwrite.py"", line 390, in read_visium. raise OSError(f""Could not find '{f}'""). OSError: Could not find '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/spatial/tissue_positions_list.csv'. ```. ### Versions. <details>. <summary>Details</summary>. ```. >>> import scanpy; scanpy.logging.print_versions(). -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. dot_parser NA. gmpy2 2.1.2. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. invgauss_ufunc NA. joblib 1.1.1. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.2.1. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.57.0. numpy 1.24.3. nvfuser NA. opt_einsum v3.3.0. packaging 23.0. pandas 2.0.1. pkg_resources NA. pydot 1.4.2. pyparsing 3.0.9. pytz 2022.7. s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2565
https://github.com/scverse/scanpy/issues/2565:232,deployability,version,version,232,"spaceranger2.0.1 outs/spatial/tissue_positions_list.csv change; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In scanpy version 1.9.3. ### Minimal code sample. ```python. adata = sc.read_visium(. '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/',. count_file='filtered_feature_bc_matrix.h5',. source_image_path='/data_disk/ST01/User/zhanghh/project/GSE206391/GSM6252954_11-V19T12-012-V4.jpg',. ). ```. ### Error output. ```pytb. /data_disk/ST01/Software/conda_env/zhhenv/lib/python3.9/site-packages/anndata/_core/anndata.py:1832: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`. utils.warn_names_duplicates(""var""). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/zhanghaihao/.local/lib/python3.9/site-packages/scanpy/readwrite.py"", line 390, in read_visium. raise OSError(f""Could not find '{f}'""). OSError: Could not find '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/spatial/tissue_positions_list.csv'. ```. ### Versions. <details>. <summary>Details</summary>. ```. >>> import scanpy; scanpy.logging.print_versions(). -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. dot_parser NA. gmpy2 2.1.2. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. invgauss_ufunc NA. joblib 1.1.1. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.2.1. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.57.0. numpy 1.24.3. nvfuser NA. opt_einsum v3.3.0. packaging 23.0. pandas 2.0.1. pkg_resources NA. pydot 1.4.2. pyparsing 3.0.9. pytz 2022.7. s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2565
https://github.com/scverse/scanpy/issues/2565:362,deployability,version,version,362,"spaceranger2.0.1 outs/spatial/tissue_positions_list.csv change; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In scanpy version 1.9.3. ### Minimal code sample. ```python. adata = sc.read_visium(. '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/',. count_file='filtered_feature_bc_matrix.h5',. source_image_path='/data_disk/ST01/User/zhanghh/project/GSE206391/GSM6252954_11-V19T12-012-V4.jpg',. ). ```. ### Error output. ```pytb. /data_disk/ST01/Software/conda_env/zhhenv/lib/python3.9/site-packages/anndata/_core/anndata.py:1832: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`. utils.warn_names_duplicates(""var""). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/zhanghaihao/.local/lib/python3.9/site-packages/scanpy/readwrite.py"", line 390, in read_visium. raise OSError(f""Could not find '{f}'""). OSError: Could not find '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/spatial/tissue_positions_list.csv'. ```. ### Versions. <details>. <summary>Details</summary>. ```. >>> import scanpy; scanpy.logging.print_versions(). -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. dot_parser NA. gmpy2 2.1.2. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. invgauss_ufunc NA. joblib 1.1.1. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.2.1. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.57.0. numpy 1.24.3. nvfuser NA. opt_einsum v3.3.0. packaging 23.0. pandas 2.0.1. pkg_resources NA. pydot 1.4.2. pyparsing 3.0.9. pytz 2022.7. s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2565
https://github.com/scverse/scanpy/issues/2565:983,deployability,modul,module,983,"spaceranger2.0.1 outs/spatial/tissue_positions_list.csv change; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In scanpy version 1.9.3. ### Minimal code sample. ```python. adata = sc.read_visium(. '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/',. count_file='filtered_feature_bc_matrix.h5',. source_image_path='/data_disk/ST01/User/zhanghh/project/GSE206391/GSM6252954_11-V19T12-012-V4.jpg',. ). ```. ### Error output. ```pytb. /data_disk/ST01/Software/conda_env/zhhenv/lib/python3.9/site-packages/anndata/_core/anndata.py:1832: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`. utils.warn_names_duplicates(""var""). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/zhanghaihao/.local/lib/python3.9/site-packages/scanpy/readwrite.py"", line 390, in read_visium. raise OSError(f""Could not find '{f}'""). OSError: Could not find '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/spatial/tissue_positions_list.csv'. ```. ### Versions. <details>. <summary>Details</summary>. ```. >>> import scanpy; scanpy.logging.print_versions(). -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. dot_parser NA. gmpy2 2.1.2. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. invgauss_ufunc NA. joblib 1.1.1. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.2.1. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.57.0. numpy 1.24.3. nvfuser NA. opt_einsum v3.3.0. packaging 23.0. pandas 2.0.1. pkg_resources NA. pydot 1.4.2. pyparsing 3.0.9. pytz 2022.7. s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2565
https://github.com/scverse/scanpy/issues/2565:1271,deployability,Version,Versions,1271,"confirmed this bug exists on the master branch of scanpy. ### What happened? In scanpy version 1.9.3. ### Minimal code sample. ```python. adata = sc.read_visium(. '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/',. count_file='filtered_feature_bc_matrix.h5',. source_image_path='/data_disk/ST01/User/zhanghh/project/GSE206391/GSM6252954_11-V19T12-012-V4.jpg',. ). ```. ### Error output. ```pytb. /data_disk/ST01/Software/conda_env/zhhenv/lib/python3.9/site-packages/anndata/_core/anndata.py:1832: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`. utils.warn_names_duplicates(""var""). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/zhanghaihao/.local/lib/python3.9/site-packages/scanpy/readwrite.py"", line 390, in read_visium. raise OSError(f""Could not find '{f}'""). OSError: Could not find '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/spatial/tissue_positions_list.csv'. ```. ### Versions. <details>. <summary>Details</summary>. ```. >>> import scanpy; scanpy.logging.print_versions(). -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. dot_parser NA. gmpy2 2.1.2. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. invgauss_ufunc NA. joblib 1.1.1. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.2.1. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.57.0. numpy 1.24.3. nvfuser NA. opt_einsum v3.3.0. packaging 23.0. pandas 2.0.1. pkg_resources NA. pydot 1.4.2. pyparsing 3.0.9. pytz 2022.7. scipy 1.10.1. session_info 1.0.0. setuptools 66.0.0. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. sympy 1.11.1. texttable 1.6.7. threadpoolctl 2.2.0. torch 2.0.1. tqdm 4.65.0. typing_extensions NA. yaml 6.0. zipp NA. zoneinfo NA. -----. Python 3.9.16 (main, Mar 8 2023, 14:00",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2565
https://github.com/scverse/scanpy/issues/2565:1351,deployability,log,logging,1351,"scanpy version 1.9.3. ### Minimal code sample. ```python. adata = sc.read_visium(. '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/',. count_file='filtered_feature_bc_matrix.h5',. source_image_path='/data_disk/ST01/User/zhanghh/project/GSE206391/GSM6252954_11-V19T12-012-V4.jpg',. ). ```. ### Error output. ```pytb. /data_disk/ST01/Software/conda_env/zhhenv/lib/python3.9/site-packages/anndata/_core/anndata.py:1832: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`. utils.warn_names_duplicates(""var""). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/zhanghaihao/.local/lib/python3.9/site-packages/scanpy/readwrite.py"", line 390, in read_visium. raise OSError(f""Could not find '{f}'""). OSError: Could not find '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/spatial/tissue_positions_list.csv'. ```. ### Versions. <details>. <summary>Details</summary>. ```. >>> import scanpy; scanpy.logging.print_versions(). -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. dot_parser NA. gmpy2 2.1.2. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. invgauss_ufunc NA. joblib 1.1.1. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.2.1. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.57.0. numpy 1.24.3. nvfuser NA. opt_einsum v3.3.0. packaging 23.0. pandas 2.0.1. pkg_resources NA. pydot 1.4.2. pyparsing 3.0.9. pytz 2022.7. scipy 1.10.1. session_info 1.0.0. setuptools 66.0.0. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. sympy 1.11.1. texttable 1.6.7. threadpoolctl 2.2.0. torch 2.0.1. tqdm 4.65.0. typing_extensions NA. yaml 6.0. zipp NA. zoneinfo NA. -----. Python 3.9.16 (main, Mar 8 2023, 14:00:05) [GCC 11.2.0]. Linux-5.15.0-71-generic-x86_64-with-glibc2.35. -----. Session",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2565
https://github.com/scverse/scanpy/issues/2565:2368,deployability,updat,updated,2368," adata = sc.read_visium(. '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/',. count_file='filtered_feature_bc_matrix.h5',. source_image_path='/data_disk/ST01/User/zhanghh/project/GSE206391/GSM6252954_11-V19T12-012-V4.jpg',. ). ```. ### Error output. ```pytb. /data_disk/ST01/Software/conda_env/zhhenv/lib/python3.9/site-packages/anndata/_core/anndata.py:1832: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`. utils.warn_names_duplicates(""var""). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/zhanghaihao/.local/lib/python3.9/site-packages/scanpy/readwrite.py"", line 390, in read_visium. raise OSError(f""Could not find '{f}'""). OSError: Could not find '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/spatial/tissue_positions_list.csv'. ```. ### Versions. <details>. <summary>Details</summary>. ```. >>> import scanpy; scanpy.logging.print_versions(). -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. dot_parser NA. gmpy2 2.1.2. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. invgauss_ufunc NA. joblib 1.1.1. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.2.1. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.57.0. numpy 1.24.3. nvfuser NA. opt_einsum v3.3.0. packaging 23.0. pandas 2.0.1. pkg_resources NA. pydot 1.4.2. pyparsing 3.0.9. pytz 2022.7. scipy 1.10.1. session_info 1.0.0. setuptools 66.0.0. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. sympy 1.11.1. texttable 1.6.7. threadpoolctl 2.2.0. torch 2.0.1. tqdm 4.65.0. typing_extensions NA. yaml 6.0. zipp NA. zoneinfo NA. -----. Python 3.9.16 (main, Mar 8 2023, 14:00:05) [GCC 11.2.0]. Linux-5.15.0-71-generic-x86_64-with-glibc2.35. -----. Session information updated at 2023-07-20 15:11. ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2565
https://github.com/scverse/scanpy/issues/2565:232,integrability,version,version,232,"spaceranger2.0.1 outs/spatial/tissue_positions_list.csv change; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In scanpy version 1.9.3. ### Minimal code sample. ```python. adata = sc.read_visium(. '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/',. count_file='filtered_feature_bc_matrix.h5',. source_image_path='/data_disk/ST01/User/zhanghh/project/GSE206391/GSM6252954_11-V19T12-012-V4.jpg',. ). ```. ### Error output. ```pytb. /data_disk/ST01/Software/conda_env/zhhenv/lib/python3.9/site-packages/anndata/_core/anndata.py:1832: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`. utils.warn_names_duplicates(""var""). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/zhanghaihao/.local/lib/python3.9/site-packages/scanpy/readwrite.py"", line 390, in read_visium. raise OSError(f""Could not find '{f}'""). OSError: Could not find '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/spatial/tissue_positions_list.csv'. ```. ### Versions. <details>. <summary>Details</summary>. ```. >>> import scanpy; scanpy.logging.print_versions(). -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. dot_parser NA. gmpy2 2.1.2. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. invgauss_ufunc NA. joblib 1.1.1. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.2.1. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.57.0. numpy 1.24.3. nvfuser NA. opt_einsum v3.3.0. packaging 23.0. pandas 2.0.1. pkg_resources NA. pydot 1.4.2. pyparsing 3.0.9. pytz 2022.7. s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2565
https://github.com/scverse/scanpy/issues/2565:362,integrability,version,version,362,"spaceranger2.0.1 outs/spatial/tissue_positions_list.csv change; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In scanpy version 1.9.3. ### Minimal code sample. ```python. adata = sc.read_visium(. '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/',. count_file='filtered_feature_bc_matrix.h5',. source_image_path='/data_disk/ST01/User/zhanghh/project/GSE206391/GSM6252954_11-V19T12-012-V4.jpg',. ). ```. ### Error output. ```pytb. /data_disk/ST01/Software/conda_env/zhhenv/lib/python3.9/site-packages/anndata/_core/anndata.py:1832: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`. utils.warn_names_duplicates(""var""). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/zhanghaihao/.local/lib/python3.9/site-packages/scanpy/readwrite.py"", line 390, in read_visium. raise OSError(f""Could not find '{f}'""). OSError: Could not find '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/spatial/tissue_positions_list.csv'. ```. ### Versions. <details>. <summary>Details</summary>. ```. >>> import scanpy; scanpy.logging.print_versions(). -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. dot_parser NA. gmpy2 2.1.2. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. invgauss_ufunc NA. joblib 1.1.1. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.2.1. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.57.0. numpy 1.24.3. nvfuser NA. opt_einsum v3.3.0. packaging 23.0. pandas 2.0.1. pkg_resources NA. pydot 1.4.2. pyparsing 3.0.9. pytz 2022.7. s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2565
https://github.com/scverse/scanpy/issues/2565:1271,integrability,Version,Versions,1271,"confirmed this bug exists on the master branch of scanpy. ### What happened? In scanpy version 1.9.3. ### Minimal code sample. ```python. adata = sc.read_visium(. '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/',. count_file='filtered_feature_bc_matrix.h5',. source_image_path='/data_disk/ST01/User/zhanghh/project/GSE206391/GSM6252954_11-V19T12-012-V4.jpg',. ). ```. ### Error output. ```pytb. /data_disk/ST01/Software/conda_env/zhhenv/lib/python3.9/site-packages/anndata/_core/anndata.py:1832: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`. utils.warn_names_duplicates(""var""). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/zhanghaihao/.local/lib/python3.9/site-packages/scanpy/readwrite.py"", line 390, in read_visium. raise OSError(f""Could not find '{f}'""). OSError: Could not find '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/spatial/tissue_positions_list.csv'. ```. ### Versions. <details>. <summary>Details</summary>. ```. >>> import scanpy; scanpy.logging.print_versions(). -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. dot_parser NA. gmpy2 2.1.2. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. invgauss_ufunc NA. joblib 1.1.1. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.2.1. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.57.0. numpy 1.24.3. nvfuser NA. opt_einsum v3.3.0. packaging 23.0. pandas 2.0.1. pkg_resources NA. pydot 1.4.2. pyparsing 3.0.9. pytz 2022.7. scipy 1.10.1. session_info 1.0.0. setuptools 66.0.0. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. sympy 1.11.1. texttable 1.6.7. threadpoolctl 2.2.0. torch 2.0.1. tqdm 4.65.0. typing_extensions NA. yaml 6.0. zipp NA. zoneinfo NA. -----. Python 3.9.16 (main, Mar 8 2023, 14:00",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2565
https://github.com/scverse/scanpy/issues/2565:232,modifiability,version,version,232,"spaceranger2.0.1 outs/spatial/tissue_positions_list.csv change; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In scanpy version 1.9.3. ### Minimal code sample. ```python. adata = sc.read_visium(. '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/',. count_file='filtered_feature_bc_matrix.h5',. source_image_path='/data_disk/ST01/User/zhanghh/project/GSE206391/GSM6252954_11-V19T12-012-V4.jpg',. ). ```. ### Error output. ```pytb. /data_disk/ST01/Software/conda_env/zhhenv/lib/python3.9/site-packages/anndata/_core/anndata.py:1832: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`. utils.warn_names_duplicates(""var""). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/zhanghaihao/.local/lib/python3.9/site-packages/scanpy/readwrite.py"", line 390, in read_visium. raise OSError(f""Could not find '{f}'""). OSError: Could not find '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/spatial/tissue_positions_list.csv'. ```. ### Versions. <details>. <summary>Details</summary>. ```. >>> import scanpy; scanpy.logging.print_versions(). -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. dot_parser NA. gmpy2 2.1.2. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. invgauss_ufunc NA. joblib 1.1.1. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.2.1. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.57.0. numpy 1.24.3. nvfuser NA. opt_einsum v3.3.0. packaging 23.0. pandas 2.0.1. pkg_resources NA. pydot 1.4.2. pyparsing 3.0.9. pytz 2022.7. s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2565
https://github.com/scverse/scanpy/issues/2565:362,modifiability,version,version,362,"spaceranger2.0.1 outs/spatial/tissue_positions_list.csv change; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In scanpy version 1.9.3. ### Minimal code sample. ```python. adata = sc.read_visium(. '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/',. count_file='filtered_feature_bc_matrix.h5',. source_image_path='/data_disk/ST01/User/zhanghh/project/GSE206391/GSM6252954_11-V19T12-012-V4.jpg',. ). ```. ### Error output. ```pytb. /data_disk/ST01/Software/conda_env/zhhenv/lib/python3.9/site-packages/anndata/_core/anndata.py:1832: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`. utils.warn_names_duplicates(""var""). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/zhanghaihao/.local/lib/python3.9/site-packages/scanpy/readwrite.py"", line 390, in read_visium. raise OSError(f""Could not find '{f}'""). OSError: Could not find '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/spatial/tissue_positions_list.csv'. ```. ### Versions. <details>. <summary>Details</summary>. ```. >>> import scanpy; scanpy.logging.print_versions(). -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. dot_parser NA. gmpy2 2.1.2. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. invgauss_ufunc NA. joblib 1.1.1. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.2.1. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.57.0. numpy 1.24.3. nvfuser NA. opt_einsum v3.3.0. packaging 23.0. pandas 2.0.1. pkg_resources NA. pydot 1.4.2. pyparsing 3.0.9. pytz 2022.7. s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2565
https://github.com/scverse/scanpy/issues/2565:747,modifiability,pac,packages,747,"spaceranger2.0.1 outs/spatial/tissue_positions_list.csv change; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In scanpy version 1.9.3. ### Minimal code sample. ```python. adata = sc.read_visium(. '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/',. count_file='filtered_feature_bc_matrix.h5',. source_image_path='/data_disk/ST01/User/zhanghh/project/GSE206391/GSM6252954_11-V19T12-012-V4.jpg',. ). ```. ### Error output. ```pytb. /data_disk/ST01/Software/conda_env/zhhenv/lib/python3.9/site-packages/anndata/_core/anndata.py:1832: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`. utils.warn_names_duplicates(""var""). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/zhanghaihao/.local/lib/python3.9/site-packages/scanpy/readwrite.py"", line 390, in read_visium. raise OSError(f""Could not find '{f}'""). OSError: Could not find '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/spatial/tissue_positions_list.csv'. ```. ### Versions. <details>. <summary>Details</summary>. ```. >>> import scanpy; scanpy.logging.print_versions(). -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. dot_parser NA. gmpy2 2.1.2. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. invgauss_ufunc NA. joblib 1.1.1. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.2.1. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.57.0. numpy 1.24.3. nvfuser NA. opt_einsum v3.3.0. packaging 23.0. pandas 2.0.1. pkg_resources NA. pydot 1.4.2. pyparsing 3.0.9. pytz 2022.7. s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2565
https://github.com/scverse/scanpy/issues/2565:800,modifiability,Variab,Variable,800,"spaceranger2.0.1 outs/spatial/tissue_positions_list.csv change; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In scanpy version 1.9.3. ### Minimal code sample. ```python. adata = sc.read_visium(. '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/',. count_file='filtered_feature_bc_matrix.h5',. source_image_path='/data_disk/ST01/User/zhanghh/project/GSE206391/GSM6252954_11-V19T12-012-V4.jpg',. ). ```. ### Error output. ```pytb. /data_disk/ST01/Software/conda_env/zhhenv/lib/python3.9/site-packages/anndata/_core/anndata.py:1832: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`. utils.warn_names_duplicates(""var""). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/zhanghaihao/.local/lib/python3.9/site-packages/scanpy/readwrite.py"", line 390, in read_visium. raise OSError(f""Could not find '{f}'""). OSError: Could not find '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/spatial/tissue_positions_list.csv'. ```. ### Versions. <details>. <summary>Details</summary>. ```. >>> import scanpy; scanpy.logging.print_versions(). -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. dot_parser NA. gmpy2 2.1.2. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. invgauss_ufunc NA. joblib 1.1.1. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.2.1. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.57.0. numpy 1.24.3. nvfuser NA. opt_einsum v3.3.0. packaging 23.0. pandas 2.0.1. pkg_resources NA. pydot 1.4.2. pyparsing 3.0.9. pytz 2022.7. s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2565
https://github.com/scverse/scanpy/issues/2565:983,modifiability,modul,module,983,"spaceranger2.0.1 outs/spatial/tissue_positions_list.csv change; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In scanpy version 1.9.3. ### Minimal code sample. ```python. adata = sc.read_visium(. '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/',. count_file='filtered_feature_bc_matrix.h5',. source_image_path='/data_disk/ST01/User/zhanghh/project/GSE206391/GSM6252954_11-V19T12-012-V4.jpg',. ). ```. ### Error output. ```pytb. /data_disk/ST01/Software/conda_env/zhhenv/lib/python3.9/site-packages/anndata/_core/anndata.py:1832: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`. utils.warn_names_duplicates(""var""). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/zhanghaihao/.local/lib/python3.9/site-packages/scanpy/readwrite.py"", line 390, in read_visium. raise OSError(f""Could not find '{f}'""). OSError: Could not find '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/spatial/tissue_positions_list.csv'. ```. ### Versions. <details>. <summary>Details</summary>. ```. >>> import scanpy; scanpy.logging.print_versions(). -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. dot_parser NA. gmpy2 2.1.2. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. invgauss_ufunc NA. joblib 1.1.1. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.2.1. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.57.0. numpy 1.24.3. nvfuser NA. opt_einsum v3.3.0. packaging 23.0. pandas 2.0.1. pkg_resources NA. pydot 1.4.2. pyparsing 3.0.9. pytz 2022.7. s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2565
https://github.com/scverse/scanpy/issues/2565:1042,modifiability,pac,packages,1042,"_list.csv change; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In scanpy version 1.9.3. ### Minimal code sample. ```python. adata = sc.read_visium(. '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/',. count_file='filtered_feature_bc_matrix.h5',. source_image_path='/data_disk/ST01/User/zhanghh/project/GSE206391/GSM6252954_11-V19T12-012-V4.jpg',. ). ```. ### Error output. ```pytb. /data_disk/ST01/Software/conda_env/zhhenv/lib/python3.9/site-packages/anndata/_core/anndata.py:1832: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`. utils.warn_names_duplicates(""var""). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/zhanghaihao/.local/lib/python3.9/site-packages/scanpy/readwrite.py"", line 390, in read_visium. raise OSError(f""Could not find '{f}'""). OSError: Could not find '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/spatial/tissue_positions_list.csv'. ```. ### Versions. <details>. <summary>Details</summary>. ```. >>> import scanpy; scanpy.logging.print_versions(). -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. dot_parser NA. gmpy2 2.1.2. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. invgauss_ufunc NA. joblib 1.1.1. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.2.1. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.57.0. numpy 1.24.3. nvfuser NA. opt_einsum v3.3.0. packaging 23.0. pandas 2.0.1. pkg_resources NA. pydot 1.4.2. pyparsing 3.0.9. pytz 2022.7. scipy 1.10.1. session_info 1.0.0. setuptools 66",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2565
https://github.com/scverse/scanpy/issues/2565:1271,modifiability,Version,Versions,1271,"confirmed this bug exists on the master branch of scanpy. ### What happened? In scanpy version 1.9.3. ### Minimal code sample. ```python. adata = sc.read_visium(. '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/',. count_file='filtered_feature_bc_matrix.h5',. source_image_path='/data_disk/ST01/User/zhanghh/project/GSE206391/GSM6252954_11-V19T12-012-V4.jpg',. ). ```. ### Error output. ```pytb. /data_disk/ST01/Software/conda_env/zhhenv/lib/python3.9/site-packages/anndata/_core/anndata.py:1832: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`. utils.warn_names_duplicates(""var""). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/zhanghaihao/.local/lib/python3.9/site-packages/scanpy/readwrite.py"", line 390, in read_visium. raise OSError(f""Could not find '{f}'""). OSError: Could not find '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/spatial/tissue_positions_list.csv'. ```. ### Versions. <details>. <summary>Details</summary>. ```. >>> import scanpy; scanpy.logging.print_versions(). -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. dot_parser NA. gmpy2 2.1.2. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. invgauss_ufunc NA. joblib 1.1.1. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.2.1. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.57.0. numpy 1.24.3. nvfuser NA. opt_einsum v3.3.0. packaging 23.0. pandas 2.0.1. pkg_resources NA. pydot 1.4.2. pyparsing 3.0.9. pytz 2022.7. scipy 1.10.1. session_info 1.0.0. setuptools 66.0.0. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. sympy 1.11.1. texttable 1.6.7. threadpoolctl 2.2.0. torch 2.0.1. tqdm 4.65.0. typing_extensions NA. yaml 6.0. zipp NA. zoneinfo NA. -----. Python 3.9.16 (main, Mar 8 2023, 14:00",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2565
https://github.com/scverse/scanpy/issues/2565:1908,modifiability,pac,packaging,1908," adata = sc.read_visium(. '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/',. count_file='filtered_feature_bc_matrix.h5',. source_image_path='/data_disk/ST01/User/zhanghh/project/GSE206391/GSM6252954_11-V19T12-012-V4.jpg',. ). ```. ### Error output. ```pytb. /data_disk/ST01/Software/conda_env/zhhenv/lib/python3.9/site-packages/anndata/_core/anndata.py:1832: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`. utils.warn_names_duplicates(""var""). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/zhanghaihao/.local/lib/python3.9/site-packages/scanpy/readwrite.py"", line 390, in read_visium. raise OSError(f""Could not find '{f}'""). OSError: Could not find '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/spatial/tissue_positions_list.csv'. ```. ### Versions. <details>. <summary>Details</summary>. ```. >>> import scanpy; scanpy.logging.print_versions(). -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. dot_parser NA. gmpy2 2.1.2. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. invgauss_ufunc NA. joblib 1.1.1. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.2.1. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.57.0. numpy 1.24.3. nvfuser NA. opt_einsum v3.3.0. packaging 23.0. pandas 2.0.1. pkg_resources NA. pydot 1.4.2. pyparsing 3.0.9. pytz 2022.7. scipy 1.10.1. session_info 1.0.0. setuptools 66.0.0. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. sympy 1.11.1. texttable 1.6.7. threadpoolctl 2.2.0. torch 2.0.1. tqdm 4.65.0. typing_extensions NA. yaml 6.0. zipp NA. zoneinfo NA. -----. Python 3.9.16 (main, Mar 8 2023, 14:00:05) [GCC 11.2.0]. Linux-5.15.0-71-generic-x86_64-with-glibc2.35. -----. Session information updated at 2023-07-20 15:11. ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2565
https://github.com/scverse/scanpy/issues/2565:663,performance,Error,Error,663,"spaceranger2.0.1 outs/spatial/tissue_positions_list.csv change; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In scanpy version 1.9.3. ### Minimal code sample. ```python. adata = sc.read_visium(. '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/',. count_file='filtered_feature_bc_matrix.h5',. source_image_path='/data_disk/ST01/User/zhanghh/project/GSE206391/GSM6252954_11-V19T12-012-V4.jpg',. ). ```. ### Error output. ```pytb. /data_disk/ST01/Software/conda_env/zhhenv/lib/python3.9/site-packages/anndata/_core/anndata.py:1832: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`. utils.warn_names_duplicates(""var""). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/zhanghaihao/.local/lib/python3.9/site-packages/scanpy/readwrite.py"", line 390, in read_visium. raise OSError(f""Could not find '{f}'""). OSError: Could not find '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/spatial/tissue_positions_list.csv'. ```. ### Versions. <details>. <summary>Details</summary>. ```. >>> import scanpy; scanpy.logging.print_versions(). -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. dot_parser NA. gmpy2 2.1.2. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. invgauss_ufunc NA. joblib 1.1.1. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.2.1. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.57.0. numpy 1.24.3. nvfuser NA. opt_einsum v3.3.0. packaging 23.0. pandas 2.0.1. pkg_resources NA. pydot 1.4.2. pyparsing 3.0.9. pytz 2022.7. s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2565
https://github.com/scverse/scanpy/issues/2565:663,safety,Error,Error,663,"spaceranger2.0.1 outs/spatial/tissue_positions_list.csv change; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In scanpy version 1.9.3. ### Minimal code sample. ```python. adata = sc.read_visium(. '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/',. count_file='filtered_feature_bc_matrix.h5',. source_image_path='/data_disk/ST01/User/zhanghh/project/GSE206391/GSM6252954_11-V19T12-012-V4.jpg',. ). ```. ### Error output. ```pytb. /data_disk/ST01/Software/conda_env/zhhenv/lib/python3.9/site-packages/anndata/_core/anndata.py:1832: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`. utils.warn_names_duplicates(""var""). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/zhanghaihao/.local/lib/python3.9/site-packages/scanpy/readwrite.py"", line 390, in read_visium. raise OSError(f""Could not find '{f}'""). OSError: Could not find '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/spatial/tissue_positions_list.csv'. ```. ### Versions. <details>. <summary>Details</summary>. ```. >>> import scanpy; scanpy.logging.print_versions(). -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. dot_parser NA. gmpy2 2.1.2. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. invgauss_ufunc NA. joblib 1.1.1. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.2.1. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.57.0. numpy 1.24.3. nvfuser NA. opt_einsum v3.3.0. packaging 23.0. pandas 2.0.1. pkg_resources NA. pydot 1.4.2. pyparsing 3.0.9. pytz 2022.7. s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2565
https://github.com/scverse/scanpy/issues/2565:983,safety,modul,module,983,"spaceranger2.0.1 outs/spatial/tissue_positions_list.csv change; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In scanpy version 1.9.3. ### Minimal code sample. ```python. adata = sc.read_visium(. '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/',. count_file='filtered_feature_bc_matrix.h5',. source_image_path='/data_disk/ST01/User/zhanghh/project/GSE206391/GSM6252954_11-V19T12-012-V4.jpg',. ). ```. ### Error output. ```pytb. /data_disk/ST01/Software/conda_env/zhhenv/lib/python3.9/site-packages/anndata/_core/anndata.py:1832: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`. utils.warn_names_duplicates(""var""). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/zhanghaihao/.local/lib/python3.9/site-packages/scanpy/readwrite.py"", line 390, in read_visium. raise OSError(f""Could not find '{f}'""). OSError: Could not find '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/spatial/tissue_positions_list.csv'. ```. ### Versions. <details>. <summary>Details</summary>. ```. >>> import scanpy; scanpy.logging.print_versions(). -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. dot_parser NA. gmpy2 2.1.2. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. invgauss_ufunc NA. joblib 1.1.1. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.2.1. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.57.0. numpy 1.24.3. nvfuser NA. opt_einsum v3.3.0. packaging 23.0. pandas 2.0.1. pkg_resources NA. pydot 1.4.2. pyparsing 3.0.9. pytz 2022.7. s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2565
https://github.com/scverse/scanpy/issues/2565:1351,safety,log,logging,1351,"scanpy version 1.9.3. ### Minimal code sample. ```python. adata = sc.read_visium(. '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/',. count_file='filtered_feature_bc_matrix.h5',. source_image_path='/data_disk/ST01/User/zhanghh/project/GSE206391/GSM6252954_11-V19T12-012-V4.jpg',. ). ```. ### Error output. ```pytb. /data_disk/ST01/Software/conda_env/zhhenv/lib/python3.9/site-packages/anndata/_core/anndata.py:1832: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`. utils.warn_names_duplicates(""var""). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/zhanghaihao/.local/lib/python3.9/site-packages/scanpy/readwrite.py"", line 390, in read_visium. raise OSError(f""Could not find '{f}'""). OSError: Could not find '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/spatial/tissue_positions_list.csv'. ```. ### Versions. <details>. <summary>Details</summary>. ```. >>> import scanpy; scanpy.logging.print_versions(). -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. dot_parser NA. gmpy2 2.1.2. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. invgauss_ufunc NA. joblib 1.1.1. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.2.1. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.57.0. numpy 1.24.3. nvfuser NA. opt_einsum v3.3.0. packaging 23.0. pandas 2.0.1. pkg_resources NA. pydot 1.4.2. pyparsing 3.0.9. pytz 2022.7. scipy 1.10.1. session_info 1.0.0. setuptools 66.0.0. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. sympy 1.11.1. texttable 1.6.7. threadpoolctl 2.2.0. torch 2.0.1. tqdm 4.65.0. typing_extensions NA. yaml 6.0. zipp NA. zoneinfo NA. -----. Python 3.9.16 (main, Mar 8 2023, 14:00:05) [GCC 11.2.0]. Linux-5.15.0-71-generic-x86_64-with-glibc2.35. -----. Session",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2565
https://github.com/scverse/scanpy/issues/2565:2368,safety,updat,updated,2368," adata = sc.read_visium(. '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/',. count_file='filtered_feature_bc_matrix.h5',. source_image_path='/data_disk/ST01/User/zhanghh/project/GSE206391/GSM6252954_11-V19T12-012-V4.jpg',. ). ```. ### Error output. ```pytb. /data_disk/ST01/Software/conda_env/zhhenv/lib/python3.9/site-packages/anndata/_core/anndata.py:1832: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`. utils.warn_names_duplicates(""var""). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/zhanghaihao/.local/lib/python3.9/site-packages/scanpy/readwrite.py"", line 390, in read_visium. raise OSError(f""Could not find '{f}'""). OSError: Could not find '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/spatial/tissue_positions_list.csv'. ```. ### Versions. <details>. <summary>Details</summary>. ```. >>> import scanpy; scanpy.logging.print_versions(). -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. dot_parser NA. gmpy2 2.1.2. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. invgauss_ufunc NA. joblib 1.1.1. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.2.1. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.57.0. numpy 1.24.3. nvfuser NA. opt_einsum v3.3.0. packaging 23.0. pandas 2.0.1. pkg_resources NA. pydot 1.4.2. pyparsing 3.0.9. pytz 2022.7. scipy 1.10.1. session_info 1.0.0. setuptools 66.0.0. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. sympy 1.11.1. texttable 1.6.7. threadpoolctl 2.2.0. torch 2.0.1. tqdm 4.65.0. typing_extensions NA. yaml 6.0. zipp NA. zoneinfo NA. -----. Python 3.9.16 (main, Mar 8 2023, 14:00:05) [GCC 11.2.0]. Linux-5.15.0-71-generic-x86_64-with-glibc2.35. -----. Session information updated at 2023-07-20 15:11. ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2565
https://github.com/scverse/scanpy/issues/2565:1351,security,log,logging,1351,"scanpy version 1.9.3. ### Minimal code sample. ```python. adata = sc.read_visium(. '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/',. count_file='filtered_feature_bc_matrix.h5',. source_image_path='/data_disk/ST01/User/zhanghh/project/GSE206391/GSM6252954_11-V19T12-012-V4.jpg',. ). ```. ### Error output. ```pytb. /data_disk/ST01/Software/conda_env/zhhenv/lib/python3.9/site-packages/anndata/_core/anndata.py:1832: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`. utils.warn_names_duplicates(""var""). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/zhanghaihao/.local/lib/python3.9/site-packages/scanpy/readwrite.py"", line 390, in read_visium. raise OSError(f""Could not find '{f}'""). OSError: Could not find '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/spatial/tissue_positions_list.csv'. ```. ### Versions. <details>. <summary>Details</summary>. ```. >>> import scanpy; scanpy.logging.print_versions(). -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. dot_parser NA. gmpy2 2.1.2. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. invgauss_ufunc NA. joblib 1.1.1. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.2.1. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.57.0. numpy 1.24.3. nvfuser NA. opt_einsum v3.3.0. packaging 23.0. pandas 2.0.1. pkg_resources NA. pydot 1.4.2. pyparsing 3.0.9. pytz 2022.7. scipy 1.10.1. session_info 1.0.0. setuptools 66.0.0. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. sympy 1.11.1. texttable 1.6.7. threadpoolctl 2.2.0. torch 2.0.1. tqdm 4.65.0. typing_extensions NA. yaml 6.0. zipp NA. zoneinfo NA. -----. Python 3.9.16 (main, Mar 8 2023, 14:00:05) [GCC 11.2.0]. Linux-5.15.0-71-generic-x86_64-with-glibc2.35. -----. Session",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2565
https://github.com/scverse/scanpy/issues/2565:2348,security,Session,Session,2348," adata = sc.read_visium(. '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/',. count_file='filtered_feature_bc_matrix.h5',. source_image_path='/data_disk/ST01/User/zhanghh/project/GSE206391/GSM6252954_11-V19T12-012-V4.jpg',. ). ```. ### Error output. ```pytb. /data_disk/ST01/Software/conda_env/zhhenv/lib/python3.9/site-packages/anndata/_core/anndata.py:1832: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`. utils.warn_names_duplicates(""var""). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/zhanghaihao/.local/lib/python3.9/site-packages/scanpy/readwrite.py"", line 390, in read_visium. raise OSError(f""Could not find '{f}'""). OSError: Could not find '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/spatial/tissue_positions_list.csv'. ```. ### Versions. <details>. <summary>Details</summary>. ```. >>> import scanpy; scanpy.logging.print_versions(). -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. dot_parser NA. gmpy2 2.1.2. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. invgauss_ufunc NA. joblib 1.1.1. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.2.1. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.57.0. numpy 1.24.3. nvfuser NA. opt_einsum v3.3.0. packaging 23.0. pandas 2.0.1. pkg_resources NA. pydot 1.4.2. pyparsing 3.0.9. pytz 2022.7. scipy 1.10.1. session_info 1.0.0. setuptools 66.0.0. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. sympy 1.11.1. texttable 1.6.7. threadpoolctl 2.2.0. torch 2.0.1. tqdm 4.65.0. typing_extensions NA. yaml 6.0. zipp NA. zoneinfo NA. -----. Python 3.9.16 (main, Mar 8 2023, 14:00:05) [GCC 11.2.0]. Linux-5.15.0-71-generic-x86_64-with-glibc2.35. -----. Session information updated at 2023-07-20 15:11. ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2565
https://github.com/scverse/scanpy/issues/2565:2368,security,updat,updated,2368," adata = sc.read_visium(. '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/',. count_file='filtered_feature_bc_matrix.h5',. source_image_path='/data_disk/ST01/User/zhanghh/project/GSE206391/GSM6252954_11-V19T12-012-V4.jpg',. ). ```. ### Error output. ```pytb. /data_disk/ST01/Software/conda_env/zhhenv/lib/python3.9/site-packages/anndata/_core/anndata.py:1832: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`. utils.warn_names_duplicates(""var""). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/zhanghaihao/.local/lib/python3.9/site-packages/scanpy/readwrite.py"", line 390, in read_visium. raise OSError(f""Could not find '{f}'""). OSError: Could not find '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/spatial/tissue_positions_list.csv'. ```. ### Versions. <details>. <summary>Details</summary>. ```. >>> import scanpy; scanpy.logging.print_versions(). -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. dot_parser NA. gmpy2 2.1.2. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. invgauss_ufunc NA. joblib 1.1.1. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.2.1. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.57.0. numpy 1.24.3. nvfuser NA. opt_einsum v3.3.0. packaging 23.0. pandas 2.0.1. pkg_resources NA. pydot 1.4.2. pyparsing 3.0.9. pytz 2022.7. scipy 1.10.1. session_info 1.0.0. setuptools 66.0.0. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. sympy 1.11.1. texttable 1.6.7. threadpoolctl 2.2.0. torch 2.0.1. tqdm 4.65.0. typing_extensions NA. yaml 6.0. zipp NA. zoneinfo NA. -----. Python 3.9.16 (main, Mar 8 2023, 14:00:05) [GCC 11.2.0]. Linux-5.15.0-71-generic-x86_64-with-glibc2.35. -----. Session information updated at 2023-07-20 15:11. ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2565
https://github.com/scverse/scanpy/issues/2565:919,testability,Trace,Traceback,919,"spaceranger2.0.1 outs/spatial/tissue_positions_list.csv change; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In scanpy version 1.9.3. ### Minimal code sample. ```python. adata = sc.read_visium(. '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/',. count_file='filtered_feature_bc_matrix.h5',. source_image_path='/data_disk/ST01/User/zhanghh/project/GSE206391/GSM6252954_11-V19T12-012-V4.jpg',. ). ```. ### Error output. ```pytb. /data_disk/ST01/Software/conda_env/zhhenv/lib/python3.9/site-packages/anndata/_core/anndata.py:1832: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`. utils.warn_names_duplicates(""var""). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/zhanghaihao/.local/lib/python3.9/site-packages/scanpy/readwrite.py"", line 390, in read_visium. raise OSError(f""Could not find '{f}'""). OSError: Could not find '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/spatial/tissue_positions_list.csv'. ```. ### Versions. <details>. <summary>Details</summary>. ```. >>> import scanpy; scanpy.logging.print_versions(). -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. dot_parser NA. gmpy2 2.1.2. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. invgauss_ufunc NA. joblib 1.1.1. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.2.1. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.57.0. numpy 1.24.3. nvfuser NA. opt_einsum v3.3.0. packaging 23.0. pandas 2.0.1. pkg_resources NA. pydot 1.4.2. pyparsing 3.0.9. pytz 2022.7. s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2565
https://github.com/scverse/scanpy/issues/2565:1351,testability,log,logging,1351,"scanpy version 1.9.3. ### Minimal code sample. ```python. adata = sc.read_visium(. '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/',. count_file='filtered_feature_bc_matrix.h5',. source_image_path='/data_disk/ST01/User/zhanghh/project/GSE206391/GSM6252954_11-V19T12-012-V4.jpg',. ). ```. ### Error output. ```pytb. /data_disk/ST01/Software/conda_env/zhhenv/lib/python3.9/site-packages/anndata/_core/anndata.py:1832: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`. utils.warn_names_duplicates(""var""). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/zhanghaihao/.local/lib/python3.9/site-packages/scanpy/readwrite.py"", line 390, in read_visium. raise OSError(f""Could not find '{f}'""). OSError: Could not find '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/spatial/tissue_positions_list.csv'. ```. ### Versions. <details>. <summary>Details</summary>. ```. >>> import scanpy; scanpy.logging.print_versions(). -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. dot_parser NA. gmpy2 2.1.2. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. invgauss_ufunc NA. joblib 1.1.1. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.2.1. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.57.0. numpy 1.24.3. nvfuser NA. opt_einsum v3.3.0. packaging 23.0. pandas 2.0.1. pkg_resources NA. pydot 1.4.2. pyparsing 3.0.9. pytz 2022.7. scipy 1.10.1. session_info 1.0.0. setuptools 66.0.0. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. sympy 1.11.1. texttable 1.6.7. threadpoolctl 2.2.0. torch 2.0.1. tqdm 4.65.0. typing_extensions NA. yaml 6.0. zipp NA. zoneinfo NA. -----. Python 3.9.16 (main, Mar 8 2023, 14:00:05) [GCC 11.2.0]. Linux-5.15.0-71-generic-x86_64-with-glibc2.35. -----. Session",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2565
https://github.com/scverse/scanpy/issues/2565:192,usability,confirm,confirmed,192,"spaceranger2.0.1 outs/spatial/tissue_positions_list.csv change; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In scanpy version 1.9.3. ### Minimal code sample. ```python. adata = sc.read_visium(. '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/',. count_file='filtered_feature_bc_matrix.h5',. source_image_path='/data_disk/ST01/User/zhanghh/project/GSE206391/GSM6252954_11-V19T12-012-V4.jpg',. ). ```. ### Error output. ```pytb. /data_disk/ST01/Software/conda_env/zhhenv/lib/python3.9/site-packages/anndata/_core/anndata.py:1832: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`. utils.warn_names_duplicates(""var""). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/zhanghaihao/.local/lib/python3.9/site-packages/scanpy/readwrite.py"", line 390, in read_visium. raise OSError(f""Could not find '{f}'""). OSError: Could not find '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/spatial/tissue_positions_list.csv'. ```. ### Versions. <details>. <summary>Details</summary>. ```. >>> import scanpy; scanpy.logging.print_versions(). -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. dot_parser NA. gmpy2 2.1.2. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. invgauss_ufunc NA. joblib 1.1.1. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.2.1. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.57.0. numpy 1.24.3. nvfuser NA. opt_einsum v3.3.0. packaging 23.0. pandas 2.0.1. pkg_resources NA. pydot 1.4.2. pyparsing 3.0.9. pytz 2022.7. s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2565
https://github.com/scverse/scanpy/issues/2565:275,usability,confirm,confirmed,275,"spaceranger2.0.1 outs/spatial/tissue_positions_list.csv change; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In scanpy version 1.9.3. ### Minimal code sample. ```python. adata = sc.read_visium(. '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/',. count_file='filtered_feature_bc_matrix.h5',. source_image_path='/data_disk/ST01/User/zhanghh/project/GSE206391/GSM6252954_11-V19T12-012-V4.jpg',. ). ```. ### Error output. ```pytb. /data_disk/ST01/Software/conda_env/zhhenv/lib/python3.9/site-packages/anndata/_core/anndata.py:1832: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`. utils.warn_names_duplicates(""var""). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/zhanghaihao/.local/lib/python3.9/site-packages/scanpy/readwrite.py"", line 390, in read_visium. raise OSError(f""Could not find '{f}'""). OSError: Could not find '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/spatial/tissue_positions_list.csv'. ```. ### Versions. <details>. <summary>Details</summary>. ```. >>> import scanpy; scanpy.logging.print_versions(). -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. dot_parser NA. gmpy2 2.1.2. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. invgauss_ufunc NA. joblib 1.1.1. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.2.1. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.57.0. numpy 1.24.3. nvfuser NA. opt_einsum v3.3.0. packaging 23.0. pandas 2.0.1. pkg_resources NA. pydot 1.4.2. pyparsing 3.0.9. pytz 2022.7. s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2565
https://github.com/scverse/scanpy/issues/2565:381,usability,Minim,Minimal,381,"spaceranger2.0.1 outs/spatial/tissue_positions_list.csv change; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In scanpy version 1.9.3. ### Minimal code sample. ```python. adata = sc.read_visium(. '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/',. count_file='filtered_feature_bc_matrix.h5',. source_image_path='/data_disk/ST01/User/zhanghh/project/GSE206391/GSM6252954_11-V19T12-012-V4.jpg',. ). ```. ### Error output. ```pytb. /data_disk/ST01/Software/conda_env/zhhenv/lib/python3.9/site-packages/anndata/_core/anndata.py:1832: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`. utils.warn_names_duplicates(""var""). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/zhanghaihao/.local/lib/python3.9/site-packages/scanpy/readwrite.py"", line 390, in read_visium. raise OSError(f""Could not find '{f}'""). OSError: Could not find '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/spatial/tissue_positions_list.csv'. ```. ### Versions. <details>. <summary>Details</summary>. ```. >>> import scanpy; scanpy.logging.print_versions(). -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. dot_parser NA. gmpy2 2.1.2. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. invgauss_ufunc NA. joblib 1.1.1. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.2.1. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.57.0. numpy 1.24.3. nvfuser NA. opt_einsum v3.3.0. packaging 23.0. pandas 2.0.1. pkg_resources NA. pydot 1.4.2. pyparsing 3.0.9. pytz 2022.7. s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2565
https://github.com/scverse/scanpy/issues/2565:455,usability,User,User,455,"spaceranger2.0.1 outs/spatial/tissue_positions_list.csv change; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In scanpy version 1.9.3. ### Minimal code sample. ```python. adata = sc.read_visium(. '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/',. count_file='filtered_feature_bc_matrix.h5',. source_image_path='/data_disk/ST01/User/zhanghh/project/GSE206391/GSM6252954_11-V19T12-012-V4.jpg',. ). ```. ### Error output. ```pytb. /data_disk/ST01/Software/conda_env/zhhenv/lib/python3.9/site-packages/anndata/_core/anndata.py:1832: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`. utils.warn_names_duplicates(""var""). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/zhanghaihao/.local/lib/python3.9/site-packages/scanpy/readwrite.py"", line 390, in read_visium. raise OSError(f""Could not find '{f}'""). OSError: Could not find '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/spatial/tissue_positions_list.csv'. ```. ### Versions. <details>. <summary>Details</summary>. ```. >>> import scanpy; scanpy.logging.print_versions(). -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. dot_parser NA. gmpy2 2.1.2. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. invgauss_ufunc NA. joblib 1.1.1. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.2.1. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.57.0. numpy 1.24.3. nvfuser NA. opt_einsum v3.3.0. packaging 23.0. pandas 2.0.1. pkg_resources NA. pydot 1.4.2. pyparsing 3.0.9. pytz 2022.7. s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2565
https://github.com/scverse/scanpy/issues/2565:585,usability,User,User,585,"spaceranger2.0.1 outs/spatial/tissue_positions_list.csv change; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In scanpy version 1.9.3. ### Minimal code sample. ```python. adata = sc.read_visium(. '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/',. count_file='filtered_feature_bc_matrix.h5',. source_image_path='/data_disk/ST01/User/zhanghh/project/GSE206391/GSM6252954_11-V19T12-012-V4.jpg',. ). ```. ### Error output. ```pytb. /data_disk/ST01/Software/conda_env/zhhenv/lib/python3.9/site-packages/anndata/_core/anndata.py:1832: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`. utils.warn_names_duplicates(""var""). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/zhanghaihao/.local/lib/python3.9/site-packages/scanpy/readwrite.py"", line 390, in read_visium. raise OSError(f""Could not find '{f}'""). OSError: Could not find '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/spatial/tissue_positions_list.csv'. ```. ### Versions. <details>. <summary>Details</summary>. ```. >>> import scanpy; scanpy.logging.print_versions(). -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. dot_parser NA. gmpy2 2.1.2. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. invgauss_ufunc NA. joblib 1.1.1. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.2.1. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.57.0. numpy 1.24.3. nvfuser NA. opt_einsum v3.3.0. packaging 23.0. pandas 2.0.1. pkg_resources NA. pydot 1.4.2. pyparsing 3.0.9. pytz 2022.7. s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2565
https://github.com/scverse/scanpy/issues/2565:663,usability,Error,Error,663,"spaceranger2.0.1 outs/spatial/tissue_positions_list.csv change; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In scanpy version 1.9.3. ### Minimal code sample. ```python. adata = sc.read_visium(. '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/',. count_file='filtered_feature_bc_matrix.h5',. source_image_path='/data_disk/ST01/User/zhanghh/project/GSE206391/GSM6252954_11-V19T12-012-V4.jpg',. ). ```. ### Error output. ```pytb. /data_disk/ST01/Software/conda_env/zhhenv/lib/python3.9/site-packages/anndata/_core/anndata.py:1832: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`. utils.warn_names_duplicates(""var""). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/zhanghaihao/.local/lib/python3.9/site-packages/scanpy/readwrite.py"", line 390, in read_visium. raise OSError(f""Could not find '{f}'""). OSError: Could not find '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/spatial/tissue_positions_list.csv'. ```. ### Versions. <details>. <summary>Details</summary>. ```. >>> import scanpy; scanpy.logging.print_versions(). -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. dot_parser NA. gmpy2 2.1.2. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. invgauss_ufunc NA. joblib 1.1.1. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.2.1. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.57.0. numpy 1.24.3. nvfuser NA. opt_einsum v3.3.0. packaging 23.0. pandas 2.0.1. pkg_resources NA. pydot 1.4.2. pyparsing 3.0.9. pytz 2022.7. s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2565
https://github.com/scverse/scanpy/issues/2565:787,usability,User,UserWarning,787,"spaceranger2.0.1 outs/spatial/tissue_positions_list.csv change; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In scanpy version 1.9.3. ### Minimal code sample. ```python. adata = sc.read_visium(. '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/',. count_file='filtered_feature_bc_matrix.h5',. source_image_path='/data_disk/ST01/User/zhanghh/project/GSE206391/GSM6252954_11-V19T12-012-V4.jpg',. ). ```. ### Error output. ```pytb. /data_disk/ST01/Software/conda_env/zhhenv/lib/python3.9/site-packages/anndata/_core/anndata.py:1832: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`. utils.warn_names_duplicates(""var""). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/zhanghaihao/.local/lib/python3.9/site-packages/scanpy/readwrite.py"", line 390, in read_visium. raise OSError(f""Could not find '{f}'""). OSError: Could not find '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/spatial/tissue_positions_list.csv'. ```. ### Versions. <details>. <summary>Details</summary>. ```. >>> import scanpy; scanpy.logging.print_versions(). -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. dot_parser NA. gmpy2 2.1.2. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. invgauss_ufunc NA. joblib 1.1.1. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.2.1. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.57.0. numpy 1.24.3. nvfuser NA. opt_einsum v3.3.0. packaging 23.0. pandas 2.0.1. pkg_resources NA. pydot 1.4.2. pyparsing 3.0.9. pytz 2022.7. s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2565
https://github.com/scverse/scanpy/issues/2565:1180,usability,User,User,1180,"X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? In scanpy version 1.9.3. ### Minimal code sample. ```python. adata = sc.read_visium(. '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/',. count_file='filtered_feature_bc_matrix.h5',. source_image_path='/data_disk/ST01/User/zhanghh/project/GSE206391/GSM6252954_11-V19T12-012-V4.jpg',. ). ```. ### Error output. ```pytb. /data_disk/ST01/Software/conda_env/zhhenv/lib/python3.9/site-packages/anndata/_core/anndata.py:1832: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`. utils.warn_names_duplicates(""var""). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/zhanghaihao/.local/lib/python3.9/site-packages/scanpy/readwrite.py"", line 390, in read_visium. raise OSError(f""Could not find '{f}'""). OSError: Could not find '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/spatial/tissue_positions_list.csv'. ```. ### Versions. <details>. <summary>Details</summary>. ```. >>> import scanpy; scanpy.logging.print_versions(). -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. beta_ufunc NA. binom_ufunc NA. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. dot_parser NA. gmpy2 2.1.2. h5py 3.7.0. hypergeom_ufunc NA. igraph 0.10.4. importlib_resources NA. invgauss_ufunc NA. joblib 1.1.1. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.0. matplotlib 3.7.1. mpl_toolkits NA. mpmath 1.2.1. natsort 8.3.1. nbinom_ufunc NA. ncf_ufunc NA. nct_ufunc NA. ncx2_ufunc NA. numba 0.57.0. numpy 1.24.3. nvfuser NA. opt_einsum v3.3.0. packaging 23.0. pandas 2.0.1. pkg_resources NA. pydot 1.4.2. pyparsing 3.0.9. pytz 2022.7. scipy 1.10.1. session_info 1.0.0. setuptools 66.0.0. six 1.16.0. skewnorm_ufunc NA. sklearn 1.2.2. sympy 1.11.1. texttable 1.6.7. threadpoolctl 2.2.0. torch 2.0.1. tqdm 4.65.0. typing",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2565
https://github.com/scverse/scanpy/pull/2566:55,deployability,version,versions,55,"Switch to `igraph` name; Fixes #2562, fixes #2341. All versions of the old package that used to be published under that name are yanked, so no need to specify a minimum version",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2566
https://github.com/scverse/scanpy/pull/2566:169,deployability,version,version,169,"Switch to `igraph` name; Fixes #2562, fixes #2341. All versions of the old package that used to be published under that name are yanked, so no need to specify a minimum version",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2566
https://github.com/scverse/scanpy/pull/2566:55,integrability,version,versions,55,"Switch to `igraph` name; Fixes #2562, fixes #2341. All versions of the old package that used to be published under that name are yanked, so no need to specify a minimum version",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2566
https://github.com/scverse/scanpy/pull/2566:99,integrability,pub,published,99,"Switch to `igraph` name; Fixes #2562, fixes #2341. All versions of the old package that used to be published under that name are yanked, so no need to specify a minimum version",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2566
https://github.com/scverse/scanpy/pull/2566:169,integrability,version,version,169,"Switch to `igraph` name; Fixes #2562, fixes #2341. All versions of the old package that used to be published under that name are yanked, so no need to specify a minimum version",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2566
https://github.com/scverse/scanpy/pull/2566:151,interoperability,specif,specify,151,"Switch to `igraph` name; Fixes #2562, fixes #2341. All versions of the old package that used to be published under that name are yanked, so no need to specify a minimum version",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2566
https://github.com/scverse/scanpy/pull/2566:55,modifiability,version,versions,55,"Switch to `igraph` name; Fixes #2562, fixes #2341. All versions of the old package that used to be published under that name are yanked, so no need to specify a minimum version",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2566
https://github.com/scverse/scanpy/pull/2566:75,modifiability,pac,package,75,"Switch to `igraph` name; Fixes #2562, fixes #2341. All versions of the old package that used to be published under that name are yanked, so no need to specify a minimum version",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2566
https://github.com/scverse/scanpy/pull/2566:169,modifiability,version,version,169,"Switch to `igraph` name; Fixes #2562, fixes #2341. All versions of the old package that used to be published under that name are yanked, so no need to specify a minimum version",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2566
https://github.com/scverse/scanpy/pull/2566:161,usability,minim,minimum,161,"Switch to `igraph` name; Fixes #2562, fixes #2341. All versions of the old package that used to be published under that name are yanked, so no need to specify a minimum version",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2566
https://github.com/scverse/scanpy/pull/2569:14,deployability,releas,release,14,"Add check for release notes; \<!-- Please check (- [x]) and fill in the following two boxes --> . - [x] Closes N/A. - [x] Tests included or not required because:. \<!-- Only check the following box if you did not include release nodes --> . - [x] Release notes not necessary because: Its a dev process PR. (made comments visible to show what PR authors see). Current design is that the jobs fail, respectively:. - If theres no milestone. - If the Development process label isnt there and the Release notes  checkbox is not checked, and there are not edits to the relevant release notes file. This is how it looks like if that check fails:. ![grafik](https://github.com/scverse/scanpy/assets/291575/08c324a5-a580-4b30-a838-b7f98fa9787e).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2569
https://github.com/scverse/scanpy/pull/2569:223,deployability,releas,release,223,"Add check for release notes; \<!-- Please check (- [x]) and fill in the following two boxes --> . - [x] Closes N/A. - [x] Tests included or not required because:. \<!-- Only check the following box if you did not include release nodes --> . - [x] Release notes not necessary because: Its a dev process PR. (made comments visible to show what PR authors see). Current design is that the jobs fail, respectively:. - If theres no milestone. - If the Development process label isnt there and the Release notes  checkbox is not checked, and there are not edits to the relevant release notes file. This is how it looks like if that check fails:. ![grafik](https://github.com/scverse/scanpy/assets/291575/08c324a5-a580-4b30-a838-b7f98fa9787e).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2569
https://github.com/scverse/scanpy/pull/2569:249,deployability,Releas,Release,249,"Add check for release notes; \<!-- Please check (- [x]) and fill in the following two boxes --> . - [x] Closes N/A. - [x] Tests included or not required because:. \<!-- Only check the following box if you did not include release nodes --> . - [x] Release notes not necessary because: Its a dev process PR. (made comments visible to show what PR authors see). Current design is that the jobs fail, respectively:. - If theres no milestone. - If the Development process label isnt there and the Release notes  checkbox is not checked, and there are not edits to the relevant release notes file. This is how it looks like if that check fails:. ![grafik](https://github.com/scverse/scanpy/assets/291575/08c324a5-a580-4b30-a838-b7f98fa9787e).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2569
https://github.com/scverse/scanpy/pull/2569:394,deployability,fail,fail,394,"Add check for release notes; \<!-- Please check (- [x]) and fill in the following two boxes --> . - [x] Closes N/A. - [x] Tests included or not required because:. \<!-- Only check the following box if you did not include release nodes --> . - [x] Release notes not necessary because: Its a dev process PR. (made comments visible to show what PR authors see). Current design is that the jobs fail, respectively:. - If theres no milestone. - If the Development process label isnt there and the Release notes  checkbox is not checked, and there are not edits to the relevant release notes file. This is how it looks like if that check fails:. ![grafik](https://github.com/scverse/scanpy/assets/291575/08c324a5-a580-4b30-a838-b7f98fa9787e).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2569
https://github.com/scverse/scanpy/pull/2569:500,deployability,Releas,Release,500,"Add check for release notes; \<!-- Please check (- [x]) and fill in the following two boxes --> . - [x] Closes N/A. - [x] Tests included or not required because:. \<!-- Only check the following box if you did not include release nodes --> . - [x] Release notes not necessary because: Its a dev process PR. (made comments visible to show what PR authors see). Current design is that the jobs fail, respectively:. - If theres no milestone. - If the Development process label isnt there and the Release notes  checkbox is not checked, and there are not edits to the relevant release notes file. This is how it looks like if that check fails:. ![grafik](https://github.com/scverse/scanpy/assets/291575/08c324a5-a580-4b30-a838-b7f98fa9787e).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2569
https://github.com/scverse/scanpy/pull/2569:582,deployability,releas,release,582,"Add check for release notes; \<!-- Please check (- [x]) and fill in the following two boxes --> . - [x] Closes N/A. - [x] Tests included or not required because:. \<!-- Only check the following box if you did not include release nodes --> . - [x] Release notes not necessary because: Its a dev process PR. (made comments visible to show what PR authors see). Current design is that the jobs fail, respectively:. - If theres no milestone. - If the Development process label isnt there and the Release notes  checkbox is not checked, and there are not edits to the relevant release notes file. This is how it looks like if that check fails:. ![grafik](https://github.com/scverse/scanpy/assets/291575/08c324a5-a580-4b30-a838-b7f98fa9787e).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2569
https://github.com/scverse/scanpy/pull/2569:642,deployability,fail,fails,642,"Add check for release notes; \<!-- Please check (- [x]) and fill in the following two boxes --> . - [x] Closes N/A. - [x] Tests included or not required because:. \<!-- Only check the following box if you did not include release nodes --> . - [x] Release notes not necessary because: Its a dev process PR. (made comments visible to show what PR authors see). Current design is that the jobs fail, respectively:. - If theres no milestone. - If the Development process label isnt there and the Release notes  checkbox is not checked, and there are not edits to the relevant release notes file. This is how it looks like if that check fails:. ![grafik](https://github.com/scverse/scanpy/assets/291575/08c324a5-a580-4b30-a838-b7f98fa9787e).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2569
https://github.com/scverse/scanpy/pull/2569:362,energy efficiency,Current,Current,362,"Add check for release notes; \<!-- Please check (- [x]) and fill in the following two boxes --> . - [x] Closes N/A. - [x] Tests included or not required because:. \<!-- Only check the following box if you did not include release nodes --> . - [x] Release notes not necessary because: Its a dev process PR. (made comments visible to show what PR authors see). Current design is that the jobs fail, respectively:. - If theres no milestone. - If the Development process label isnt there and the Release notes  checkbox is not checked, and there are not edits to the relevant release notes file. This is how it looks like if that check fails:. ![grafik](https://github.com/scverse/scanpy/assets/291575/08c324a5-a580-4b30-a838-b7f98fa9787e).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2569
https://github.com/scverse/scanpy/pull/2569:394,reliability,fail,fail,394,"Add check for release notes; \<!-- Please check (- [x]) and fill in the following two boxes --> . - [x] Closes N/A. - [x] Tests included or not required because:. \<!-- Only check the following box if you did not include release nodes --> . - [x] Release notes not necessary because: Its a dev process PR. (made comments visible to show what PR authors see). Current design is that the jobs fail, respectively:. - If theres no milestone. - If the Development process label isnt there and the Release notes  checkbox is not checked, and there are not edits to the relevant release notes file. This is how it looks like if that check fails:. ![grafik](https://github.com/scverse/scanpy/assets/291575/08c324a5-a580-4b30-a838-b7f98fa9787e).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2569
https://github.com/scverse/scanpy/pull/2569:642,reliability,fail,fails,642,"Add check for release notes; \<!-- Please check (- [x]) and fill in the following two boxes --> . - [x] Closes N/A. - [x] Tests included or not required because:. \<!-- Only check the following box if you did not include release nodes --> . - [x] Release notes not necessary because: Its a dev process PR. (made comments visible to show what PR authors see). Current design is that the jobs fail, respectively:. - If theres no milestone. - If the Development process label isnt there and the Release notes  checkbox is not checked, and there are not edits to the relevant release notes file. This is how it looks like if that check fails:. ![grafik](https://github.com/scverse/scanpy/assets/291575/08c324a5-a580-4b30-a838-b7f98fa9787e).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2569
https://github.com/scverse/scanpy/pull/2569:124,safety,Test,Tests,124,"Add check for release notes; \<!-- Please check (- [x]) and fill in the following two boxes --> . - [x] Closes N/A. - [x] Tests included or not required because:. \<!-- Only check the following box if you did not include release nodes --> . - [x] Release notes not necessary because: Its a dev process PR. (made comments visible to show what PR authors see). Current design is that the jobs fail, respectively:. - If theres no milestone. - If the Development process label isnt there and the Release notes  checkbox is not checked, and there are not edits to the relevant release notes file. This is how it looks like if that check fails:. ![grafik](https://github.com/scverse/scanpy/assets/291575/08c324a5-a580-4b30-a838-b7f98fa9787e).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2569
https://github.com/scverse/scanpy/pull/2569:348,security,auth,authors,348,"Add check for release notes; \<!-- Please check (- [x]) and fill in the following two boxes --> . - [x] Closes N/A. - [x] Tests included or not required because:. \<!-- Only check the following box if you did not include release nodes --> . - [x] Release notes not necessary because: Its a dev process PR. (made comments visible to show what PR authors see). Current design is that the jobs fail, respectively:. - If theres no milestone. - If the Development process label isnt there and the Release notes  checkbox is not checked, and there are not edits to the relevant release notes file. This is how it looks like if that check fails:. ![grafik](https://github.com/scverse/scanpy/assets/291575/08c324a5-a580-4b30-a838-b7f98fa9787e).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2569
https://github.com/scverse/scanpy/pull/2569:124,testability,Test,Tests,124,"Add check for release notes; \<!-- Please check (- [x]) and fill in the following two boxes --> . - [x] Closes N/A. - [x] Tests included or not required because:. \<!-- Only check the following box if you did not include release nodes --> . - [x] Release notes not necessary because: Its a dev process PR. (made comments visible to show what PR authors see). Current design is that the jobs fail, respectively:. - If theres no milestone. - If the Development process label isnt there and the Release notes  checkbox is not checked, and there are not edits to the relevant release notes file. This is how it looks like if that check fails:. ![grafik](https://github.com/scverse/scanpy/assets/291575/08c324a5-a580-4b30-a838-b7f98fa9787e).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2569
https://github.com/scverse/scanpy/pull/2569:106,usability,Close,Closes,106,"Add check for release notes; \<!-- Please check (- [x]) and fill in the following two boxes --> . - [x] Closes N/A. - [x] Tests included or not required because:. \<!-- Only check the following box if you did not include release nodes --> . - [x] Release notes not necessary because: Its a dev process PR. (made comments visible to show what PR authors see). Current design is that the jobs fail, respectively:. - If theres no milestone. - If the Development process label isnt there and the Release notes  checkbox is not checked, and there are not edits to the relevant release notes file. This is how it looks like if that check fails:. ![grafik](https://github.com/scverse/scanpy/assets/291575/08c324a5-a580-4b30-a838-b7f98fa9787e).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2569
https://github.com/scverse/scanpy/issues/2570:516,availability,error,error,516,"read_10x_mtx cannot find matrix.mtx.gz file; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've been having an issue trying to load published single cell data (GEO GSE123366) onto my jupyter notebook. Despite providing the directory where all 3 files are located, I get the error that it cannot find my matrix.mtx.gz file. . ### Minimal code sample. ```python. sc.read_10x_mtx(""GSE123366_Combined""). ```. ### Error output. ```pytb. FileNotFoundError Traceback (most recent call last). Cell In[31], line 1. ----> 1 sc.read_10x_mtx(""GSE123366_Combined"", cache=True). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:490, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix). 488 genefile_exists = (path / f'{prefix}genes.tsv').is_file(). 489 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx. --> 490 adata = read(. 491 str(path),. 492 var_names=var_names,. 493 make_unique=make_unique,. 494 cache=cache,. 495 cache_compression=cache_compression,. 496 prefix=prefix,. 497 ). 498 if genefile_exists or not gex_only:. 499 return adata. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:554, in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix). 550 """""". 551 Read mtx from output from Cell Ranger v3 or later versions. 552 """""". 553 path = Path(path). --> 554 adata = read(. 555 path / f'{prefix}matrix.mtx.gz',. 556 cache=cache,. 557 cache_compression=cache_compression,. 558 ).T # transpose the data. 559 genes = pd.read_csv(path / f'{prefix}features.tsv.gz', header=None, sep='\t'). 560 if var_names == 'gene_symbols':. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, ba",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570
https://github.com/scverse/scanpy/issues/2570:651,availability,Error,Error,651,"read_10x_mtx cannot find matrix.mtx.gz file; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've been having an issue trying to load published single cell data (GEO GSE123366) onto my jupyter notebook. Despite providing the directory where all 3 files are located, I get the error that it cannot find my matrix.mtx.gz file. . ### Minimal code sample. ```python. sc.read_10x_mtx(""GSE123366_Combined""). ```. ### Error output. ```pytb. FileNotFoundError Traceback (most recent call last). Cell In[31], line 1. ----> 1 sc.read_10x_mtx(""GSE123366_Combined"", cache=True). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:490, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix). 488 genefile_exists = (path / f'{prefix}genes.tsv').is_file(). 489 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx. --> 490 adata = read(. 491 str(path),. 492 var_names=var_names,. 493 make_unique=make_unique,. 494 cache=cache,. 495 cache_compression=cache_compression,. 496 prefix=prefix,. 497 ). 498 if genefile_exists or not gex_only:. 499 return adata. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:554, in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix). 550 """""". 551 Read mtx from output from Cell Ranger v3 or later versions. 552 """""". 553 path = Path(path). --> 554 adata = read(. 555 path / f'{prefix}matrix.mtx.gz',. 556 cache=cache,. 557 cache_compression=cache_compression,. 558 ).T # transpose the data. 559 genes = pd.read_csv(path / f'{prefix}features.tsv.gz', header=None, sep='\t'). 560 if var_names == 'gene_symbols':. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, ba",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570
https://github.com/scverse/scanpy/issues/2570:213,deployability,version,version,213,"read_10x_mtx cannot find matrix.mtx.gz file; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've been having an issue trying to load published single cell data (GEO GSE123366) onto my jupyter notebook. Despite providing the directory where all 3 files are located, I get the error that it cannot find my matrix.mtx.gz file. . ### Minimal code sample. ```python. sc.read_10x_mtx(""GSE123366_Combined""). ```. ### Error output. ```pytb. FileNotFoundError Traceback (most recent call last). Cell In[31], line 1. ----> 1 sc.read_10x_mtx(""GSE123366_Combined"", cache=True). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:490, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix). 488 genefile_exists = (path / f'{prefix}genes.tsv').is_file(). 489 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx. --> 490 adata = read(. 491 str(path),. 492 var_names=var_names,. 493 make_unique=make_unique,. 494 cache=cache,. 495 cache_compression=cache_compression,. 496 prefix=prefix,. 497 ). 498 if genefile_exists or not gex_only:. 499 return adata. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:554, in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix). 550 """""". 551 Read mtx from output from Cell Ranger v3 or later versions. 552 """""". 553 path = Path(path). --> 554 adata = read(. 555 path / f'{prefix}matrix.mtx.gz',. 556 cache=cache,. 557 cache_compression=cache_compression,. 558 ).T # transpose the data. 559 genes = pd.read_csv(path / f'{prefix}features.tsv.gz', header=None, sep='\t'). 560 if var_names == 'gene_symbols':. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, ba",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570
https://github.com/scverse/scanpy/issues/2570:1586,deployability,version,versions,1586,". ```python. sc.read_10x_mtx(""GSE123366_Combined""). ```. ### Error output. ```pytb. FileNotFoundError Traceback (most recent call last). Cell In[31], line 1. ----> 1 sc.read_10x_mtx(""GSE123366_Combined"", cache=True). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:490, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix). 488 genefile_exists = (path / f'{prefix}genes.tsv').is_file(). 489 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx. --> 490 adata = read(. 491 str(path),. 492 var_names=var_names,. 493 make_unique=make_unique,. 494 cache=cache,. 495 cache_compression=cache_compression,. 496 prefix=prefix,. 497 ). 498 if genefile_exists or not gex_only:. 499 return adata. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:554, in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix). 550 """""". 551 Read mtx from output from Cell Ranger v3 or later versions. 552 """""". 553 path = Path(path). --> 554 adata = read(. 555 path / f'{prefix}matrix.mtx.gz',. 556 cache=cache,. 557 cache_compression=cache_compression,. 558 ).T # transpose the data. 559 genes = pd.read_csv(path / f'{prefix}features.tsv.gz', header=None, sep='\t'). 560 if var_names == 'gene_symbols':. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). 110 filename = Path(filename) # allow passing strings. 111 if is_valid_filename(filename):. --> 112 return _read(. 113 filename,. 114 backed=backed,. 115 sheet=sheet,. 116 ext=ext,. 117 delimiter=delimiter,. 118 first_column_names=first_column_names,. 119 backup_url=backup_url,. 120 cache=cache,. 121 cache_compression=cache_compression,. 122 **kwargs,. 123 ). 124 # generate filename and read to dict. 125 filekey = str(filename). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/sca",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570
https://github.com/scverse/scanpy/issues/2570:2884,deployability,log,logg,2884,"e_symbols':. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). 110 filename = Path(filename) # allow passing strings. 111 if is_valid_filename(filename):. --> 112 return _read(. 113 filename,. 114 backed=backed,. 115 sheet=sheet,. 116 ext=ext,. 117 delimiter=delimiter,. 118 first_column_names=first_column_names,. 119 backup_url=backup_url,. 120 cache=cache,. 121 cache_compression=cache_compression,. 122 **kwargs,. 123 ). 124 # generate filename and read to dict. 125 filekey = str(filename). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:737, in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, suppress_cache_warning, **kwargs). 734 return read_h5ad(path_cache). 736 if not is_present:. --> 737 raise FileNotFoundError(f'Did not find file {filename}.'). 738 logg.debug(f'reading {filename}'). 739 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file GSE123366_Combined/matrix.mtx.gz. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 10.0.0. appnope 0.1.3. asttokens NA. attr 23.1.0. backcall 0.2.0. boltons NA. cffi 1.15.1. cloudpickle 2.2.1. comm 0.1.3. ctxcore 0.2.0. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.1. dask 2023.7.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. frozendict 2.3.8. h5py 3.9.0. ikarus NA. importlib_resources NA. ipykernel 6.24.0. ipython_genutils 0.2.0. jedi 0.18.2. jinja2 3.1.2. joblib 1.3.1. kiwisolver 1.4.4. llvmlite 0.40.1. lz4 4.3.2. markupsafe 2.1.3. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numexpr 2.8.4. numpy 1.24.4. packaging 23.1. pandas 2.0.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.8.1. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570
https://github.com/scverse/scanpy/issues/2570:3049,deployability,Version,Versions,3049,"up_url, cache, cache_compression, **kwargs). 110 filename = Path(filename) # allow passing strings. 111 if is_valid_filename(filename):. --> 112 return _read(. 113 filename,. 114 backed=backed,. 115 sheet=sheet,. 116 ext=ext,. 117 delimiter=delimiter,. 118 first_column_names=first_column_names,. 119 backup_url=backup_url,. 120 cache=cache,. 121 cache_compression=cache_compression,. 122 **kwargs,. 123 ). 124 # generate filename and read to dict. 125 filekey = str(filename). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:737, in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, suppress_cache_warning, **kwargs). 734 return read_h5ad(path_cache). 736 if not is_present:. --> 737 raise FileNotFoundError(f'Did not find file {filename}.'). 738 logg.debug(f'reading {filename}'). 739 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file GSE123366_Combined/matrix.mtx.gz. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 10.0.0. appnope 0.1.3. asttokens NA. attr 23.1.0. backcall 0.2.0. boltons NA. cffi 1.15.1. cloudpickle 2.2.1. comm 0.1.3. ctxcore 0.2.0. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.1. dask 2023.7.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. frozendict 2.3.8. h5py 3.9.0. ikarus NA. importlib_resources NA. ipykernel 6.24.0. ipython_genutils 0.2.0. jedi 0.18.2. jinja2 3.1.2. joblib 1.3.1. kiwisolver 1.4.4. llvmlite 0.40.1. lz4 4.3.2. markupsafe 2.1.3. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numexpr 2.8.4. numpy 1.24.4. packaging 23.1. pandas 2.0.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.8.1. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pyarrow 12.0.1. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570
https://github.com/scverse/scanpy/issues/2570:4560,deployability,updat,updated,4560,"y:737, in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, suppress_cache_warning, **kwargs). 734 return read_h5ad(path_cache). 736 if not is_present:. --> 737 raise FileNotFoundError(f'Did not find file {filename}.'). 738 logg.debug(f'reading {filename}'). 739 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file GSE123366_Combined/matrix.mtx.gz. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 10.0.0. appnope 0.1.3. asttokens NA. attr 23.1.0. backcall 0.2.0. boltons NA. cffi 1.15.1. cloudpickle 2.2.1. comm 0.1.3. ctxcore 0.2.0. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.1. dask 2023.7.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. frozendict 2.3.8. h5py 3.9.0. ikarus NA. importlib_resources NA. ipykernel 6.24.0. ipython_genutils 0.2.0. jedi 0.18.2. jinja2 3.1.2. joblib 1.3.1. kiwisolver 1.4.4. llvmlite 0.40.1. lz4 4.3.2. markupsafe 2.1.3. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numexpr 2.8.4. numpy 1.24.4. packaging 23.1. pandas 2.0.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.8.1. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pyarrow 12.0.1. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.0.9. pyscenic 0.12.1. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. six 1.16.0. sklearn 1.3.0. stack_data 0.6.2. tblib 2.0.0. threadpoolctl 3.2.0. tlz 0.12.1. toolz 0.12.0. tornado 6.3.2. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. wcwidth 0.2.6. yaml 6.0. zipp NA. zmq 25.1.0. zoneinfo NA. -----. IPython 8.14.0. jupyter_client 8.3.0. jupyter_core 5.3.1. notebook 6.5.4. -----. Python 3.9.13 (main, Aug 25 2022, 18:29:29) [Clang 12.0.0 ]. macOS-10.16-x86_64-i386-64bit. -----. Session information updated at 2023-07-21 20:08. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570
https://github.com/scverse/scanpy/issues/2570:369,energy efficiency,load,load,369,"read_10x_mtx cannot find matrix.mtx.gz file; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've been having an issue trying to load published single cell data (GEO GSE123366) onto my jupyter notebook. Despite providing the directory where all 3 files are located, I get the error that it cannot find my matrix.mtx.gz file. . ### Minimal code sample. ```python. sc.read_10x_mtx(""GSE123366_Combined""). ```. ### Error output. ```pytb. FileNotFoundError Traceback (most recent call last). Cell In[31], line 1. ----> 1 sc.read_10x_mtx(""GSE123366_Combined"", cache=True). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:490, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix). 488 genefile_exists = (path / f'{prefix}genes.tsv').is_file(). 489 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx. --> 490 adata = read(. 491 str(path),. 492 var_names=var_names,. 493 make_unique=make_unique,. 494 cache=cache,. 495 cache_compression=cache_compression,. 496 prefix=prefix,. 497 ). 498 if genefile_exists or not gex_only:. 499 return adata. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:554, in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix). 550 """""". 551 Read mtx from output from Cell Ranger v3 or later versions. 552 """""". 553 path = Path(path). --> 554 adata = read(. 555 path / f'{prefix}matrix.mtx.gz',. 556 cache=cache,. 557 cache_compression=cache_compression,. 558 ).T # transpose the data. 559 genes = pd.read_csv(path / f'{prefix}features.tsv.gz', header=None, sep='\t'). 560 if var_names == 'gene_symbols':. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, ba",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570
https://github.com/scverse/scanpy/issues/2570:3213,energy efficiency,cloud,cloudpickle,3213,"lename,. 114 backed=backed,. 115 sheet=sheet,. 116 ext=ext,. 117 delimiter=delimiter,. 118 first_column_names=first_column_names,. 119 backup_url=backup_url,. 120 cache=cache,. 121 cache_compression=cache_compression,. 122 **kwargs,. 123 ). 124 # generate filename and read to dict. 125 filekey = str(filename). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:737, in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, suppress_cache_warning, **kwargs). 734 return read_h5ad(path_cache). 736 if not is_present:. --> 737 raise FileNotFoundError(f'Did not find file {filename}.'). 738 logg.debug(f'reading {filename}'). 739 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file GSE123366_Combined/matrix.mtx.gz. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 10.0.0. appnope 0.1.3. asttokens NA. attr 23.1.0. backcall 0.2.0. boltons NA. cffi 1.15.1. cloudpickle 2.2.1. comm 0.1.3. ctxcore 0.2.0. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.1. dask 2023.7.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. frozendict 2.3.8. h5py 3.9.0. ikarus NA. importlib_resources NA. ipykernel 6.24.0. ipython_genutils 0.2.0. jedi 0.18.2. jinja2 3.1.2. joblib 1.3.1. kiwisolver 1.4.4. llvmlite 0.40.1. lz4 4.3.2. markupsafe 2.1.3. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numexpr 2.8.4. numpy 1.24.4. packaging 23.1. pandas 2.0.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.8.1. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pyarrow 12.0.1. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.0.9. pyscenic 0.12.1. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. six 1.16.0. sklearn 1.3.0. stack_data 0.6.2. tblib 2.0.0. threadpoolctl 3.2.0. tlz 0.12.1. toolz ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570
https://github.com/scverse/scanpy/issues/2570:213,integrability,version,version,213,"read_10x_mtx cannot find matrix.mtx.gz file; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've been having an issue trying to load published single cell data (GEO GSE123366) onto my jupyter notebook. Despite providing the directory where all 3 files are located, I get the error that it cannot find my matrix.mtx.gz file. . ### Minimal code sample. ```python. sc.read_10x_mtx(""GSE123366_Combined""). ```. ### Error output. ```pytb. FileNotFoundError Traceback (most recent call last). Cell In[31], line 1. ----> 1 sc.read_10x_mtx(""GSE123366_Combined"", cache=True). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:490, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix). 488 genefile_exists = (path / f'{prefix}genes.tsv').is_file(). 489 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx. --> 490 adata = read(. 491 str(path),. 492 var_names=var_names,. 493 make_unique=make_unique,. 494 cache=cache,. 495 cache_compression=cache_compression,. 496 prefix=prefix,. 497 ). 498 if genefile_exists or not gex_only:. 499 return adata. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:554, in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix). 550 """""". 551 Read mtx from output from Cell Ranger v3 or later versions. 552 """""". 553 path = Path(path). --> 554 adata = read(. 555 path / f'{prefix}matrix.mtx.gz',. 556 cache=cache,. 557 cache_compression=cache_compression,. 558 ).T # transpose the data. 559 genes = pd.read_csv(path / f'{prefix}features.tsv.gz', header=None, sep='\t'). 560 if var_names == 'gene_symbols':. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, ba",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570
https://github.com/scverse/scanpy/issues/2570:374,integrability,pub,published,374,"read_10x_mtx cannot find matrix.mtx.gz file; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've been having an issue trying to load published single cell data (GEO GSE123366) onto my jupyter notebook. Despite providing the directory where all 3 files are located, I get the error that it cannot find my matrix.mtx.gz file. . ### Minimal code sample. ```python. sc.read_10x_mtx(""GSE123366_Combined""). ```. ### Error output. ```pytb. FileNotFoundError Traceback (most recent call last). Cell In[31], line 1. ----> 1 sc.read_10x_mtx(""GSE123366_Combined"", cache=True). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:490, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix). 488 genefile_exists = (path / f'{prefix}genes.tsv').is_file(). 489 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx. --> 490 adata = read(. 491 str(path),. 492 var_names=var_names,. 493 make_unique=make_unique,. 494 cache=cache,. 495 cache_compression=cache_compression,. 496 prefix=prefix,. 497 ). 498 if genefile_exists or not gex_only:. 499 return adata. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:554, in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix). 550 """""". 551 Read mtx from output from Cell Ranger v3 or later versions. 552 """""". 553 path = Path(path). --> 554 adata = read(. 555 path / f'{prefix}matrix.mtx.gz',. 556 cache=cache,. 557 cache_compression=cache_compression,. 558 ).T # transpose the data. 559 genes = pd.read_csv(path / f'{prefix}features.tsv.gz', header=None, sep='\t'). 560 if var_names == 'gene_symbols':. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, ba",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570
https://github.com/scverse/scanpy/issues/2570:1586,integrability,version,versions,1586,". ```python. sc.read_10x_mtx(""GSE123366_Combined""). ```. ### Error output. ```pytb. FileNotFoundError Traceback (most recent call last). Cell In[31], line 1. ----> 1 sc.read_10x_mtx(""GSE123366_Combined"", cache=True). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:490, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix). 488 genefile_exists = (path / f'{prefix}genes.tsv').is_file(). 489 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx. --> 490 adata = read(. 491 str(path),. 492 var_names=var_names,. 493 make_unique=make_unique,. 494 cache=cache,. 495 cache_compression=cache_compression,. 496 prefix=prefix,. 497 ). 498 if genefile_exists or not gex_only:. 499 return adata. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:554, in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix). 550 """""". 551 Read mtx from output from Cell Ranger v3 or later versions. 552 """""". 553 path = Path(path). --> 554 adata = read(. 555 path / f'{prefix}matrix.mtx.gz',. 556 cache=cache,. 557 cache_compression=cache_compression,. 558 ).T # transpose the data. 559 genes = pd.read_csv(path / f'{prefix}features.tsv.gz', header=None, sep='\t'). 560 if var_names == 'gene_symbols':. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). 110 filename = Path(filename) # allow passing strings. 111 if is_valid_filename(filename):. --> 112 return _read(. 113 filename,. 114 backed=backed,. 115 sheet=sheet,. 116 ext=ext,. 117 delimiter=delimiter,. 118 first_column_names=first_column_names,. 119 backup_url=backup_url,. 120 cache=cache,. 121 cache_compression=cache_compression,. 122 **kwargs,. 123 ). 124 # generate filename and read to dict. 125 filekey = str(filename). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/sca",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570
https://github.com/scverse/scanpy/issues/2570:3049,integrability,Version,Versions,3049,"up_url, cache, cache_compression, **kwargs). 110 filename = Path(filename) # allow passing strings. 111 if is_valid_filename(filename):. --> 112 return _read(. 113 filename,. 114 backed=backed,. 115 sheet=sheet,. 116 ext=ext,. 117 delimiter=delimiter,. 118 first_column_names=first_column_names,. 119 backup_url=backup_url,. 120 cache=cache,. 121 cache_compression=cache_compression,. 122 **kwargs,. 123 ). 124 # generate filename and read to dict. 125 filekey = str(filename). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:737, in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, suppress_cache_warning, **kwargs). 734 return read_h5ad(path_cache). 736 if not is_present:. --> 737 raise FileNotFoundError(f'Did not find file {filename}.'). 738 logg.debug(f'reading {filename}'). 739 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file GSE123366_Combined/matrix.mtx.gz. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 10.0.0. appnope 0.1.3. asttokens NA. attr 23.1.0. backcall 0.2.0. boltons NA. cffi 1.15.1. cloudpickle 2.2.1. comm 0.1.3. ctxcore 0.2.0. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.1. dask 2023.7.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. frozendict 2.3.8. h5py 3.9.0. ikarus NA. importlib_resources NA. ipykernel 6.24.0. ipython_genutils 0.2.0. jedi 0.18.2. jinja2 3.1.2. joblib 1.3.1. kiwisolver 1.4.4. llvmlite 0.40.1. lz4 4.3.2. markupsafe 2.1.3. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numexpr 2.8.4. numpy 1.24.4. packaging 23.1. pandas 2.0.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.8.1. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pyarrow 12.0.1. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570
https://github.com/scverse/scanpy/issues/2570:3807,interoperability,platform,platformdirs,3807,"y:737, in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, suppress_cache_warning, **kwargs). 734 return read_h5ad(path_cache). 736 if not is_present:. --> 737 raise FileNotFoundError(f'Did not find file {filename}.'). 738 logg.debug(f'reading {filename}'). 739 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file GSE123366_Combined/matrix.mtx.gz. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 10.0.0. appnope 0.1.3. asttokens NA. attr 23.1.0. backcall 0.2.0. boltons NA. cffi 1.15.1. cloudpickle 2.2.1. comm 0.1.3. ctxcore 0.2.0. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.1. dask 2023.7.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. frozendict 2.3.8. h5py 3.9.0. ikarus NA. importlib_resources NA. ipykernel 6.24.0. ipython_genutils 0.2.0. jedi 0.18.2. jinja2 3.1.2. joblib 1.3.1. kiwisolver 1.4.4. llvmlite 0.40.1. lz4 4.3.2. markupsafe 2.1.3. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numexpr 2.8.4. numpy 1.24.4. packaging 23.1. pandas 2.0.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.8.1. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pyarrow 12.0.1. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.0.9. pyscenic 0.12.1. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. six 1.16.0. sklearn 1.3.0. stack_data 0.6.2. tblib 2.0.0. threadpoolctl 3.2.0. tlz 0.12.1. toolz 0.12.0. tornado 6.3.2. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. wcwidth 0.2.6. yaml 6.0. zipp NA. zmq 25.1.0. zoneinfo NA. -----. IPython 8.14.0. jupyter_client 8.3.0. jupyter_core 5.3.1. notebook 6.5.4. -----. Python 3.9.13 (main, Aug 25 2022, 18:29:29) [Clang 12.0.0 ]. macOS-10.16-x86_64-i386-64bit. -----. Session information updated at 2023-07-21 20:08. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570
https://github.com/scverse/scanpy/issues/2570:213,modifiability,version,version,213,"read_10x_mtx cannot find matrix.mtx.gz file; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've been having an issue trying to load published single cell data (GEO GSE123366) onto my jupyter notebook. Despite providing the directory where all 3 files are located, I get the error that it cannot find my matrix.mtx.gz file. . ### Minimal code sample. ```python. sc.read_10x_mtx(""GSE123366_Combined""). ```. ### Error output. ```pytb. FileNotFoundError Traceback (most recent call last). Cell In[31], line 1. ----> 1 sc.read_10x_mtx(""GSE123366_Combined"", cache=True). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:490, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix). 488 genefile_exists = (path / f'{prefix}genes.tsv').is_file(). 489 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx. --> 490 adata = read(. 491 str(path),. 492 var_names=var_names,. 493 make_unique=make_unique,. 494 cache=cache,. 495 cache_compression=cache_compression,. 496 prefix=prefix,. 497 ). 498 if genefile_exists or not gex_only:. 499 return adata. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:554, in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix). 550 """""". 551 Read mtx from output from Cell Ranger v3 or later versions. 552 """""". 553 path = Path(path). --> 554 adata = read(. 555 path / f'{prefix}matrix.mtx.gz',. 556 cache=cache,. 557 cache_compression=cache_compression,. 558 ).T # transpose the data. 559 genes = pd.read_csv(path / f'{prefix}features.tsv.gz', header=None, sep='\t'). 560 if var_names == 'gene_symbols':. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, ba",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570
https://github.com/scverse/scanpy/issues/2570:854,modifiability,pac,packages,854,"read_10x_mtx cannot find matrix.mtx.gz file; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've been having an issue trying to load published single cell data (GEO GSE123366) onto my jupyter notebook. Despite providing the directory where all 3 files are located, I get the error that it cannot find my matrix.mtx.gz file. . ### Minimal code sample. ```python. sc.read_10x_mtx(""GSE123366_Combined""). ```. ### Error output. ```pytb. FileNotFoundError Traceback (most recent call last). Cell In[31], line 1. ----> 1 sc.read_10x_mtx(""GSE123366_Combined"", cache=True). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:490, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix). 488 genefile_exists = (path / f'{prefix}genes.tsv').is_file(). 489 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx. --> 490 adata = read(. 491 str(path),. 492 var_names=var_names,. 493 make_unique=make_unique,. 494 cache=cache,. 495 cache_compression=cache_compression,. 496 prefix=prefix,. 497 ). 498 if genefile_exists or not gex_only:. 499 return adata. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:554, in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix). 550 """""". 551 Read mtx from output from Cell Ranger v3 or later versions. 552 """""". 553 path = Path(path). --> 554 adata = read(. 555 path / f'{prefix}matrix.mtx.gz',. 556 cache=cache,. 557 cache_compression=cache_compression,. 558 ).T # transpose the data. 559 genes = pd.read_csv(path / f'{prefix}features.tsv.gz', header=None, sep='\t'). 560 if var_names == 'gene_symbols':. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, ba",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570
https://github.com/scverse/scanpy/issues/2570:1404,modifiability,pac,packages,1404,"E123366) onto my jupyter notebook. Despite providing the directory where all 3 files are located, I get the error that it cannot find my matrix.mtx.gz file. . ### Minimal code sample. ```python. sc.read_10x_mtx(""GSE123366_Combined""). ```. ### Error output. ```pytb. FileNotFoundError Traceback (most recent call last). Cell In[31], line 1. ----> 1 sc.read_10x_mtx(""GSE123366_Combined"", cache=True). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:490, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix). 488 genefile_exists = (path / f'{prefix}genes.tsv').is_file(). 489 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx. --> 490 adata = read(. 491 str(path),. 492 var_names=var_names,. 493 make_unique=make_unique,. 494 cache=cache,. 495 cache_compression=cache_compression,. 496 prefix=prefix,. 497 ). 498 if genefile_exists or not gex_only:. 499 return adata. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:554, in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix). 550 """""". 551 Read mtx from output from Cell Ranger v3 or later versions. 552 """""". 553 path = Path(path). --> 554 adata = read(. 555 path / f'{prefix}matrix.mtx.gz',. 556 cache=cache,. 557 cache_compression=cache_compression,. 558 ).T # transpose the data. 559 genes = pd.read_csv(path / f'{prefix}features.tsv.gz', header=None, sep='\t'). 560 if var_names == 'gene_symbols':. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). 110 filename = Path(filename) # allow passing strings. 111 if is_valid_filename(filename):. --> 112 return _read(. 113 filename,. 114 backed=backed,. 115 sheet=sheet,. 116 ext=ext,. 117 delimiter=delimiter,. 118 first_column_names=first_column_names,. 119 backup_url=backup_url,. 120 cache=cache,. 121 cache_co",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570
https://github.com/scverse/scanpy/issues/2570:1586,modifiability,version,versions,1586,". ```python. sc.read_10x_mtx(""GSE123366_Combined""). ```. ### Error output. ```pytb. FileNotFoundError Traceback (most recent call last). Cell In[31], line 1. ----> 1 sc.read_10x_mtx(""GSE123366_Combined"", cache=True). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:490, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix). 488 genefile_exists = (path / f'{prefix}genes.tsv').is_file(). 489 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx. --> 490 adata = read(. 491 str(path),. 492 var_names=var_names,. 493 make_unique=make_unique,. 494 cache=cache,. 495 cache_compression=cache_compression,. 496 prefix=prefix,. 497 ). 498 if genefile_exists or not gex_only:. 499 return adata. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:554, in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix). 550 """""". 551 Read mtx from output from Cell Ranger v3 or later versions. 552 """""". 553 path = Path(path). --> 554 adata = read(. 555 path / f'{prefix}matrix.mtx.gz',. 556 cache=cache,. 557 cache_compression=cache_compression,. 558 ).T # transpose the data. 559 genes = pd.read_csv(path / f'{prefix}features.tsv.gz', header=None, sep='\t'). 560 if var_names == 'gene_symbols':. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). 110 filename = Path(filename) # allow passing strings. 111 if is_valid_filename(filename):. --> 112 return _read(. 113 filename,. 114 backed=backed,. 115 sheet=sheet,. 116 ext=ext,. 117 delimiter=delimiter,. 118 first_column_names=first_column_names,. 119 backup_url=backup_url,. 120 cache=cache,. 121 cache_compression=cache_compression,. 122 **kwargs,. 123 ). 124 # generate filename and read to dict. 125 filekey = str(filename). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/sca",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570
https://github.com/scverse/scanpy/issues/2570:1946,modifiability,pac,packages,1946,"pression, gex_only, prefix). 488 genefile_exists = (path / f'{prefix}genes.tsv').is_file(). 489 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx. --> 490 adata = read(. 491 str(path),. 492 var_names=var_names,. 493 make_unique=make_unique,. 494 cache=cache,. 495 cache_compression=cache_compression,. 496 prefix=prefix,. 497 ). 498 if genefile_exists or not gex_only:. 499 return adata. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:554, in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix). 550 """""". 551 Read mtx from output from Cell Ranger v3 or later versions. 552 """""". 553 path = Path(path). --> 554 adata = read(. 555 path / f'{prefix}matrix.mtx.gz',. 556 cache=cache,. 557 cache_compression=cache_compression,. 558 ).T # transpose the data. 559 genes = pd.read_csv(path / f'{prefix}features.tsv.gz', header=None, sep='\t'). 560 if var_names == 'gene_symbols':. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). 110 filename = Path(filename) # allow passing strings. 111 if is_valid_filename(filename):. --> 112 return _read(. 113 filename,. 114 backed=backed,. 115 sheet=sheet,. 116 ext=ext,. 117 delimiter=delimiter,. 118 first_column_names=first_column_names,. 119 backup_url=backup_url,. 120 cache=cache,. 121 cache_compression=cache_compression,. 122 **kwargs,. 123 ). 124 # generate filename and read to dict. 125 filekey = str(filename). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:737, in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, suppress_cache_warning, **kwargs). 734 return read_h5ad(path_cache). 736 if not is_present:. --> 737 raise FileNotFoundError(f'Did not find file {filename}.'). 738 logg.debug(f'reading {filename}'). 739 if not cache and not suppre",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570
https://github.com/scverse/scanpy/issues/2570:2578,modifiability,pac,packages,2578,"ter versions. 552 """""". 553 path = Path(path). --> 554 adata = read(. 555 path / f'{prefix}matrix.mtx.gz',. 556 cache=cache,. 557 cache_compression=cache_compression,. 558 ).T # transpose the data. 559 genes = pd.read_csv(path / f'{prefix}features.tsv.gz', header=None, sep='\t'). 560 if var_names == 'gene_symbols':. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). 110 filename = Path(filename) # allow passing strings. 111 if is_valid_filename(filename):. --> 112 return _read(. 113 filename,. 114 backed=backed,. 115 sheet=sheet,. 116 ext=ext,. 117 delimiter=delimiter,. 118 first_column_names=first_column_names,. 119 backup_url=backup_url,. 120 cache=cache,. 121 cache_compression=cache_compression,. 122 **kwargs,. 123 ). 124 # generate filename and read to dict. 125 filekey = str(filename). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:737, in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, suppress_cache_warning, **kwargs). 734 return read_h5ad(path_cache). 736 if not is_present:. --> 737 raise FileNotFoundError(f'Did not find file {filename}.'). 738 logg.debug(f'reading {filename}'). 739 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file GSE123366_Combined/matrix.mtx.gz. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 10.0.0. appnope 0.1.3. asttokens NA. attr 23.1.0. backcall 0.2.0. boltons NA. cffi 1.15.1. cloudpickle 2.2.1. comm 0.1.3. ctxcore 0.2.0. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.1. dask 2023.7.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. frozendict 2.3.8. h5py 3.9.0. ikarus NA. importlib_resources NA. ipykernel 6.24.0. ipython_genutils 0.2.0. jedi 0.18.2. jinja2 3.1.2. joblib 1.3.1. kiwisolver 1.4.4. llvmlite ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570
https://github.com/scverse/scanpy/issues/2570:3049,modifiability,Version,Versions,3049,"up_url, cache, cache_compression, **kwargs). 110 filename = Path(filename) # allow passing strings. 111 if is_valid_filename(filename):. --> 112 return _read(. 113 filename,. 114 backed=backed,. 115 sheet=sheet,. 116 ext=ext,. 117 delimiter=delimiter,. 118 first_column_names=first_column_names,. 119 backup_url=backup_url,. 120 cache=cache,. 121 cache_compression=cache_compression,. 122 **kwargs,. 123 ). 124 # generate filename and read to dict. 125 filekey = str(filename). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:737, in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, suppress_cache_warning, **kwargs). 734 return read_h5ad(path_cache). 736 if not is_present:. --> 737 raise FileNotFoundError(f'Did not find file {filename}.'). 738 logg.debug(f'reading {filename}'). 739 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file GSE123366_Combined/matrix.mtx.gz. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 10.0.0. appnope 0.1.3. asttokens NA. attr 23.1.0. backcall 0.2.0. boltons NA. cffi 1.15.1. cloudpickle 2.2.1. comm 0.1.3. ctxcore 0.2.0. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.1. dask 2023.7.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. frozendict 2.3.8. h5py 3.9.0. ikarus NA. importlib_resources NA. ipykernel 6.24.0. ipython_genutils 0.2.0. jedi 0.18.2. jinja2 3.1.2. joblib 1.3.1. kiwisolver 1.4.4. llvmlite 0.40.1. lz4 4.3.2. markupsafe 2.1.3. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numexpr 2.8.4. numpy 1.24.4. packaging 23.1. pandas 2.0.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.8.1. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pyarrow 12.0.1. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570
https://github.com/scverse/scanpy/issues/2570:3355,modifiability,deco,decorator,3355,"_url=backup_url,. 120 cache=cache,. 121 cache_compression=cache_compression,. 122 **kwargs,. 123 ). 124 # generate filename and read to dict. 125 filekey = str(filename). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:737, in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, suppress_cache_warning, **kwargs). 734 return read_h5ad(path_cache). 736 if not is_present:. --> 737 raise FileNotFoundError(f'Did not find file {filename}.'). 738 logg.debug(f'reading {filename}'). 739 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file GSE123366_Combined/matrix.mtx.gz. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 10.0.0. appnope 0.1.3. asttokens NA. attr 23.1.0. backcall 0.2.0. boltons NA. cffi 1.15.1. cloudpickle 2.2.1. comm 0.1.3. ctxcore 0.2.0. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.1. dask 2023.7.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. frozendict 2.3.8. h5py 3.9.0. ikarus NA. importlib_resources NA. ipykernel 6.24.0. ipython_genutils 0.2.0. jedi 0.18.2. jinja2 3.1.2. joblib 1.3.1. kiwisolver 1.4.4. llvmlite 0.40.1. lz4 4.3.2. markupsafe 2.1.3. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numexpr 2.8.4. numpy 1.24.4. packaging 23.1. pandas 2.0.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.8.1. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pyarrow 12.0.1. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.0.9. pyscenic 0.12.1. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. six 1.16.0. sklearn 1.3.0. stack_data 0.6.2. tblib 2.0.0. threadpoolctl 3.2.0. tlz 0.12.1. toolz 0.12.0. tornado 6.3.2. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. wcwidth 0.2.6. yaml 6.0. zipp NA. zmq 25.1.0. zoneinfo NA. -----. ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570
https://github.com/scverse/scanpy/issues/2570:3712,modifiability,pac,packaging,3712,"y:737, in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, suppress_cache_warning, **kwargs). 734 return read_h5ad(path_cache). 736 if not is_present:. --> 737 raise FileNotFoundError(f'Did not find file {filename}.'). 738 logg.debug(f'reading {filename}'). 739 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file GSE123366_Combined/matrix.mtx.gz. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 10.0.0. appnope 0.1.3. asttokens NA. attr 23.1.0. backcall 0.2.0. boltons NA. cffi 1.15.1. cloudpickle 2.2.1. comm 0.1.3. ctxcore 0.2.0. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.1. dask 2023.7.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. frozendict 2.3.8. h5py 3.9.0. ikarus NA. importlib_resources NA. ipykernel 6.24.0. ipython_genutils 0.2.0. jedi 0.18.2. jinja2 3.1.2. joblib 1.3.1. kiwisolver 1.4.4. llvmlite 0.40.1. lz4 4.3.2. markupsafe 2.1.3. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numexpr 2.8.4. numpy 1.24.4. packaging 23.1. pandas 2.0.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.8.1. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pyarrow 12.0.1. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.0.9. pyscenic 0.12.1. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. six 1.16.0. sklearn 1.3.0. stack_data 0.6.2. tblib 2.0.0. threadpoolctl 3.2.0. tlz 0.12.1. toolz 0.12.0. tornado 6.3.2. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. wcwidth 0.2.6. yaml 6.0. zipp NA. zmq 25.1.0. zoneinfo NA. -----. IPython 8.14.0. jupyter_client 8.3.0. jupyter_core 5.3.1. notebook 6.5.4. -----. Python 3.9.13 (main, Aug 25 2022, 18:29:29) [Clang 12.0.0 ]. macOS-10.16-x86_64-i386-64bit. -----. Session information updated at 2023-07-21 20:08. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570
https://github.com/scverse/scanpy/issues/2570:369,performance,load,load,369,"read_10x_mtx cannot find matrix.mtx.gz file; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've been having an issue trying to load published single cell data (GEO GSE123366) onto my jupyter notebook. Despite providing the directory where all 3 files are located, I get the error that it cannot find my matrix.mtx.gz file. . ### Minimal code sample. ```python. sc.read_10x_mtx(""GSE123366_Combined""). ```. ### Error output. ```pytb. FileNotFoundError Traceback (most recent call last). Cell In[31], line 1. ----> 1 sc.read_10x_mtx(""GSE123366_Combined"", cache=True). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:490, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix). 488 genefile_exists = (path / f'{prefix}genes.tsv').is_file(). 489 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx. --> 490 adata = read(. 491 str(path),. 492 var_names=var_names,. 493 make_unique=make_unique,. 494 cache=cache,. 495 cache_compression=cache_compression,. 496 prefix=prefix,. 497 ). 498 if genefile_exists or not gex_only:. 499 return adata. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:554, in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix). 550 """""". 551 Read mtx from output from Cell Ranger v3 or later versions. 552 """""". 553 path = Path(path). --> 554 adata = read(. 555 path / f'{prefix}matrix.mtx.gz',. 556 cache=cache,. 557 cache_compression=cache_compression,. 558 ).T # transpose the data. 559 genes = pd.read_csv(path / f'{prefix}features.tsv.gz', header=None, sep='\t'). 560 if var_names == 'gene_symbols':. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, ba",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570
https://github.com/scverse/scanpy/issues/2570:516,performance,error,error,516,"read_10x_mtx cannot find matrix.mtx.gz file; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've been having an issue trying to load published single cell data (GEO GSE123366) onto my jupyter notebook. Despite providing the directory where all 3 files are located, I get the error that it cannot find my matrix.mtx.gz file. . ### Minimal code sample. ```python. sc.read_10x_mtx(""GSE123366_Combined""). ```. ### Error output. ```pytb. FileNotFoundError Traceback (most recent call last). Cell In[31], line 1. ----> 1 sc.read_10x_mtx(""GSE123366_Combined"", cache=True). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:490, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix). 488 genefile_exists = (path / f'{prefix}genes.tsv').is_file(). 489 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx. --> 490 adata = read(. 491 str(path),. 492 var_names=var_names,. 493 make_unique=make_unique,. 494 cache=cache,. 495 cache_compression=cache_compression,. 496 prefix=prefix,. 497 ). 498 if genefile_exists or not gex_only:. 499 return adata. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:554, in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix). 550 """""". 551 Read mtx from output from Cell Ranger v3 or later versions. 552 """""". 553 path = Path(path). --> 554 adata = read(. 555 path / f'{prefix}matrix.mtx.gz',. 556 cache=cache,. 557 cache_compression=cache_compression,. 558 ).T # transpose the data. 559 genes = pd.read_csv(path / f'{prefix}features.tsv.gz', header=None, sep='\t'). 560 if var_names == 'gene_symbols':. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, ba",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570
https://github.com/scverse/scanpy/issues/2570:651,performance,Error,Error,651,"read_10x_mtx cannot find matrix.mtx.gz file; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've been having an issue trying to load published single cell data (GEO GSE123366) onto my jupyter notebook. Despite providing the directory where all 3 files are located, I get the error that it cannot find my matrix.mtx.gz file. . ### Minimal code sample. ```python. sc.read_10x_mtx(""GSE123366_Combined""). ```. ### Error output. ```pytb. FileNotFoundError Traceback (most recent call last). Cell In[31], line 1. ----> 1 sc.read_10x_mtx(""GSE123366_Combined"", cache=True). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:490, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix). 488 genefile_exists = (path / f'{prefix}genes.tsv').is_file(). 489 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx. --> 490 adata = read(. 491 str(path),. 492 var_names=var_names,. 493 make_unique=make_unique,. 494 cache=cache,. 495 cache_compression=cache_compression,. 496 prefix=prefix,. 497 ). 498 if genefile_exists or not gex_only:. 499 return adata. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:554, in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix). 550 """""". 551 Read mtx from output from Cell Ranger v3 or later versions. 552 """""". 553 path = Path(path). --> 554 adata = read(. 555 path / f'{prefix}matrix.mtx.gz',. 556 cache=cache,. 557 cache_compression=cache_compression,. 558 ).T # transpose the data. 559 genes = pd.read_csv(path / f'{prefix}features.tsv.gz', header=None, sep='\t'). 560 if var_names == 'gene_symbols':. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, ba",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570
https://github.com/scverse/scanpy/issues/2570:794,performance,cach,cache,794,"read_10x_mtx cannot find matrix.mtx.gz file; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've been having an issue trying to load published single cell data (GEO GSE123366) onto my jupyter notebook. Despite providing the directory where all 3 files are located, I get the error that it cannot find my matrix.mtx.gz file. . ### Minimal code sample. ```python. sc.read_10x_mtx(""GSE123366_Combined""). ```. ### Error output. ```pytb. FileNotFoundError Traceback (most recent call last). Cell In[31], line 1. ----> 1 sc.read_10x_mtx(""GSE123366_Combined"", cache=True). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:490, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix). 488 genefile_exists = (path / f'{prefix}genes.tsv').is_file(). 489 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx. --> 490 adata = read(. 491 str(path),. 492 var_names=var_names,. 493 make_unique=make_unique,. 494 cache=cache,. 495 cache_compression=cache_compression,. 496 prefix=prefix,. 497 ). 498 if genefile_exists or not gex_only:. 499 return adata. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:554, in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix). 550 """""". 551 Read mtx from output from Cell Ranger v3 or later versions. 552 """""". 553 path = Path(path). --> 554 adata = read(. 555 path / f'{prefix}matrix.mtx.gz',. 556 cache=cache,. 557 cache_compression=cache_compression,. 558 ).T # transpose the data. 559 genes = pd.read_csv(path / f'{prefix}features.tsv.gz', header=None, sep='\t'). 560 if var_names == 'gene_symbols':. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, ba",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570
https://github.com/scverse/scanpy/issues/2570:934,performance,cach,cache,934,"read_10x_mtx cannot find matrix.mtx.gz file; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've been having an issue trying to load published single cell data (GEO GSE123366) onto my jupyter notebook. Despite providing the directory where all 3 files are located, I get the error that it cannot find my matrix.mtx.gz file. . ### Minimal code sample. ```python. sc.read_10x_mtx(""GSE123366_Combined""). ```. ### Error output. ```pytb. FileNotFoundError Traceback (most recent call last). Cell In[31], line 1. ----> 1 sc.read_10x_mtx(""GSE123366_Combined"", cache=True). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:490, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix). 488 genefile_exists = (path / f'{prefix}genes.tsv').is_file(). 489 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx. --> 490 adata = read(. 491 str(path),. 492 var_names=var_names,. 493 make_unique=make_unique,. 494 cache=cache,. 495 cache_compression=cache_compression,. 496 prefix=prefix,. 497 ). 498 if genefile_exists or not gex_only:. 499 return adata. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:554, in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix). 550 """""". 551 Read mtx from output from Cell Ranger v3 or later versions. 552 """""". 553 path = Path(path). --> 554 adata = read(. 555 path / f'{prefix}matrix.mtx.gz',. 556 cache=cache,. 557 cache_compression=cache_compression,. 558 ).T # transpose the data. 559 genes = pd.read_csv(path / f'{prefix}features.tsv.gz', header=None, sep='\t'). 560 if var_names == 'gene_symbols':. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, ba",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570
https://github.com/scverse/scanpy/issues/2570:1215,performance,cach,cache,1215,"on of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've been having an issue trying to load published single cell data (GEO GSE123366) onto my jupyter notebook. Despite providing the directory where all 3 files are located, I get the error that it cannot find my matrix.mtx.gz file. . ### Minimal code sample. ```python. sc.read_10x_mtx(""GSE123366_Combined""). ```. ### Error output. ```pytb. FileNotFoundError Traceback (most recent call last). Cell In[31], line 1. ----> 1 sc.read_10x_mtx(""GSE123366_Combined"", cache=True). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:490, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix). 488 genefile_exists = (path / f'{prefix}genes.tsv').is_file(). 489 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx. --> 490 adata = read(. 491 str(path),. 492 var_names=var_names,. 493 make_unique=make_unique,. 494 cache=cache,. 495 cache_compression=cache_compression,. 496 prefix=prefix,. 497 ). 498 if genefile_exists or not gex_only:. 499 return adata. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:554, in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix). 550 """""". 551 Read mtx from output from Cell Ranger v3 or later versions. 552 """""". 553 path = Path(path). --> 554 adata = read(. 555 path / f'{prefix}matrix.mtx.gz',. 556 cache=cache,. 557 cache_compression=cache_compression,. 558 ).T # transpose the data. 559 genes = pd.read_csv(path / f'{prefix}features.tsv.gz', header=None, sep='\t'). 560 if var_names == 'gene_symbols':. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). 110 filename = Path(filename) # allow passing strings. 111 if is_valid_filename(filename):. --> 112 return _read(. 113 f",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570
https://github.com/scverse/scanpy/issues/2570:1221,performance,cach,cache,1221,"scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've been having an issue trying to load published single cell data (GEO GSE123366) onto my jupyter notebook. Despite providing the directory where all 3 files are located, I get the error that it cannot find my matrix.mtx.gz file. . ### Minimal code sample. ```python. sc.read_10x_mtx(""GSE123366_Combined""). ```. ### Error output. ```pytb. FileNotFoundError Traceback (most recent call last). Cell In[31], line 1. ----> 1 sc.read_10x_mtx(""GSE123366_Combined"", cache=True). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:490, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix). 488 genefile_exists = (path / f'{prefix}genes.tsv').is_file(). 489 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx. --> 490 adata = read(. 491 str(path),. 492 var_names=var_names,. 493 make_unique=make_unique,. 494 cache=cache,. 495 cache_compression=cache_compression,. 496 prefix=prefix,. 497 ). 498 if genefile_exists or not gex_only:. 499 return adata. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:554, in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix). 550 """""". 551 Read mtx from output from Cell Ranger v3 or later versions. 552 """""". 553 path = Path(path). --> 554 adata = read(. 555 path / f'{prefix}matrix.mtx.gz',. 556 cache=cache,. 557 cache_compression=cache_compression,. 558 ).T # transpose the data. 559 genes = pd.read_csv(path / f'{prefix}features.tsv.gz', header=None, sep='\t'). 560 if var_names == 'gene_symbols':. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). 110 filename = Path(filename) # allow passing strings. 111 if is_valid_filename(filename):. --> 112 return _read(. 113 filenam",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570
https://github.com/scverse/scanpy/issues/2570:1488,performance,cach,cache,1488,"s are located, I get the error that it cannot find my matrix.mtx.gz file. . ### Minimal code sample. ```python. sc.read_10x_mtx(""GSE123366_Combined""). ```. ### Error output. ```pytb. FileNotFoundError Traceback (most recent call last). Cell In[31], line 1. ----> 1 sc.read_10x_mtx(""GSE123366_Combined"", cache=True). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:490, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix). 488 genefile_exists = (path / f'{prefix}genes.tsv').is_file(). 489 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx. --> 490 adata = read(. 491 str(path),. 492 var_names=var_names,. 493 make_unique=make_unique,. 494 cache=cache,. 495 cache_compression=cache_compression,. 496 prefix=prefix,. 497 ). 498 if genefile_exists or not gex_only:. 499 return adata. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:554, in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix). 550 """""". 551 Read mtx from output from Cell Ranger v3 or later versions. 552 """""". 553 path = Path(path). --> 554 adata = read(. 555 path / f'{prefix}matrix.mtx.gz',. 556 cache=cache,. 557 cache_compression=cache_compression,. 558 ).T # transpose the data. 559 genes = pd.read_csv(path / f'{prefix}features.tsv.gz', header=None, sep='\t'). 560 if var_names == 'gene_symbols':. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). 110 filename = Path(filename) # allow passing strings. 111 if is_valid_filename(filename):. --> 112 return _read(. 113 filename,. 114 backed=backed,. 115 sheet=sheet,. 116 ext=ext,. 117 delimiter=delimiter,. 118 first_column_names=first_column_names,. 119 backup_url=backup_url,. 120 cache=cache,. 121 cache_compression=cache_compression,. 122 **kwargs,. 123 ). 124 # generate filename and rea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570
https://github.com/scverse/scanpy/issues/2570:1693,performance,cach,cache,1693,"eback (most recent call last). Cell In[31], line 1. ----> 1 sc.read_10x_mtx(""GSE123366_Combined"", cache=True). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:490, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix). 488 genefile_exists = (path / f'{prefix}genes.tsv').is_file(). 489 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx. --> 490 adata = read(. 491 str(path),. 492 var_names=var_names,. 493 make_unique=make_unique,. 494 cache=cache,. 495 cache_compression=cache_compression,. 496 prefix=prefix,. 497 ). 498 if genefile_exists or not gex_only:. 499 return adata. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:554, in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix). 550 """""". 551 Read mtx from output from Cell Ranger v3 or later versions. 552 """""". 553 path = Path(path). --> 554 adata = read(. 555 path / f'{prefix}matrix.mtx.gz',. 556 cache=cache,. 557 cache_compression=cache_compression,. 558 ).T # transpose the data. 559 genes = pd.read_csv(path / f'{prefix}features.tsv.gz', header=None, sep='\t'). 560 if var_names == 'gene_symbols':. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). 110 filename = Path(filename) # allow passing strings. 111 if is_valid_filename(filename):. --> 112 return _read(. 113 filename,. 114 backed=backed,. 115 sheet=sheet,. 116 ext=ext,. 117 delimiter=delimiter,. 118 first_column_names=first_column_names,. 119 backup_url=backup_url,. 120 cache=cache,. 121 cache_compression=cache_compression,. 122 **kwargs,. 123 ). 124 # generate filename and read to dict. 125 filekey = str(filename). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:737, in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, ca",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570
https://github.com/scverse/scanpy/issues/2570:1699,performance,cach,cache,1699,"(most recent call last). Cell In[31], line 1. ----> 1 sc.read_10x_mtx(""GSE123366_Combined"", cache=True). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:490, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix). 488 genefile_exists = (path / f'{prefix}genes.tsv').is_file(). 489 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx. --> 490 adata = read(. 491 str(path),. 492 var_names=var_names,. 493 make_unique=make_unique,. 494 cache=cache,. 495 cache_compression=cache_compression,. 496 prefix=prefix,. 497 ). 498 if genefile_exists or not gex_only:. 499 return adata. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:554, in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix). 550 """""". 551 Read mtx from output from Cell Ranger v3 or later versions. 552 """""". 553 path = Path(path). --> 554 adata = read(. 555 path / f'{prefix}matrix.mtx.gz',. 556 cache=cache,. 557 cache_compression=cache_compression,. 558 ).T # transpose the data. 559 genes = pd.read_csv(path / f'{prefix}features.tsv.gz', header=None, sep='\t'). 560 if var_names == 'gene_symbols':. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). 110 filename = Path(filename) # allow passing strings. 111 if is_valid_filename(filename):. --> 112 return _read(. 113 filename,. 114 backed=backed,. 115 sheet=sheet,. 116 ext=ext,. 117 delimiter=delimiter,. 118 first_column_names=first_column_names,. 119 backup_url=backup_url,. 120 cache=cache,. 121 cache_compression=cache_compression,. 122 **kwargs,. 123 ). 124 # generate filename and read to dict. 125 filekey = str(filename). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:737, in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570
https://github.com/scverse/scanpy/issues/2570:2061,performance,cach,cache,2061,"y_10x_mtx if genefile_exists else _read_v3_10x_mtx. --> 490 adata = read(. 491 str(path),. 492 var_names=var_names,. 493 make_unique=make_unique,. 494 cache=cache,. 495 cache_compression=cache_compression,. 496 prefix=prefix,. 497 ). 498 if genefile_exists or not gex_only:. 499 return adata. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:554, in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix). 550 """""". 551 Read mtx from output from Cell Ranger v3 or later versions. 552 """""". 553 path = Path(path). --> 554 adata = read(. 555 path / f'{prefix}matrix.mtx.gz',. 556 cache=cache,. 557 cache_compression=cache_compression,. 558 ).T # transpose the data. 559 genes = pd.read_csv(path / f'{prefix}features.tsv.gz', header=None, sep='\t'). 560 if var_names == 'gene_symbols':. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). 110 filename = Path(filename) # allow passing strings. 111 if is_valid_filename(filename):. --> 112 return _read(. 113 filename,. 114 backed=backed,. 115 sheet=sheet,. 116 ext=ext,. 117 delimiter=delimiter,. 118 first_column_names=first_column_names,. 119 backup_url=backup_url,. 120 cache=cache,. 121 cache_compression=cache_compression,. 122 **kwargs,. 123 ). 124 # generate filename and read to dict. 125 filekey = str(filename). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:737, in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, suppress_cache_warning, **kwargs). 734 return read_h5ad(path_cache). 736 if not is_present:. --> 737 raise FileNotFoundError(f'Did not find file {filename}.'). 738 logg.debug(f'reading {filename}'). 739 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file GSE123366_Combined/matrix.mtx.gz. ```. ### Versions. <deta",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570
https://github.com/scverse/scanpy/issues/2570:2382,performance,cach,cache,2382,"lib/python3.9/site-packages/scanpy/readwrite.py:554, in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix). 550 """""". 551 Read mtx from output from Cell Ranger v3 or later versions. 552 """""". 553 path = Path(path). --> 554 adata = read(. 555 path / f'{prefix}matrix.mtx.gz',. 556 cache=cache,. 557 cache_compression=cache_compression,. 558 ).T # transpose the data. 559 genes = pd.read_csv(path / f'{prefix}features.tsv.gz', header=None, sep='\t'). 560 if var_names == 'gene_symbols':. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). 110 filename = Path(filename) # allow passing strings. 111 if is_valid_filename(filename):. --> 112 return _read(. 113 filename,. 114 backed=backed,. 115 sheet=sheet,. 116 ext=ext,. 117 delimiter=delimiter,. 118 first_column_names=first_column_names,. 119 backup_url=backup_url,. 120 cache=cache,. 121 cache_compression=cache_compression,. 122 **kwargs,. 123 ). 124 # generate filename and read to dict. 125 filekey = str(filename). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:737, in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, suppress_cache_warning, **kwargs). 734 return read_h5ad(path_cache). 736 if not is_present:. --> 737 raise FileNotFoundError(f'Did not find file {filename}.'). 738 logg.debug(f'reading {filename}'). 739 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file GSE123366_Combined/matrix.mtx.gz. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 10.0.0. appnope 0.1.3. asttokens NA. attr 23.1.0. backcall 0.2.0. boltons NA. cffi 1.15.1. cloudpickle 2.2.1. comm 0.1.3. ctxcore 0.2.0. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.1. dask 2023.7.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570
https://github.com/scverse/scanpy/issues/2570:2388,performance,cach,cache,2388,"thon3.9/site-packages/scanpy/readwrite.py:554, in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix). 550 """""". 551 Read mtx from output from Cell Ranger v3 or later versions. 552 """""". 553 path = Path(path). --> 554 adata = read(. 555 path / f'{prefix}matrix.mtx.gz',. 556 cache=cache,. 557 cache_compression=cache_compression,. 558 ).T # transpose the data. 559 genes = pd.read_csv(path / f'{prefix}features.tsv.gz', header=None, sep='\t'). 560 if var_names == 'gene_symbols':. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). 110 filename = Path(filename) # allow passing strings. 111 if is_valid_filename(filename):. --> 112 return _read(. 113 filename,. 114 backed=backed,. 115 sheet=sheet,. 116 ext=ext,. 117 delimiter=delimiter,. 118 first_column_names=first_column_names,. 119 backup_url=backup_url,. 120 cache=cache,. 121 cache_compression=cache_compression,. 122 **kwargs,. 123 ). 124 # generate filename and read to dict. 125 filekey = str(filename). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:737, in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, suppress_cache_warning, **kwargs). 734 return read_h5ad(path_cache). 736 if not is_present:. --> 737 raise FileNotFoundError(f'Did not find file {filename}.'). 738 logg.debug(f'reading {filename}'). 739 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file GSE123366_Combined/matrix.mtx.gz. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 10.0.0. appnope 0.1.3. asttokens NA. attr 23.1.0. backcall 0.2.0. boltons NA. cffi 1.15.1. cloudpickle 2.2.1. comm 0.1.3. ctxcore 0.2.0. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.1. dask 2023.7.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. e",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570
https://github.com/scverse/scanpy/issues/2570:2694,performance,cach,cache,2694,"e=cache,. 557 cache_compression=cache_compression,. 558 ).T # transpose the data. 559 genes = pd.read_csv(path / f'{prefix}features.tsv.gz', header=None, sep='\t'). 560 if var_names == 'gene_symbols':. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). 110 filename = Path(filename) # allow passing strings. 111 if is_valid_filename(filename):. --> 112 return _read(. 113 filename,. 114 backed=backed,. 115 sheet=sheet,. 116 ext=ext,. 117 delimiter=delimiter,. 118 first_column_names=first_column_names,. 119 backup_url=backup_url,. 120 cache=cache,. 121 cache_compression=cache_compression,. 122 **kwargs,. 123 ). 124 # generate filename and read to dict. 125 filekey = str(filename). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:737, in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, suppress_cache_warning, **kwargs). 734 return read_h5ad(path_cache). 736 if not is_present:. --> 737 raise FileNotFoundError(f'Did not find file {filename}.'). 738 logg.debug(f'reading {filename}'). 739 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file GSE123366_Combined/matrix.mtx.gz. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 10.0.0. appnope 0.1.3. asttokens NA. attr 23.1.0. backcall 0.2.0. boltons NA. cffi 1.15.1. cloudpickle 2.2.1. comm 0.1.3. ctxcore 0.2.0. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.1. dask 2023.7.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. frozendict 2.3.8. h5py 3.9.0. ikarus NA. importlib_resources NA. ipykernel 6.24.0. ipython_genutils 0.2.0. jedi 0.18.2. jinja2 3.1.2. joblib 1.3.1. kiwisolver 1.4.4. llvmlite 0.40.1. lz4 4.3.2. markupsafe 2.1.3. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numexpr 2.8.4.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570
https://github.com/scverse/scanpy/issues/2570:2930,performance,cach,cache,2930,"thon3.9/site-packages/scanpy/readwrite.py:112, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). 110 filename = Path(filename) # allow passing strings. 111 if is_valid_filename(filename):. --> 112 return _read(. 113 filename,. 114 backed=backed,. 115 sheet=sheet,. 116 ext=ext,. 117 delimiter=delimiter,. 118 first_column_names=first_column_names,. 119 backup_url=backup_url,. 120 cache=cache,. 121 cache_compression=cache_compression,. 122 **kwargs,. 123 ). 124 # generate filename and read to dict. 125 filekey = str(filename). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:737, in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, suppress_cache_warning, **kwargs). 734 return read_h5ad(path_cache). 736 if not is_present:. --> 737 raise FileNotFoundError(f'Did not find file {filename}.'). 738 logg.debug(f'reading {filename}'). 739 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file GSE123366_Combined/matrix.mtx.gz. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 10.0.0. appnope 0.1.3. asttokens NA. attr 23.1.0. backcall 0.2.0. boltons NA. cffi 1.15.1. cloudpickle 2.2.1. comm 0.1.3. ctxcore 0.2.0. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.1. dask 2023.7.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. frozendict 2.3.8. h5py 3.9.0. ikarus NA. importlib_resources NA. ipykernel 6.24.0. ipython_genutils 0.2.0. jedi 0.18.2. jinja2 3.1.2. joblib 1.3.1. kiwisolver 1.4.4. llvmlite 0.40.1. lz4 4.3.2. markupsafe 2.1.3. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numexpr 2.8.4. numpy 1.24.4. packaging 23.1. pandas 2.0.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.8.1. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pyarrow 12.0.1. pydev_ipython NA. ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570
https://github.com/scverse/scanpy/issues/2570:516,safety,error,error,516,"read_10x_mtx cannot find matrix.mtx.gz file; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've been having an issue trying to load published single cell data (GEO GSE123366) onto my jupyter notebook. Despite providing the directory where all 3 files are located, I get the error that it cannot find my matrix.mtx.gz file. . ### Minimal code sample. ```python. sc.read_10x_mtx(""GSE123366_Combined""). ```. ### Error output. ```pytb. FileNotFoundError Traceback (most recent call last). Cell In[31], line 1. ----> 1 sc.read_10x_mtx(""GSE123366_Combined"", cache=True). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:490, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix). 488 genefile_exists = (path / f'{prefix}genes.tsv').is_file(). 489 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx. --> 490 adata = read(. 491 str(path),. 492 var_names=var_names,. 493 make_unique=make_unique,. 494 cache=cache,. 495 cache_compression=cache_compression,. 496 prefix=prefix,. 497 ). 498 if genefile_exists or not gex_only:. 499 return adata. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:554, in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix). 550 """""". 551 Read mtx from output from Cell Ranger v3 or later versions. 552 """""". 553 path = Path(path). --> 554 adata = read(. 555 path / f'{prefix}matrix.mtx.gz',. 556 cache=cache,. 557 cache_compression=cache_compression,. 558 ).T # transpose the data. 559 genes = pd.read_csv(path / f'{prefix}features.tsv.gz', header=None, sep='\t'). 560 if var_names == 'gene_symbols':. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, ba",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570
https://github.com/scverse/scanpy/issues/2570:651,safety,Error,Error,651,"read_10x_mtx cannot find matrix.mtx.gz file; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've been having an issue trying to load published single cell data (GEO GSE123366) onto my jupyter notebook. Despite providing the directory where all 3 files are located, I get the error that it cannot find my matrix.mtx.gz file. . ### Minimal code sample. ```python. sc.read_10x_mtx(""GSE123366_Combined""). ```. ### Error output. ```pytb. FileNotFoundError Traceback (most recent call last). Cell In[31], line 1. ----> 1 sc.read_10x_mtx(""GSE123366_Combined"", cache=True). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:490, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix). 488 genefile_exists = (path / f'{prefix}genes.tsv').is_file(). 489 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx. --> 490 adata = read(. 491 str(path),. 492 var_names=var_names,. 493 make_unique=make_unique,. 494 cache=cache,. 495 cache_compression=cache_compression,. 496 prefix=prefix,. 497 ). 498 if genefile_exists or not gex_only:. 499 return adata. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:554, in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix). 550 """""". 551 Read mtx from output from Cell Ranger v3 or later versions. 552 """""". 553 path = Path(path). --> 554 adata = read(. 555 path / f'{prefix}matrix.mtx.gz',. 556 cache=cache,. 557 cache_compression=cache_compression,. 558 ).T # transpose the data. 559 genes = pd.read_csv(path / f'{prefix}features.tsv.gz', header=None, sep='\t'). 560 if var_names == 'gene_symbols':. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, ba",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570
https://github.com/scverse/scanpy/issues/2570:2884,safety,log,logg,2884,"e_symbols':. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). 110 filename = Path(filename) # allow passing strings. 111 if is_valid_filename(filename):. --> 112 return _read(. 113 filename,. 114 backed=backed,. 115 sheet=sheet,. 116 ext=ext,. 117 delimiter=delimiter,. 118 first_column_names=first_column_names,. 119 backup_url=backup_url,. 120 cache=cache,. 121 cache_compression=cache_compression,. 122 **kwargs,. 123 ). 124 # generate filename and read to dict. 125 filekey = str(filename). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:737, in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, suppress_cache_warning, **kwargs). 734 return read_h5ad(path_cache). 736 if not is_present:. --> 737 raise FileNotFoundError(f'Did not find file {filename}.'). 738 logg.debug(f'reading {filename}'). 739 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file GSE123366_Combined/matrix.mtx.gz. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 10.0.0. appnope 0.1.3. asttokens NA. attr 23.1.0. backcall 0.2.0. boltons NA. cffi 1.15.1. cloudpickle 2.2.1. comm 0.1.3. ctxcore 0.2.0. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.1. dask 2023.7.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. frozendict 2.3.8. h5py 3.9.0. ikarus NA. importlib_resources NA. ipykernel 6.24.0. ipython_genutils 0.2.0. jedi 0.18.2. jinja2 3.1.2. joblib 1.3.1. kiwisolver 1.4.4. llvmlite 0.40.1. lz4 4.3.2. markupsafe 2.1.3. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numexpr 2.8.4. numpy 1.24.4. packaging 23.1. pandas 2.0.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.8.1. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570
https://github.com/scverse/scanpy/issues/2570:4560,safety,updat,updated,4560,"y:737, in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, suppress_cache_warning, **kwargs). 734 return read_h5ad(path_cache). 736 if not is_present:. --> 737 raise FileNotFoundError(f'Did not find file {filename}.'). 738 logg.debug(f'reading {filename}'). 739 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file GSE123366_Combined/matrix.mtx.gz. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 10.0.0. appnope 0.1.3. asttokens NA. attr 23.1.0. backcall 0.2.0. boltons NA. cffi 1.15.1. cloudpickle 2.2.1. comm 0.1.3. ctxcore 0.2.0. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.1. dask 2023.7.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. frozendict 2.3.8. h5py 3.9.0. ikarus NA. importlib_resources NA. ipykernel 6.24.0. ipython_genutils 0.2.0. jedi 0.18.2. jinja2 3.1.2. joblib 1.3.1. kiwisolver 1.4.4. llvmlite 0.40.1. lz4 4.3.2. markupsafe 2.1.3. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numexpr 2.8.4. numpy 1.24.4. packaging 23.1. pandas 2.0.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.8.1. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pyarrow 12.0.1. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.0.9. pyscenic 0.12.1. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. six 1.16.0. sklearn 1.3.0. stack_data 0.6.2. tblib 2.0.0. threadpoolctl 3.2.0. tlz 0.12.1. toolz 0.12.0. tornado 6.3.2. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. wcwidth 0.2.6. yaml 6.0. zipp NA. zmq 25.1.0. zoneinfo NA. -----. IPython 8.14.0. jupyter_client 8.3.0. jupyter_core 5.3.1. notebook 6.5.4. -----. Python 3.9.13 (main, Aug 25 2022, 18:29:29) [Clang 12.0.0 ]. macOS-10.16-x86_64-i386-64bit. -----. Session information updated at 2023-07-21 20:08. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570
https://github.com/scverse/scanpy/issues/2570:2884,security,log,logg,2884,"e_symbols':. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). 110 filename = Path(filename) # allow passing strings. 111 if is_valid_filename(filename):. --> 112 return _read(. 113 filename,. 114 backed=backed,. 115 sheet=sheet,. 116 ext=ext,. 117 delimiter=delimiter,. 118 first_column_names=first_column_names,. 119 backup_url=backup_url,. 120 cache=cache,. 121 cache_compression=cache_compression,. 122 **kwargs,. 123 ). 124 # generate filename and read to dict. 125 filekey = str(filename). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:737, in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, suppress_cache_warning, **kwargs). 734 return read_h5ad(path_cache). 736 if not is_present:. --> 737 raise FileNotFoundError(f'Did not find file {filename}.'). 738 logg.debug(f'reading {filename}'). 739 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file GSE123366_Combined/matrix.mtx.gz. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 10.0.0. appnope 0.1.3. asttokens NA. attr 23.1.0. backcall 0.2.0. boltons NA. cffi 1.15.1. cloudpickle 2.2.1. comm 0.1.3. ctxcore 0.2.0. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.1. dask 2023.7.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. frozendict 2.3.8. h5py 3.9.0. ikarus NA. importlib_resources NA. ipykernel 6.24.0. ipython_genutils 0.2.0. jedi 0.18.2. jinja2 3.1.2. joblib 1.3.1. kiwisolver 1.4.4. llvmlite 0.40.1. lz4 4.3.2. markupsafe 2.1.3. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numexpr 2.8.4. numpy 1.24.4. packaging 23.1. pandas 2.0.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.8.1. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570
https://github.com/scverse/scanpy/issues/2570:4540,security,Session,Session,4540,"y:737, in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, suppress_cache_warning, **kwargs). 734 return read_h5ad(path_cache). 736 if not is_present:. --> 737 raise FileNotFoundError(f'Did not find file {filename}.'). 738 logg.debug(f'reading {filename}'). 739 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file GSE123366_Combined/matrix.mtx.gz. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 10.0.0. appnope 0.1.3. asttokens NA. attr 23.1.0. backcall 0.2.0. boltons NA. cffi 1.15.1. cloudpickle 2.2.1. comm 0.1.3. ctxcore 0.2.0. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.1. dask 2023.7.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. frozendict 2.3.8. h5py 3.9.0. ikarus NA. importlib_resources NA. ipykernel 6.24.0. ipython_genutils 0.2.0. jedi 0.18.2. jinja2 3.1.2. joblib 1.3.1. kiwisolver 1.4.4. llvmlite 0.40.1. lz4 4.3.2. markupsafe 2.1.3. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numexpr 2.8.4. numpy 1.24.4. packaging 23.1. pandas 2.0.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.8.1. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pyarrow 12.0.1. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.0.9. pyscenic 0.12.1. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. six 1.16.0. sklearn 1.3.0. stack_data 0.6.2. tblib 2.0.0. threadpoolctl 3.2.0. tlz 0.12.1. toolz 0.12.0. tornado 6.3.2. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. wcwidth 0.2.6. yaml 6.0. zipp NA. zmq 25.1.0. zoneinfo NA. -----. IPython 8.14.0. jupyter_client 8.3.0. jupyter_core 5.3.1. notebook 6.5.4. -----. Python 3.9.13 (main, Aug 25 2022, 18:29:29) [Clang 12.0.0 ]. macOS-10.16-x86_64-i386-64bit. -----. Session information updated at 2023-07-21 20:08. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570
https://github.com/scverse/scanpy/issues/2570:4560,security,updat,updated,4560,"y:737, in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, suppress_cache_warning, **kwargs). 734 return read_h5ad(path_cache). 736 if not is_present:. --> 737 raise FileNotFoundError(f'Did not find file {filename}.'). 738 logg.debug(f'reading {filename}'). 739 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file GSE123366_Combined/matrix.mtx.gz. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 10.0.0. appnope 0.1.3. asttokens NA. attr 23.1.0. backcall 0.2.0. boltons NA. cffi 1.15.1. cloudpickle 2.2.1. comm 0.1.3. ctxcore 0.2.0. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.1. dask 2023.7.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. frozendict 2.3.8. h5py 3.9.0. ikarus NA. importlib_resources NA. ipykernel 6.24.0. ipython_genutils 0.2.0. jedi 0.18.2. jinja2 3.1.2. joblib 1.3.1. kiwisolver 1.4.4. llvmlite 0.40.1. lz4 4.3.2. markupsafe 2.1.3. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numexpr 2.8.4. numpy 1.24.4. packaging 23.1. pandas 2.0.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.8.1. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pyarrow 12.0.1. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.0.9. pyscenic 0.12.1. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. six 1.16.0. sklearn 1.3.0. stack_data 0.6.2. tblib 2.0.0. threadpoolctl 3.2.0. tlz 0.12.1. toolz 0.12.0. tornado 6.3.2. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. wcwidth 0.2.6. yaml 6.0. zipp NA. zmq 25.1.0. zoneinfo NA. -----. IPython 8.14.0. jupyter_client 8.3.0. jupyter_core 5.3.1. notebook 6.5.4. -----. Python 3.9.13 (main, Aug 25 2022, 18:29:29) [Clang 12.0.0 ]. macOS-10.16-x86_64-i386-64bit. -----. Session information updated at 2023-07-21 20:08. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570
https://github.com/scverse/scanpy/issues/2570:692,testability,Trace,Traceback,692,"read_10x_mtx cannot find matrix.mtx.gz file; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've been having an issue trying to load published single cell data (GEO GSE123366) onto my jupyter notebook. Despite providing the directory where all 3 files are located, I get the error that it cannot find my matrix.mtx.gz file. . ### Minimal code sample. ```python. sc.read_10x_mtx(""GSE123366_Combined""). ```. ### Error output. ```pytb. FileNotFoundError Traceback (most recent call last). Cell In[31], line 1. ----> 1 sc.read_10x_mtx(""GSE123366_Combined"", cache=True). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:490, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix). 488 genefile_exists = (path / f'{prefix}genes.tsv').is_file(). 489 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx. --> 490 adata = read(. 491 str(path),. 492 var_names=var_names,. 493 make_unique=make_unique,. 494 cache=cache,. 495 cache_compression=cache_compression,. 496 prefix=prefix,. 497 ). 498 if genefile_exists or not gex_only:. 499 return adata. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:554, in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix). 550 """""". 551 Read mtx from output from Cell Ranger v3 or later versions. 552 """""". 553 path = Path(path). --> 554 adata = read(. 555 path / f'{prefix}matrix.mtx.gz',. 556 cache=cache,. 557 cache_compression=cache_compression,. 558 ).T # transpose the data. 559 genes = pd.read_csv(path / f'{prefix}features.tsv.gz', header=None, sep='\t'). 560 if var_names == 'gene_symbols':. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, ba",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570
https://github.com/scverse/scanpy/issues/2570:2884,testability,log,logg,2884,"e_symbols':. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). 110 filename = Path(filename) # allow passing strings. 111 if is_valid_filename(filename):. --> 112 return _read(. 113 filename,. 114 backed=backed,. 115 sheet=sheet,. 116 ext=ext,. 117 delimiter=delimiter,. 118 first_column_names=first_column_names,. 119 backup_url=backup_url,. 120 cache=cache,. 121 cache_compression=cache_compression,. 122 **kwargs,. 123 ). 124 # generate filename and read to dict. 125 filekey = str(filename). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:737, in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, suppress_cache_warning, **kwargs). 734 return read_h5ad(path_cache). 736 if not is_present:. --> 737 raise FileNotFoundError(f'Did not find file {filename}.'). 738 logg.debug(f'reading {filename}'). 739 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file GSE123366_Combined/matrix.mtx.gz. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 10.0.0. appnope 0.1.3. asttokens NA. attr 23.1.0. backcall 0.2.0. boltons NA. cffi 1.15.1. cloudpickle 2.2.1. comm 0.1.3. ctxcore 0.2.0. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.1. dask 2023.7.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. frozendict 2.3.8. h5py 3.9.0. ikarus NA. importlib_resources NA. ipykernel 6.24.0. ipython_genutils 0.2.0. jedi 0.18.2. jinja2 3.1.2. joblib 1.3.1. kiwisolver 1.4.4. llvmlite 0.40.1. lz4 4.3.2. markupsafe 2.1.3. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numexpr 2.8.4. numpy 1.24.4. packaging 23.1. pandas 2.0.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.8.1. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570
https://github.com/scverse/scanpy/issues/2570:173,usability,confirm,confirmed,173,"read_10x_mtx cannot find matrix.mtx.gz file; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've been having an issue trying to load published single cell data (GEO GSE123366) onto my jupyter notebook. Despite providing the directory where all 3 files are located, I get the error that it cannot find my matrix.mtx.gz file. . ### Minimal code sample. ```python. sc.read_10x_mtx(""GSE123366_Combined""). ```. ### Error output. ```pytb. FileNotFoundError Traceback (most recent call last). Cell In[31], line 1. ----> 1 sc.read_10x_mtx(""GSE123366_Combined"", cache=True). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:490, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix). 488 genefile_exists = (path / f'{prefix}genes.tsv').is_file(). 489 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx. --> 490 adata = read(. 491 str(path),. 492 var_names=var_names,. 493 make_unique=make_unique,. 494 cache=cache,. 495 cache_compression=cache_compression,. 496 prefix=prefix,. 497 ). 498 if genefile_exists or not gex_only:. 499 return adata. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:554, in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix). 550 """""". 551 Read mtx from output from Cell Ranger v3 or later versions. 552 """""". 553 path = Path(path). --> 554 adata = read(. 555 path / f'{prefix}matrix.mtx.gz',. 556 cache=cache,. 557 cache_compression=cache_compression,. 558 ).T # transpose the data. 559 genes = pd.read_csv(path / f'{prefix}features.tsv.gz', header=None, sep='\t'). 560 if var_names == 'gene_symbols':. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, ba",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570
https://github.com/scverse/scanpy/issues/2570:256,usability,confirm,confirmed,256,"read_10x_mtx cannot find matrix.mtx.gz file; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've been having an issue trying to load published single cell data (GEO GSE123366) onto my jupyter notebook. Despite providing the directory where all 3 files are located, I get the error that it cannot find my matrix.mtx.gz file. . ### Minimal code sample. ```python. sc.read_10x_mtx(""GSE123366_Combined""). ```. ### Error output. ```pytb. FileNotFoundError Traceback (most recent call last). Cell In[31], line 1. ----> 1 sc.read_10x_mtx(""GSE123366_Combined"", cache=True). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:490, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix). 488 genefile_exists = (path / f'{prefix}genes.tsv').is_file(). 489 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx. --> 490 adata = read(. 491 str(path),. 492 var_names=var_names,. 493 make_unique=make_unique,. 494 cache=cache,. 495 cache_compression=cache_compression,. 496 prefix=prefix,. 497 ). 498 if genefile_exists or not gex_only:. 499 return adata. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:554, in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix). 550 """""". 551 Read mtx from output from Cell Ranger v3 or later versions. 552 """""". 553 path = Path(path). --> 554 adata = read(. 555 path / f'{prefix}matrix.mtx.gz',. 556 cache=cache,. 557 cache_compression=cache_compression,. 558 ).T # transpose the data. 559 genes = pd.read_csv(path / f'{prefix}features.tsv.gz', header=None, sep='\t'). 560 if var_names == 'gene_symbols':. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, ba",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570
https://github.com/scverse/scanpy/issues/2570:516,usability,error,error,516,"read_10x_mtx cannot find matrix.mtx.gz file; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've been having an issue trying to load published single cell data (GEO GSE123366) onto my jupyter notebook. Despite providing the directory where all 3 files are located, I get the error that it cannot find my matrix.mtx.gz file. . ### Minimal code sample. ```python. sc.read_10x_mtx(""GSE123366_Combined""). ```. ### Error output. ```pytb. FileNotFoundError Traceback (most recent call last). Cell In[31], line 1. ----> 1 sc.read_10x_mtx(""GSE123366_Combined"", cache=True). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:490, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix). 488 genefile_exists = (path / f'{prefix}genes.tsv').is_file(). 489 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx. --> 490 adata = read(. 491 str(path),. 492 var_names=var_names,. 493 make_unique=make_unique,. 494 cache=cache,. 495 cache_compression=cache_compression,. 496 prefix=prefix,. 497 ). 498 if genefile_exists or not gex_only:. 499 return adata. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:554, in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix). 550 """""". 551 Read mtx from output from Cell Ranger v3 or later versions. 552 """""". 553 path = Path(path). --> 554 adata = read(. 555 path / f'{prefix}matrix.mtx.gz',. 556 cache=cache,. 557 cache_compression=cache_compression,. 558 ).T # transpose the data. 559 genes = pd.read_csv(path / f'{prefix}features.tsv.gz', header=None, sep='\t'). 560 if var_names == 'gene_symbols':. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, ba",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570
https://github.com/scverse/scanpy/issues/2570:571,usability,Minim,Minimal,571,"read_10x_mtx cannot find matrix.mtx.gz file; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've been having an issue trying to load published single cell data (GEO GSE123366) onto my jupyter notebook. Despite providing the directory where all 3 files are located, I get the error that it cannot find my matrix.mtx.gz file. . ### Minimal code sample. ```python. sc.read_10x_mtx(""GSE123366_Combined""). ```. ### Error output. ```pytb. FileNotFoundError Traceback (most recent call last). Cell In[31], line 1. ----> 1 sc.read_10x_mtx(""GSE123366_Combined"", cache=True). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:490, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix). 488 genefile_exists = (path / f'{prefix}genes.tsv').is_file(). 489 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx. --> 490 adata = read(. 491 str(path),. 492 var_names=var_names,. 493 make_unique=make_unique,. 494 cache=cache,. 495 cache_compression=cache_compression,. 496 prefix=prefix,. 497 ). 498 if genefile_exists or not gex_only:. 499 return adata. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:554, in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix). 550 """""". 551 Read mtx from output from Cell Ranger v3 or later versions. 552 """""". 553 path = Path(path). --> 554 adata = read(. 555 path / f'{prefix}matrix.mtx.gz',. 556 cache=cache,. 557 cache_compression=cache_compression,. 558 ).T # transpose the data. 559 genes = pd.read_csv(path / f'{prefix}features.tsv.gz', header=None, sep='\t'). 560 if var_names == 'gene_symbols':. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, ba",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570
https://github.com/scverse/scanpy/issues/2570:651,usability,Error,Error,651,"read_10x_mtx cannot find matrix.mtx.gz file; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I've been having an issue trying to load published single cell data (GEO GSE123366) onto my jupyter notebook. Despite providing the directory where all 3 files are located, I get the error that it cannot find my matrix.mtx.gz file. . ### Minimal code sample. ```python. sc.read_10x_mtx(""GSE123366_Combined""). ```. ### Error output. ```pytb. FileNotFoundError Traceback (most recent call last). Cell In[31], line 1. ----> 1 sc.read_10x_mtx(""GSE123366_Combined"", cache=True). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:490, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix). 488 genefile_exists = (path / f'{prefix}genes.tsv').is_file(). 489 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx. --> 490 adata = read(. 491 str(path),. 492 var_names=var_names,. 493 make_unique=make_unique,. 494 cache=cache,. 495 cache_compression=cache_compression,. 496 prefix=prefix,. 497 ). 498 if genefile_exists or not gex_only:. 499 return adata. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:554, in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix). 550 """""". 551 Read mtx from output from Cell Ranger v3 or later versions. 552 """""". 553 path = Path(path). --> 554 adata = read(. 555 path / f'{prefix}matrix.mtx.gz',. 556 cache=cache,. 557 cache_compression=cache_compression,. 558 ).T # transpose the data. 559 genes = pd.read_csv(path / f'{prefix}features.tsv.gz', header=None, sep='\t'). 560 if var_names == 'gene_symbols':. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, ba",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570
https://github.com/scverse/scanpy/issues/2570:4213,usability,tool,toolz,4213,"y:737, in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, suppress_cache_warning, **kwargs). 734 return read_h5ad(path_cache). 736 if not is_present:. --> 737 raise FileNotFoundError(f'Did not find file {filename}.'). 738 logg.debug(f'reading {filename}'). 739 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file GSE123366_Combined/matrix.mtx.gz. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 10.0.0. appnope 0.1.3. asttokens NA. attr 23.1.0. backcall 0.2.0. boltons NA. cffi 1.15.1. cloudpickle 2.2.1. comm 0.1.3. ctxcore 0.2.0. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.1. dask 2023.7.0. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. executing 1.2.0. frozendict 2.3.8. h5py 3.9.0. ikarus NA. importlib_resources NA. ipykernel 6.24.0. ipython_genutils 0.2.0. jedi 0.18.2. jinja2 3.1.2. joblib 1.3.1. kiwisolver 1.4.4. llvmlite 0.40.1. lz4 4.3.2. markupsafe 2.1.3. matplotlib 3.7.2. mpl_toolkits NA. natsort 8.4.0. numba 0.57.1. numexpr 2.8.4. numpy 1.24.4. packaging 23.1. pandas 2.0.3. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.8.1. prompt_toolkit 3.0.39. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pyarrow 12.0.1. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 3.0.9. pyscenic 0.12.1. pytz 2023.3. scipy 1.11.1. session_info 1.0.0. six 1.16.0. sklearn 1.3.0. stack_data 0.6.2. tblib 2.0.0. threadpoolctl 3.2.0. tlz 0.12.1. toolz 0.12.0. tornado 6.3.2. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. wcwidth 0.2.6. yaml 6.0. zipp NA. zmq 25.1.0. zoneinfo NA. -----. IPython 8.14.0. jupyter_client 8.3.0. jupyter_core 5.3.1. notebook 6.5.4. -----. Python 3.9.13 (main, Aug 25 2022, 18:29:29) [Clang 12.0.0 ]. macOS-10.16-x86_64-i386-64bit. -----. Session information updated at 2023-07-21 20:08. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570
https://github.com/scverse/scanpy/issues/2572:31,deployability,scale,scale,31,"Add a ``batch_key`` to ``sc.pp.scale()``; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Being able to z-score each batch separately is a pretty easy way to overcome some technical effect in the data. Inspired by multiMAP and its per-batch scaling, I just ran BBKNN on the pancreas having z-scored each of the four batches separately, and got an improved output:. ![image](https://github.com/scverse/scanpy/assets/14993986/29ec310d-b9ad-4a50-9cbb-ae9329648eb7). Seems like it might be worth offering this as an option in the scaling function?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2572
https://github.com/scverse/scanpy/issues/2572:31,energy efficiency,scale,scale,31,"Add a ``batch_key`` to ``sc.pp.scale()``; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Being able to z-score each batch separately is a pretty easy way to overcome some technical effect in the data. Inspired by multiMAP and its per-batch scaling, I just ran BBKNN on the pancreas having z-scored each of the four batches separately, and got an improved output:. ![image](https://github.com/scverse/scanpy/assets/14993986/29ec310d-b9ad-4a50-9cbb-ae9329648eb7). Seems like it might be worth offering this as an option in the scaling function?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2572
https://github.com/scverse/scanpy/issues/2572:229,integrability,batch,batch,229,"Add a ``batch_key`` to ``sc.pp.scale()``; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Being able to z-score each batch separately is a pretty easy way to overcome some technical effect in the data. Inspired by multiMAP and its per-batch scaling, I just ran BBKNN on the pancreas having z-scored each of the four batches separately, and got an improved output:. ![image](https://github.com/scverse/scanpy/assets/14993986/29ec310d-b9ad-4a50-9cbb-ae9329648eb7). Seems like it might be worth offering this as an option in the scaling function?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2572
https://github.com/scverse/scanpy/issues/2572:347,integrability,batch,batch,347,"Add a ``batch_key`` to ``sc.pp.scale()``; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Being able to z-score each batch separately is a pretty easy way to overcome some technical effect in the data. Inspired by multiMAP and its per-batch scaling, I just ran BBKNN on the pancreas having z-scored each of the four batches separately, and got an improved output:. ![image](https://github.com/scverse/scanpy/assets/14993986/29ec310d-b9ad-4a50-9cbb-ae9329648eb7). Seems like it might be worth offering this as an option in the scaling function?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2572
https://github.com/scverse/scanpy/issues/2572:428,integrability,batch,batches,428,"Add a ``batch_key`` to ``sc.pp.scale()``; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Being able to z-score each batch separately is a pretty easy way to overcome some technical effect in the data. Inspired by multiMAP and its per-batch scaling, I just ran BBKNN on the pancreas having z-scored each of the four batches separately, and got an improved output:. ![image](https://github.com/scverse/scanpy/assets/14993986/29ec310d-b9ad-4a50-9cbb-ae9329648eb7). Seems like it might be worth offering this as an option in the scaling function?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2572
https://github.com/scverse/scanpy/issues/2572:31,modifiability,scal,scale,31,"Add a ``batch_key`` to ``sc.pp.scale()``; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Being able to z-score each batch separately is a pretty easy way to overcome some technical effect in the data. Inspired by multiMAP and its per-batch scaling, I just ran BBKNN on the pancreas having z-scored each of the four batches separately, and got an improved output:. ![image](https://github.com/scverse/scanpy/assets/14993986/29ec310d-b9ad-4a50-9cbb-ae9329648eb7). Seems like it might be worth offering this as an option in the scaling function?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2572
https://github.com/scverse/scanpy/issues/2572:114,modifiability,paramet,parameters,114,"Add a ``batch_key`` to ``sc.pp.scale()``; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Being able to z-score each batch separately is a pretty easy way to overcome some technical effect in the data. Inspired by multiMAP and its per-batch scaling, I just ran BBKNN on the pancreas having z-scored each of the four batches separately, and got an improved output:. ![image](https://github.com/scverse/scanpy/assets/14993986/29ec310d-b9ad-4a50-9cbb-ae9329648eb7). Seems like it might be worth offering this as an option in the scaling function?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2572
https://github.com/scverse/scanpy/issues/2572:353,modifiability,scal,scaling,353,"Add a ``batch_key`` to ``sc.pp.scale()``; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Being able to z-score each batch separately is a pretty easy way to overcome some technical effect in the data. Inspired by multiMAP and its per-batch scaling, I just ran BBKNN on the pancreas having z-scored each of the four batches separately, and got an improved output:. ![image](https://github.com/scverse/scanpy/assets/14993986/29ec310d-b9ad-4a50-9cbb-ae9329648eb7). Seems like it might be worth offering this as an option in the scaling function?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2572
https://github.com/scverse/scanpy/issues/2572:638,modifiability,scal,scaling,638,"Add a ``batch_key`` to ``sc.pp.scale()``; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Being able to z-score each batch separately is a pretty easy way to overcome some technical effect in the data. Inspired by multiMAP and its per-batch scaling, I just ran BBKNN on the pancreas having z-scored each of the four batches separately, and got an improved output:. ![image](https://github.com/scverse/scanpy/assets/14993986/29ec310d-b9ad-4a50-9cbb-ae9329648eb7). Seems like it might be worth offering this as an option in the scaling function?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2572
https://github.com/scverse/scanpy/issues/2572:31,performance,scale,scale,31,"Add a ``batch_key`` to ``sc.pp.scale()``; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Being able to z-score each batch separately is a pretty easy way to overcome some technical effect in the data. Inspired by multiMAP and its per-batch scaling, I just ran BBKNN on the pancreas having z-scored each of the four batches separately, and got an improved output:. ![image](https://github.com/scverse/scanpy/assets/14993986/29ec310d-b9ad-4a50-9cbb-ae9329648eb7). Seems like it might be worth offering this as an option in the scaling function?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2572
https://github.com/scverse/scanpy/issues/2572:229,performance,batch,batch,229,"Add a ``batch_key`` to ``sc.pp.scale()``; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Being able to z-score each batch separately is a pretty easy way to overcome some technical effect in the data. Inspired by multiMAP and its per-batch scaling, I just ran BBKNN on the pancreas having z-scored each of the four batches separately, and got an improved output:. ![image](https://github.com/scverse/scanpy/assets/14993986/29ec310d-b9ad-4a50-9cbb-ae9329648eb7). Seems like it might be worth offering this as an option in the scaling function?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2572
https://github.com/scverse/scanpy/issues/2572:347,performance,batch,batch,347,"Add a ``batch_key`` to ``sc.pp.scale()``; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Being able to z-score each batch separately is a pretty easy way to overcome some technical effect in the data. Inspired by multiMAP and its per-batch scaling, I just ran BBKNN on the pancreas having z-scored each of the four batches separately, and got an improved output:. ![image](https://github.com/scverse/scanpy/assets/14993986/29ec310d-b9ad-4a50-9cbb-ae9329648eb7). Seems like it might be worth offering this as an option in the scaling function?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2572
https://github.com/scverse/scanpy/issues/2572:428,performance,batch,batches,428,"Add a ``batch_key`` to ``sc.pp.scale()``; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Being able to z-score each batch separately is a pretty easy way to overcome some technical effect in the data. Inspired by multiMAP and its per-batch scaling, I just ran BBKNN on the pancreas having z-scored each of the four batches separately, and got an improved output:. ![image](https://github.com/scverse/scanpy/assets/14993986/29ec310d-b9ad-4a50-9cbb-ae9329648eb7). Seems like it might be worth offering this as an option in the scaling function?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2572
https://github.com/scverse/scanpy/pull/2574:277,safety,review,review,277,Backport PR #2573: [pre-commit.ci] pre-commit autoupdate; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2574
https://github.com/scverse/scanpy/pull/2574:277,testability,review,review,277,Backport PR #2573: [pre-commit.ci] pre-commit autoupdate; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2574
https://github.com/scverse/scanpy/pull/2574:128,usability,guid,guidelines,128,Backport PR #2573: [pre-commit.ci] pre-commit autoupdate; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2574
https://github.com/scverse/scanpy/pull/2574:159,usability,guid,guide,159,Backport PR #2573: [pre-commit.ci] pre-commit autoupdate; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2574
https://github.com/scverse/scanpy/pull/2574:255,usability,workflow,workflow,255,Backport PR #2573: [pre-commit.ci] pre-commit autoupdate; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2574
https://github.com/scverse/scanpy/pull/2575:89,deployability,version,version,89,"Simplify tests; - Even our `test-min` extra has `pytest-nunit`, so we dont need a dummy version. - `--internet-tests` can just be replaced by the mark itself.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2575
https://github.com/scverse/scanpy/pull/2575:89,integrability,version,version,89,"Simplify tests; - Even our `test-min` extra has `pytest-nunit`, so we dont need a dummy version. - `--internet-tests` can just be replaced by the mark itself.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2575
https://github.com/scverse/scanpy/pull/2575:89,modifiability,version,version,89,"Simplify tests; - Even our `test-min` extra has `pytest-nunit`, so we dont need a dummy version. - `--internet-tests` can just be replaced by the mark itself.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2575
https://github.com/scverse/scanpy/pull/2575:9,safety,test,tests,9,"Simplify tests; - Even our `test-min` extra has `pytest-nunit`, so we dont need a dummy version. - `--internet-tests` can just be replaced by the mark itself.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2575
https://github.com/scverse/scanpy/pull/2575:28,safety,test,test-min,28,"Simplify tests; - Even our `test-min` extra has `pytest-nunit`, so we dont need a dummy version. - `--internet-tests` can just be replaced by the mark itself.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2575
https://github.com/scverse/scanpy/pull/2575:112,safety,test,tests,112,"Simplify tests; - Even our `test-min` extra has `pytest-nunit`, so we dont need a dummy version. - `--internet-tests` can just be replaced by the mark itself.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2575
https://github.com/scverse/scanpy/pull/2575:0,testability,Simpl,Simplify,0,"Simplify tests; - Even our `test-min` extra has `pytest-nunit`, so we dont need a dummy version. - `--internet-tests` can just be replaced by the mark itself.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2575
https://github.com/scverse/scanpy/pull/2575:9,testability,test,tests,9,"Simplify tests; - Even our `test-min` extra has `pytest-nunit`, so we dont need a dummy version. - `--internet-tests` can just be replaced by the mark itself.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2575
https://github.com/scverse/scanpy/pull/2575:28,testability,test,test-min,28,"Simplify tests; - Even our `test-min` extra has `pytest-nunit`, so we dont need a dummy version. - `--internet-tests` can just be replaced by the mark itself.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2575
https://github.com/scverse/scanpy/pull/2575:112,testability,test,tests,112,"Simplify tests; - Even our `test-min` extra has `pytest-nunit`, so we dont need a dummy version. - `--internet-tests` can just be replaced by the mark itself.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2575
https://github.com/scverse/scanpy/pull/2575:0,usability,Simpl,Simplify,0,"Simplify tests; - Even our `test-min` extra has `pytest-nunit`, so we dont need a dummy version. - `--internet-tests` can just be replaced by the mark itself.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2575
https://github.com/scverse/scanpy/pull/2576:252,safety,review,review,252,Corrected year of scVerse paper; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2576
https://github.com/scverse/scanpy/pull/2576:252,testability,review,review,252,Corrected year of scVerse paper; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2576
https://github.com/scverse/scanpy/pull/2576:103,usability,guid,guidelines,103,Corrected year of scVerse paper; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2576
https://github.com/scverse/scanpy/pull/2576:134,usability,guid,guide,134,Corrected year of scVerse paper; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2576
https://github.com/scverse/scanpy/pull/2576:230,usability,workflow,workflow,230,Corrected year of scVerse paper; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2576
https://github.com/scverse/scanpy/pull/2577:119,availability,state,state,119,Mellon; This implements `scanpy.external.tl.mellon` using [Mellon](https://github.com/settylab/mellon) to compute cell-state density.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2577
https://github.com/scverse/scanpy/pull/2577:119,integrability,state,state,119,Mellon; This implements `scanpy.external.tl.mellon` using [Mellon](https://github.com/settylab/mellon) to compute cell-state density.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2577
https://github.com/scverse/scanpy/issues/2578:1150,deployability,scale,scale,1150,"Expand dask support; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Meta issue tracking scanpy functions without dask support. Related #921. First we should test things more generically:. - [x] add global `array_type` fixture to use in all tests for features below. - #2595. @ivirshup came up with places where we should prioritize dask support:. - [x] `normalize_total`. - seems to work: scverse/scanpy#2466. - has tests: scverse/scanpy#1663. - [x] `log1p`, `normalize_per_cell`, `filter_cells`/`*_genes`:. - [x] accept Dask arrays: scverse/scanpy#283. - [x] Test that they are kept as Dask arrays throughout: scverse/scanpy#2814. - [x] `sc.pp.pca`: scverse/scanpy#2563 scanpy/scanpy#3263. - [ ] `sc.pp.calculate_qc_metrics` #3307. - `sc.pp.highly_variable_genes`:. - [x] scverse/scanpy#2777. - [x] scverse/scanpy#2807. - [ ] scverse/scanpy#2808. - [ ] experimental pearson_residuals flavor. - [x] `sc.tl.rank_genes_groups`: scverse/scanpy#2621. - [ ] Subsample. - [ ] Aggregate: https://flox.readthedocs.io/en/latest/. Later:. - [x] scale. - [ ] nearest-neighbors: https://github.com/dask/dask-ml/issues/982 may need to implement ourselves... - [ ] umap: implement ourselves? - [ ] metrics: depednent on squidpy decisions but should be dask-ifiable",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2578
https://github.com/scverse/scanpy/issues/2578:1150,energy efficiency,scale,scale,1150,"Expand dask support; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Meta issue tracking scanpy functions without dask support. Related #921. First we should test things more generically:. - [x] add global `array_type` fixture to use in all tests for features below. - #2595. @ivirshup came up with places where we should prioritize dask support:. - [x] `normalize_total`. - seems to work: scverse/scanpy#2466. - has tests: scverse/scanpy#1663. - [x] `log1p`, `normalize_per_cell`, `filter_cells`/`*_genes`:. - [x] accept Dask arrays: scverse/scanpy#283. - [x] Test that they are kept as Dask arrays throughout: scverse/scanpy#2814. - [x] `sc.pp.pca`: scverse/scanpy#2563 scanpy/scanpy#3263. - [ ] `sc.pp.calculate_qc_metrics` #3307. - `sc.pp.highly_variable_genes`:. - [x] scverse/scanpy#2777. - [x] scverse/scanpy#2807. - [ ] scverse/scanpy#2808. - [ ] experimental pearson_residuals flavor. - [x] `sc.tl.rank_genes_groups`: scverse/scanpy#2621. - [ ] Subsample. - [ ] Aggregate: https://flox.readthedocs.io/en/latest/. Later:. - [x] scale. - [ ] nearest-neighbors: https://github.com/dask/dask-ml/issues/982 may need to implement ourselves... - [ ] umap: implement ourselves? - [ ] metrics: depednent on squidpy decisions but should be dask-ifiable",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2578
https://github.com/scverse/scanpy/issues/2578:1068,integrability,Sub,Subsample,1068,"Expand dask support; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Meta issue tracking scanpy functions without dask support. Related #921. First we should test things more generically:. - [x] add global `array_type` fixture to use in all tests for features below. - #2595. @ivirshup came up with places where we should prioritize dask support:. - [x] `normalize_total`. - seems to work: scverse/scanpy#2466. - has tests: scverse/scanpy#1663. - [x] `log1p`, `normalize_per_cell`, `filter_cells`/`*_genes`:. - [x] accept Dask arrays: scverse/scanpy#283. - [x] Test that they are kept as Dask arrays throughout: scverse/scanpy#2814. - [x] `sc.pp.pca`: scverse/scanpy#2563 scanpy/scanpy#3263. - [ ] `sc.pp.calculate_qc_metrics` #3307. - `sc.pp.highly_variable_genes`:. - [x] scverse/scanpy#2777. - [x] scverse/scanpy#2807. - [ ] scverse/scanpy#2808. - [ ] experimental pearson_residuals flavor. - [x] `sc.tl.rank_genes_groups`: scverse/scanpy#2621. - [ ] Subsample. - [ ] Aggregate: https://flox.readthedocs.io/en/latest/. Later:. - [x] scale. - [ ] nearest-neighbors: https://github.com/dask/dask-ml/issues/982 may need to implement ourselves... - [ ] umap: implement ourselves? - [ ] metrics: depednent on squidpy decisions but should be dask-ifiable",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2578
https://github.com/scverse/scanpy/issues/2578:93,modifiability,paramet,parameters,93,"Expand dask support; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Meta issue tracking scanpy functions without dask support. Related #921. First we should test things more generically:. - [x] add global `array_type` fixture to use in all tests for features below. - #2595. @ivirshup came up with places where we should prioritize dask support:. - [x] `normalize_total`. - seems to work: scverse/scanpy#2466. - has tests: scverse/scanpy#1663. - [x] `log1p`, `normalize_per_cell`, `filter_cells`/`*_genes`:. - [x] accept Dask arrays: scverse/scanpy#283. - [x] Test that they are kept as Dask arrays throughout: scverse/scanpy#2814. - [x] `sc.pp.pca`: scverse/scanpy#2563 scanpy/scanpy#3263. - [ ] `sc.pp.calculate_qc_metrics` #3307. - `sc.pp.highly_variable_genes`:. - [x] scverse/scanpy#2777. - [x] scverse/scanpy#2807. - [ ] scverse/scanpy#2808. - [ ] experimental pearson_residuals flavor. - [x] `sc.tl.rank_genes_groups`: scverse/scanpy#2621. - [ ] Subsample. - [ ] Aggregate: https://flox.readthedocs.io/en/latest/. Later:. - [x] scale. - [ ] nearest-neighbors: https://github.com/dask/dask-ml/issues/982 may need to implement ourselves... - [ ] umap: implement ourselves? - [ ] metrics: depednent on squidpy decisions but should be dask-ifiable",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2578
https://github.com/scverse/scanpy/issues/2578:1150,modifiability,scal,scale,1150,"Expand dask support; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Meta issue tracking scanpy functions without dask support. Related #921. First we should test things more generically:. - [x] add global `array_type` fixture to use in all tests for features below. - #2595. @ivirshup came up with places where we should prioritize dask support:. - [x] `normalize_total`. - seems to work: scverse/scanpy#2466. - has tests: scverse/scanpy#1663. - [x] `log1p`, `normalize_per_cell`, `filter_cells`/`*_genes`:. - [x] accept Dask arrays: scverse/scanpy#283. - [x] Test that they are kept as Dask arrays throughout: scverse/scanpy#2814. - [x] `sc.pp.pca`: scverse/scanpy#2563 scanpy/scanpy#3263. - [ ] `sc.pp.calculate_qc_metrics` #3307. - `sc.pp.highly_variable_genes`:. - [x] scverse/scanpy#2777. - [x] scverse/scanpy#2807. - [ ] scverse/scanpy#2808. - [ ] experimental pearson_residuals flavor. - [x] `sc.tl.rank_genes_groups`: scverse/scanpy#2621. - [ ] Subsample. - [ ] Aggregate: https://flox.readthedocs.io/en/latest/. Later:. - [x] scale. - [ ] nearest-neighbors: https://github.com/dask/dask-ml/issues/982 may need to implement ourselves... - [ ] umap: implement ourselves? - [ ] metrics: depednent on squidpy decisions but should be dask-ifiable",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2578
https://github.com/scverse/scanpy/issues/2578:1150,performance,scale,scale,1150,"Expand dask support; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Meta issue tracking scanpy functions without dask support. Related #921. First we should test things more generically:. - [x] add global `array_type` fixture to use in all tests for features below. - #2595. @ivirshup came up with places where we should prioritize dask support:. - [x] `normalize_total`. - seems to work: scverse/scanpy#2466. - has tests: scverse/scanpy#1663. - [x] `log1p`, `normalize_per_cell`, `filter_cells`/`*_genes`:. - [x] accept Dask arrays: scverse/scanpy#283. - [x] Test that they are kept as Dask arrays throughout: scverse/scanpy#2814. - [x] `sc.pp.pca`: scverse/scanpy#2563 scanpy/scanpy#3263. - [ ] `sc.pp.calculate_qc_metrics` #3307. - `sc.pp.highly_variable_genes`:. - [x] scverse/scanpy#2777. - [x] scverse/scanpy#2807. - [ ] scverse/scanpy#2808. - [ ] experimental pearson_residuals flavor. - [x] `sc.tl.rank_genes_groups`: scverse/scanpy#2621. - [ ] Subsample. - [ ] Aggregate: https://flox.readthedocs.io/en/latest/. Later:. - [x] scale. - [ ] nearest-neighbors: https://github.com/dask/dask-ml/issues/982 may need to implement ourselves... - [ ] umap: implement ourselves? - [ ] metrics: depednent on squidpy decisions but should be dask-ifiable",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2578
https://github.com/scverse/scanpy/issues/2578:270,safety,test,test,270,"Expand dask support; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Meta issue tracking scanpy functions without dask support. Related #921. First we should test things more generically:. - [x] add global `array_type` fixture to use in all tests for features below. - #2595. @ivirshup came up with places where we should prioritize dask support:. - [x] `normalize_total`. - seems to work: scverse/scanpy#2466. - has tests: scverse/scanpy#1663. - [x] `log1p`, `normalize_per_cell`, `filter_cells`/`*_genes`:. - [x] accept Dask arrays: scverse/scanpy#283. - [x] Test that they are kept as Dask arrays throughout: scverse/scanpy#2814. - [x] `sc.pp.pca`: scverse/scanpy#2563 scanpy/scanpy#3263. - [ ] `sc.pp.calculate_qc_metrics` #3307. - `sc.pp.highly_variable_genes`:. - [x] scverse/scanpy#2777. - [x] scverse/scanpy#2807. - [ ] scverse/scanpy#2808. - [ ] experimental pearson_residuals flavor. - [x] `sc.tl.rank_genes_groups`: scverse/scanpy#2621. - [ ] Subsample. - [ ] Aggregate: https://flox.readthedocs.io/en/latest/. Later:. - [x] scale. - [ ] nearest-neighbors: https://github.com/dask/dask-ml/issues/982 may need to implement ourselves... - [ ] umap: implement ourselves? - [ ] metrics: depednent on squidpy decisions but should be dask-ifiable",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2578
https://github.com/scverse/scanpy/issues/2578:353,safety,test,tests,353,"Expand dask support; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Meta issue tracking scanpy functions without dask support. Related #921. First we should test things more generically:. - [x] add global `array_type` fixture to use in all tests for features below. - #2595. @ivirshup came up with places where we should prioritize dask support:. - [x] `normalize_total`. - seems to work: scverse/scanpy#2466. - has tests: scverse/scanpy#1663. - [x] `log1p`, `normalize_per_cell`, `filter_cells`/`*_genes`:. - [x] accept Dask arrays: scverse/scanpy#283. - [x] Test that they are kept as Dask arrays throughout: scverse/scanpy#2814. - [x] `sc.pp.pca`: scverse/scanpy#2563 scanpy/scanpy#3263. - [ ] `sc.pp.calculate_qc_metrics` #3307. - `sc.pp.highly_variable_genes`:. - [x] scverse/scanpy#2777. - [x] scverse/scanpy#2807. - [ ] scverse/scanpy#2808. - [ ] experimental pearson_residuals flavor. - [x] `sc.tl.rank_genes_groups`: scverse/scanpy#2621. - [ ] Subsample. - [ ] Aggregate: https://flox.readthedocs.io/en/latest/. Later:. - [x] scale. - [ ] nearest-neighbors: https://github.com/dask/dask-ml/issues/982 may need to implement ourselves... - [ ] umap: implement ourselves? - [ ] metrics: depednent on squidpy decisions but should be dask-ifiable",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2578
https://github.com/scverse/scanpy/issues/2578:529,safety,test,tests,529,"Expand dask support; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Meta issue tracking scanpy functions without dask support. Related #921. First we should test things more generically:. - [x] add global `array_type` fixture to use in all tests for features below. - #2595. @ivirshup came up with places where we should prioritize dask support:. - [x] `normalize_total`. - seems to work: scverse/scanpy#2466. - has tests: scverse/scanpy#1663. - [x] `log1p`, `normalize_per_cell`, `filter_cells`/`*_genes`:. - [x] accept Dask arrays: scverse/scanpy#283. - [x] Test that they are kept as Dask arrays throughout: scverse/scanpy#2814. - [x] `sc.pp.pca`: scverse/scanpy#2563 scanpy/scanpy#3263. - [ ] `sc.pp.calculate_qc_metrics` #3307. - `sc.pp.highly_variable_genes`:. - [x] scverse/scanpy#2777. - [x] scverse/scanpy#2807. - [ ] scverse/scanpy#2808. - [ ] experimental pearson_residuals flavor. - [x] `sc.tl.rank_genes_groups`: scverse/scanpy#2621. - [ ] Subsample. - [ ] Aggregate: https://flox.readthedocs.io/en/latest/. Later:. - [x] scale. - [ ] nearest-neighbors: https://github.com/dask/dask-ml/issues/982 may need to implement ourselves... - [ ] umap: implement ourselves? - [ ] metrics: depednent on squidpy decisions but should be dask-ifiable",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2578
https://github.com/scverse/scanpy/issues/2578:673,safety,Test,Test,673,"Expand dask support; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Meta issue tracking scanpy functions without dask support. Related #921. First we should test things more generically:. - [x] add global `array_type` fixture to use in all tests for features below. - #2595. @ivirshup came up with places where we should prioritize dask support:. - [x] `normalize_total`. - seems to work: scverse/scanpy#2466. - has tests: scverse/scanpy#1663. - [x] `log1p`, `normalize_per_cell`, `filter_cells`/`*_genes`:. - [x] accept Dask arrays: scverse/scanpy#283. - [x] Test that they are kept as Dask arrays throughout: scverse/scanpy#2814. - [x] `sc.pp.pca`: scverse/scanpy#2563 scanpy/scanpy#3263. - [ ] `sc.pp.calculate_qc_metrics` #3307. - `sc.pp.highly_variable_genes`:. - [x] scverse/scanpy#2777. - [x] scverse/scanpy#2807. - [ ] scverse/scanpy#2808. - [ ] experimental pearson_residuals flavor. - [x] `sc.tl.rank_genes_groups`: scverse/scanpy#2621. - [ ] Subsample. - [ ] Aggregate: https://flox.readthedocs.io/en/latest/. Later:. - [x] scale. - [ ] nearest-neighbors: https://github.com/dask/dask-ml/issues/982 may need to implement ourselves... - [ ] umap: implement ourselves? - [ ] metrics: depednent on squidpy decisions but should be dask-ifiable",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2578
https://github.com/scverse/scanpy/issues/2578:270,testability,test,test,270,"Expand dask support; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Meta issue tracking scanpy functions without dask support. Related #921. First we should test things more generically:. - [x] add global `array_type` fixture to use in all tests for features below. - #2595. @ivirshup came up with places where we should prioritize dask support:. - [x] `normalize_total`. - seems to work: scverse/scanpy#2466. - has tests: scverse/scanpy#1663. - [x] `log1p`, `normalize_per_cell`, `filter_cells`/`*_genes`:. - [x] accept Dask arrays: scverse/scanpy#283. - [x] Test that they are kept as Dask arrays throughout: scverse/scanpy#2814. - [x] `sc.pp.pca`: scverse/scanpy#2563 scanpy/scanpy#3263. - [ ] `sc.pp.calculate_qc_metrics` #3307. - `sc.pp.highly_variable_genes`:. - [x] scverse/scanpy#2777. - [x] scverse/scanpy#2807. - [ ] scverse/scanpy#2808. - [ ] experimental pearson_residuals flavor. - [x] `sc.tl.rank_genes_groups`: scverse/scanpy#2621. - [ ] Subsample. - [ ] Aggregate: https://flox.readthedocs.io/en/latest/. Later:. - [x] scale. - [ ] nearest-neighbors: https://github.com/dask/dask-ml/issues/982 may need to implement ourselves... - [ ] umap: implement ourselves? - [ ] metrics: depednent on squidpy decisions but should be dask-ifiable",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2578
https://github.com/scverse/scanpy/issues/2578:353,testability,test,tests,353,"Expand dask support; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Meta issue tracking scanpy functions without dask support. Related #921. First we should test things more generically:. - [x] add global `array_type` fixture to use in all tests for features below. - #2595. @ivirshup came up with places where we should prioritize dask support:. - [x] `normalize_total`. - seems to work: scverse/scanpy#2466. - has tests: scverse/scanpy#1663. - [x] `log1p`, `normalize_per_cell`, `filter_cells`/`*_genes`:. - [x] accept Dask arrays: scverse/scanpy#283. - [x] Test that they are kept as Dask arrays throughout: scverse/scanpy#2814. - [x] `sc.pp.pca`: scverse/scanpy#2563 scanpy/scanpy#3263. - [ ] `sc.pp.calculate_qc_metrics` #3307. - `sc.pp.highly_variable_genes`:. - [x] scverse/scanpy#2777. - [x] scverse/scanpy#2807. - [ ] scverse/scanpy#2808. - [ ] experimental pearson_residuals flavor. - [x] `sc.tl.rank_genes_groups`: scverse/scanpy#2621. - [ ] Subsample. - [ ] Aggregate: https://flox.readthedocs.io/en/latest/. Later:. - [x] scale. - [ ] nearest-neighbors: https://github.com/dask/dask-ml/issues/982 may need to implement ourselves... - [ ] umap: implement ourselves? - [ ] metrics: depednent on squidpy decisions but should be dask-ifiable",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2578
https://github.com/scverse/scanpy/issues/2578:529,testability,test,tests,529,"Expand dask support; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Meta issue tracking scanpy functions without dask support. Related #921. First we should test things more generically:. - [x] add global `array_type` fixture to use in all tests for features below. - #2595. @ivirshup came up with places where we should prioritize dask support:. - [x] `normalize_total`. - seems to work: scverse/scanpy#2466. - has tests: scverse/scanpy#1663. - [x] `log1p`, `normalize_per_cell`, `filter_cells`/`*_genes`:. - [x] accept Dask arrays: scverse/scanpy#283. - [x] Test that they are kept as Dask arrays throughout: scverse/scanpy#2814. - [x] `sc.pp.pca`: scverse/scanpy#2563 scanpy/scanpy#3263. - [ ] `sc.pp.calculate_qc_metrics` #3307. - `sc.pp.highly_variable_genes`:. - [x] scverse/scanpy#2777. - [x] scverse/scanpy#2807. - [ ] scverse/scanpy#2808. - [ ] experimental pearson_residuals flavor. - [x] `sc.tl.rank_genes_groups`: scverse/scanpy#2621. - [ ] Subsample. - [ ] Aggregate: https://flox.readthedocs.io/en/latest/. Later:. - [x] scale. - [ ] nearest-neighbors: https://github.com/dask/dask-ml/issues/982 may need to implement ourselves... - [ ] umap: implement ourselves? - [ ] metrics: depednent on squidpy decisions but should be dask-ifiable",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2578
https://github.com/scverse/scanpy/issues/2578:673,testability,Test,Test,673,"Expand dask support; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Meta issue tracking scanpy functions without dask support. Related #921. First we should test things more generically:. - [x] add global `array_type` fixture to use in all tests for features below. - #2595. @ivirshup came up with places where we should prioritize dask support:. - [x] `normalize_total`. - seems to work: scverse/scanpy#2466. - has tests: scverse/scanpy#1663. - [x] `log1p`, `normalize_per_cell`, `filter_cells`/`*_genes`:. - [x] accept Dask arrays: scverse/scanpy#283. - [x] Test that they are kept as Dask arrays throughout: scverse/scanpy#2814. - [x] `sc.pp.pca`: scverse/scanpy#2563 scanpy/scanpy#3263. - [ ] `sc.pp.calculate_qc_metrics` #3307. - `sc.pp.highly_variable_genes`:. - [x] scverse/scanpy#2777. - [x] scverse/scanpy#2807. - [ ] scverse/scanpy#2808. - [ ] experimental pearson_residuals flavor. - [x] `sc.tl.rank_genes_groups`: scverse/scanpy#2621. - [ ] Subsample. - [ ] Aggregate: https://flox.readthedocs.io/en/latest/. Later:. - [x] scale. - [ ] nearest-neighbors: https://github.com/dask/dask-ml/issues/982 may need to implement ourselves... - [ ] umap: implement ourselves? - [ ] metrics: depednent on squidpy decisions but should be dask-ifiable",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2578
https://github.com/scverse/scanpy/issues/2578:12,usability,support,support,12,"Expand dask support; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Meta issue tracking scanpy functions without dask support. Related #921. First we should test things more generically:. - [x] add global `array_type` fixture to use in all tests for features below. - #2595. @ivirshup came up with places where we should prioritize dask support:. - [x] `normalize_total`. - seems to work: scverse/scanpy#2466. - has tests: scverse/scanpy#1663. - [x] `log1p`, `normalize_per_cell`, `filter_cells`/`*_genes`:. - [x] accept Dask arrays: scverse/scanpy#283. - [x] Test that they are kept as Dask arrays throughout: scverse/scanpy#2814. - [x] `sc.pp.pca`: scverse/scanpy#2563 scanpy/scanpy#3263. - [ ] `sc.pp.calculate_qc_metrics` #3307. - `sc.pp.highly_variable_genes`:. - [x] scverse/scanpy#2777. - [x] scverse/scanpy#2807. - [ ] scverse/scanpy#2808. - [ ] experimental pearson_residuals flavor. - [x] `sc.tl.rank_genes_groups`: scverse/scanpy#2621. - [ ] Subsample. - [ ] Aggregate: https://flox.readthedocs.io/en/latest/. Later:. - [x] scale. - [ ] nearest-neighbors: https://github.com/dask/dask-ml/issues/982 may need to implement ourselves... - [ ] umap: implement ourselves? - [ ] metrics: depednent on squidpy decisions but should be dask-ifiable",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2578
https://github.com/scverse/scanpy/issues/2578:231,usability,support,support,231,"Expand dask support; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Meta issue tracking scanpy functions without dask support. Related #921. First we should test things more generically:. - [x] add global `array_type` fixture to use in all tests for features below. - #2595. @ivirshup came up with places where we should prioritize dask support:. - [x] `normalize_total`. - seems to work: scverse/scanpy#2466. - has tests: scverse/scanpy#1663. - [x] `log1p`, `normalize_per_cell`, `filter_cells`/`*_genes`:. - [x] accept Dask arrays: scverse/scanpy#283. - [x] Test that they are kept as Dask arrays throughout: scverse/scanpy#2814. - [x] `sc.pp.pca`: scverse/scanpy#2563 scanpy/scanpy#3263. - [ ] `sc.pp.calculate_qc_metrics` #3307. - `sc.pp.highly_variable_genes`:. - [x] scverse/scanpy#2777. - [x] scverse/scanpy#2807. - [ ] scverse/scanpy#2808. - [ ] experimental pearson_residuals flavor. - [x] `sc.tl.rank_genes_groups`: scverse/scanpy#2621. - [ ] Subsample. - [ ] Aggregate: https://flox.readthedocs.io/en/latest/. Later:. - [x] scale. - [ ] nearest-neighbors: https://github.com/dask/dask-ml/issues/982 may need to implement ourselves... - [ ] umap: implement ourselves? - [ ] metrics: depednent on squidpy decisions but should be dask-ifiable",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2578
https://github.com/scverse/scanpy/issues/2578:450,usability,support,support,450,"Expand dask support; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Meta issue tracking scanpy functions without dask support. Related #921. First we should test things more generically:. - [x] add global `array_type` fixture to use in all tests for features below. - #2595. @ivirshup came up with places where we should prioritize dask support:. - [x] `normalize_total`. - seems to work: scverse/scanpy#2466. - has tests: scverse/scanpy#1663. - [x] `log1p`, `normalize_per_cell`, `filter_cells`/`*_genes`:. - [x] accept Dask arrays: scverse/scanpy#283. - [x] Test that they are kept as Dask arrays throughout: scverse/scanpy#2814. - [x] `sc.pp.pca`: scverse/scanpy#2563 scanpy/scanpy#3263. - [ ] `sc.pp.calculate_qc_metrics` #3307. - `sc.pp.highly_variable_genes`:. - [x] scverse/scanpy#2777. - [x] scverse/scanpy#2807. - [ ] scverse/scanpy#2808. - [ ] experimental pearson_residuals flavor. - [x] `sc.tl.rank_genes_groups`: scverse/scanpy#2621. - [ ] Subsample. - [ ] Aggregate: https://flox.readthedocs.io/en/latest/. Later:. - [x] scale. - [ ] nearest-neighbors: https://github.com/dask/dask-ml/issues/982 may need to implement ourselves... - [ ] umap: implement ourselves? - [ ] metrics: depednent on squidpy decisions but should be dask-ifiable",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2578
https://github.com/scverse/scanpy/issues/2579:1098,availability,Error,Error,1098,"ase make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I would expect that when you call sc.pp.highly_variable_genes on the same dataset and request the same number of genes, that you would get the same output. The below example suggests that this is not the case. . ### Minimal code sample. ```python. adata_sub = sc.read_h5ad(""your_favourite_object.h5ad""). n_genes = 1491. for i in range(10):. sc.pp.highly_variable_genes(adata_sub, n_top_genes=n_genes). unique_genes = list(adata_sub.var['highly_variable'][adata_sub.var['highly_variable'] == True].index). if i == 0:. all_unique = list(set(unique_genes)). print(f""total {len(all_unique)} unique genes""). else:. all_unique = list(set(all_unique+unique_genes)). print(f""total {len(all_unique)} unique genes""). ```. ### Error output. ```pytb. total 1491 unique genes. total 1814 unique genes. total 2042 unique genes. total 2163 unique genes. total 2237 unique genes. total 2305 unique genes. total 2356 unique genes. total 2401 unique genes. total 2437 unique genes. total 2453 unique genes. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 8.4.0. asciitree NA. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. bottleneck 1.3.4. cffi 1.15.0. cloudpickle 2.0.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2022.02.1. dateutil 2.8.1. debugpy 1.5.1. decorator 5.1.1. defusedxml 0.7.1. django 4.1.3. entrypoints 0.4. executing 0.8.3. fasteners 0.18. fsspec 2023.4.0. google NA. h5py 3.6.0. igraph 0.10.6. ipykernel 6.9.1. ipython_genutils 0.2.0. ipywidgets 7.6.5. jedi 0.18.1. jinja2 3.1.2. joblib 1.1.0. jupyter_server 1.13.5. kiwisolver 1.2.0. kneed 0.8.3. leidenalg 0.10.1. llvmlite 0.38.0. markupsafe 2.0.1. matplotlib 3.5.1. matplotlib_inline NA. mi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2579
https://github.com/scverse/scanpy/issues/2579:262,deployability,version,version,262,"sc.pp.highly_variable_genes produces different hvg when run on same object with same n_genes; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I would expect that when you call sc.pp.highly_variable_genes on the same dataset and request the same number of genes, that you would get the same output. The below example suggests that this is not the case. . ### Minimal code sample. ```python. adata_sub = sc.read_h5ad(""your_favourite_object.h5ad""). n_genes = 1491. for i in range(10):. sc.pp.highly_variable_genes(adata_sub, n_top_genes=n_genes). unique_genes = list(adata_sub.var['highly_variable'][adata_sub.var['highly_variable'] == True].index). if i == 0:. all_unique = list(set(unique_genes)). print(f""total {len(all_unique)} unique genes""). else:. all_unique = list(set(all_unique+unique_genes)). print(f""total {len(all_unique)} unique genes""). ```. ### Error output. ```pytb. total 1491 unique genes. total 1814 unique genes. total 2042 unique genes. total 2163 unique genes. total 2237 unique genes. total 2305 unique genes. total 2356 unique genes. total 2401 unique genes. total 2437 unique genes. total 2453 unique genes. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 8.4.0. asciitree NA. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. bottleneck 1.3.4. cffi 1.15.0. cloudpickle 2.0.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2022.02.1. dateutil 2.8.1. debugpy 1.5.1. decorator 5.1.1. defusedxml 0.7.1. django 4.1.3. entrypoints 0.4. executing 0.8.3. fasteners 0.18. fsspec 2023.4.0. google NA. h5py 3.6.0. igraph 0.10.6. ipykernel 6.9.1. ipython_genutils 0.2.0. ipywidgets 7.6.5. jedi 0.18.1. jinja2 3.1.2. joblib 1.1.0. jupyter_server 1.13.5. kiwisolver 1.2.0. kneed 0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2579
https://github.com/scverse/scanpy/issues/2579:1380,deployability,Version,Versions,1380,"would expect that when you call sc.pp.highly_variable_genes on the same dataset and request the same number of genes, that you would get the same output. The below example suggests that this is not the case. . ### Minimal code sample. ```python. adata_sub = sc.read_h5ad(""your_favourite_object.h5ad""). n_genes = 1491. for i in range(10):. sc.pp.highly_variable_genes(adata_sub, n_top_genes=n_genes). unique_genes = list(adata_sub.var['highly_variable'][adata_sub.var['highly_variable'] == True].index). if i == 0:. all_unique = list(set(unique_genes)). print(f""total {len(all_unique)} unique genes""). else:. all_unique = list(set(all_unique+unique_genes)). print(f""total {len(all_unique)} unique genes""). ```. ### Error output. ```pytb. total 1491 unique genes. total 1814 unique genes. total 2042 unique genes. total 2163 unique genes. total 2237 unique genes. total 2305 unique genes. total 2356 unique genes. total 2401 unique genes. total 2437 unique genes. total 2453 unique genes. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 8.4.0. asciitree NA. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. bottleneck 1.3.4. cffi 1.15.0. cloudpickle 2.0.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2022.02.1. dateutil 2.8.1. debugpy 1.5.1. decorator 5.1.1. defusedxml 0.7.1. django 4.1.3. entrypoints 0.4. executing 0.8.3. fasteners 0.18. fsspec 2023.4.0. google NA. h5py 3.6.0. igraph 0.10.6. ipykernel 6.9.1. ipython_genutils 0.2.0. ipywidgets 7.6.5. jedi 0.18.1. jinja2 3.1.2. joblib 1.1.0. jupyter_server 1.13.5. kiwisolver 1.2.0. kneed 0.8.3. leidenalg 0.10.1. llvmlite 0.38.0. markupsafe 2.0.1. matplotlib 3.5.1. matplotlib_inline NA. mishalpy NA. mpl_toolkits NA. mpmath 1.2.1. msgpack 1.0.2. natsort 8.3.1. nbinom_ufunc NA. nt NA. ntsecuritycon NA. numba 0.55.1. numcodecs 0.11.0. numexpr 2.8.1. numpy 1.21.6. opt_einsum v3.3.0. packaging 21.3. pandas 1.4.3. parso 0.8.3. pickleshare 0.7.5. pkg_resources NA. plotly 5.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2579
https://github.com/scverse/scanpy/issues/2579:3381,deployability,updat,updated,3381,". scanpy 1.9.3. -----. PIL 8.4.0. asciitree NA. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. bottleneck 1.3.4. cffi 1.15.0. cloudpickle 2.0.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2022.02.1. dateutil 2.8.1. debugpy 1.5.1. decorator 5.1.1. defusedxml 0.7.1. django 4.1.3. entrypoints 0.4. executing 0.8.3. fasteners 0.18. fsspec 2023.4.0. google NA. h5py 3.6.0. igraph 0.10.6. ipykernel 6.9.1. ipython_genutils 0.2.0. ipywidgets 7.6.5. jedi 0.18.1. jinja2 3.1.2. joblib 1.1.0. jupyter_server 1.13.5. kiwisolver 1.2.0. kneed 0.8.3. leidenalg 0.10.1. llvmlite 0.38.0. markupsafe 2.0.1. matplotlib 3.5.1. matplotlib_inline NA. mishalpy NA. mpl_toolkits NA. mpmath 1.2.1. msgpack 1.0.2. natsort 8.3.1. nbinom_ufunc NA. nt NA. ntsecuritycon NA. numba 0.55.1. numcodecs 0.11.0. numexpr 2.8.1. numpy 1.21.6. opt_einsum v3.3.0. packaging 21.3. pandas 1.4.3. parso 0.8.3. pickleshare 0.7.5. pkg_resources NA. plotly 5.6.0. prompt_toolkit 3.0.20. psutil 5.8.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.6.0. pydevd_concurrency_analyser NA. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 2.4.7. pythoncom NA. pytz 2020.1. pywintypes NA. ruamel NA. scipy 1.7.3. seaborn 0.11.2. session_info 1.0.0. setuptools 61.2.0. six 1.15.0. sklearn 1.0.2. snappy NA. sparse 0.14.0. sphinxcontrib NA. stack_data 0.2.0. statsmodels 0.13.2. sympy 1.10.1. tblib 1.7.0. texttable 1.6.7. threadpoolctl 2.2.0. tlz 0.11.0. toolz 0.11.2. torch 2.0.1+cpu. tornado 6.1. tqdm 4.64.0. traitlets 5.9.0. typing_extensions NA. wcwidth 0.2.5. win32api NA. win32com NA. win32security NA. yaml 6.0. zarr 2.14.2. zipp NA. zmq 22.3.0. zope NA. -----. IPython 8.2.0. jupyter_client 6.1.12. jupyter_core 4.9.2. jupyterlab 3.3.2. notebook 6.4.8. -----. Python 3.9.12 (main, Apr 4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]. Windows-10-10.0.22000-SP0. -----. Session information updated at 2023-07-31 10:13. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2579
https://github.com/scverse/scanpy/issues/2579:1566,energy efficiency,cloud,cloudpickle,1566,"is is not the case. . ### Minimal code sample. ```python. adata_sub = sc.read_h5ad(""your_favourite_object.h5ad""). n_genes = 1491. for i in range(10):. sc.pp.highly_variable_genes(adata_sub, n_top_genes=n_genes). unique_genes = list(adata_sub.var['highly_variable'][adata_sub.var['highly_variable'] == True].index). if i == 0:. all_unique = list(set(unique_genes)). print(f""total {len(all_unique)} unique genes""). else:. all_unique = list(set(all_unique+unique_genes)). print(f""total {len(all_unique)} unique genes""). ```. ### Error output. ```pytb. total 1491 unique genes. total 1814 unique genes. total 2042 unique genes. total 2163 unique genes. total 2237 unique genes. total 2305 unique genes. total 2356 unique genes. total 2401 unique genes. total 2437 unique genes. total 2453 unique genes. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 8.4.0. asciitree NA. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. bottleneck 1.3.4. cffi 1.15.0. cloudpickle 2.0.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2022.02.1. dateutil 2.8.1. debugpy 1.5.1. decorator 5.1.1. defusedxml 0.7.1. django 4.1.3. entrypoints 0.4. executing 0.8.3. fasteners 0.18. fsspec 2023.4.0. google NA. h5py 3.6.0. igraph 0.10.6. ipykernel 6.9.1. ipython_genutils 0.2.0. ipywidgets 7.6.5. jedi 0.18.1. jinja2 3.1.2. joblib 1.1.0. jupyter_server 1.13.5. kiwisolver 1.2.0. kneed 0.8.3. leidenalg 0.10.1. llvmlite 0.38.0. markupsafe 2.0.1. matplotlib 3.5.1. matplotlib_inline NA. mishalpy NA. mpl_toolkits NA. mpmath 1.2.1. msgpack 1.0.2. natsort 8.3.1. nbinom_ufunc NA. nt NA. ntsecuritycon NA. numba 0.55.1. numcodecs 0.11.0. numexpr 2.8.1. numpy 1.21.6. opt_einsum v3.3.0. packaging 21.3. pandas 1.4.3. parso 0.8.3. pickleshare 0.7.5. pkg_resources NA. plotly 5.6.0. prompt_toolkit 3.0.20. psutil 5.8.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.6.0. pydevd_concurrency_analyser NA. pydevd_file_utils NA. pydevd_plu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2579
https://github.com/scverse/scanpy/issues/2579:2967,energy efficiency,cpu,cpu,2967,". scanpy 1.9.3. -----. PIL 8.4.0. asciitree NA. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. bottleneck 1.3.4. cffi 1.15.0. cloudpickle 2.0.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2022.02.1. dateutil 2.8.1. debugpy 1.5.1. decorator 5.1.1. defusedxml 0.7.1. django 4.1.3. entrypoints 0.4. executing 0.8.3. fasteners 0.18. fsspec 2023.4.0. google NA. h5py 3.6.0. igraph 0.10.6. ipykernel 6.9.1. ipython_genutils 0.2.0. ipywidgets 7.6.5. jedi 0.18.1. jinja2 3.1.2. joblib 1.1.0. jupyter_server 1.13.5. kiwisolver 1.2.0. kneed 0.8.3. leidenalg 0.10.1. llvmlite 0.38.0. markupsafe 2.0.1. matplotlib 3.5.1. matplotlib_inline NA. mishalpy NA. mpl_toolkits NA. mpmath 1.2.1. msgpack 1.0.2. natsort 8.3.1. nbinom_ufunc NA. nt NA. ntsecuritycon NA. numba 0.55.1. numcodecs 0.11.0. numexpr 2.8.1. numpy 1.21.6. opt_einsum v3.3.0. packaging 21.3. pandas 1.4.3. parso 0.8.3. pickleshare 0.7.5. pkg_resources NA. plotly 5.6.0. prompt_toolkit 3.0.20. psutil 5.8.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.6.0. pydevd_concurrency_analyser NA. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 2.4.7. pythoncom NA. pytz 2020.1. pywintypes NA. ruamel NA. scipy 1.7.3. seaborn 0.11.2. session_info 1.0.0. setuptools 61.2.0. six 1.15.0. sklearn 1.0.2. snappy NA. sparse 0.14.0. sphinxcontrib NA. stack_data 0.2.0. statsmodels 0.13.2. sympy 1.10.1. tblib 1.7.0. texttable 1.6.7. threadpoolctl 2.2.0. tlz 0.11.0. toolz 0.11.2. torch 2.0.1+cpu. tornado 6.1. tqdm 4.64.0. traitlets 5.9.0. typing_extensions NA. wcwidth 0.2.5. win32api NA. win32com NA. win32security NA. yaml 6.0. zarr 2.14.2. zipp NA. zmq 22.3.0. zope NA. -----. IPython 8.2.0. jupyter_client 6.1.12. jupyter_core 4.9.2. jupyterlab 3.3.2. notebook 6.4.8. -----. Python 3.9.12 (main, Apr 4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]. Windows-10-10.0.22000-SP0. -----. Session information updated at 2023-07-31 10:13. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2579
https://github.com/scverse/scanpy/issues/2579:262,integrability,version,version,262,"sc.pp.highly_variable_genes produces different hvg when run on same object with same n_genes; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I would expect that when you call sc.pp.highly_variable_genes on the same dataset and request the same number of genes, that you would get the same output. The below example suggests that this is not the case. . ### Minimal code sample. ```python. adata_sub = sc.read_h5ad(""your_favourite_object.h5ad""). n_genes = 1491. for i in range(10):. sc.pp.highly_variable_genes(adata_sub, n_top_genes=n_genes). unique_genes = list(adata_sub.var['highly_variable'][adata_sub.var['highly_variable'] == True].index). if i == 0:. all_unique = list(set(unique_genes)). print(f""total {len(all_unique)} unique genes""). else:. all_unique = list(set(all_unique+unique_genes)). print(f""total {len(all_unique)} unique genes""). ```. ### Error output. ```pytb. total 1491 unique genes. total 1814 unique genes. total 2042 unique genes. total 2163 unique genes. total 2237 unique genes. total 2305 unique genes. total 2356 unique genes. total 2401 unique genes. total 2437 unique genes. total 2453 unique genes. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 8.4.0. asciitree NA. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. bottleneck 1.3.4. cffi 1.15.0. cloudpickle 2.0.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2022.02.1. dateutil 2.8.1. debugpy 1.5.1. decorator 5.1.1. defusedxml 0.7.1. django 4.1.3. entrypoints 0.4. executing 0.8.3. fasteners 0.18. fsspec 2023.4.0. google NA. h5py 3.6.0. igraph 0.10.6. ipykernel 6.9.1. ipython_genutils 0.2.0. ipywidgets 7.6.5. jedi 0.18.1. jinja2 3.1.2. joblib 1.1.0. jupyter_server 1.13.5. kiwisolver 1.2.0. kneed 0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2579
https://github.com/scverse/scanpy/issues/2579:1380,integrability,Version,Versions,1380,"would expect that when you call sc.pp.highly_variable_genes on the same dataset and request the same number of genes, that you would get the same output. The below example suggests that this is not the case. . ### Minimal code sample. ```python. adata_sub = sc.read_h5ad(""your_favourite_object.h5ad""). n_genes = 1491. for i in range(10):. sc.pp.highly_variable_genes(adata_sub, n_top_genes=n_genes). unique_genes = list(adata_sub.var['highly_variable'][adata_sub.var['highly_variable'] == True].index). if i == 0:. all_unique = list(set(unique_genes)). print(f""total {len(all_unique)} unique genes""). else:. all_unique = list(set(all_unique+unique_genes)). print(f""total {len(all_unique)} unique genes""). ```. ### Error output. ```pytb. total 1491 unique genes. total 1814 unique genes. total 2042 unique genes. total 2163 unique genes. total 2237 unique genes. total 2305 unique genes. total 2356 unique genes. total 2401 unique genes. total 2437 unique genes. total 2453 unique genes. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 8.4.0. asciitree NA. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. bottleneck 1.3.4. cffi 1.15.0. cloudpickle 2.0.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2022.02.1. dateutil 2.8.1. debugpy 1.5.1. decorator 5.1.1. defusedxml 0.7.1. django 4.1.3. entrypoints 0.4. executing 0.8.3. fasteners 0.18. fsspec 2023.4.0. google NA. h5py 3.6.0. igraph 0.10.6. ipykernel 6.9.1. ipython_genutils 0.2.0. ipywidgets 7.6.5. jedi 0.18.1. jinja2 3.1.2. joblib 1.1.0. jupyter_server 1.13.5. kiwisolver 1.2.0. kneed 0.8.3. leidenalg 0.10.1. llvmlite 0.38.0. markupsafe 2.0.1. matplotlib 3.5.1. matplotlib_inline NA. mishalpy NA. mpl_toolkits NA. mpmath 1.2.1. msgpack 1.0.2. natsort 8.3.1. nbinom_ufunc NA. nt NA. ntsecuritycon NA. numba 0.55.1. numcodecs 0.11.0. numexpr 2.8.1. numpy 1.21.6. opt_einsum v3.3.0. packaging 21.3. pandas 1.4.3. parso 0.8.3. pickleshare 0.7.5. pkg_resources NA. plotly 5.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2579
https://github.com/scverse/scanpy/issues/2579:262,modifiability,version,version,262,"sc.pp.highly_variable_genes produces different hvg when run on same object with same n_genes; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I would expect that when you call sc.pp.highly_variable_genes on the same dataset and request the same number of genes, that you would get the same output. The below example suggests that this is not the case. . ### Minimal code sample. ```python. adata_sub = sc.read_h5ad(""your_favourite_object.h5ad""). n_genes = 1491. for i in range(10):. sc.pp.highly_variable_genes(adata_sub, n_top_genes=n_genes). unique_genes = list(adata_sub.var['highly_variable'][adata_sub.var['highly_variable'] == True].index). if i == 0:. all_unique = list(set(unique_genes)). print(f""total {len(all_unique)} unique genes""). else:. all_unique = list(set(all_unique+unique_genes)). print(f""total {len(all_unique)} unique genes""). ```. ### Error output. ```pytb. total 1491 unique genes. total 1814 unique genes. total 2042 unique genes. total 2163 unique genes. total 2237 unique genes. total 2305 unique genes. total 2356 unique genes. total 2401 unique genes. total 2437 unique genes. total 2453 unique genes. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 8.4.0. asciitree NA. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. bottleneck 1.3.4. cffi 1.15.0. cloudpickle 2.0.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2022.02.1. dateutil 2.8.1. debugpy 1.5.1. decorator 5.1.1. defusedxml 0.7.1. django 4.1.3. entrypoints 0.4. executing 0.8.3. fasteners 0.18. fsspec 2023.4.0. google NA. h5py 3.6.0. igraph 0.10.6. ipykernel 6.9.1. ipython_genutils 0.2.0. ipywidgets 7.6.5. jedi 0.18.1. jinja2 3.1.2. joblib 1.1.0. jupyter_server 1.13.5. kiwisolver 1.2.0. kneed 0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2579
https://github.com/scverse/scanpy/issues/2579:1380,modifiability,Version,Versions,1380,"would expect that when you call sc.pp.highly_variable_genes on the same dataset and request the same number of genes, that you would get the same output. The below example suggests that this is not the case. . ### Minimal code sample. ```python. adata_sub = sc.read_h5ad(""your_favourite_object.h5ad""). n_genes = 1491. for i in range(10):. sc.pp.highly_variable_genes(adata_sub, n_top_genes=n_genes). unique_genes = list(adata_sub.var['highly_variable'][adata_sub.var['highly_variable'] == True].index). if i == 0:. all_unique = list(set(unique_genes)). print(f""total {len(all_unique)} unique genes""). else:. all_unique = list(set(all_unique+unique_genes)). print(f""total {len(all_unique)} unique genes""). ```. ### Error output. ```pytb. total 1491 unique genes. total 1814 unique genes. total 2042 unique genes. total 2163 unique genes. total 2237 unique genes. total 2305 unique genes. total 2356 unique genes. total 2401 unique genes. total 2437 unique genes. total 2453 unique genes. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 8.4.0. asciitree NA. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. bottleneck 1.3.4. cffi 1.15.0. cloudpickle 2.0.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2022.02.1. dateutil 2.8.1. debugpy 1.5.1. decorator 5.1.1. defusedxml 0.7.1. django 4.1.3. entrypoints 0.4. executing 0.8.3. fasteners 0.18. fsspec 2023.4.0. google NA. h5py 3.6.0. igraph 0.10.6. ipykernel 6.9.1. ipython_genutils 0.2.0. ipywidgets 7.6.5. jedi 0.18.1. jinja2 3.1.2. joblib 1.1.0. jupyter_server 1.13.5. kiwisolver 1.2.0. kneed 0.8.3. leidenalg 0.10.1. llvmlite 0.38.0. markupsafe 2.0.1. matplotlib 3.5.1. matplotlib_inline NA. mishalpy NA. mpl_toolkits NA. mpmath 1.2.1. msgpack 1.0.2. natsort 8.3.1. nbinom_ufunc NA. nt NA. ntsecuritycon NA. numba 0.55.1. numcodecs 0.11.0. numexpr 2.8.1. numpy 1.21.6. opt_einsum v3.3.0. packaging 21.3. pandas 1.4.3. parso 0.8.3. pickleshare 0.7.5. pkg_resources NA. plotly 5.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2579
https://github.com/scverse/scanpy/issues/2579:1698,modifiability,deco,decorator,1698,"or i in range(10):. sc.pp.highly_variable_genes(adata_sub, n_top_genes=n_genes). unique_genes = list(adata_sub.var['highly_variable'][adata_sub.var['highly_variable'] == True].index). if i == 0:. all_unique = list(set(unique_genes)). print(f""total {len(all_unique)} unique genes""). else:. all_unique = list(set(all_unique+unique_genes)). print(f""total {len(all_unique)} unique genes""). ```. ### Error output. ```pytb. total 1491 unique genes. total 1814 unique genes. total 2042 unique genes. total 2163 unique genes. total 2237 unique genes. total 2305 unique genes. total 2356 unique genes. total 2401 unique genes. total 2437 unique genes. total 2453 unique genes. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 8.4.0. asciitree NA. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. bottleneck 1.3.4. cffi 1.15.0. cloudpickle 2.0.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2022.02.1. dateutil 2.8.1. debugpy 1.5.1. decorator 5.1.1. defusedxml 0.7.1. django 4.1.3. entrypoints 0.4. executing 0.8.3. fasteners 0.18. fsspec 2023.4.0. google NA. h5py 3.6.0. igraph 0.10.6. ipykernel 6.9.1. ipython_genutils 0.2.0. ipywidgets 7.6.5. jedi 0.18.1. jinja2 3.1.2. joblib 1.1.0. jupyter_server 1.13.5. kiwisolver 1.2.0. kneed 0.8.3. leidenalg 0.10.1. llvmlite 0.38.0. markupsafe 2.0.1. matplotlib 3.5.1. matplotlib_inline NA. mishalpy NA. mpl_toolkits NA. mpmath 1.2.1. msgpack 1.0.2. natsort 8.3.1. nbinom_ufunc NA. nt NA. ntsecuritycon NA. numba 0.55.1. numcodecs 0.11.0. numexpr 2.8.1. numpy 1.21.6. opt_einsum v3.3.0. packaging 21.3. pandas 1.4.3. parso 0.8.3. pickleshare 0.7.5. pkg_resources NA. plotly 5.6.0. prompt_toolkit 3.0.20. psutil 5.8.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.6.0. pydevd_concurrency_analyser NA. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 2.4.7. pythoncom NA. pytz 2020.1. pywintypes NA. ruamel NA. scipy 1.7.3. sea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2579
https://github.com/scverse/scanpy/issues/2579:2295,modifiability,pac,packaging,2295,"l 2401 unique genes. total 2437 unique genes. total 2453 unique genes. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 8.4.0. asciitree NA. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. bottleneck 1.3.4. cffi 1.15.0. cloudpickle 2.0.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2022.02.1. dateutil 2.8.1. debugpy 1.5.1. decorator 5.1.1. defusedxml 0.7.1. django 4.1.3. entrypoints 0.4. executing 0.8.3. fasteners 0.18. fsspec 2023.4.0. google NA. h5py 3.6.0. igraph 0.10.6. ipykernel 6.9.1. ipython_genutils 0.2.0. ipywidgets 7.6.5. jedi 0.18.1. jinja2 3.1.2. joblib 1.1.0. jupyter_server 1.13.5. kiwisolver 1.2.0. kneed 0.8.3. leidenalg 0.10.1. llvmlite 0.38.0. markupsafe 2.0.1. matplotlib 3.5.1. matplotlib_inline NA. mishalpy NA. mpl_toolkits NA. mpmath 1.2.1. msgpack 1.0.2. natsort 8.3.1. nbinom_ufunc NA. nt NA. ntsecuritycon NA. numba 0.55.1. numcodecs 0.11.0. numexpr 2.8.1. numpy 1.21.6. opt_einsum v3.3.0. packaging 21.3. pandas 1.4.3. parso 0.8.3. pickleshare 0.7.5. pkg_resources NA. plotly 5.6.0. prompt_toolkit 3.0.20. psutil 5.8.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.6.0. pydevd_concurrency_analyser NA. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 2.4.7. pythoncom NA. pytz 2020.1. pywintypes NA. ruamel NA. scipy 1.7.3. seaborn 0.11.2. session_info 1.0.0. setuptools 61.2.0. six 1.15.0. sklearn 1.0.2. snappy NA. sparse 0.14.0. sphinxcontrib NA. stack_data 0.2.0. statsmodels 0.13.2. sympy 1.10.1. tblib 1.7.0. texttable 1.6.7. threadpoolctl 2.2.0. tlz 0.11.0. toolz 0.11.2. torch 2.0.1+cpu. tornado 6.1. tqdm 4.64.0. traitlets 5.9.0. typing_extensions NA. wcwidth 0.2.5. win32api NA. win32com NA. win32security NA. yaml 6.0. zarr 2.14.2. zipp NA. zmq 22.3.0. zope NA. -----. IPython 8.2.0. jupyter_client 6.1.12. jupyter_core 4.9.2. jupyterlab 3.3.2. notebook 6.4.8. -----. Python 3.9.12 (main, Apr 4 2022, 05:22:27) [M",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2579
https://github.com/scverse/scanpy/issues/2579:1098,performance,Error,Error,1098,"ase make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I would expect that when you call sc.pp.highly_variable_genes on the same dataset and request the same number of genes, that you would get the same output. The below example suggests that this is not the case. . ### Minimal code sample. ```python. adata_sub = sc.read_h5ad(""your_favourite_object.h5ad""). n_genes = 1491. for i in range(10):. sc.pp.highly_variable_genes(adata_sub, n_top_genes=n_genes). unique_genes = list(adata_sub.var['highly_variable'][adata_sub.var['highly_variable'] == True].index). if i == 0:. all_unique = list(set(unique_genes)). print(f""total {len(all_unique)} unique genes""). else:. all_unique = list(set(all_unique+unique_genes)). print(f""total {len(all_unique)} unique genes""). ```. ### Error output. ```pytb. total 1491 unique genes. total 1814 unique genes. total 2042 unique genes. total 2163 unique genes. total 2237 unique genes. total 2305 unique genes. total 2356 unique genes. total 2401 unique genes. total 2437 unique genes. total 2453 unique genes. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 8.4.0. asciitree NA. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. bottleneck 1.3.4. cffi 1.15.0. cloudpickle 2.0.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2022.02.1. dateutil 2.8.1. debugpy 1.5.1. decorator 5.1.1. defusedxml 0.7.1. django 4.1.3. entrypoints 0.4. executing 0.8.3. fasteners 0.18. fsspec 2023.4.0. google NA. h5py 3.6.0. igraph 0.10.6. ipykernel 6.9.1. ipython_genutils 0.2.0. ipywidgets 7.6.5. jedi 0.18.1. jinja2 3.1.2. joblib 1.1.0. jupyter_server 1.13.5. kiwisolver 1.2.0. kneed 0.8.3. leidenalg 0.10.1. llvmlite 0.38.0. markupsafe 2.0.1. matplotlib 3.5.1. matplotlib_inline NA. mi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2579
https://github.com/scverse/scanpy/issues/2579:1535,performance,bottleneck,bottleneck,1535,"e below example suggests that this is not the case. . ### Minimal code sample. ```python. adata_sub = sc.read_h5ad(""your_favourite_object.h5ad""). n_genes = 1491. for i in range(10):. sc.pp.highly_variable_genes(adata_sub, n_top_genes=n_genes). unique_genes = list(adata_sub.var['highly_variable'][adata_sub.var['highly_variable'] == True].index). if i == 0:. all_unique = list(set(unique_genes)). print(f""total {len(all_unique)} unique genes""). else:. all_unique = list(set(all_unique+unique_genes)). print(f""total {len(all_unique)} unique genes""). ```. ### Error output. ```pytb. total 1491 unique genes. total 1814 unique genes. total 2042 unique genes. total 2163 unique genes. total 2237 unique genes. total 2305 unique genes. total 2356 unique genes. total 2401 unique genes. total 2437 unique genes. total 2453 unique genes. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 8.4.0. asciitree NA. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. bottleneck 1.3.4. cffi 1.15.0. cloudpickle 2.0.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2022.02.1. dateutil 2.8.1. debugpy 1.5.1. decorator 5.1.1. defusedxml 0.7.1. django 4.1.3. entrypoints 0.4. executing 0.8.3. fasteners 0.18. fsspec 2023.4.0. google NA. h5py 3.6.0. igraph 0.10.6. ipykernel 6.9.1. ipython_genutils 0.2.0. ipywidgets 7.6.5. jedi 0.18.1. jinja2 3.1.2. joblib 1.1.0. jupyter_server 1.13.5. kiwisolver 1.2.0. kneed 0.8.3. leidenalg 0.10.1. llvmlite 0.38.0. markupsafe 2.0.1. matplotlib 3.5.1. matplotlib_inline NA. mishalpy NA. mpl_toolkits NA. mpmath 1.2.1. msgpack 1.0.2. natsort 8.3.1. nbinom_ufunc NA. nt NA. ntsecuritycon NA. numba 0.55.1. numcodecs 0.11.0. numexpr 2.8.1. numpy 1.21.6. opt_einsum v3.3.0. packaging 21.3. pandas 1.4.3. parso 0.8.3. pickleshare 0.7.5. pkg_resources NA. plotly 5.6.0. prompt_toolkit 3.0.20. psutil 5.8.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.6.0. pydevd_concurrency_analyser NA. ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2579
https://github.com/scverse/scanpy/issues/2579:2967,performance,cpu,cpu,2967,". scanpy 1.9.3. -----. PIL 8.4.0. asciitree NA. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. bottleneck 1.3.4. cffi 1.15.0. cloudpickle 2.0.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2022.02.1. dateutil 2.8.1. debugpy 1.5.1. decorator 5.1.1. defusedxml 0.7.1. django 4.1.3. entrypoints 0.4. executing 0.8.3. fasteners 0.18. fsspec 2023.4.0. google NA. h5py 3.6.0. igraph 0.10.6. ipykernel 6.9.1. ipython_genutils 0.2.0. ipywidgets 7.6.5. jedi 0.18.1. jinja2 3.1.2. joblib 1.1.0. jupyter_server 1.13.5. kiwisolver 1.2.0. kneed 0.8.3. leidenalg 0.10.1. llvmlite 0.38.0. markupsafe 2.0.1. matplotlib 3.5.1. matplotlib_inline NA. mishalpy NA. mpl_toolkits NA. mpmath 1.2.1. msgpack 1.0.2. natsort 8.3.1. nbinom_ufunc NA. nt NA. ntsecuritycon NA. numba 0.55.1. numcodecs 0.11.0. numexpr 2.8.1. numpy 1.21.6. opt_einsum v3.3.0. packaging 21.3. pandas 1.4.3. parso 0.8.3. pickleshare 0.7.5. pkg_resources NA. plotly 5.6.0. prompt_toolkit 3.0.20. psutil 5.8.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.6.0. pydevd_concurrency_analyser NA. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 2.4.7. pythoncom NA. pytz 2020.1. pywintypes NA. ruamel NA. scipy 1.7.3. seaborn 0.11.2. session_info 1.0.0. setuptools 61.2.0. six 1.15.0. sklearn 1.0.2. snappy NA. sparse 0.14.0. sphinxcontrib NA. stack_data 0.2.0. statsmodels 0.13.2. sympy 1.10.1. tblib 1.7.0. texttable 1.6.7. threadpoolctl 2.2.0. tlz 0.11.0. toolz 0.11.2. torch 2.0.1+cpu. tornado 6.1. tqdm 4.64.0. traitlets 5.9.0. typing_extensions NA. wcwidth 0.2.5. win32api NA. win32com NA. win32security NA. yaml 6.0. zarr 2.14.2. zipp NA. zmq 22.3.0. zope NA. -----. IPython 8.2.0. jupyter_client 6.1.12. jupyter_core 4.9.2. jupyterlab 3.3.2. notebook 6.4.8. -----. Python 3.9.12 (main, Apr 4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]. Windows-10-10.0.22000-SP0. -----. Session information updated at 2023-07-31 10:13. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2579
https://github.com/scverse/scanpy/issues/2579:1098,safety,Error,Error,1098,"ase make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I would expect that when you call sc.pp.highly_variable_genes on the same dataset and request the same number of genes, that you would get the same output. The below example suggests that this is not the case. . ### Minimal code sample. ```python. adata_sub = sc.read_h5ad(""your_favourite_object.h5ad""). n_genes = 1491. for i in range(10):. sc.pp.highly_variable_genes(adata_sub, n_top_genes=n_genes). unique_genes = list(adata_sub.var['highly_variable'][adata_sub.var['highly_variable'] == True].index). if i == 0:. all_unique = list(set(unique_genes)). print(f""total {len(all_unique)} unique genes""). else:. all_unique = list(set(all_unique+unique_genes)). print(f""total {len(all_unique)} unique genes""). ```. ### Error output. ```pytb. total 1491 unique genes. total 1814 unique genes. total 2042 unique genes. total 2163 unique genes. total 2237 unique genes. total 2305 unique genes. total 2356 unique genes. total 2401 unique genes. total 2437 unique genes. total 2453 unique genes. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 8.4.0. asciitree NA. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. bottleneck 1.3.4. cffi 1.15.0. cloudpickle 2.0.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2022.02.1. dateutil 2.8.1. debugpy 1.5.1. decorator 5.1.1. defusedxml 0.7.1. django 4.1.3. entrypoints 0.4. executing 0.8.3. fasteners 0.18. fsspec 2023.4.0. google NA. h5py 3.6.0. igraph 0.10.6. ipykernel 6.9.1. ipython_genutils 0.2.0. ipywidgets 7.6.5. jedi 0.18.1. jinja2 3.1.2. joblib 1.1.0. jupyter_server 1.13.5. kiwisolver 1.2.0. kneed 0.8.3. leidenalg 0.10.1. llvmlite 0.38.0. markupsafe 2.0.1. matplotlib 3.5.1. matplotlib_inline NA. mi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2579
https://github.com/scverse/scanpy/issues/2579:3381,safety,updat,updated,3381,". scanpy 1.9.3. -----. PIL 8.4.0. asciitree NA. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. bottleneck 1.3.4. cffi 1.15.0. cloudpickle 2.0.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2022.02.1. dateutil 2.8.1. debugpy 1.5.1. decorator 5.1.1. defusedxml 0.7.1. django 4.1.3. entrypoints 0.4. executing 0.8.3. fasteners 0.18. fsspec 2023.4.0. google NA. h5py 3.6.0. igraph 0.10.6. ipykernel 6.9.1. ipython_genutils 0.2.0. ipywidgets 7.6.5. jedi 0.18.1. jinja2 3.1.2. joblib 1.1.0. jupyter_server 1.13.5. kiwisolver 1.2.0. kneed 0.8.3. leidenalg 0.10.1. llvmlite 0.38.0. markupsafe 2.0.1. matplotlib 3.5.1. matplotlib_inline NA. mishalpy NA. mpl_toolkits NA. mpmath 1.2.1. msgpack 1.0.2. natsort 8.3.1. nbinom_ufunc NA. nt NA. ntsecuritycon NA. numba 0.55.1. numcodecs 0.11.0. numexpr 2.8.1. numpy 1.21.6. opt_einsum v3.3.0. packaging 21.3. pandas 1.4.3. parso 0.8.3. pickleshare 0.7.5. pkg_resources NA. plotly 5.6.0. prompt_toolkit 3.0.20. psutil 5.8.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.6.0. pydevd_concurrency_analyser NA. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 2.4.7. pythoncom NA. pytz 2020.1. pywintypes NA. ruamel NA. scipy 1.7.3. seaborn 0.11.2. session_info 1.0.0. setuptools 61.2.0. six 1.15.0. sklearn 1.0.2. snappy NA. sparse 0.14.0. sphinxcontrib NA. stack_data 0.2.0. statsmodels 0.13.2. sympy 1.10.1. tblib 1.7.0. texttable 1.6.7. threadpoolctl 2.2.0. tlz 0.11.0. toolz 0.11.2. torch 2.0.1+cpu. tornado 6.1. tqdm 4.64.0. traitlets 5.9.0. typing_extensions NA. wcwidth 0.2.5. win32api NA. win32com NA. win32security NA. yaml 6.0. zarr 2.14.2. zipp NA. zmq 22.3.0. zope NA. -----. IPython 8.2.0. jupyter_client 6.1.12. jupyter_core 4.9.2. jupyterlab 3.3.2. notebook 6.4.8. -----. Python 3.9.12 (main, Apr 4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]. Windows-10-10.0.22000-SP0. -----. Session information updated at 2023-07-31 10:13. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2579
https://github.com/scverse/scanpy/issues/2579:3361,security,Session,Session,3361,". scanpy 1.9.3. -----. PIL 8.4.0. asciitree NA. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. bottleneck 1.3.4. cffi 1.15.0. cloudpickle 2.0.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2022.02.1. dateutil 2.8.1. debugpy 1.5.1. decorator 5.1.1. defusedxml 0.7.1. django 4.1.3. entrypoints 0.4. executing 0.8.3. fasteners 0.18. fsspec 2023.4.0. google NA. h5py 3.6.0. igraph 0.10.6. ipykernel 6.9.1. ipython_genutils 0.2.0. ipywidgets 7.6.5. jedi 0.18.1. jinja2 3.1.2. joblib 1.1.0. jupyter_server 1.13.5. kiwisolver 1.2.0. kneed 0.8.3. leidenalg 0.10.1. llvmlite 0.38.0. markupsafe 2.0.1. matplotlib 3.5.1. matplotlib_inline NA. mishalpy NA. mpl_toolkits NA. mpmath 1.2.1. msgpack 1.0.2. natsort 8.3.1. nbinom_ufunc NA. nt NA. ntsecuritycon NA. numba 0.55.1. numcodecs 0.11.0. numexpr 2.8.1. numpy 1.21.6. opt_einsum v3.3.0. packaging 21.3. pandas 1.4.3. parso 0.8.3. pickleshare 0.7.5. pkg_resources NA. plotly 5.6.0. prompt_toolkit 3.0.20. psutil 5.8.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.6.0. pydevd_concurrency_analyser NA. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 2.4.7. pythoncom NA. pytz 2020.1. pywintypes NA. ruamel NA. scipy 1.7.3. seaborn 0.11.2. session_info 1.0.0. setuptools 61.2.0. six 1.15.0. sklearn 1.0.2. snappy NA. sparse 0.14.0. sphinxcontrib NA. stack_data 0.2.0. statsmodels 0.13.2. sympy 1.10.1. tblib 1.7.0. texttable 1.6.7. threadpoolctl 2.2.0. tlz 0.11.0. toolz 0.11.2. torch 2.0.1+cpu. tornado 6.1. tqdm 4.64.0. traitlets 5.9.0. typing_extensions NA. wcwidth 0.2.5. win32api NA. win32com NA. win32security NA. yaml 6.0. zarr 2.14.2. zipp NA. zmq 22.3.0. zope NA. -----. IPython 8.2.0. jupyter_client 6.1.12. jupyter_core 4.9.2. jupyterlab 3.3.2. notebook 6.4.8. -----. Python 3.9.12 (main, Apr 4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]. Windows-10-10.0.22000-SP0. -----. Session information updated at 2023-07-31 10:13. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2579
https://github.com/scverse/scanpy/issues/2579:3381,security,updat,updated,3381,". scanpy 1.9.3. -----. PIL 8.4.0. asciitree NA. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. bottleneck 1.3.4. cffi 1.15.0. cloudpickle 2.0.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2022.02.1. dateutil 2.8.1. debugpy 1.5.1. decorator 5.1.1. defusedxml 0.7.1. django 4.1.3. entrypoints 0.4. executing 0.8.3. fasteners 0.18. fsspec 2023.4.0. google NA. h5py 3.6.0. igraph 0.10.6. ipykernel 6.9.1. ipython_genutils 0.2.0. ipywidgets 7.6.5. jedi 0.18.1. jinja2 3.1.2. joblib 1.1.0. jupyter_server 1.13.5. kiwisolver 1.2.0. kneed 0.8.3. leidenalg 0.10.1. llvmlite 0.38.0. markupsafe 2.0.1. matplotlib 3.5.1. matplotlib_inline NA. mishalpy NA. mpl_toolkits NA. mpmath 1.2.1. msgpack 1.0.2. natsort 8.3.1. nbinom_ufunc NA. nt NA. ntsecuritycon NA. numba 0.55.1. numcodecs 0.11.0. numexpr 2.8.1. numpy 1.21.6. opt_einsum v3.3.0. packaging 21.3. pandas 1.4.3. parso 0.8.3. pickleshare 0.7.5. pkg_resources NA. plotly 5.6.0. prompt_toolkit 3.0.20. psutil 5.8.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.6.0. pydevd_concurrency_analyser NA. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 2.4.7. pythoncom NA. pytz 2020.1. pywintypes NA. ruamel NA. scipy 1.7.3. seaborn 0.11.2. session_info 1.0.0. setuptools 61.2.0. six 1.15.0. sklearn 1.0.2. snappy NA. sparse 0.14.0. sphinxcontrib NA. stack_data 0.2.0. statsmodels 0.13.2. sympy 1.10.1. tblib 1.7.0. texttable 1.6.7. threadpoolctl 2.2.0. tlz 0.11.0. toolz 0.11.2. torch 2.0.1+cpu. tornado 6.1. tqdm 4.64.0. traitlets 5.9.0. typing_extensions NA. wcwidth 0.2.5. win32api NA. win32com NA. win32security NA. yaml 6.0. zarr 2.14.2. zipp NA. zmq 22.3.0. zope NA. -----. IPython 8.2.0. jupyter_client 6.1.12. jupyter_core 4.9.2. jupyterlab 3.3.2. notebook 6.4.8. -----. Python 3.9.12 (main, Apr 4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]. Windows-10-10.0.22000-SP0. -----. Session information updated at 2023-07-31 10:13. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2579
https://github.com/scverse/scanpy/issues/2579:222,usability,confirm,confirmed,222,"sc.pp.highly_variable_genes produces different hvg when run on same object with same n_genes; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I would expect that when you call sc.pp.highly_variable_genes on the same dataset and request the same number of genes, that you would get the same output. The below example suggests that this is not the case. . ### Minimal code sample. ```python. adata_sub = sc.read_h5ad(""your_favourite_object.h5ad""). n_genes = 1491. for i in range(10):. sc.pp.highly_variable_genes(adata_sub, n_top_genes=n_genes). unique_genes = list(adata_sub.var['highly_variable'][adata_sub.var['highly_variable'] == True].index). if i == 0:. all_unique = list(set(unique_genes)). print(f""total {len(all_unique)} unique genes""). else:. all_unique = list(set(all_unique+unique_genes)). print(f""total {len(all_unique)} unique genes""). ```. ### Error output. ```pytb. total 1491 unique genes. total 1814 unique genes. total 2042 unique genes. total 2163 unique genes. total 2237 unique genes. total 2305 unique genes. total 2356 unique genes. total 2401 unique genes. total 2437 unique genes. total 2453 unique genes. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 8.4.0. asciitree NA. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. bottleneck 1.3.4. cffi 1.15.0. cloudpickle 2.0.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2022.02.1. dateutil 2.8.1. debugpy 1.5.1. decorator 5.1.1. defusedxml 0.7.1. django 4.1.3. entrypoints 0.4. executing 0.8.3. fasteners 0.18. fsspec 2023.4.0. google NA. h5py 3.6.0. igraph 0.10.6. ipykernel 6.9.1. ipython_genutils 0.2.0. ipywidgets 7.6.5. jedi 0.18.1. jinja2 3.1.2. joblib 1.1.0. jupyter_server 1.13.5. kiwisolver 1.2.0. kneed 0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2579
https://github.com/scverse/scanpy/issues/2579:305,usability,confirm,confirmed,305,"sc.pp.highly_variable_genes produces different hvg when run on same object with same n_genes; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I would expect that when you call sc.pp.highly_variable_genes on the same dataset and request the same number of genes, that you would get the same output. The below example suggests that this is not the case. . ### Minimal code sample. ```python. adata_sub = sc.read_h5ad(""your_favourite_object.h5ad""). n_genes = 1491. for i in range(10):. sc.pp.highly_variable_genes(adata_sub, n_top_genes=n_genes). unique_genes = list(adata_sub.var['highly_variable'][adata_sub.var['highly_variable'] == True].index). if i == 0:. all_unique = list(set(unique_genes)). print(f""total {len(all_unique)} unique genes""). else:. all_unique = list(set(all_unique+unique_genes)). print(f""total {len(all_unique)} unique genes""). ```. ### Error output. ```pytb. total 1491 unique genes. total 1814 unique genes. total 2042 unique genes. total 2163 unique genes. total 2237 unique genes. total 2305 unique genes. total 2356 unique genes. total 2401 unique genes. total 2437 unique genes. total 2453 unique genes. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 8.4.0. asciitree NA. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. bottleneck 1.3.4. cffi 1.15.0. cloudpickle 2.0.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2022.02.1. dateutil 2.8.1. debugpy 1.5.1. decorator 5.1.1. defusedxml 0.7.1. django 4.1.3. entrypoints 0.4. executing 0.8.3. fasteners 0.18. fsspec 2023.4.0. google NA. h5py 3.6.0. igraph 0.10.6. ipykernel 6.9.1. ipython_genutils 0.2.0. ipywidgets 7.6.5. jedi 0.18.1. jinja2 3.1.2. joblib 1.1.0. jupyter_server 1.13.5. kiwisolver 1.2.0. kneed 0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2579
https://github.com/scverse/scanpy/issues/2579:598,usability,Minim,Minimal,598,"sc.pp.highly_variable_genes produces different hvg when run on same object with same n_genes; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I would expect that when you call sc.pp.highly_variable_genes on the same dataset and request the same number of genes, that you would get the same output. The below example suggests that this is not the case. . ### Minimal code sample. ```python. adata_sub = sc.read_h5ad(""your_favourite_object.h5ad""). n_genes = 1491. for i in range(10):. sc.pp.highly_variable_genes(adata_sub, n_top_genes=n_genes). unique_genes = list(adata_sub.var['highly_variable'][adata_sub.var['highly_variable'] == True].index). if i == 0:. all_unique = list(set(unique_genes)). print(f""total {len(all_unique)} unique genes""). else:. all_unique = list(set(all_unique+unique_genes)). print(f""total {len(all_unique)} unique genes""). ```. ### Error output. ```pytb. total 1491 unique genes. total 1814 unique genes. total 2042 unique genes. total 2163 unique genes. total 2237 unique genes. total 2305 unique genes. total 2356 unique genes. total 2401 unique genes. total 2437 unique genes. total 2453 unique genes. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 8.4.0. asciitree NA. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. bottleneck 1.3.4. cffi 1.15.0. cloudpickle 2.0.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2022.02.1. dateutil 2.8.1. debugpy 1.5.1. decorator 5.1.1. defusedxml 0.7.1. django 4.1.3. entrypoints 0.4. executing 0.8.3. fasteners 0.18. fsspec 2023.4.0. google NA. h5py 3.6.0. igraph 0.10.6. ipykernel 6.9.1. ipython_genutils 0.2.0. ipywidgets 7.6.5. jedi 0.18.1. jinja2 3.1.2. joblib 1.1.0. jupyter_server 1.13.5. kiwisolver 1.2.0. kneed 0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2579
https://github.com/scverse/scanpy/issues/2579:1098,usability,Error,Error,1098,"ase make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I would expect that when you call sc.pp.highly_variable_genes on the same dataset and request the same number of genes, that you would get the same output. The below example suggests that this is not the case. . ### Minimal code sample. ```python. adata_sub = sc.read_h5ad(""your_favourite_object.h5ad""). n_genes = 1491. for i in range(10):. sc.pp.highly_variable_genes(adata_sub, n_top_genes=n_genes). unique_genes = list(adata_sub.var['highly_variable'][adata_sub.var['highly_variable'] == True].index). if i == 0:. all_unique = list(set(unique_genes)). print(f""total {len(all_unique)} unique genes""). else:. all_unique = list(set(all_unique+unique_genes)). print(f""total {len(all_unique)} unique genes""). ```. ### Error output. ```pytb. total 1491 unique genes. total 1814 unique genes. total 2042 unique genes. total 2163 unique genes. total 2237 unique genes. total 2305 unique genes. total 2356 unique genes. total 2401 unique genes. total 2437 unique genes. total 2453 unique genes. ```. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 8.4.0. asciitree NA. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. bottleneck 1.3.4. cffi 1.15.0. cloudpickle 2.0.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2022.02.1. dateutil 2.8.1. debugpy 1.5.1. decorator 5.1.1. defusedxml 0.7.1. django 4.1.3. entrypoints 0.4. executing 0.8.3. fasteners 0.18. fsspec 2023.4.0. google NA. h5py 3.6.0. igraph 0.10.6. ipykernel 6.9.1. ipython_genutils 0.2.0. ipywidgets 7.6.5. jedi 0.18.1. jinja2 3.1.2. joblib 1.1.0. jupyter_server 1.13.5. kiwisolver 1.2.0. kneed 0.8.3. leidenalg 0.10.1. llvmlite 0.38.0. markupsafe 2.0.1. matplotlib 3.5.1. matplotlib_inline NA. mi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2579
https://github.com/scverse/scanpy/issues/2579:2941,usability,tool,toolz,2941,". scanpy 1.9.3. -----. PIL 8.4.0. asciitree NA. asttokens NA. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. bottleneck 1.3.4. cffi 1.15.0. cloudpickle 2.0.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. cytoolz 0.11.0. dask 2022.02.1. dateutil 2.8.1. debugpy 1.5.1. decorator 5.1.1. defusedxml 0.7.1. django 4.1.3. entrypoints 0.4. executing 0.8.3. fasteners 0.18. fsspec 2023.4.0. google NA. h5py 3.6.0. igraph 0.10.6. ipykernel 6.9.1. ipython_genutils 0.2.0. ipywidgets 7.6.5. jedi 0.18.1. jinja2 3.1.2. joblib 1.1.0. jupyter_server 1.13.5. kiwisolver 1.2.0. kneed 0.8.3. leidenalg 0.10.1. llvmlite 0.38.0. markupsafe 2.0.1. matplotlib 3.5.1. matplotlib_inline NA. mishalpy NA. mpl_toolkits NA. mpmath 1.2.1. msgpack 1.0.2. natsort 8.3.1. nbinom_ufunc NA. nt NA. ntsecuritycon NA. numba 0.55.1. numcodecs 0.11.0. numexpr 2.8.1. numpy 1.21.6. opt_einsum v3.3.0. packaging 21.3. pandas 1.4.3. parso 0.8.3. pickleshare 0.7.5. pkg_resources NA. plotly 5.6.0. prompt_toolkit 3.0.20. psutil 5.8.0. pure_eval 0.2.2. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.6.0. pydevd_concurrency_analyser NA. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pyparsing 2.4.7. pythoncom NA. pytz 2020.1. pywintypes NA. ruamel NA. scipy 1.7.3. seaborn 0.11.2. session_info 1.0.0. setuptools 61.2.0. six 1.15.0. sklearn 1.0.2. snappy NA. sparse 0.14.0. sphinxcontrib NA. stack_data 0.2.0. statsmodels 0.13.2. sympy 1.10.1. tblib 1.7.0. texttable 1.6.7. threadpoolctl 2.2.0. tlz 0.11.0. toolz 0.11.2. torch 2.0.1+cpu. tornado 6.1. tqdm 4.64.0. traitlets 5.9.0. typing_extensions NA. wcwidth 0.2.5. win32api NA. win32com NA. win32security NA. yaml 6.0. zarr 2.14.2. zipp NA. zmq 22.3.0. zope NA. -----. IPython 8.2.0. jupyter_client 6.1.12. jupyter_core 4.9.2. jupyterlab 3.3.2. notebook 6.4.8. -----. Python 3.9.12 (main, Apr 4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]. Windows-10-10.0.22000-SP0. -----. Session information updated at 2023-07-31 10:13. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2579
https://github.com/scverse/scanpy/issues/2580:54,availability,Error,Error,54,"Minor Bug: scanpy.logging.print_versions() throws Key Error; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:572,availability,error,errored,572,"Minor Bug: scanpy.logging.print_versions() throws Key Error; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:700,availability,Error,Error,700,"Minor Bug: scanpy.logging.print_versions() throws Key Error; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:1870,availability,error,error,1870,"ersion of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes several versions in the output. ```. scanpy==1.10.0.dev88+gedd61302 anndata==0.9.2 umap==0.5.3 numpy==1.24.4 scipy==1.11.1 pandas==2.0.3 scikit-learn==1.3.0 statsmodels==0.14.0 igraph==0.10.6 pynndescent==0.5.10. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:18,deployability,log,logging,18,"Minor Bug: scanpy.logging.print_versions() throws Key Error; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:229,deployability,version,version,229,"Minor Bug: scanpy.logging.print_versions() throws Key Error; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:450,deployability,log,logging,450,"Minor Bug: scanpy.logging.print_versions() throws Key Error; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:665,deployability,log,logging,665,"Minor Bug: scanpy.logging.print_versions() throws Key Error; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:876,deployability,log,logging,876,"Minor Bug: scanpy.logging.print_versions() throws Key Error; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:957,deployability,log,logging,957,"Minor Bug: scanpy.logging.print_versions() throws Key Error; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:1063,deployability,depend,dependencies,1063,"se make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes several versions in the output. ```. scanpy==1.10.0.dev88+gedd61302 ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:1194,deployability,modul,module,1194,"d this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes several versions in the output. ```. scanpy==1.10.0.dev88+gedd61302 anndata==0.9.2 umap==0.5.3 numpy==1.24.4 scipy==1.11.1 pandas==2.0.3 scikit-learn==1.3.0 statsmodels==0.14.0 igraph==0.10.6 pynn",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:1449,deployability,depend,dependencies,1449,"ersion of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes several versions in the output. ```. scanpy==1.10.0.dev88+gedd61302 anndata==0.9.2 umap==0.5.3 numpy==1.24.4 scipy==1.11.1 pandas==2.0.3 scikit-learn==1.3.0 statsmodels==0.14.0 igraph==0.10.6 pynndescent==0.5.10. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:1615,deployability,modul,modules,1615,"ersion of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes several versions in the output. ```. scanpy==1.10.0.dev88+gedd61302 anndata==0.9.2 umap==0.5.3 numpy==1.24.4 scipy==1.11.1 pandas==2.0.3 scikit-learn==1.3.0 statsmodels==0.14.0 igraph==0.10.6 pynndescent==0.5.10. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:1646,deployability,modul,modules,1646,"ersion of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes several versions in the output. ```. scanpy==1.10.0.dev88+gedd61302 anndata==0.9.2 umap==0.5.3 numpy==1.24.4 scipy==1.11.1 pandas==2.0.3 scikit-learn==1.3.0 statsmodels==0.14.0 igraph==0.10.6 pynndescent==0.5.10. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:1693,deployability,version,version,1693,"ersion of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes several versions in the output. ```. scanpy==1.10.0.dev88+gedd61302 anndata==0.9.2 umap==0.5.3 numpy==1.24.4 scipy==1.11.1 pandas==2.0.3 scikit-learn==1.3.0 statsmodels==0.14.0 igraph==0.10.6 pynndescent==0.5.10. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:1782,deployability,Version,Versions,1782,"ersion of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes several versions in the output. ```. scanpy==1.10.0.dev88+gedd61302 anndata==0.9.2 umap==0.5.3 numpy==1.24.4 scipy==1.11.1 pandas==2.0.3 scikit-learn==1.3.0 statsmodels==0.14.0 igraph==0.10.6 pynndescent==0.5.10. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:2009,deployability,version,versions,2009,"ersion of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes several versions in the output. ```. scanpy==1.10.0.dev88+gedd61302 anndata==0.9.2 umap==0.5.3 numpy==1.24.4 scipy==1.11.1 pandas==2.0.3 scikit-learn==1.3.0 statsmodels==0.14.0 igraph==0.10.6 pynndescent==0.5.10. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:534,energy efficiency,current,current,534,"Minor Bug: scanpy.logging.print_versions() throws Key Error; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:1435,energy efficiency,cpu,cpu,1435,"ersion of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes several versions in the output. ```. scanpy==1.10.0.dev88+gedd61302 anndata==0.9.2 umap==0.5.3 numpy==1.24.4 scipy==1.11.1 pandas==2.0.3 scikit-learn==1.3.0 statsmodels==0.14.0 igraph==0.10.6 pynndescent==0.5.10. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:229,integrability,version,version,229,"Minor Bug: scanpy.logging.print_versions() throws Key Error; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:1063,integrability,depend,dependencies,1063,"se make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes several versions in the output. ```. scanpy==1.10.0.dev88+gedd61302 ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:1449,integrability,depend,dependencies,1449,"ersion of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes several versions in the output. ```. scanpy==1.10.0.dev88+gedd61302 anndata==0.9.2 umap==0.5.3 numpy==1.24.4 scipy==1.11.1 pandas==2.0.3 scikit-learn==1.3.0 statsmodels==0.14.0 igraph==0.10.6 pynndescent==0.5.10. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:1693,integrability,version,version,1693,"ersion of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes several versions in the output. ```. scanpy==1.10.0.dev88+gedd61302 anndata==0.9.2 umap==0.5.3 numpy==1.24.4 scipy==1.11.1 pandas==2.0.3 scikit-learn==1.3.0 statsmodels==0.14.0 igraph==0.10.6 pynndescent==0.5.10. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:1782,integrability,Version,Versions,1782,"ersion of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes several versions in the output. ```. scanpy==1.10.0.dev88+gedd61302 anndata==0.9.2 umap==0.5.3 numpy==1.24.4 scipy==1.11.1 pandas==2.0.3 scikit-learn==1.3.0 statsmodels==0.14.0 igraph==0.10.6 pynndescent==0.5.10. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:2009,integrability,version,versions,2009,"ersion of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes several versions in the output. ```. scanpy==1.10.0.dev88+gedd61302 anndata==0.9.2 umap==0.5.3 numpy==1.24.4 scipy==1.11.1 pandas==2.0.3 scikit-learn==1.3.0 statsmodels==0.14.0 igraph==0.10.6 pynndescent==0.5.10. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:229,modifiability,version,version,229,"Minor Bug: scanpy.logging.print_versions() throws Key Error; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:1063,modifiability,depend,dependencies,1063,"se make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes several versions in the output. ```. scanpy==1.10.0.dev88+gedd61302 ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:1194,modifiability,modul,module,1194,"d this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes several versions in the output. ```. scanpy==1.10.0.dev88+gedd61302 anndata==0.9.2 umap==0.5.3 numpy==1.24.4 scipy==1.11.1 pandas==2.0.3 scikit-learn==1.3.0 statsmodels==0.14.0 igraph==0.10.6 pynn",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:1384,modifiability,pac,packages,1384,"ersion of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes several versions in the output. ```. scanpy==1.10.0.dev88+gedd61302 anndata==0.9.2 umap==0.5.3 numpy==1.24.4 scipy==1.11.1 pandas==2.0.3 scikit-learn==1.3.0 statsmodels==0.14.0 igraph==0.10.6 pynndescent==0.5.10. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:1449,modifiability,depend,dependencies,1449,"ersion of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes several versions in the output. ```. scanpy==1.10.0.dev88+gedd61302 anndata==0.9.2 umap==0.5.3 numpy==1.24.4 scipy==1.11.1 pandas==2.0.3 scikit-learn==1.3.0 statsmodels==0.14.0 igraph==0.10.6 pynndescent==0.5.10. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:1615,modifiability,modul,modules,1615,"ersion of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes several versions in the output. ```. scanpy==1.10.0.dev88+gedd61302 anndata==0.9.2 umap==0.5.3 numpy==1.24.4 scipy==1.11.1 pandas==2.0.3 scikit-learn==1.3.0 statsmodels==0.14.0 igraph==0.10.6 pynndescent==0.5.10. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:1646,modifiability,modul,modules,1646,"ersion of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes several versions in the output. ```. scanpy==1.10.0.dev88+gedd61302 anndata==0.9.2 umap==0.5.3 numpy==1.24.4 scipy==1.11.1 pandas==2.0.3 scikit-learn==1.3.0 statsmodels==0.14.0 igraph==0.10.6 pynndescent==0.5.10. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:1693,modifiability,version,version,1693,"ersion of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes several versions in the output. ```. scanpy==1.10.0.dev88+gedd61302 anndata==0.9.2 umap==0.5.3 numpy==1.24.4 scipy==1.11.1 pandas==2.0.3 scikit-learn==1.3.0 statsmodels==0.14.0 igraph==0.10.6 pynndescent==0.5.10. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:1782,modifiability,Version,Versions,1782,"ersion of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes several versions in the output. ```. scanpy==1.10.0.dev88+gedd61302 anndata==0.9.2 umap==0.5.3 numpy==1.24.4 scipy==1.11.1 pandas==2.0.3 scikit-learn==1.3.0 statsmodels==0.14.0 igraph==0.10.6 pynndescent==0.5.10. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:2009,modifiability,version,versions,2009,"ersion of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes several versions in the output. ```. scanpy==1.10.0.dev88+gedd61302 anndata==0.9.2 umap==0.5.3 numpy==1.24.4 scipy==1.11.1 pandas==2.0.3 scikit-learn==1.3.0 statsmodels==0.14.0 igraph==0.10.6 pynndescent==0.5.10. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:54,performance,Error,Error,54,"Minor Bug: scanpy.logging.print_versions() throws Key Error; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:572,performance,error,errored,572,"Minor Bug: scanpy.logging.print_versions() throws Key Error; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:700,performance,Error,Error,700,"Minor Bug: scanpy.logging.print_versions() throws Key Error; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:1435,performance,cpu,cpu,1435,"ersion of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes several versions in the output. ```. scanpy==1.10.0.dev88+gedd61302 anndata==0.9.2 umap==0.5.3 numpy==1.24.4 scipy==1.11.1 pandas==2.0.3 scikit-learn==1.3.0 statsmodels==0.14.0 igraph==0.10.6 pynndescent==0.5.10. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:1870,performance,error,error,1870,"ersion of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes several versions in the output. ```. scanpy==1.10.0.dev88+gedd61302 anndata==0.9.2 umap==0.5.3 numpy==1.24.4 scipy==1.11.1 pandas==2.0.3 scikit-learn==1.3.0 statsmodels==0.14.0 igraph==0.10.6 pynndescent==0.5.10. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:18,safety,log,logging,18,"Minor Bug: scanpy.logging.print_versions() throws Key Error; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:54,safety,Error,Error,54,"Minor Bug: scanpy.logging.print_versions() throws Key Error; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:450,safety,log,logging,450,"Minor Bug: scanpy.logging.print_versions() throws Key Error; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:572,safety,error,errored,572,"Minor Bug: scanpy.logging.print_versions() throws Key Error; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:665,safety,log,logging,665,"Minor Bug: scanpy.logging.print_versions() throws Key Error; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:700,safety,Error,Error,700,"Minor Bug: scanpy.logging.print_versions() throws Key Error; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:876,safety,log,logging,876,"Minor Bug: scanpy.logging.print_versions() throws Key Error; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:957,safety,log,logging,957,"Minor Bug: scanpy.logging.print_versions() throws Key Error; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:1063,safety,depend,dependencies,1063,"se make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes several versions in the output. ```. scanpy==1.10.0.dev88+gedd61302 ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:1194,safety,modul,module,1194,"d this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes several versions in the output. ```. scanpy==1.10.0.dev88+gedd61302 anndata==0.9.2 umap==0.5.3 numpy==1.24.4 scipy==1.11.1 pandas==2.0.3 scikit-learn==1.3.0 statsmodels==0.14.0 igraph==0.10.6 pynn",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:1212,safety,test,test,1212," on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes several versions in the output. ```. scanpy==1.10.0.dev88+gedd61302 anndata==0.9.2 umap==0.5.3 numpy==1.24.4 scipy==1.11.1 pandas==2.0.3 scikit-learn==1.3.0 statsmodels==0.14.0 igraph==0.10.6 pynndescent==0.5.10. ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:1449,safety,depend,dependencies,1449,"ersion of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes several versions in the output. ```. scanpy==1.10.0.dev88+gedd61302 anndata==0.9.2 umap==0.5.3 numpy==1.24.4 scipy==1.11.1 pandas==2.0.3 scikit-learn==1.3.0 statsmodels==0.14.0 igraph==0.10.6 pynndescent==0.5.10. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:1615,safety,modul,modules,1615,"ersion of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes several versions in the output. ```. scanpy==1.10.0.dev88+gedd61302 anndata==0.9.2 umap==0.5.3 numpy==1.24.4 scipy==1.11.1 pandas==2.0.3 scikit-learn==1.3.0 statsmodels==0.14.0 igraph==0.10.6 pynndescent==0.5.10. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:1646,safety,modul,modules,1646,"ersion of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes several versions in the output. ```. scanpy==1.10.0.dev88+gedd61302 anndata==0.9.2 umap==0.5.3 numpy==1.24.4 scipy==1.11.1 pandas==2.0.3 scikit-learn==1.3.0 statsmodels==0.14.0 igraph==0.10.6 pynndescent==0.5.10. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:1870,safety,error,error,1870,"ersion of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes several versions in the output. ```. scanpy==1.10.0.dev88+gedd61302 anndata==0.9.2 umap==0.5.3 numpy==1.24.4 scipy==1.11.1 pandas==2.0.3 scikit-learn==1.3.0 statsmodels==0.14.0 igraph==0.10.6 pynndescent==0.5.10. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:18,security,log,logging,18,"Minor Bug: scanpy.logging.print_versions() throws Key Error; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:450,security,log,logging,450,"Minor Bug: scanpy.logging.print_versions() throws Key Error; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:665,security,log,logging,665,"Minor Bug: scanpy.logging.print_versions() throws Key Error; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:876,security,log,logging,876,"Minor Bug: scanpy.logging.print_versions() throws Key Error; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:957,security,log,logging,957,"Minor Bug: scanpy.logging.print_versions() throws Key Error; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:18,testability,log,logging,18,"Minor Bug: scanpy.logging.print_versions() throws Key Error; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:450,testability,log,logging,450,"Minor Bug: scanpy.logging.print_versions() throws Key Error; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:665,testability,log,logging,665,"Minor Bug: scanpy.logging.print_versions() throws Key Error; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:809,testability,Trace,Traceback,809,"Minor Bug: scanpy.logging.print_versions() throws Key Error; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:876,testability,log,logging,876,"Minor Bug: scanpy.logging.print_versions() throws Key Error; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:957,testability,log,logging,957,"Minor Bug: scanpy.logging.print_versions() throws Key Error; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:1063,testability,depend,dependencies,1063,"se make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes several versions in the output. ```. scanpy==1.10.0.dev88+gedd61302 ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:1212,testability,test,test,1212," on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes several versions in the output. ```. scanpy==1.10.0.dev88+gedd61302 anndata==0.9.2 umap==0.5.3 numpy==1.24.4 scipy==1.11.1 pandas==2.0.3 scikit-learn==1.3.0 statsmodels==0.14.0 igraph==0.10.6 pynndescent==0.5.10. ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:1217,testability,coverag,coverage,1217," latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes several versions in the output. ```. scanpy==1.10.0.dev88+gedd61302 anndata==0.9.2 umap==0.5.3 numpy==1.24.4 scipy==1.11.1 pandas==2.0.3 scikit-learn==1.3.0 statsmodels==0.14.0 igraph==0.10.6 pynndescent==0.5.10. ```. </",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:1313,testability,coverag,coverage,1313,"ersion of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes several versions in the output. ```. scanpy==1.10.0.dev88+gedd61302 anndata==0.9.2 umap==0.5.3 numpy==1.24.4 scipy==1.11.1 pandas==2.0.3 scikit-learn==1.3.0 statsmodels==0.14.0 igraph==0.10.6 pynndescent==0.5.10. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:1449,testability,depend,dependencies,1449,"ersion of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes several versions in the output. ```. scanpy==1.10.0.dev88+gedd61302 anndata==0.9.2 umap==0.5.3 numpy==1.24.4 scipy==1.11.1 pandas==2.0.3 scikit-learn==1.3.0 statsmodels==0.14.0 igraph==0.10.6 pynndescent==0.5.10. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:54,usability,Error,Error,54,"Minor Bug: scanpy.logging.print_versions() throws Key Error; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:189,usability,confirm,confirmed,189,"Minor Bug: scanpy.logging.print_versions() throws Key Error; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:272,usability,confirm,confirmed,272,"Minor Bug: scanpy.logging.print_versions() throws Key Error; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:572,usability,error,errored,572,"Minor Bug: scanpy.logging.print_versions() throws Key Error; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:611,usability,Minim,Minimal,611,"Minor Bug: scanpy.logging.print_versions() throws Key Error; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:700,usability,Error,Error,700,"Minor Bug: scanpy.logging.print_versions() throws Key Error; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:909,usability,Document,Documents,909,"Minor Bug: scanpy.logging.print_versions() throws Key Error; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:1870,usability,error,error,1870,"ersion of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes several versions in the output. ```. scanpy==1.10.0.dev88+gedd61302 anndata==0.9.2 umap==0.5.3 numpy==1.24.4 scipy==1.11.1 pandas==2.0.3 scikit-learn==1.3.0 statsmodels==0.14.0 igraph==0.10.6 pynndescent==0.5.10. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2580:2145,usability,learn,learn,2145,"ersion of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python. import scanpy. scanpy.logging.print_versions(). ```. ### Error output. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). Cell In[44], line 1. ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file). 178 print_versions(). 179 else:. --> 180 session_info.show(. 181 dependencies=True,. 182 html=False,. 183 excludes=[. 184 'builtins',. 185 'stdlib_list',. 186 'importlib_metadata',. 187 # Special module present if test coverage being calculated. 188 # https://gitlab.com/joelostblom/session_info/-/issues/10. 189 ""$coverage"",. 190 ],. 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes). 207 for mod_name in clean_modules:. 208 mod_names.append(mod_name). --> 209 mod = sys.modules[mod_name]. 210 # Since modules use different attribute names to store version info,. 211 # try the most common ones. 212 try:. KeyError: 'numcodecs'. ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes several versions in the output. ```. scanpy==1.10.0.dev88+gedd61302 anndata==0.9.2 umap==0.5.3 numpy==1.24.4 scipy==1.11.1 pandas==2.0.3 scikit-learn==1.3.0 statsmodels==0.14.0 igraph==0.10.6 pynndescent==0.5.10. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580
https://github.com/scverse/scanpy/issues/2583:613,deployability,stack,stackoverflow,613,"Remove inplace argument in scanpy 2.0; ### What kind of feature would you like to request? Other? ### Please describe your wishes. Another point that I'd like to throw into the scanpy 2.0 discussion: . Right now, many functions have the `inplace` argument, that determines if a function should write back to `adata` or return the result instead. . With this behavior it is hard to make correct type hints. While it is possible with `@overload`, it is cumbersome because it requires to type out the entire function signature twice. When I asked if it is possible to write these overloads in a more concise way [on stackoverflow](https://stackoverflow.com/questions/75757890/python-overload-single-argument?noredirect=1#comment133653817_75757890) several users argued that changing the return type based on an argument is an anti-pattern, and I think they convinced me. . ## Alternative approach. Have two API levels, e.g. ```py. def scanpy.tl.pca(adata: AnnData, **kwargs) -> None: ... ```. and. ```py. def scanpy.lowlevel.tl.pca(data: np.ndarray | sp.spmatrix, n_pcs) -> np.ndarray: ... ```. Where the former is a wrapper for the latter. This allows to separate the implementation of the actual method using only numpy/scipy data types from the scverse-specific behavior. .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:636,deployability,stack,stackoverflow,636,"Remove inplace argument in scanpy 2.0; ### What kind of feature would you like to request? Other? ### Please describe your wishes. Another point that I'd like to throw into the scanpy 2.0 discussion: . Right now, many functions have the `inplace` argument, that determines if a function should write back to `adata` or return the result instead. . With this behavior it is hard to make correct type hints. While it is possible with `@overload`, it is cumbersome because it requires to type out the entire function signature twice. When I asked if it is possible to write these overloads in a more concise way [on stackoverflow](https://stackoverflow.com/questions/75757890/python-overload-single-argument?noredirect=1#comment133653817_75757890) several users argued that changing the return type based on an argument is an anti-pattern, and I think they convinced me. . ## Alternative approach. Have two API levels, e.g. ```py. def scanpy.tl.pca(adata: AnnData, **kwargs) -> None: ... ```. and. ```py. def scanpy.lowlevel.tl.pca(data: np.ndarray | sp.spmatrix, n_pcs) -> np.ndarray: ... ```. Where the former is a wrapper for the latter. This allows to separate the implementation of the actual method using only numpy/scipy data types from the scverse-specific behavior. .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:904,deployability,API,API,904,"Remove inplace argument in scanpy 2.0; ### What kind of feature would you like to request? Other? ### Please describe your wishes. Another point that I'd like to throw into the scanpy 2.0 discussion: . Right now, many functions have the `inplace` argument, that determines if a function should write back to `adata` or return the result instead. . With this behavior it is hard to make correct type hints. While it is possible with `@overload`, it is cumbersome because it requires to type out the entire function signature twice. When I asked if it is possible to write these overloads in a more concise way [on stackoverflow](https://stackoverflow.com/questions/75757890/python-overload-single-argument?noredirect=1#comment133653817_75757890) several users argued that changing the return type based on an argument is an anti-pattern, and I think they convinced me. . ## Alternative approach. Have two API levels, e.g. ```py. def scanpy.tl.pca(adata: AnnData, **kwargs) -> None: ... ```. and. ```py. def scanpy.lowlevel.tl.pca(data: np.ndarray | sp.spmatrix, n_pcs) -> np.ndarray: ... ```. Where the former is a wrapper for the latter. This allows to separate the implementation of the actual method using only numpy/scipy data types from the scverse-specific behavior. .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:904,integrability,API,API,904,"Remove inplace argument in scanpy 2.0; ### What kind of feature would you like to request? Other? ### Please describe your wishes. Another point that I'd like to throw into the scanpy 2.0 discussion: . Right now, many functions have the `inplace` argument, that determines if a function should write back to `adata` or return the result instead. . With this behavior it is hard to make correct type hints. While it is possible with `@overload`, it is cumbersome because it requires to type out the entire function signature twice. When I asked if it is possible to write these overloads in a more concise way [on stackoverflow](https://stackoverflow.com/questions/75757890/python-overload-single-argument?noredirect=1#comment133653817_75757890) several users argued that changing the return type based on an argument is an anti-pattern, and I think they convinced me. . ## Alternative approach. Have two API levels, e.g. ```py. def scanpy.tl.pca(adata: AnnData, **kwargs) -> None: ... ```. and. ```py. def scanpy.lowlevel.tl.pca(data: np.ndarray | sp.spmatrix, n_pcs) -> np.ndarray: ... ```. Where the former is a wrapper for the latter. This allows to separate the implementation of the actual method using only numpy/scipy data types from the scverse-specific behavior. .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:1114,integrability,wrap,wrapper,1114,"Remove inplace argument in scanpy 2.0; ### What kind of feature would you like to request? Other? ### Please describe your wishes. Another point that I'd like to throw into the scanpy 2.0 discussion: . Right now, many functions have the `inplace` argument, that determines if a function should write back to `adata` or return the result instead. . With this behavior it is hard to make correct type hints. While it is possible with `@overload`, it is cumbersome because it requires to type out the entire function signature twice. When I asked if it is possible to write these overloads in a more concise way [on stackoverflow](https://stackoverflow.com/questions/75757890/python-overload-single-argument?noredirect=1#comment133653817_75757890) several users argued that changing the return type based on an argument is an anti-pattern, and I think they convinced me. . ## Alternative approach. Have two API levels, e.g. ```py. def scanpy.tl.pca(adata: AnnData, **kwargs) -> None: ... ```. and. ```py. def scanpy.lowlevel.tl.pca(data: np.ndarray | sp.spmatrix, n_pcs) -> np.ndarray: ... ```. Where the former is a wrapper for the latter. This allows to separate the implementation of the actual method using only numpy/scipy data types from the scverse-specific behavior. .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:904,interoperability,API,API,904,"Remove inplace argument in scanpy 2.0; ### What kind of feature would you like to request? Other? ### Please describe your wishes. Another point that I'd like to throw into the scanpy 2.0 discussion: . Right now, many functions have the `inplace` argument, that determines if a function should write back to `adata` or return the result instead. . With this behavior it is hard to make correct type hints. While it is possible with `@overload`, it is cumbersome because it requires to type out the entire function signature twice. When I asked if it is possible to write these overloads in a more concise way [on stackoverflow](https://stackoverflow.com/questions/75757890/python-overload-single-argument?noredirect=1#comment133653817_75757890) several users argued that changing the return type based on an argument is an anti-pattern, and I think they convinced me. . ## Alternative approach. Have two API levels, e.g. ```py. def scanpy.tl.pca(adata: AnnData, **kwargs) -> None: ... ```. and. ```py. def scanpy.lowlevel.tl.pca(data: np.ndarray | sp.spmatrix, n_pcs) -> np.ndarray: ... ```. Where the former is a wrapper for the latter. This allows to separate the implementation of the actual method using only numpy/scipy data types from the scverse-specific behavior. .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:1114,interoperability,wrapper,wrapper,1114,"Remove inplace argument in scanpy 2.0; ### What kind of feature would you like to request? Other? ### Please describe your wishes. Another point that I'd like to throw into the scanpy 2.0 discussion: . Right now, many functions have the `inplace` argument, that determines if a function should write back to `adata` or return the result instead. . With this behavior it is hard to make correct type hints. While it is possible with `@overload`, it is cumbersome because it requires to type out the entire function signature twice. When I asked if it is possible to write these overloads in a more concise way [on stackoverflow](https://stackoverflow.com/questions/75757890/python-overload-single-argument?noredirect=1#comment133653817_75757890) several users argued that changing the return type based on an argument is an anti-pattern, and I think they convinced me. . ## Alternative approach. Have two API levels, e.g. ```py. def scanpy.tl.pca(adata: AnnData, **kwargs) -> None: ... ```. and. ```py. def scanpy.lowlevel.tl.pca(data: np.ndarray | sp.spmatrix, n_pcs) -> np.ndarray: ... ```. Where the former is a wrapper for the latter. This allows to separate the implementation of the actual method using only numpy/scipy data types from the scverse-specific behavior. .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:1253,interoperability,specif,specific,1253,"Remove inplace argument in scanpy 2.0; ### What kind of feature would you like to request? Other? ### Please describe your wishes. Another point that I'd like to throw into the scanpy 2.0 discussion: . Right now, many functions have the `inplace` argument, that determines if a function should write back to `adata` or return the result instead. . With this behavior it is hard to make correct type hints. While it is possible with `@overload`, it is cumbersome because it requires to type out the entire function signature twice. When I asked if it is possible to write these overloads in a more concise way [on stackoverflow](https://stackoverflow.com/questions/75757890/python-overload-single-argument?noredirect=1#comment133653817_75757890) several users argued that changing the return type based on an argument is an anti-pattern, and I think they convinced me. . ## Alternative approach. Have two API levels, e.g. ```py. def scanpy.tl.pca(adata: AnnData, **kwargs) -> None: ... ```. and. ```py. def scanpy.lowlevel.tl.pca(data: np.ndarray | sp.spmatrix, n_pcs) -> np.ndarray: ... ```. Where the former is a wrapper for the latter. This allows to separate the implementation of the actual method using only numpy/scipy data types from the scverse-specific behavior. .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:514,security,sign,signature,514,"Remove inplace argument in scanpy 2.0; ### What kind of feature would you like to request? Other? ### Please describe your wishes. Another point that I'd like to throw into the scanpy 2.0 discussion: . Right now, many functions have the `inplace` argument, that determines if a function should write back to `adata` or return the result instead. . With this behavior it is hard to make correct type hints. While it is possible with `@overload`, it is cumbersome because it requires to type out the entire function signature twice. When I asked if it is possible to write these overloads in a more concise way [on stackoverflow](https://stackoverflow.com/questions/75757890/python-overload-single-argument?noredirect=1#comment133653817_75757890) several users argued that changing the return type based on an argument is an anti-pattern, and I think they convinced me. . ## Alternative approach. Have two API levels, e.g. ```py. def scanpy.tl.pca(adata: AnnData, **kwargs) -> None: ... ```. and. ```py. def scanpy.lowlevel.tl.pca(data: np.ndarray | sp.spmatrix, n_pcs) -> np.ndarray: ... ```. Where the former is a wrapper for the latter. This allows to separate the implementation of the actual method using only numpy/scipy data types from the scverse-specific behavior. .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:358,usability,behavi,behavior,358,"Remove inplace argument in scanpy 2.0; ### What kind of feature would you like to request? Other? ### Please describe your wishes. Another point that I'd like to throw into the scanpy 2.0 discussion: . Right now, many functions have the `inplace` argument, that determines if a function should write back to `adata` or return the result instead. . With this behavior it is hard to make correct type hints. While it is possible with `@overload`, it is cumbersome because it requires to type out the entire function signature twice. When I asked if it is possible to write these overloads in a more concise way [on stackoverflow](https://stackoverflow.com/questions/75757890/python-overload-single-argument?noredirect=1#comment133653817_75757890) several users argued that changing the return type based on an argument is an anti-pattern, and I think they convinced me. . ## Alternative approach. Have two API levels, e.g. ```py. def scanpy.tl.pca(adata: AnnData, **kwargs) -> None: ... ```. and. ```py. def scanpy.lowlevel.tl.pca(data: np.ndarray | sp.spmatrix, n_pcs) -> np.ndarray: ... ```. Where the former is a wrapper for the latter. This allows to separate the implementation of the actual method using only numpy/scipy data types from the scverse-specific behavior. .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:399,usability,hint,hints,399,"Remove inplace argument in scanpy 2.0; ### What kind of feature would you like to request? Other? ### Please describe your wishes. Another point that I'd like to throw into the scanpy 2.0 discussion: . Right now, many functions have the `inplace` argument, that determines if a function should write back to `adata` or return the result instead. . With this behavior it is hard to make correct type hints. While it is possible with `@overload`, it is cumbersome because it requires to type out the entire function signature twice. When I asked if it is possible to write these overloads in a more concise way [on stackoverflow](https://stackoverflow.com/questions/75757890/python-overload-single-argument?noredirect=1#comment133653817_75757890) several users argued that changing the return type based on an argument is an anti-pattern, and I think they convinced me. . ## Alternative approach. Have two API levels, e.g. ```py. def scanpy.tl.pca(adata: AnnData, **kwargs) -> None: ... ```. and. ```py. def scanpy.lowlevel.tl.pca(data: np.ndarray | sp.spmatrix, n_pcs) -> np.ndarray: ... ```. Where the former is a wrapper for the latter. This allows to separate the implementation of the actual method using only numpy/scipy data types from the scverse-specific behavior. .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:753,usability,user,users,753,"Remove inplace argument in scanpy 2.0; ### What kind of feature would you like to request? Other? ### Please describe your wishes. Another point that I'd like to throw into the scanpy 2.0 discussion: . Right now, many functions have the `inplace` argument, that determines if a function should write back to `adata` or return the result instead. . With this behavior it is hard to make correct type hints. While it is possible with `@overload`, it is cumbersome because it requires to type out the entire function signature twice. When I asked if it is possible to write these overloads in a more concise way [on stackoverflow](https://stackoverflow.com/questions/75757890/python-overload-single-argument?noredirect=1#comment133653817_75757890) several users argued that changing the return type based on an argument is an anti-pattern, and I think they convinced me. . ## Alternative approach. Have two API levels, e.g. ```py. def scanpy.tl.pca(adata: AnnData, **kwargs) -> None: ... ```. and. ```py. def scanpy.lowlevel.tl.pca(data: np.ndarray | sp.spmatrix, n_pcs) -> np.ndarray: ... ```. Where the former is a wrapper for the latter. This allows to separate the implementation of the actual method using only numpy/scipy data types from the scverse-specific behavior. .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2583:1262,usability,behavi,behavior,1262,"Remove inplace argument in scanpy 2.0; ### What kind of feature would you like to request? Other? ### Please describe your wishes. Another point that I'd like to throw into the scanpy 2.0 discussion: . Right now, many functions have the `inplace` argument, that determines if a function should write back to `adata` or return the result instead. . With this behavior it is hard to make correct type hints. While it is possible with `@overload`, it is cumbersome because it requires to type out the entire function signature twice. When I asked if it is possible to write these overloads in a more concise way [on stackoverflow](https://stackoverflow.com/questions/75757890/python-overload-single-argument?noredirect=1#comment133653817_75757890) several users argued that changing the return type based on an argument is an anti-pattern, and I think they convinced me. . ## Alternative approach. Have two API levels, e.g. ```py. def scanpy.tl.pca(adata: AnnData, **kwargs) -> None: ... ```. and. ```py. def scanpy.lowlevel.tl.pca(data: np.ndarray | sp.spmatrix, n_pcs) -> np.ndarray: ... ```. Where the former is a wrapper for the latter. This allows to separate the implementation of the actual method using only numpy/scipy data types from the scverse-specific behavior. .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583
https://github.com/scverse/scanpy/issues/2584:108,modifiability,paramet,parameters,108,"Transparent spots for spatial plot; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Maybe it is already possible. I would like to have transparent spots for spots that have zero counts (or near zero). I am using the function `scanpy.pl.spatial` An example shown below for a 10x Visium dataset. Can this be done? . <img width=""516"" alt=""Screen Shot 2023-08-02 at 7 48 59 pm"" src=""https://github.com/scverse/scanpy/assets/32261323/00b4f62f-6808-4226-93f9-d973f01274bd"">.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2584
https://github.com/scverse/scanpy/pull/2585:96,availability,consist,consistent,96,"Fix returns in the doc of sc.pl.violin; Close #2135. Changed the docstring of `pl.violin` to be consistent with the code, and docs of other similar plotting functions. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2585
https://github.com/scverse/scanpy/pull/2585:387,safety,review,review,387,"Fix returns in the doc of sc.pl.violin; Close #2135. Changed the docstring of `pl.violin` to be consistent with the code, and docs of other similar plotting functions. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2585
https://github.com/scverse/scanpy/pull/2585:387,testability,review,review,387,"Fix returns in the doc of sc.pl.violin; Close #2135. Changed the docstring of `pl.violin` to be consistent with the code, and docs of other similar plotting functions. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2585
https://github.com/scverse/scanpy/pull/2585:40,usability,Close,Close,40,"Fix returns in the doc of sc.pl.violin; Close #2135. Changed the docstring of `pl.violin` to be consistent with the code, and docs of other similar plotting functions. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2585
https://github.com/scverse/scanpy/pull/2585:96,usability,consist,consistent,96,"Fix returns in the doc of sc.pl.violin; Close #2135. Changed the docstring of `pl.violin` to be consistent with the code, and docs of other similar plotting functions. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2585
https://github.com/scverse/scanpy/pull/2585:238,usability,guid,guidelines,238,"Fix returns in the doc of sc.pl.violin; Close #2135. Changed the docstring of `pl.violin` to be consistent with the code, and docs of other similar plotting functions. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2585
https://github.com/scverse/scanpy/pull/2585:269,usability,guid,guide,269,"Fix returns in the doc of sc.pl.violin; Close #2135. Changed the docstring of `pl.violin` to be consistent with the code, and docs of other similar plotting functions. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2585
https://github.com/scverse/scanpy/pull/2585:365,usability,workflow,workflow,365,"Fix returns in the doc of sc.pl.violin; Close #2135. Changed the docstring of `pl.violin` to be consistent with the code, and docs of other similar plotting functions. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2585
https://github.com/scverse/scanpy/issues/2586:1255,availability,cluster,cluster-,1255,"irmed this bug exists on the master branch of scanpy. ### What happened? When I run sc.rank_genes_groups() on my gene expression data, I do get separate matrices for names, scores, pvals, and pvals_adj. However, the scores don't match the pvals. In other words, with decreasing pvals, we don't always have increasing score. I did check out the description on the main documentations page, and they say they're calculating the zscores underlying the distribution, however if that's the case shouldn't it always be higher with decreasing pval? Also, I went through the code: it looks like they're calculating the scores on the absolute values instead of the real values--why is this? Are the scores basically U1 values corresponding to the pvalues, in whcih case once again lower pvalues should always have higher scores right? ares calculated from the p-values? What's the relation between the two. I have ran sc.rank_genes_groups() on my gene expression data, and I have generated the matrix for cluster-1 versus the rest, for reference. You can see that one, the pavlues don't increase as we go down the rows; and two, the scores seem kinda arbitrary to the p-values. What am I missing here? Thanks a lot, and sorry for the wordy question. [NOTE: This doesn't just depend on a specific type of clustering technique. Irrespective of whether I try the in-built Leiden clustering technique, or the Schit library, or even cluster the cells randomly, this issue keeps occuring--which makes me think it's ether a bug, or I don't quite understand how the score generation works, maybe both. I've provided a little code snipper below.]. [ClusterOneVsRest.csv](https://github.com/scverse/scanpy/files/12243486/ClusterOneVsRest.csv). ### Minimal code sample. ```python. import pickle. import numpy as np. import pandas as pd. from PIL import Image. import glob. import matplotlib.pyplot as plt. from skimage.morphology import convex_hull_image. from skimage import data, img_as_float. from skimage.util import",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:1355,availability,down,down,1355,"groups() on my gene expression data, I do get separate matrices for names, scores, pvals, and pvals_adj. However, the scores don't match the pvals. In other words, with decreasing pvals, we don't always have increasing score. I did check out the description on the main documentations page, and they say they're calculating the zscores underlying the distribution, however if that's the case shouldn't it always be higher with decreasing pval? Also, I went through the code: it looks like they're calculating the scores on the absolute values instead of the real values--why is this? Are the scores basically U1 values corresponding to the pvalues, in whcih case once again lower pvalues should always have higher scores right? ares calculated from the p-values? What's the relation between the two. I have ran sc.rank_genes_groups() on my gene expression data, and I have generated the matrix for cluster-1 versus the rest, for reference. You can see that one, the pavlues don't increase as we go down the rows; and two, the scores seem kinda arbitrary to the p-values. What am I missing here? Thanks a lot, and sorry for the wordy question. [NOTE: This doesn't just depend on a specific type of clustering technique. Irrespective of whether I try the in-built Leiden clustering technique, or the Schit library, or even cluster the cells randomly, this issue keeps occuring--which makes me think it's ether a bug, or I don't quite understand how the score generation works, maybe both. I've provided a little code snipper below.]. [ClusterOneVsRest.csv](https://github.com/scverse/scanpy/files/12243486/ClusterOneVsRest.csv). ### Minimal code sample. ```python. import pickle. import numpy as np. import pandas as pd. from PIL import Image. import glob. import matplotlib.pyplot as plt. from skimage.morphology import convex_hull_image. from skimage import data, img_as_float. from skimage.util import invert. from scipy.spatial import ConvexHull, convex_hull_plot_2d. from multiprocessing import Po",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:1554,availability,cluster,clustering,1554," have increasing score. I did check out the description on the main documentations page, and they say they're calculating the zscores underlying the distribution, however if that's the case shouldn't it always be higher with decreasing pval? Also, I went through the code: it looks like they're calculating the scores on the absolute values instead of the real values--why is this? Are the scores basically U1 values corresponding to the pvalues, in whcih case once again lower pvalues should always have higher scores right? ares calculated from the p-values? What's the relation between the two. I have ran sc.rank_genes_groups() on my gene expression data, and I have generated the matrix for cluster-1 versus the rest, for reference. You can see that one, the pavlues don't increase as we go down the rows; and two, the scores seem kinda arbitrary to the p-values. What am I missing here? Thanks a lot, and sorry for the wordy question. [NOTE: This doesn't just depend on a specific type of clustering technique. Irrespective of whether I try the in-built Leiden clustering technique, or the Schit library, or even cluster the cells randomly, this issue keeps occuring--which makes me think it's ether a bug, or I don't quite understand how the score generation works, maybe both. I've provided a little code snipper below.]. [ClusterOneVsRest.csv](https://github.com/scverse/scanpy/files/12243486/ClusterOneVsRest.csv). ### Minimal code sample. ```python. import pickle. import numpy as np. import pandas as pd. from PIL import Image. import glob. import matplotlib.pyplot as plt. from skimage.morphology import convex_hull_image. from skimage import data, img_as_float. from skimage.util import invert. from scipy.spatial import ConvexHull, convex_hull_plot_2d. from multiprocessing import Pool. import time. import math. from collections import Counter. import scanpy as sc. import networkx as nx. import pandas as pd. import numpy as np. import itertools. import random. from scipy.stats impo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:1626,availability,cluster,clustering,1626,"mentations page, and they say they're calculating the zscores underlying the distribution, however if that's the case shouldn't it always be higher with decreasing pval? Also, I went through the code: it looks like they're calculating the scores on the absolute values instead of the real values--why is this? Are the scores basically U1 values corresponding to the pvalues, in whcih case once again lower pvalues should always have higher scores right? ares calculated from the p-values? What's the relation between the two. I have ran sc.rank_genes_groups() on my gene expression data, and I have generated the matrix for cluster-1 versus the rest, for reference. You can see that one, the pavlues don't increase as we go down the rows; and two, the scores seem kinda arbitrary to the p-values. What am I missing here? Thanks a lot, and sorry for the wordy question. [NOTE: This doesn't just depend on a specific type of clustering technique. Irrespective of whether I try the in-built Leiden clustering technique, or the Schit library, or even cluster the cells randomly, this issue keeps occuring--which makes me think it's ether a bug, or I don't quite understand how the score generation works, maybe both. I've provided a little code snipper below.]. [ClusterOneVsRest.csv](https://github.com/scverse/scanpy/files/12243486/ClusterOneVsRest.csv). ### Minimal code sample. ```python. import pickle. import numpy as np. import pandas as pd. from PIL import Image. import glob. import matplotlib.pyplot as plt. from skimage.morphology import convex_hull_image. from skimage import data, img_as_float. from skimage.util import invert. from scipy.spatial import ConvexHull, convex_hull_plot_2d. from multiprocessing import Pool. import time. import math. from collections import Counter. import scanpy as sc. import networkx as nx. import pandas as pd. import numpy as np. import itertools. import random. from scipy.stats import mannwhitneyu. import os. import warnings. import pickle. import matpl",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:1678,availability,cluster,cluster,1678,"he zscores underlying the distribution, however if that's the case shouldn't it always be higher with decreasing pval? Also, I went through the code: it looks like they're calculating the scores on the absolute values instead of the real values--why is this? Are the scores basically U1 values corresponding to the pvalues, in whcih case once again lower pvalues should always have higher scores right? ares calculated from the p-values? What's the relation between the two. I have ran sc.rank_genes_groups() on my gene expression data, and I have generated the matrix for cluster-1 versus the rest, for reference. You can see that one, the pavlues don't increase as we go down the rows; and two, the scores seem kinda arbitrary to the p-values. What am I missing here? Thanks a lot, and sorry for the wordy question. [NOTE: This doesn't just depend on a specific type of clustering technique. Irrespective of whether I try the in-built Leiden clustering technique, or the Schit library, or even cluster the cells randomly, this issue keeps occuring--which makes me think it's ether a bug, or I don't quite understand how the score generation works, maybe both. I've provided a little code snipper below.]. [ClusterOneVsRest.csv](https://github.com/scverse/scanpy/files/12243486/ClusterOneVsRest.csv). ### Minimal code sample. ```python. import pickle. import numpy as np. import pandas as pd. from PIL import Image. import glob. import matplotlib.pyplot as plt. from skimage.morphology import convex_hull_image. from skimage import data, img_as_float. from skimage.util import invert. from scipy.spatial import ConvexHull, convex_hull_plot_2d. from multiprocessing import Pool. import time. import math. from collections import Counter. import scanpy as sc. import networkx as nx. import pandas as pd. import numpy as np. import itertools. import random. from scipy.stats import mannwhitneyu. import os. import warnings. import pickle. import matplotlib.pyplot as plt. import pandas as pd. import gs",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:1890,availability,Cluster,ClusterOneVsRest,1890,"s instead of the real values--why is this? Are the scores basically U1 values corresponding to the pvalues, in whcih case once again lower pvalues should always have higher scores right? ares calculated from the p-values? What's the relation between the two. I have ran sc.rank_genes_groups() on my gene expression data, and I have generated the matrix for cluster-1 versus the rest, for reference. You can see that one, the pavlues don't increase as we go down the rows; and two, the scores seem kinda arbitrary to the p-values. What am I missing here? Thanks a lot, and sorry for the wordy question. [NOTE: This doesn't just depend on a specific type of clustering technique. Irrespective of whether I try the in-built Leiden clustering technique, or the Schit library, or even cluster the cells randomly, this issue keeps occuring--which makes me think it's ether a bug, or I don't quite understand how the score generation works, maybe both. I've provided a little code snipper below.]. [ClusterOneVsRest.csv](https://github.com/scverse/scanpy/files/12243486/ClusterOneVsRest.csv). ### Minimal code sample. ```python. import pickle. import numpy as np. import pandas as pd. from PIL import Image. import glob. import matplotlib.pyplot as plt. from skimage.morphology import convex_hull_image. from skimage import data, img_as_float. from skimage.util import invert. from scipy.spatial import ConvexHull, convex_hull_plot_2d. from multiprocessing import Pool. import time. import math. from collections import Counter. import scanpy as sc. import networkx as nx. import pandas as pd. import numpy as np. import itertools. import random. from scipy.stats import mannwhitneyu. import os. import warnings. import pickle. import matplotlib.pyplot as plt. import pandas as pd. import gseapy as gp. from matplotlib import pyplot as plt. from matplotlib_venn import venn2. import mygene. import seaborn as sns. from gseapy import barplot, dotplot. import random. import matplotlib.pyplot as plt. import n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:1961,availability,Cluster,ClusterOneVsRest,1961,"values corresponding to the pvalues, in whcih case once again lower pvalues should always have higher scores right? ares calculated from the p-values? What's the relation between the two. I have ran sc.rank_genes_groups() on my gene expression data, and I have generated the matrix for cluster-1 versus the rest, for reference. You can see that one, the pavlues don't increase as we go down the rows; and two, the scores seem kinda arbitrary to the p-values. What am I missing here? Thanks a lot, and sorry for the wordy question. [NOTE: This doesn't just depend on a specific type of clustering technique. Irrespective of whether I try the in-built Leiden clustering technique, or the Schit library, or even cluster the cells randomly, this issue keeps occuring--which makes me think it's ether a bug, or I don't quite understand how the score generation works, maybe both. I've provided a little code snipper below.]. [ClusterOneVsRest.csv](https://github.com/scverse/scanpy/files/12243486/ClusterOneVsRest.csv). ### Minimal code sample. ```python. import pickle. import numpy as np. import pandas as pd. from PIL import Image. import glob. import matplotlib.pyplot as plt. from skimage.morphology import convex_hull_image. from skimage import data, img_as_float. from skimage.util import invert. from scipy.spatial import ConvexHull, convex_hull_plot_2d. from multiprocessing import Pool. import time. import math. from collections import Counter. import scanpy as sc. import networkx as nx. import pandas as pd. import numpy as np. import itertools. import random. from scipy.stats import mannwhitneyu. import os. import warnings. import pickle. import matplotlib.pyplot as plt. import pandas as pd. import gseapy as gp. from matplotlib import pyplot as plt. from matplotlib_venn import venn2. import mygene. import seaborn as sns. from gseapy import barplot, dotplot. import random. import matplotlib.pyplot as plt. import numpy as np. import decoupler as dc. from numpy.random import default_rn",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:4093,availability,Error,Error,4093,"borhood_enrichment. import schist as scs. with open('TNBC_41patients_KerenEtAl.pkl', 'rb') as f:. pickle_= pickle.load(f). pickle_=dict(list(pickle_.items())[7:8]). for i in pickle_:. cnt+=1. adata=pickle_[i]. sc.pp.neighbors(adata, n_neighbors=10, n_pcs=3). sc.tl.leiden(adata). scs.inference.planted_model(adata). sc.pp.scale(adata). sc.tl.rank_genes_groups(adata, groupby='ppbm', pts=True, method='wilcoxon'). adata_rankGenes_names, adata_rankGenes_scores, adata_rankGenes_logfoldchanges, adata_rankGenes_pvals, adata_rankGenes_pvals_adj, adata_rankGenes_pts, adata_rankGenes_pts_rest=pd.DataFrame(adata.uns['rank_genes_groups']['names']), pd.DataFrame(adata.uns['rank_genes_groups']['scores']), pd.DataFrame(adata.uns['rank_genes_groups']['logfoldchanges']), pd.DataFrame(adata.uns['rank_genes_groups']['pvals']), pd.DataFrame(adata.uns['rank_genes_groups']['pvals_adj']), pd.DataFrame(adata.uns['rank_genes_groups']['pts']), pd.DataFrame(adata.uns['rank_genes_groups']['pts_rest']). ```. ### Error output. ```pytb. It's not really an error, it's just unexpected output. 1. https://github.com/scverse/scanpy/issues/701 says the p-values should always decrease as we go down the rows of the output matrix, which would make sense because the scores are ordered in decreasing order. 2. https://github.com/scverse/scanpy/issues/1688 says that higher score should always correspond to lower p-value, but that's not what's happening. 3. The official documentation says: ""Structured array to be indexed by group id storing the z-score underlying the computation of a p-value for each gene for each group. Ordered according to scores."" --> But if that's the case, why are the scores and p-values not perfectly opposite (that is lower score always equals higher p-value. Also, aren't you generating the U-value corresponding to the p-value?). ```. ### Versions. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 9.0.1. PyQt5 NA. appdirs 1.4.4. appnope 0.1.2. asciitree NA. asttokens NA. atomi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:4135,availability,error,error,4135,"with open('TNBC_41patients_KerenEtAl.pkl', 'rb') as f:. pickle_= pickle.load(f). pickle_=dict(list(pickle_.items())[7:8]). for i in pickle_:. cnt+=1. adata=pickle_[i]. sc.pp.neighbors(adata, n_neighbors=10, n_pcs=3). sc.tl.leiden(adata). scs.inference.planted_model(adata). sc.pp.scale(adata). sc.tl.rank_genes_groups(adata, groupby='ppbm', pts=True, method='wilcoxon'). adata_rankGenes_names, adata_rankGenes_scores, adata_rankGenes_logfoldchanges, adata_rankGenes_pvals, adata_rankGenes_pvals_adj, adata_rankGenes_pts, adata_rankGenes_pts_rest=pd.DataFrame(adata.uns['rank_genes_groups']['names']), pd.DataFrame(adata.uns['rank_genes_groups']['scores']), pd.DataFrame(adata.uns['rank_genes_groups']['logfoldchanges']), pd.DataFrame(adata.uns['rank_genes_groups']['pvals']), pd.DataFrame(adata.uns['rank_genes_groups']['pvals_adj']), pd.DataFrame(adata.uns['rank_genes_groups']['pts']), pd.DataFrame(adata.uns['rank_genes_groups']['pts_rest']). ```. ### Error output. ```pytb. It's not really an error, it's just unexpected output. 1. https://github.com/scverse/scanpy/issues/701 says the p-values should always decrease as we go down the rows of the output matrix, which would make sense because the scores are ordered in decreasing order. 2. https://github.com/scverse/scanpy/issues/1688 says that higher score should always correspond to lower p-value, but that's not what's happening. 3. The official documentation says: ""Structured array to be indexed by group id storing the z-score underlying the computation of a p-value for each gene for each group. Ordered according to scores."" --> But if that's the case, why are the scores and p-values not perfectly opposite (that is lower score always equals higher p-value. Also, aren't you generating the U-value corresponding to the p-value?). ```. ### Versions. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 9.0.1. PyQt5 NA. appdirs 1.4.4. appnope 0.1.2. asciitree NA. asttokens NA. atomicwrites 1.4.0. attr 22.1.0. backcall 0.2.0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:4269,availability,down,down,4269,"ickle_:. cnt+=1. adata=pickle_[i]. sc.pp.neighbors(adata, n_neighbors=10, n_pcs=3). sc.tl.leiden(adata). scs.inference.planted_model(adata). sc.pp.scale(adata). sc.tl.rank_genes_groups(adata, groupby='ppbm', pts=True, method='wilcoxon'). adata_rankGenes_names, adata_rankGenes_scores, adata_rankGenes_logfoldchanges, adata_rankGenes_pvals, adata_rankGenes_pvals_adj, adata_rankGenes_pts, adata_rankGenes_pts_rest=pd.DataFrame(adata.uns['rank_genes_groups']['names']), pd.DataFrame(adata.uns['rank_genes_groups']['scores']), pd.DataFrame(adata.uns['rank_genes_groups']['logfoldchanges']), pd.DataFrame(adata.uns['rank_genes_groups']['pvals']), pd.DataFrame(adata.uns['rank_genes_groups']['pvals_adj']), pd.DataFrame(adata.uns['rank_genes_groups']['pts']), pd.DataFrame(adata.uns['rank_genes_groups']['pts_rest']). ```. ### Error output. ```pytb. It's not really an error, it's just unexpected output. 1. https://github.com/scverse/scanpy/issues/701 says the p-values should always decrease as we go down the rows of the output matrix, which would make sense because the scores are ordered in decreasing order. 2. https://github.com/scverse/scanpy/issues/1688 says that higher score should always correspond to lower p-value, but that's not what's happening. 3. The official documentation says: ""Structured array to be indexed by group id storing the z-score underlying the computation of a p-value for each gene for each group. Ordered according to scores."" --> But if that's the case, why are the scores and p-values not perfectly opposite (that is lower score always equals higher p-value. Also, aren't you generating the U-value corresponding to the p-value?). ```. ### Versions. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 9.0.1. PyQt5 NA. appdirs 1.4.4. appnope 0.1.2. asciitree NA. asttokens NA. atomicwrites 1.4.0. attr 22.1.0. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. biothings_client 0.3.0. brotli NA. cairo 1.23.0. certifi 2023.07.22. cffi 1.15.1. chardet 4.0.0. cha",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:212,deployability,version,version,212,"Question regarding: sc.rank_genes_groups(); ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When I run sc.rank_genes_groups() on my gene expression data, I do get separate matrices for names, scores, pvals, and pvals_adj. However, the scores don't match the pvals. In other words, with decreasing pvals, we don't always have increasing score. I did check out the description on the main documentations page, and they say they're calculating the zscores underlying the distribution, however if that's the case shouldn't it always be higher with decreasing pval? Also, I went through the code: it looks like they're calculating the scores on the absolute values instead of the real values--why is this? Are the scores basically U1 values corresponding to the pvalues, in whcih case once again lower pvalues should always have higher scores right? ares calculated from the p-values? What's the relation between the two. I have ran sc.rank_genes_groups() on my gene expression data, and I have generated the matrix for cluster-1 versus the rest, for reference. You can see that one, the pavlues don't increase as we go down the rows; and two, the scores seem kinda arbitrary to the p-values. What am I missing here? Thanks a lot, and sorry for the wordy question. [NOTE: This doesn't just depend on a specific type of clustering technique. Irrespective of whether I try the in-built Leiden clustering technique, or the Schit library, or even cluster the cells randomly, this issue keeps occuring--which makes me think it's ether a bug, or I don't quite understand how the score generation works, maybe both. I've provided a little code snipper below.]. [ClusterOneVsRest.csv](https://github.com/scverse/scanpy/files/12243486/ClusterOneVsRest.csv). ### Minimal code",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:1255,deployability,cluster,cluster-,1255,"irmed this bug exists on the master branch of scanpy. ### What happened? When I run sc.rank_genes_groups() on my gene expression data, I do get separate matrices for names, scores, pvals, and pvals_adj. However, the scores don't match the pvals. In other words, with decreasing pvals, we don't always have increasing score. I did check out the description on the main documentations page, and they say they're calculating the zscores underlying the distribution, however if that's the case shouldn't it always be higher with decreasing pval? Also, I went through the code: it looks like they're calculating the scores on the absolute values instead of the real values--why is this? Are the scores basically U1 values corresponding to the pvalues, in whcih case once again lower pvalues should always have higher scores right? ares calculated from the p-values? What's the relation between the two. I have ran sc.rank_genes_groups() on my gene expression data, and I have generated the matrix for cluster-1 versus the rest, for reference. You can see that one, the pavlues don't increase as we go down the rows; and two, the scores seem kinda arbitrary to the p-values. What am I missing here? Thanks a lot, and sorry for the wordy question. [NOTE: This doesn't just depend on a specific type of clustering technique. Irrespective of whether I try the in-built Leiden clustering technique, or the Schit library, or even cluster the cells randomly, this issue keeps occuring--which makes me think it's ether a bug, or I don't quite understand how the score generation works, maybe both. I've provided a little code snipper below.]. [ClusterOneVsRest.csv](https://github.com/scverse/scanpy/files/12243486/ClusterOneVsRest.csv). ### Minimal code sample. ```python. import pickle. import numpy as np. import pandas as pd. from PIL import Image. import glob. import matplotlib.pyplot as plt. from skimage.morphology import convex_hull_image. from skimage import data, img_as_float. from skimage.util import",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:1525,deployability,depend,depend,1525,"creasing pvals, we don't always have increasing score. I did check out the description on the main documentations page, and they say they're calculating the zscores underlying the distribution, however if that's the case shouldn't it always be higher with decreasing pval? Also, I went through the code: it looks like they're calculating the scores on the absolute values instead of the real values--why is this? Are the scores basically U1 values corresponding to the pvalues, in whcih case once again lower pvalues should always have higher scores right? ares calculated from the p-values? What's the relation between the two. I have ran sc.rank_genes_groups() on my gene expression data, and I have generated the matrix for cluster-1 versus the rest, for reference. You can see that one, the pavlues don't increase as we go down the rows; and two, the scores seem kinda arbitrary to the p-values. What am I missing here? Thanks a lot, and sorry for the wordy question. [NOTE: This doesn't just depend on a specific type of clustering technique. Irrespective of whether I try the in-built Leiden clustering technique, or the Schit library, or even cluster the cells randomly, this issue keeps occuring--which makes me think it's ether a bug, or I don't quite understand how the score generation works, maybe both. I've provided a little code snipper below.]. [ClusterOneVsRest.csv](https://github.com/scverse/scanpy/files/12243486/ClusterOneVsRest.csv). ### Minimal code sample. ```python. import pickle. import numpy as np. import pandas as pd. from PIL import Image. import glob. import matplotlib.pyplot as plt. from skimage.morphology import convex_hull_image. from skimage import data, img_as_float. from skimage.util import invert. from scipy.spatial import ConvexHull, convex_hull_plot_2d. from multiprocessing import Pool. import time. import math. from collections import Counter. import scanpy as sc. import networkx as nx. import pandas as pd. import numpy as np. import itertools. impor",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:1554,deployability,cluster,clustering,1554," have increasing score. I did check out the description on the main documentations page, and they say they're calculating the zscores underlying the distribution, however if that's the case shouldn't it always be higher with decreasing pval? Also, I went through the code: it looks like they're calculating the scores on the absolute values instead of the real values--why is this? Are the scores basically U1 values corresponding to the pvalues, in whcih case once again lower pvalues should always have higher scores right? ares calculated from the p-values? What's the relation between the two. I have ran sc.rank_genes_groups() on my gene expression data, and I have generated the matrix for cluster-1 versus the rest, for reference. You can see that one, the pavlues don't increase as we go down the rows; and two, the scores seem kinda arbitrary to the p-values. What am I missing here? Thanks a lot, and sorry for the wordy question. [NOTE: This doesn't just depend on a specific type of clustering technique. Irrespective of whether I try the in-built Leiden clustering technique, or the Schit library, or even cluster the cells randomly, this issue keeps occuring--which makes me think it's ether a bug, or I don't quite understand how the score generation works, maybe both. I've provided a little code snipper below.]. [ClusterOneVsRest.csv](https://github.com/scverse/scanpy/files/12243486/ClusterOneVsRest.csv). ### Minimal code sample. ```python. import pickle. import numpy as np. import pandas as pd. from PIL import Image. import glob. import matplotlib.pyplot as plt. from skimage.morphology import convex_hull_image. from skimage import data, img_as_float. from skimage.util import invert. from scipy.spatial import ConvexHull, convex_hull_plot_2d. from multiprocessing import Pool. import time. import math. from collections import Counter. import scanpy as sc. import networkx as nx. import pandas as pd. import numpy as np. import itertools. import random. from scipy.stats impo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:1626,deployability,cluster,clustering,1626,"mentations page, and they say they're calculating the zscores underlying the distribution, however if that's the case shouldn't it always be higher with decreasing pval? Also, I went through the code: it looks like they're calculating the scores on the absolute values instead of the real values--why is this? Are the scores basically U1 values corresponding to the pvalues, in whcih case once again lower pvalues should always have higher scores right? ares calculated from the p-values? What's the relation between the two. I have ran sc.rank_genes_groups() on my gene expression data, and I have generated the matrix for cluster-1 versus the rest, for reference. You can see that one, the pavlues don't increase as we go down the rows; and two, the scores seem kinda arbitrary to the p-values. What am I missing here? Thanks a lot, and sorry for the wordy question. [NOTE: This doesn't just depend on a specific type of clustering technique. Irrespective of whether I try the in-built Leiden clustering technique, or the Schit library, or even cluster the cells randomly, this issue keeps occuring--which makes me think it's ether a bug, or I don't quite understand how the score generation works, maybe both. I've provided a little code snipper below.]. [ClusterOneVsRest.csv](https://github.com/scverse/scanpy/files/12243486/ClusterOneVsRest.csv). ### Minimal code sample. ```python. import pickle. import numpy as np. import pandas as pd. from PIL import Image. import glob. import matplotlib.pyplot as plt. from skimage.morphology import convex_hull_image. from skimage import data, img_as_float. from skimage.util import invert. from scipy.spatial import ConvexHull, convex_hull_plot_2d. from multiprocessing import Pool. import time. import math. from collections import Counter. import scanpy as sc. import networkx as nx. import pandas as pd. import numpy as np. import itertools. import random. from scipy.stats import mannwhitneyu. import os. import warnings. import pickle. import matpl",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:1678,deployability,cluster,cluster,1678,"he zscores underlying the distribution, however if that's the case shouldn't it always be higher with decreasing pval? Also, I went through the code: it looks like they're calculating the scores on the absolute values instead of the real values--why is this? Are the scores basically U1 values corresponding to the pvalues, in whcih case once again lower pvalues should always have higher scores right? ares calculated from the p-values? What's the relation between the two. I have ran sc.rank_genes_groups() on my gene expression data, and I have generated the matrix for cluster-1 versus the rest, for reference. You can see that one, the pavlues don't increase as we go down the rows; and two, the scores seem kinda arbitrary to the p-values. What am I missing here? Thanks a lot, and sorry for the wordy question. [NOTE: This doesn't just depend on a specific type of clustering technique. Irrespective of whether I try the in-built Leiden clustering technique, or the Schit library, or even cluster the cells randomly, this issue keeps occuring--which makes me think it's ether a bug, or I don't quite understand how the score generation works, maybe both. I've provided a little code snipper below.]. [ClusterOneVsRest.csv](https://github.com/scverse/scanpy/files/12243486/ClusterOneVsRest.csv). ### Minimal code sample. ```python. import pickle. import numpy as np. import pandas as pd. from PIL import Image. import glob. import matplotlib.pyplot as plt. from skimage.morphology import convex_hull_image. from skimage import data, img_as_float. from skimage.util import invert. from scipy.spatial import ConvexHull, convex_hull_plot_2d. from multiprocessing import Pool. import time. import math. from collections import Counter. import scanpy as sc. import networkx as nx. import pandas as pd. import numpy as np. import itertools. import random. from scipy.stats import mannwhitneyu. import os. import warnings. import pickle. import matplotlib.pyplot as plt. import pandas as pd. import gs",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:1890,deployability,Cluster,ClusterOneVsRest,1890,"s instead of the real values--why is this? Are the scores basically U1 values corresponding to the pvalues, in whcih case once again lower pvalues should always have higher scores right? ares calculated from the p-values? What's the relation between the two. I have ran sc.rank_genes_groups() on my gene expression data, and I have generated the matrix for cluster-1 versus the rest, for reference. You can see that one, the pavlues don't increase as we go down the rows; and two, the scores seem kinda arbitrary to the p-values. What am I missing here? Thanks a lot, and sorry for the wordy question. [NOTE: This doesn't just depend on a specific type of clustering technique. Irrespective of whether I try the in-built Leiden clustering technique, or the Schit library, or even cluster the cells randomly, this issue keeps occuring--which makes me think it's ether a bug, or I don't quite understand how the score generation works, maybe both. I've provided a little code snipper below.]. [ClusterOneVsRest.csv](https://github.com/scverse/scanpy/files/12243486/ClusterOneVsRest.csv). ### Minimal code sample. ```python. import pickle. import numpy as np. import pandas as pd. from PIL import Image. import glob. import matplotlib.pyplot as plt. from skimage.morphology import convex_hull_image. from skimage import data, img_as_float. from skimage.util import invert. from scipy.spatial import ConvexHull, convex_hull_plot_2d. from multiprocessing import Pool. import time. import math. from collections import Counter. import scanpy as sc. import networkx as nx. import pandas as pd. import numpy as np. import itertools. import random. from scipy.stats import mannwhitneyu. import os. import warnings. import pickle. import matplotlib.pyplot as plt. import pandas as pd. import gseapy as gp. from matplotlib import pyplot as plt. from matplotlib_venn import venn2. import mygene. import seaborn as sns. from gseapy import barplot, dotplot. import random. import matplotlib.pyplot as plt. import n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:1961,deployability,Cluster,ClusterOneVsRest,1961,"values corresponding to the pvalues, in whcih case once again lower pvalues should always have higher scores right? ares calculated from the p-values? What's the relation between the two. I have ran sc.rank_genes_groups() on my gene expression data, and I have generated the matrix for cluster-1 versus the rest, for reference. You can see that one, the pavlues don't increase as we go down the rows; and two, the scores seem kinda arbitrary to the p-values. What am I missing here? Thanks a lot, and sorry for the wordy question. [NOTE: This doesn't just depend on a specific type of clustering technique. Irrespective of whether I try the in-built Leiden clustering technique, or the Schit library, or even cluster the cells randomly, this issue keeps occuring--which makes me think it's ether a bug, or I don't quite understand how the score generation works, maybe both. I've provided a little code snipper below.]. [ClusterOneVsRest.csv](https://github.com/scverse/scanpy/files/12243486/ClusterOneVsRest.csv). ### Minimal code sample. ```python. import pickle. import numpy as np. import pandas as pd. from PIL import Image. import glob. import matplotlib.pyplot as plt. from skimage.morphology import convex_hull_image. from skimage import data, img_as_float. from skimage.util import invert. from scipy.spatial import ConvexHull, convex_hull_plot_2d. from multiprocessing import Pool. import time. import math. from collections import Counter. import scanpy as sc. import networkx as nx. import pandas as pd. import numpy as np. import itertools. import random. from scipy.stats import mannwhitneyu. import os. import warnings. import pickle. import matplotlib.pyplot as plt. import pandas as pd. import gseapy as gp. from matplotlib import pyplot as plt. from matplotlib_venn import venn2. import mygene. import seaborn as sns. from gseapy import barplot, dotplot. import random. import matplotlib.pyplot as plt. import numpy as np. import decoupler as dc. from numpy.random import default_rn",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:3418,deployability,scale,scale,3418,"mport scanpy as sc. import networkx as nx. import pandas as pd. import numpy as np. import itertools. import random. from scipy.stats import mannwhitneyu. import os. import warnings. import pickle. import matplotlib.pyplot as plt. import pandas as pd. import gseapy as gp. from matplotlib import pyplot as plt. from matplotlib_venn import venn2. import mygene. import seaborn as sns. from gseapy import barplot, dotplot. import random. import matplotlib.pyplot as plt. import numpy as np. import decoupler as dc. from numpy.random import default_rng. from copy import deepcopy. import scanpy as sc. import squidpy as sq. import time. from neighborhood_enrichment import neighborhood_enrichment. import schist as scs. with open('TNBC_41patients_KerenEtAl.pkl', 'rb') as f:. pickle_= pickle.load(f). pickle_=dict(list(pickle_.items())[7:8]). for i in pickle_:. cnt+=1. adata=pickle_[i]. sc.pp.neighbors(adata, n_neighbors=10, n_pcs=3). sc.tl.leiden(adata). scs.inference.planted_model(adata). sc.pp.scale(adata). sc.tl.rank_genes_groups(adata, groupby='ppbm', pts=True, method='wilcoxon'). adata_rankGenes_names, adata_rankGenes_scores, adata_rankGenes_logfoldchanges, adata_rankGenes_pvals, adata_rankGenes_pvals_adj, adata_rankGenes_pts, adata_rankGenes_pts_rest=pd.DataFrame(adata.uns['rank_genes_groups']['names']), pd.DataFrame(adata.uns['rank_genes_groups']['scores']), pd.DataFrame(adata.uns['rank_genes_groups']['logfoldchanges']), pd.DataFrame(adata.uns['rank_genes_groups']['pvals']), pd.DataFrame(adata.uns['rank_genes_groups']['pvals_adj']), pd.DataFrame(adata.uns['rank_genes_groups']['pts']), pd.DataFrame(adata.uns['rank_genes_groups']['pts_rest']). ```. ### Error output. ```pytb. It's not really an error, it's just unexpected output. 1. https://github.com/scverse/scanpy/issues/701 says the p-values should always decrease as we go down the rows of the output matrix, which would make sense because the scores are ordered in decreasing order. 2. https://github.com/scverse/scanpy/issu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:3840,deployability,log,logfoldchanges,3840,"t random. import matplotlib.pyplot as plt. import numpy as np. import decoupler as dc. from numpy.random import default_rng. from copy import deepcopy. import scanpy as sc. import squidpy as sq. import time. from neighborhood_enrichment import neighborhood_enrichment. import schist as scs. with open('TNBC_41patients_KerenEtAl.pkl', 'rb') as f:. pickle_= pickle.load(f). pickle_=dict(list(pickle_.items())[7:8]). for i in pickle_:. cnt+=1. adata=pickle_[i]. sc.pp.neighbors(adata, n_neighbors=10, n_pcs=3). sc.tl.leiden(adata). scs.inference.planted_model(adata). sc.pp.scale(adata). sc.tl.rank_genes_groups(adata, groupby='ppbm', pts=True, method='wilcoxon'). adata_rankGenes_names, adata_rankGenes_scores, adata_rankGenes_logfoldchanges, adata_rankGenes_pvals, adata_rankGenes_pvals_adj, adata_rankGenes_pts, adata_rankGenes_pts_rest=pd.DataFrame(adata.uns['rank_genes_groups']['names']), pd.DataFrame(adata.uns['rank_genes_groups']['scores']), pd.DataFrame(adata.uns['rank_genes_groups']['logfoldchanges']), pd.DataFrame(adata.uns['rank_genes_groups']['pvals']), pd.DataFrame(adata.uns['rank_genes_groups']['pvals_adj']), pd.DataFrame(adata.uns['rank_genes_groups']['pts']), pd.DataFrame(adata.uns['rank_genes_groups']['pts_rest']). ```. ### Error output. ```pytb. It's not really an error, it's just unexpected output. 1. https://github.com/scverse/scanpy/issues/701 says the p-values should always decrease as we go down the rows of the output matrix, which would make sense because the scores are ordered in decreasing order. 2. https://github.com/scverse/scanpy/issues/1688 says that higher score should always correspond to lower p-value, but that's not what's happening. 3. The official documentation says: ""Structured array to be indexed by group id storing the z-score underlying the computation of a p-value for each gene for each group. Ordered according to scores."" --> But if that's the case, why are the scores and p-values not perfectly opposite (that is lower score always equals h",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:4943,deployability,Version,Versions,4943,"s_groups']['pvals_adj']), pd.DataFrame(adata.uns['rank_genes_groups']['pts']), pd.DataFrame(adata.uns['rank_genes_groups']['pts_rest']). ```. ### Error output. ```pytb. It's not really an error, it's just unexpected output. 1. https://github.com/scverse/scanpy/issues/701 says the p-values should always decrease as we go down the rows of the output matrix, which would make sense because the scores are ordered in decreasing order. 2. https://github.com/scverse/scanpy/issues/1688 says that higher score should always correspond to lower p-value, but that's not what's happening. 3. The official documentation says: ""Structured array to be indexed by group id storing the z-score underlying the computation of a p-value for each gene for each group. Ordered according to scores."" --> But if that's the case, why are the scores and p-values not perfectly opposite (that is lower score always equals higher p-value. Also, aren't you generating the U-value corresponding to the p-value?). ```. ### Versions. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 9.0.1. PyQt5 NA. appdirs 1.4.4. appnope 0.1.2. asciitree NA. asttokens NA. atomicwrites 1.4.0. attr 22.1.0. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. biothings_client 0.3.0. brotli NA. cairo 1.23.0. certifi 2023.07.22. cffi 1.15.1. chardet 4.0.0. charset_normalizer 2.0.4. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.2. cv2 4.8.0. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dask_image 2023.03.0. datashader 0.15.1. datashape 0.5.2. datatree 0.0.12. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. decoupler 1.4.0. defusedxml 0.7.1. docrep 0.3.2. entrypoints 0.4. et_xmlfile 1.1.0. executing 0.8.3. fasteners 0.18. fontTools 4.25.0. fsspec 2023.4.0. geopandas 0.13.2. graph_tool 2.43 (commit , ). gseapy 1.0.5. h5py 3.7.0. hypergeom_ufunc NA. idna 3.4. igraph 0.10.6. imagecodecs 2021.8.26. imageio 2.31.1. ipykernel 6.19.2. jedi 0.18.1. jinja2 3.1.2. joblib 1.2.0. kiwisolver 1.4.4. lazy_loader NA. leide",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:7827,deployability,updat,updated,7827,"0.18.1. jinja2 3.1.2. joblib 1.2.0. kiwisolver 1.4.4. lazy_loader NA. leidenalg 0.10.1. llvmlite 0.40.0. markupsafe 2.1.1. matplotlib 3.5.3. matplotlib_inline 0.1.6. matplotlib_scalebar 0.8.1. matplotlib_venn 0.11.9. mkl 2.4.0. mpl_toolkits NA. msgpack 1.0.5. multipledispatch 0.6.0. multiscale_spatial_image 0.11.2. mygene 3.2.2. natsort 7.1.1. nbinom_ufunc NA. neighborhood_enrichment NA. networkx 3.1. numba 0.57.0. numcodecs 0.11.0. numpy 1.24.3. ome_zarr NA. openpyxl 3.1.2. packaging 23.0. pandas 2.0.3. param 1.13.0. parso 0.8.3. patsy 0.5.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 2.5.2. pooch v1.4.0. prompt_toolkit 3.0.36. psutil 5.9.0. ptyprocess 0.7.0. pure_eval 0.2.2. pyarrow 12.0.1. pycparser 2.21. pyct 0.5.0. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygeos 0.14. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pyproj 3.6.0. pytz 2022.7. pywt 1.4.1. requests 2.31.0. rich NA. rtree 1.0.1. schist v0.7.16. scipy 1.8.0. seaborn 0.12.2. session_info 1.0.0. setuptools 68.0.0. shapely 2.0.1. sip NA. six 1.16.0. skimage 0.20.0. sklearn 1.2.2. socks 1.7.1. spatial_image 0.3.0. spatialdata 0.0.12. sphinxcontrib NA. spyder 5.4.3. spyder_kernels 2.4.3. spydercustomize NA. squidpy 1.3.0. stack_data 0.2.0. statsmodels 0.14.0. tblib 2.0.0. texttable 1.6.7. threadpoolctl 2.2.0. tifffile 2021.7.2. tlz 0.12.0. toolz 0.12.0. tornado 6.3.2. tqdm 4.65.0. traitlets 5.7.1. typing_extensions NA. umap 0.5.3. urllib3 1.26.16. validators 0.20.0. wcwidth 0.2.5. wurlitzer 3.0.2. xarray 2022.12.0. xarray_dataclasses 1.6.0. xarray_schema 0.0.3. xrspatial 0.3.7. yaml 6.0. zarr 2.16.0. zipp NA. zmq 25.1.0. zoneinfo NA. zstandard 0.19.0. -----. IPython 8.12.0. jupyter_client 8.1.0. jupyter_core 5.3.0. -----. Python 3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:50:29) [Clang 14.0.6 ]. macOS-11.6-x86_64-i386-64bit. -----. Session information updated at 2023-08-02 20:20. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:3210,energy efficiency,load,load,3210,"rt data, img_as_float. from skimage.util import invert. from scipy.spatial import ConvexHull, convex_hull_plot_2d. from multiprocessing import Pool. import time. import math. from collections import Counter. import scanpy as sc. import networkx as nx. import pandas as pd. import numpy as np. import itertools. import random. from scipy.stats import mannwhitneyu. import os. import warnings. import pickle. import matplotlib.pyplot as plt. import pandas as pd. import gseapy as gp. from matplotlib import pyplot as plt. from matplotlib_venn import venn2. import mygene. import seaborn as sns. from gseapy import barplot, dotplot. import random. import matplotlib.pyplot as plt. import numpy as np. import decoupler as dc. from numpy.random import default_rng. from copy import deepcopy. import scanpy as sc. import squidpy as sq. import time. from neighborhood_enrichment import neighborhood_enrichment. import schist as scs. with open('TNBC_41patients_KerenEtAl.pkl', 'rb') as f:. pickle_= pickle.load(f). pickle_=dict(list(pickle_.items())[7:8]). for i in pickle_:. cnt+=1. adata=pickle_[i]. sc.pp.neighbors(adata, n_neighbors=10, n_pcs=3). sc.tl.leiden(adata). scs.inference.planted_model(adata). sc.pp.scale(adata). sc.tl.rank_genes_groups(adata, groupby='ppbm', pts=True, method='wilcoxon'). adata_rankGenes_names, adata_rankGenes_scores, adata_rankGenes_logfoldchanges, adata_rankGenes_pvals, adata_rankGenes_pvals_adj, adata_rankGenes_pts, adata_rankGenes_pts_rest=pd.DataFrame(adata.uns['rank_genes_groups']['names']), pd.DataFrame(adata.uns['rank_genes_groups']['scores']), pd.DataFrame(adata.uns['rank_genes_groups']['logfoldchanges']), pd.DataFrame(adata.uns['rank_genes_groups']['pvals']), pd.DataFrame(adata.uns['rank_genes_groups']['pvals_adj']), pd.DataFrame(adata.uns['rank_genes_groups']['pts']), pd.DataFrame(adata.uns['rank_genes_groups']['pts_rest']). ```. ### Error output. ```pytb. It's not really an error, it's just unexpected output. 1. https://github.com/scverse/scanpy/issu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:3418,energy efficiency,scale,scale,3418,"mport scanpy as sc. import networkx as nx. import pandas as pd. import numpy as np. import itertools. import random. from scipy.stats import mannwhitneyu. import os. import warnings. import pickle. import matplotlib.pyplot as plt. import pandas as pd. import gseapy as gp. from matplotlib import pyplot as plt. from matplotlib_venn import venn2. import mygene. import seaborn as sns. from gseapy import barplot, dotplot. import random. import matplotlib.pyplot as plt. import numpy as np. import decoupler as dc. from numpy.random import default_rng. from copy import deepcopy. import scanpy as sc. import squidpy as sq. import time. from neighborhood_enrichment import neighborhood_enrichment. import schist as scs. with open('TNBC_41patients_KerenEtAl.pkl', 'rb') as f:. pickle_= pickle.load(f). pickle_=dict(list(pickle_.items())[7:8]). for i in pickle_:. cnt+=1. adata=pickle_[i]. sc.pp.neighbors(adata, n_neighbors=10, n_pcs=3). sc.tl.leiden(adata). scs.inference.planted_model(adata). sc.pp.scale(adata). sc.tl.rank_genes_groups(adata, groupby='ppbm', pts=True, method='wilcoxon'). adata_rankGenes_names, adata_rankGenes_scores, adata_rankGenes_logfoldchanges, adata_rankGenes_pvals, adata_rankGenes_pvals_adj, adata_rankGenes_pts, adata_rankGenes_pts_rest=pd.DataFrame(adata.uns['rank_genes_groups']['names']), pd.DataFrame(adata.uns['rank_genes_groups']['scores']), pd.DataFrame(adata.uns['rank_genes_groups']['logfoldchanges']), pd.DataFrame(adata.uns['rank_genes_groups']['pvals']), pd.DataFrame(adata.uns['rank_genes_groups']['pvals_adj']), pd.DataFrame(adata.uns['rank_genes_groups']['pts']), pd.DataFrame(adata.uns['rank_genes_groups']['pts_rest']). ```. ### Error output. ```pytb. It's not really an error, it's just unexpected output. 1. https://github.com/scverse/scanpy/issues/701 says the p-values should always decrease as we go down the rows of the output matrix, which would make sense because the scores are ordered in decreasing order. 2. https://github.com/scverse/scanpy/issu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:5294,energy efficiency,cloud,cloudpickle,5294,"rix, which would make sense because the scores are ordered in decreasing order. 2. https://github.com/scverse/scanpy/issues/1688 says that higher score should always correspond to lower p-value, but that's not what's happening. 3. The official documentation says: ""Structured array to be indexed by group id storing the z-score underlying the computation of a p-value for each gene for each group. Ordered according to scores."" --> But if that's the case, why are the scores and p-values not perfectly opposite (that is lower score always equals higher p-value. Also, aren't you generating the U-value corresponding to the p-value?). ```. ### Versions. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 9.0.1. PyQt5 NA. appdirs 1.4.4. appnope 0.1.2. asciitree NA. asttokens NA. atomicwrites 1.4.0. attr 22.1.0. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. biothings_client 0.3.0. brotli NA. cairo 1.23.0. certifi 2023.07.22. cffi 1.15.1. chardet 4.0.0. charset_normalizer 2.0.4. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.2. cv2 4.8.0. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dask_image 2023.03.0. datashader 0.15.1. datashape 0.5.2. datatree 0.0.12. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. decoupler 1.4.0. defusedxml 0.7.1. docrep 0.3.2. entrypoints 0.4. et_xmlfile 1.1.0. executing 0.8.3. fasteners 0.18. fontTools 4.25.0. fsspec 2023.4.0. geopandas 0.13.2. graph_tool 2.43 (commit , ). gseapy 1.0.5. h5py 3.7.0. hypergeom_ufunc NA. idna 3.4. igraph 0.10.6. imagecodecs 2021.8.26. imageio 2.31.1. ipykernel 6.19.2. jedi 0.18.1. jinja2 3.1.2. joblib 1.2.0. kiwisolver 1.4.4. lazy_loader NA. leidenalg 0.10.1. llvmlite 0.40.0. markupsafe 2.1.1. matplotlib 3.5.3. matplotlib_inline 0.1.6. matplotlib_scalebar 0.8.1. matplotlib_venn 0.11.9. mkl 2.4.0. mpl_toolkits NA. msgpack 1.0.5. multipledispatch 0.6.0. multiscale_spatial_image 0.11.2. mygene 3.2.2. natsort 7.1.1. nbinom_ufunc NA. neighborhood_enrichment NA. networkx 3.1. numba 0.57.0. numcodecs",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:212,integrability,version,version,212,"Question regarding: sc.rank_genes_groups(); ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When I run sc.rank_genes_groups() on my gene expression data, I do get separate matrices for names, scores, pvals, and pvals_adj. However, the scores don't match the pvals. In other words, with decreasing pvals, we don't always have increasing score. I did check out the description on the main documentations page, and they say they're calculating the zscores underlying the distribution, however if that's the case shouldn't it always be higher with decreasing pval? Also, I went through the code: it looks like they're calculating the scores on the absolute values instead of the real values--why is this? Are the scores basically U1 values corresponding to the pvalues, in whcih case once again lower pvalues should always have higher scores right? ares calculated from the p-values? What's the relation between the two. I have ran sc.rank_genes_groups() on my gene expression data, and I have generated the matrix for cluster-1 versus the rest, for reference. You can see that one, the pavlues don't increase as we go down the rows; and two, the scores seem kinda arbitrary to the p-values. What am I missing here? Thanks a lot, and sorry for the wordy question. [NOTE: This doesn't just depend on a specific type of clustering technique. Irrespective of whether I try the in-built Leiden clustering technique, or the Schit library, or even cluster the cells randomly, this issue keeps occuring--which makes me think it's ether a bug, or I don't quite understand how the score generation works, maybe both. I've provided a little code snipper below.]. [ClusterOneVsRest.csv](https://github.com/scverse/scanpy/files/12243486/ClusterOneVsRest.csv). ### Minimal code",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:1525,integrability,depend,depend,1525,"creasing pvals, we don't always have increasing score. I did check out the description on the main documentations page, and they say they're calculating the zscores underlying the distribution, however if that's the case shouldn't it always be higher with decreasing pval? Also, I went through the code: it looks like they're calculating the scores on the absolute values instead of the real values--why is this? Are the scores basically U1 values corresponding to the pvalues, in whcih case once again lower pvalues should always have higher scores right? ares calculated from the p-values? What's the relation between the two. I have ran sc.rank_genes_groups() on my gene expression data, and I have generated the matrix for cluster-1 versus the rest, for reference. You can see that one, the pavlues don't increase as we go down the rows; and two, the scores seem kinda arbitrary to the p-values. What am I missing here? Thanks a lot, and sorry for the wordy question. [NOTE: This doesn't just depend on a specific type of clustering technique. Irrespective of whether I try the in-built Leiden clustering technique, or the Schit library, or even cluster the cells randomly, this issue keeps occuring--which makes me think it's ether a bug, or I don't quite understand how the score generation works, maybe both. I've provided a little code snipper below.]. [ClusterOneVsRest.csv](https://github.com/scverse/scanpy/files/12243486/ClusterOneVsRest.csv). ### Minimal code sample. ```python. import pickle. import numpy as np. import pandas as pd. from PIL import Image. import glob. import matplotlib.pyplot as plt. from skimage.morphology import convex_hull_image. from skimage import data, img_as_float. from skimage.util import invert. from scipy.spatial import ConvexHull, convex_hull_plot_2d. from multiprocessing import Pool. import time. import math. from collections import Counter. import scanpy as sc. import networkx as nx. import pandas as pd. import numpy as np. import itertools. impor",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:4943,integrability,Version,Versions,4943,"s_groups']['pvals_adj']), pd.DataFrame(adata.uns['rank_genes_groups']['pts']), pd.DataFrame(adata.uns['rank_genes_groups']['pts_rest']). ```. ### Error output. ```pytb. It's not really an error, it's just unexpected output. 1. https://github.com/scverse/scanpy/issues/701 says the p-values should always decrease as we go down the rows of the output matrix, which would make sense because the scores are ordered in decreasing order. 2. https://github.com/scverse/scanpy/issues/1688 says that higher score should always correspond to lower p-value, but that's not what's happening. 3. The official documentation says: ""Structured array to be indexed by group id storing the z-score underlying the computation of a p-value for each gene for each group. Ordered according to scores."" --> But if that's the case, why are the scores and p-values not perfectly opposite (that is lower score always equals higher p-value. Also, aren't you generating the U-value corresponding to the p-value?). ```. ### Versions. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 9.0.1. PyQt5 NA. appdirs 1.4.4. appnope 0.1.2. asciitree NA. asttokens NA. atomicwrites 1.4.0. attr 22.1.0. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. biothings_client 0.3.0. brotli NA. cairo 1.23.0. certifi 2023.07.22. cffi 1.15.1. chardet 4.0.0. charset_normalizer 2.0.4. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.2. cv2 4.8.0. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dask_image 2023.03.0. datashader 0.15.1. datashape 0.5.2. datatree 0.0.12. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. decoupler 1.4.0. defusedxml 0.7.1. docrep 0.3.2. entrypoints 0.4. et_xmlfile 1.1.0. executing 0.8.3. fasteners 0.18. fontTools 4.25.0. fsspec 2023.4.0. geopandas 0.13.2. graph_tool 2.43 (commit , ). gseapy 1.0.5. h5py 3.7.0. hypergeom_ufunc NA. idna 3.4. igraph 0.10.6. imagecodecs 2021.8.26. imageio 2.31.1. ipykernel 6.19.2. jedi 0.18.1. jinja2 3.1.2. joblib 1.2.0. kiwisolver 1.4.4. lazy_loader NA. leide",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:708,interoperability,distribut,distribution,708,"Question regarding: sc.rank_genes_groups(); ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When I run sc.rank_genes_groups() on my gene expression data, I do get separate matrices for names, scores, pvals, and pvals_adj. However, the scores don't match the pvals. In other words, with decreasing pvals, we don't always have increasing score. I did check out the description on the main documentations page, and they say they're calculating the zscores underlying the distribution, however if that's the case shouldn't it always be higher with decreasing pval? Also, I went through the code: it looks like they're calculating the scores on the absolute values instead of the real values--why is this? Are the scores basically U1 values corresponding to the pvalues, in whcih case once again lower pvalues should always have higher scores right? ares calculated from the p-values? What's the relation between the two. I have ran sc.rank_genes_groups() on my gene expression data, and I have generated the matrix for cluster-1 versus the rest, for reference. You can see that one, the pavlues don't increase as we go down the rows; and two, the scores seem kinda arbitrary to the p-values. What am I missing here? Thanks a lot, and sorry for the wordy question. [NOTE: This doesn't just depend on a specific type of clustering technique. Irrespective of whether I try the in-built Leiden clustering technique, or the Schit library, or even cluster the cells randomly, this issue keeps occuring--which makes me think it's ether a bug, or I don't quite understand how the score generation works, maybe both. I've provided a little code snipper below.]. [ClusterOneVsRest.csv](https://github.com/scverse/scanpy/files/12243486/ClusterOneVsRest.csv). ### Minimal code",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:1537,interoperability,specif,specific,1537,"s, we don't always have increasing score. I did check out the description on the main documentations page, and they say they're calculating the zscores underlying the distribution, however if that's the case shouldn't it always be higher with decreasing pval? Also, I went through the code: it looks like they're calculating the scores on the absolute values instead of the real values--why is this? Are the scores basically U1 values corresponding to the pvalues, in whcih case once again lower pvalues should always have higher scores right? ares calculated from the p-values? What's the relation between the two. I have ran sc.rank_genes_groups() on my gene expression data, and I have generated the matrix for cluster-1 versus the rest, for reference. You can see that one, the pavlues don't increase as we go down the rows; and two, the scores seem kinda arbitrary to the p-values. What am I missing here? Thanks a lot, and sorry for the wordy question. [NOTE: This doesn't just depend on a specific type of clustering technique. Irrespective of whether I try the in-built Leiden clustering technique, or the Schit library, or even cluster the cells randomly, this issue keeps occuring--which makes me think it's ether a bug, or I don't quite understand how the score generation works, maybe both. I've provided a little code snipper below.]. [ClusterOneVsRest.csv](https://github.com/scverse/scanpy/files/12243486/ClusterOneVsRest.csv). ### Minimal code sample. ```python. import pickle. import numpy as np. import pandas as pd. from PIL import Image. import glob. import matplotlib.pyplot as plt. from skimage.morphology import convex_hull_image. from skimage import data, img_as_float. from skimage.util import invert. from scipy.spatial import ConvexHull, convex_hull_plot_2d. from multiprocessing import Pool. import time. import math. from collections import Counter. import scanpy as sc. import networkx as nx. import pandas as pd. import numpy as np. import itertools. import random. fro",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:6474,interoperability,platform,platformdirs,6474,"ree 0.0.12. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. decoupler 1.4.0. defusedxml 0.7.1. docrep 0.3.2. entrypoints 0.4. et_xmlfile 1.1.0. executing 0.8.3. fasteners 0.18. fontTools 4.25.0. fsspec 2023.4.0. geopandas 0.13.2. graph_tool 2.43 (commit , ). gseapy 1.0.5. h5py 3.7.0. hypergeom_ufunc NA. idna 3.4. igraph 0.10.6. imagecodecs 2021.8.26. imageio 2.31.1. ipykernel 6.19.2. jedi 0.18.1. jinja2 3.1.2. joblib 1.2.0. kiwisolver 1.4.4. lazy_loader NA. leidenalg 0.10.1. llvmlite 0.40.0. markupsafe 2.1.1. matplotlib 3.5.3. matplotlib_inline 0.1.6. matplotlib_scalebar 0.8.1. matplotlib_venn 0.11.9. mkl 2.4.0. mpl_toolkits NA. msgpack 1.0.5. multipledispatch 0.6.0. multiscale_spatial_image 0.11.2. mygene 3.2.2. natsort 7.1.1. nbinom_ufunc NA. neighborhood_enrichment NA. networkx 3.1. numba 0.57.0. numcodecs 0.11.0. numpy 1.24.3. ome_zarr NA. openpyxl 3.1.2. packaging 23.0. pandas 2.0.3. param 1.13.0. parso 0.8.3. patsy 0.5.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 2.5.2. pooch v1.4.0. prompt_toolkit 3.0.36. psutil 5.9.0. ptyprocess 0.7.0. pure_eval 0.2.2. pyarrow 12.0.1. pycparser 2.21. pyct 0.5.0. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygeos 0.14. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pyproj 3.6.0. pytz 2022.7. pywt 1.4.1. requests 2.31.0. rich NA. rtree 1.0.1. schist v0.7.16. scipy 1.8.0. seaborn 0.12.2. session_info 1.0.0. setuptools 68.0.0. shapely 2.0.1. sip NA. six 1.16.0. skimage 0.20.0. sklearn 1.2.2. socks 1.7.1. spatial_image 0.3.0. spatialdata 0.0.12. sphinxcontrib NA. spyder 5.4.3. spyder_kernels 2.4.3. spydercustomize NA. squidpy 1.3.0. stack_data 0.2.0. statsmodels 0.14.0. tblib 2.0.0. texttable 1.6.7. threadpoolctl 2.2.0. tifffile 2021.7.2. tlz 0.12.0. toolz 0.12.0. tornado 6.3.2. tqdm 4.65.0. traitlets 5.7.1. typing_extensions NA. umap 0.5.3. urllib3 1.26.16. validators 0.20.0. wcwidth 0.2.5. wurlitzer 3.0.2. xarray 2022.12.0. xarray_dat",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:212,modifiability,version,version,212,"Question regarding: sc.rank_genes_groups(); ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When I run sc.rank_genes_groups() on my gene expression data, I do get separate matrices for names, scores, pvals, and pvals_adj. However, the scores don't match the pvals. In other words, with decreasing pvals, we don't always have increasing score. I did check out the description on the main documentations page, and they say they're calculating the zscores underlying the distribution, however if that's the case shouldn't it always be higher with decreasing pval? Also, I went through the code: it looks like they're calculating the scores on the absolute values instead of the real values--why is this? Are the scores basically U1 values corresponding to the pvalues, in whcih case once again lower pvalues should always have higher scores right? ares calculated from the p-values? What's the relation between the two. I have ran sc.rank_genes_groups() on my gene expression data, and I have generated the matrix for cluster-1 versus the rest, for reference. You can see that one, the pavlues don't increase as we go down the rows; and two, the scores seem kinda arbitrary to the p-values. What am I missing here? Thanks a lot, and sorry for the wordy question. [NOTE: This doesn't just depend on a specific type of clustering technique. Irrespective of whether I try the in-built Leiden clustering technique, or the Schit library, or even cluster the cells randomly, this issue keeps occuring--which makes me think it's ether a bug, or I don't quite understand how the score generation works, maybe both. I've provided a little code snipper below.]. [ClusterOneVsRest.csv](https://github.com/scverse/scanpy/files/12243486/ClusterOneVsRest.csv). ### Minimal code",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:1525,modifiability,depend,depend,1525,"creasing pvals, we don't always have increasing score. I did check out the description on the main documentations page, and they say they're calculating the zscores underlying the distribution, however if that's the case shouldn't it always be higher with decreasing pval? Also, I went through the code: it looks like they're calculating the scores on the absolute values instead of the real values--why is this? Are the scores basically U1 values corresponding to the pvalues, in whcih case once again lower pvalues should always have higher scores right? ares calculated from the p-values? What's the relation between the two. I have ran sc.rank_genes_groups() on my gene expression data, and I have generated the matrix for cluster-1 versus the rest, for reference. You can see that one, the pavlues don't increase as we go down the rows; and two, the scores seem kinda arbitrary to the p-values. What am I missing here? Thanks a lot, and sorry for the wordy question. [NOTE: This doesn't just depend on a specific type of clustering technique. Irrespective of whether I try the in-built Leiden clustering technique, or the Schit library, or even cluster the cells randomly, this issue keeps occuring--which makes me think it's ether a bug, or I don't quite understand how the score generation works, maybe both. I've provided a little code snipper below.]. [ClusterOneVsRest.csv](https://github.com/scverse/scanpy/files/12243486/ClusterOneVsRest.csv). ### Minimal code sample. ```python. import pickle. import numpy as np. import pandas as pd. from PIL import Image. import glob. import matplotlib.pyplot as plt. from skimage.morphology import convex_hull_image. from skimage import data, img_as_float. from skimage.util import invert. from scipy.spatial import ConvexHull, convex_hull_plot_2d. from multiprocessing import Pool. import time. import math. from collections import Counter. import scanpy as sc. import networkx as nx. import pandas as pd. import numpy as np. import itertools. impor",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:2917,modifiability,deco,decoupler,2917,"thub.com/scverse/scanpy/files/12243486/ClusterOneVsRest.csv). ### Minimal code sample. ```python. import pickle. import numpy as np. import pandas as pd. from PIL import Image. import glob. import matplotlib.pyplot as plt. from skimage.morphology import convex_hull_image. from skimage import data, img_as_float. from skimage.util import invert. from scipy.spatial import ConvexHull, convex_hull_plot_2d. from multiprocessing import Pool. import time. import math. from collections import Counter. import scanpy as sc. import networkx as nx. import pandas as pd. import numpy as np. import itertools. import random. from scipy.stats import mannwhitneyu. import os. import warnings. import pickle. import matplotlib.pyplot as plt. import pandas as pd. import gseapy as gp. from matplotlib import pyplot as plt. from matplotlib_venn import venn2. import mygene. import seaborn as sns. from gseapy import barplot, dotplot. import random. import matplotlib.pyplot as plt. import numpy as np. import decoupler as dc. from numpy.random import default_rng. from copy import deepcopy. import scanpy as sc. import squidpy as sq. import time. from neighborhood_enrichment import neighborhood_enrichment. import schist as scs. with open('TNBC_41patients_KerenEtAl.pkl', 'rb') as f:. pickle_= pickle.load(f). pickle_=dict(list(pickle_.items())[7:8]). for i in pickle_:. cnt+=1. adata=pickle_[i]. sc.pp.neighbors(adata, n_neighbors=10, n_pcs=3). sc.tl.leiden(adata). scs.inference.planted_model(adata). sc.pp.scale(adata). sc.tl.rank_genes_groups(adata, groupby='ppbm', pts=True, method='wilcoxon'). adata_rankGenes_names, adata_rankGenes_scores, adata_rankGenes_logfoldchanges, adata_rankGenes_pvals, adata_rankGenes_pvals_adj, adata_rankGenes_pts, adata_rankGenes_pts_rest=pd.DataFrame(adata.uns['rank_genes_groups']['names']), pd.DataFrame(adata.uns['rank_genes_groups']['scores']), pd.DataFrame(adata.uns['rank_genes_groups']['logfoldchanges']), pd.DataFrame(adata.uns['rank_genes_groups']['pvals']), pd.DataF",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:3418,modifiability,scal,scale,3418,"mport scanpy as sc. import networkx as nx. import pandas as pd. import numpy as np. import itertools. import random. from scipy.stats import mannwhitneyu. import os. import warnings. import pickle. import matplotlib.pyplot as plt. import pandas as pd. import gseapy as gp. from matplotlib import pyplot as plt. from matplotlib_venn import venn2. import mygene. import seaborn as sns. from gseapy import barplot, dotplot. import random. import matplotlib.pyplot as plt. import numpy as np. import decoupler as dc. from numpy.random import default_rng. from copy import deepcopy. import scanpy as sc. import squidpy as sq. import time. from neighborhood_enrichment import neighborhood_enrichment. import schist as scs. with open('TNBC_41patients_KerenEtAl.pkl', 'rb') as f:. pickle_= pickle.load(f). pickle_=dict(list(pickle_.items())[7:8]). for i in pickle_:. cnt+=1. adata=pickle_[i]. sc.pp.neighbors(adata, n_neighbors=10, n_pcs=3). sc.tl.leiden(adata). scs.inference.planted_model(adata). sc.pp.scale(adata). sc.tl.rank_genes_groups(adata, groupby='ppbm', pts=True, method='wilcoxon'). adata_rankGenes_names, adata_rankGenes_scores, adata_rankGenes_logfoldchanges, adata_rankGenes_pvals, adata_rankGenes_pvals_adj, adata_rankGenes_pts, adata_rankGenes_pts_rest=pd.DataFrame(adata.uns['rank_genes_groups']['names']), pd.DataFrame(adata.uns['rank_genes_groups']['scores']), pd.DataFrame(adata.uns['rank_genes_groups']['logfoldchanges']), pd.DataFrame(adata.uns['rank_genes_groups']['pvals']), pd.DataFrame(adata.uns['rank_genes_groups']['pvals_adj']), pd.DataFrame(adata.uns['rank_genes_groups']['pts']), pd.DataFrame(adata.uns['rank_genes_groups']['pts_rest']). ```. ### Error output. ```pytb. It's not really an error, it's just unexpected output. 1. https://github.com/scverse/scanpy/issues/701 says the p-values should always decrease as we go down the rows of the output matrix, which would make sense because the scores are ordered in decreasing order. 2. https://github.com/scverse/scanpy/issu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:4943,modifiability,Version,Versions,4943,"s_groups']['pvals_adj']), pd.DataFrame(adata.uns['rank_genes_groups']['pts']), pd.DataFrame(adata.uns['rank_genes_groups']['pts_rest']). ```. ### Error output. ```pytb. It's not really an error, it's just unexpected output. 1. https://github.com/scverse/scanpy/issues/701 says the p-values should always decrease as we go down the rows of the output matrix, which would make sense because the scores are ordered in decreasing order. 2. https://github.com/scverse/scanpy/issues/1688 says that higher score should always correspond to lower p-value, but that's not what's happening. 3. The official documentation says: ""Structured array to be indexed by group id storing the z-score underlying the computation of a p-value for each gene for each group. Ordered according to scores."" --> But if that's the case, why are the scores and p-values not perfectly opposite (that is lower score always equals higher p-value. Also, aren't you generating the U-value corresponding to the p-value?). ```. ### Versions. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 9.0.1. PyQt5 NA. appdirs 1.4.4. appnope 0.1.2. asciitree NA. asttokens NA. atomicwrites 1.4.0. attr 22.1.0. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. biothings_client 0.3.0. brotli NA. cairo 1.23.0. certifi 2023.07.22. cffi 1.15.1. chardet 4.0.0. charset_normalizer 2.0.4. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.2. cv2 4.8.0. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dask_image 2023.03.0. datashader 0.15.1. datashape 0.5.2. datatree 0.0.12. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. decoupler 1.4.0. defusedxml 0.7.1. docrep 0.3.2. entrypoints 0.4. et_xmlfile 1.1.0. executing 0.8.3. fasteners 0.18. fontTools 4.25.0. fsspec 2023.4.0. geopandas 0.13.2. graph_tool 2.43 (commit , ). gseapy 1.0.5. h5py 3.7.0. hypergeom_ufunc NA. idna 3.4. igraph 0.10.6. imagecodecs 2021.8.26. imageio 2.31.1. ipykernel 6.19.2. jedi 0.18.1. jinja2 3.1.2. joblib 1.2.0. kiwisolver 1.4.4. lazy_loader NA. leide",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:5523,modifiability,deco,decorator,5523,"3. The official documentation says: ""Structured array to be indexed by group id storing the z-score underlying the computation of a p-value for each gene for each group. Ordered according to scores."" --> But if that's the case, why are the scores and p-values not perfectly opposite (that is lower score always equals higher p-value. Also, aren't you generating the U-value corresponding to the p-value?). ```. ### Versions. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 9.0.1. PyQt5 NA. appdirs 1.4.4. appnope 0.1.2. asciitree NA. asttokens NA. atomicwrites 1.4.0. attr 22.1.0. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. biothings_client 0.3.0. brotli NA. cairo 1.23.0. certifi 2023.07.22. cffi 1.15.1. chardet 4.0.0. charset_normalizer 2.0.4. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.2. cv2 4.8.0. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dask_image 2023.03.0. datashader 0.15.1. datashape 0.5.2. datatree 0.0.12. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. decoupler 1.4.0. defusedxml 0.7.1. docrep 0.3.2. entrypoints 0.4. et_xmlfile 1.1.0. executing 0.8.3. fasteners 0.18. fontTools 4.25.0. fsspec 2023.4.0. geopandas 0.13.2. graph_tool 2.43 (commit , ). gseapy 1.0.5. h5py 3.7.0. hypergeom_ufunc NA. idna 3.4. igraph 0.10.6. imagecodecs 2021.8.26. imageio 2.31.1. ipykernel 6.19.2. jedi 0.18.1. jinja2 3.1.2. joblib 1.2.0. kiwisolver 1.4.4. lazy_loader NA. leidenalg 0.10.1. llvmlite 0.40.0. markupsafe 2.1.1. matplotlib 3.5.3. matplotlib_inline 0.1.6. matplotlib_scalebar 0.8.1. matplotlib_venn 0.11.9. mkl 2.4.0. mpl_toolkits NA. msgpack 1.0.5. multipledispatch 0.6.0. multiscale_spatial_image 0.11.2. mygene 3.2.2. natsort 7.1.1. nbinom_ufunc NA. neighborhood_enrichment NA. networkx 3.1. numba 0.57.0. numcodecs 0.11.0. numpy 1.24.3. ome_zarr NA. openpyxl 3.1.2. packaging 23.0. pandas 2.0.3. param 1.13.0. parso 0.8.3. patsy 0.5.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 2.5.2. pooch v1.4.0. prompt_toolkit 3.0.3",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:5540,modifiability,deco,decoupler,5540,"ocumentation says: ""Structured array to be indexed by group id storing the z-score underlying the computation of a p-value for each gene for each group. Ordered according to scores."" --> But if that's the case, why are the scores and p-values not perfectly opposite (that is lower score always equals higher p-value. Also, aren't you generating the U-value corresponding to the p-value?). ```. ### Versions. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 9.0.1. PyQt5 NA. appdirs 1.4.4. appnope 0.1.2. asciitree NA. asttokens NA. atomicwrites 1.4.0. attr 22.1.0. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. biothings_client 0.3.0. brotli NA. cairo 1.23.0. certifi 2023.07.22. cffi 1.15.1. chardet 4.0.0. charset_normalizer 2.0.4. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.2. cv2 4.8.0. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dask_image 2023.03.0. datashader 0.15.1. datashape 0.5.2. datatree 0.0.12. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. decoupler 1.4.0. defusedxml 0.7.1. docrep 0.3.2. entrypoints 0.4. et_xmlfile 1.1.0. executing 0.8.3. fasteners 0.18. fontTools 4.25.0. fsspec 2023.4.0. geopandas 0.13.2. graph_tool 2.43 (commit , ). gseapy 1.0.5. h5py 3.7.0. hypergeom_ufunc NA. idna 3.4. igraph 0.10.6. imagecodecs 2021.8.26. imageio 2.31.1. ipykernel 6.19.2. jedi 0.18.1. jinja2 3.1.2. joblib 1.2.0. kiwisolver 1.4.4. lazy_loader NA. leidenalg 0.10.1. llvmlite 0.40.0. markupsafe 2.1.1. matplotlib 3.5.3. matplotlib_inline 0.1.6. matplotlib_scalebar 0.8.1. matplotlib_venn 0.11.9. mkl 2.4.0. mpl_toolkits NA. msgpack 1.0.5. multipledispatch 0.6.0. multiscale_spatial_image 0.11.2. mygene 3.2.2. natsort 7.1.1. nbinom_ufunc NA. neighborhood_enrichment NA. networkx 3.1. numba 0.57.0. numcodecs 0.11.0. numpy 1.24.3. ome_zarr NA. openpyxl 3.1.2. packaging 23.0. pandas 2.0.3. param 1.13.0. parso 0.8.3. patsy 0.5.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 2.5.2. pooch v1.4.0. prompt_toolkit 3.0.36. psutil 5.9.0. ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:6352,modifiability,pac,packaging,6352,"r 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dask_image 2023.03.0. datashader 0.15.1. datashape 0.5.2. datatree 0.0.12. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. decoupler 1.4.0. defusedxml 0.7.1. docrep 0.3.2. entrypoints 0.4. et_xmlfile 1.1.0. executing 0.8.3. fasteners 0.18. fontTools 4.25.0. fsspec 2023.4.0. geopandas 0.13.2. graph_tool 2.43 (commit , ). gseapy 1.0.5. h5py 3.7.0. hypergeom_ufunc NA. idna 3.4. igraph 0.10.6. imagecodecs 2021.8.26. imageio 2.31.1. ipykernel 6.19.2. jedi 0.18.1. jinja2 3.1.2. joblib 1.2.0. kiwisolver 1.4.4. lazy_loader NA. leidenalg 0.10.1. llvmlite 0.40.0. markupsafe 2.1.1. matplotlib 3.5.3. matplotlib_inline 0.1.6. matplotlib_scalebar 0.8.1. matplotlib_venn 0.11.9. mkl 2.4.0. mpl_toolkits NA. msgpack 1.0.5. multipledispatch 0.6.0. multiscale_spatial_image 0.11.2. mygene 3.2.2. natsort 7.1.1. nbinom_ufunc NA. neighborhood_enrichment NA. networkx 3.1. numba 0.57.0. numcodecs 0.11.0. numpy 1.24.3. ome_zarr NA. openpyxl 3.1.2. packaging 23.0. pandas 2.0.3. param 1.13.0. parso 0.8.3. patsy 0.5.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 2.5.2. pooch v1.4.0. prompt_toolkit 3.0.36. psutil 5.9.0. ptyprocess 0.7.0. pure_eval 0.2.2. pyarrow 12.0.1. pycparser 2.21. pyct 0.5.0. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygeos 0.14. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pyproj 3.6.0. pytz 2022.7. pywt 1.4.1. requests 2.31.0. rich NA. rtree 1.0.1. schist v0.7.16. scipy 1.8.0. seaborn 0.12.2. session_info 1.0.0. setuptools 68.0.0. shapely 2.0.1. sip NA. six 1.16.0. skimage 0.20.0. sklearn 1.2.2. socks 1.7.1. spatial_image 0.3.0. spatialdata 0.0.12. sphinxcontrib NA. spyder 5.4.3. spyder_kernels 2.4.3. spydercustomize NA. squidpy 1.3.0. stack_data 0.2.0. statsmodels 0.14.0. tblib 2.0.0. texttable 1.6.7. threadpoolctl 2.2.0. tifffile 2021.7.2. tlz 0.12.0. toolz 0.12.0. tornado 6.3.2. tqdm 4.65.0. traitlets 5.7.1. typing_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:7697,modifiability,pac,packaged,7697,"0.18.1. jinja2 3.1.2. joblib 1.2.0. kiwisolver 1.4.4. lazy_loader NA. leidenalg 0.10.1. llvmlite 0.40.0. markupsafe 2.1.1. matplotlib 3.5.3. matplotlib_inline 0.1.6. matplotlib_scalebar 0.8.1. matplotlib_venn 0.11.9. mkl 2.4.0. mpl_toolkits NA. msgpack 1.0.5. multipledispatch 0.6.0. multiscale_spatial_image 0.11.2. mygene 3.2.2. natsort 7.1.1. nbinom_ufunc NA. neighborhood_enrichment NA. networkx 3.1. numba 0.57.0. numcodecs 0.11.0. numpy 1.24.3. ome_zarr NA. openpyxl 3.1.2. packaging 23.0. pandas 2.0.3. param 1.13.0. parso 0.8.3. patsy 0.5.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 2.5.2. pooch v1.4.0. prompt_toolkit 3.0.36. psutil 5.9.0. ptyprocess 0.7.0. pure_eval 0.2.2. pyarrow 12.0.1. pycparser 2.21. pyct 0.5.0. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygeos 0.14. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pyproj 3.6.0. pytz 2022.7. pywt 1.4.1. requests 2.31.0. rich NA. rtree 1.0.1. schist v0.7.16. scipy 1.8.0. seaborn 0.12.2. session_info 1.0.0. setuptools 68.0.0. shapely 2.0.1. sip NA. six 1.16.0. skimage 0.20.0. sklearn 1.2.2. socks 1.7.1. spatial_image 0.3.0. spatialdata 0.0.12. sphinxcontrib NA. spyder 5.4.3. spyder_kernels 2.4.3. spydercustomize NA. squidpy 1.3.0. stack_data 0.2.0. statsmodels 0.14.0. tblib 2.0.0. texttable 1.6.7. threadpoolctl 2.2.0. tifffile 2021.7.2. tlz 0.12.0. toolz 0.12.0. tornado 6.3.2. tqdm 4.65.0. traitlets 5.7.1. typing_extensions NA. umap 0.5.3. urllib3 1.26.16. validators 0.20.0. wcwidth 0.2.5. wurlitzer 3.0.2. xarray 2022.12.0. xarray_dataclasses 1.6.0. xarray_schema 0.0.3. xrspatial 0.3.7. yaml 6.0. zarr 2.16.0. zipp NA. zmq 25.1.0. zoneinfo NA. zstandard 0.19.0. -----. IPython 8.12.0. jupyter_client 8.1.0. jupyter_core 5.3.0. -----. Python 3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:50:29) [Clang 14.0.6 ]. macOS-11.6-x86_64-i386-64bit. -----. Session information updated at 2023-08-02 20:20. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:2368,performance,time,time,2368,"and two, the scores seem kinda arbitrary to the p-values. What am I missing here? Thanks a lot, and sorry for the wordy question. [NOTE: This doesn't just depend on a specific type of clustering technique. Irrespective of whether I try the in-built Leiden clustering technique, or the Schit library, or even cluster the cells randomly, this issue keeps occuring--which makes me think it's ether a bug, or I don't quite understand how the score generation works, maybe both. I've provided a little code snipper below.]. [ClusterOneVsRest.csv](https://github.com/scverse/scanpy/files/12243486/ClusterOneVsRest.csv). ### Minimal code sample. ```python. import pickle. import numpy as np. import pandas as pd. from PIL import Image. import glob. import matplotlib.pyplot as plt. from skimage.morphology import convex_hull_image. from skimage import data, img_as_float. from skimage.util import invert. from scipy.spatial import ConvexHull, convex_hull_plot_2d. from multiprocessing import Pool. import time. import math. from collections import Counter. import scanpy as sc. import networkx as nx. import pandas as pd. import numpy as np. import itertools. import random. from scipy.stats import mannwhitneyu. import os. import warnings. import pickle. import matplotlib.pyplot as plt. import pandas as pd. import gseapy as gp. from matplotlib import pyplot as plt. from matplotlib_venn import venn2. import mygene. import seaborn as sns. from gseapy import barplot, dotplot. import random. import matplotlib.pyplot as plt. import numpy as np. import decoupler as dc. from numpy.random import default_rng. from copy import deepcopy. import scanpy as sc. import squidpy as sq. import time. from neighborhood_enrichment import neighborhood_enrichment. import schist as scs. with open('TNBC_41patients_KerenEtAl.pkl', 'rb') as f:. pickle_= pickle.load(f). pickle_=dict(list(pickle_.items())[7:8]). for i in pickle_:. cnt+=1. adata=pickle_[i]. sc.pp.neighbors(adata, n_neighbors=10, n_pcs=3). sc.tl.leiden(ad",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:2448,performance,network,networkx,2448,"Thanks a lot, and sorry for the wordy question. [NOTE: This doesn't just depend on a specific type of clustering technique. Irrespective of whether I try the in-built Leiden clustering technique, or the Schit library, or even cluster the cells randomly, this issue keeps occuring--which makes me think it's ether a bug, or I don't quite understand how the score generation works, maybe both. I've provided a little code snipper below.]. [ClusterOneVsRest.csv](https://github.com/scverse/scanpy/files/12243486/ClusterOneVsRest.csv). ### Minimal code sample. ```python. import pickle. import numpy as np. import pandas as pd. from PIL import Image. import glob. import matplotlib.pyplot as plt. from skimage.morphology import convex_hull_image. from skimage import data, img_as_float. from skimage.util import invert. from scipy.spatial import ConvexHull, convex_hull_plot_2d. from multiprocessing import Pool. import time. import math. from collections import Counter. import scanpy as sc. import networkx as nx. import pandas as pd. import numpy as np. import itertools. import random. from scipy.stats import mannwhitneyu. import os. import warnings. import pickle. import matplotlib.pyplot as plt. import pandas as pd. import gseapy as gp. from matplotlib import pyplot as plt. from matplotlib_venn import venn2. import mygene. import seaborn as sns. from gseapy import barplot, dotplot. import random. import matplotlib.pyplot as plt. import numpy as np. import decoupler as dc. from numpy.random import default_rng. from copy import deepcopy. import scanpy as sc. import squidpy as sq. import time. from neighborhood_enrichment import neighborhood_enrichment. import schist as scs. with open('TNBC_41patients_KerenEtAl.pkl', 'rb') as f:. pickle_= pickle.load(f). pickle_=dict(list(pickle_.items())[7:8]). for i in pickle_:. cnt+=1. adata=pickle_[i]. sc.pp.neighbors(adata, n_neighbors=10, n_pcs=3). sc.tl.leiden(adata). scs.inference.planted_model(adata). sc.pp.scale(adata). sc.tl.rank_genes_gro",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:3049,performance,time,time,3049,"np. import pandas as pd. from PIL import Image. import glob. import matplotlib.pyplot as plt. from skimage.morphology import convex_hull_image. from skimage import data, img_as_float. from skimage.util import invert. from scipy.spatial import ConvexHull, convex_hull_plot_2d. from multiprocessing import Pool. import time. import math. from collections import Counter. import scanpy as sc. import networkx as nx. import pandas as pd. import numpy as np. import itertools. import random. from scipy.stats import mannwhitneyu. import os. import warnings. import pickle. import matplotlib.pyplot as plt. import pandas as pd. import gseapy as gp. from matplotlib import pyplot as plt. from matplotlib_venn import venn2. import mygene. import seaborn as sns. from gseapy import barplot, dotplot. import random. import matplotlib.pyplot as plt. import numpy as np. import decoupler as dc. from numpy.random import default_rng. from copy import deepcopy. import scanpy as sc. import squidpy as sq. import time. from neighborhood_enrichment import neighborhood_enrichment. import schist as scs. with open('TNBC_41patients_KerenEtAl.pkl', 'rb') as f:. pickle_= pickle.load(f). pickle_=dict(list(pickle_.items())[7:8]). for i in pickle_:. cnt+=1. adata=pickle_[i]. sc.pp.neighbors(adata, n_neighbors=10, n_pcs=3). sc.tl.leiden(adata). scs.inference.planted_model(adata). sc.pp.scale(adata). sc.tl.rank_genes_groups(adata, groupby='ppbm', pts=True, method='wilcoxon'). adata_rankGenes_names, adata_rankGenes_scores, adata_rankGenes_logfoldchanges, adata_rankGenes_pvals, adata_rankGenes_pvals_adj, adata_rankGenes_pts, adata_rankGenes_pts_rest=pd.DataFrame(adata.uns['rank_genes_groups']['names']), pd.DataFrame(adata.uns['rank_genes_groups']['scores']), pd.DataFrame(adata.uns['rank_genes_groups']['logfoldchanges']), pd.DataFrame(adata.uns['rank_genes_groups']['pvals']), pd.DataFrame(adata.uns['rank_genes_groups']['pvals_adj']), pd.DataFrame(adata.uns['rank_genes_groups']['pts']), pd.DataFrame(adata.uns['r",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:3210,performance,load,load,3210,"rt data, img_as_float. from skimage.util import invert. from scipy.spatial import ConvexHull, convex_hull_plot_2d. from multiprocessing import Pool. import time. import math. from collections import Counter. import scanpy as sc. import networkx as nx. import pandas as pd. import numpy as np. import itertools. import random. from scipy.stats import mannwhitneyu. import os. import warnings. import pickle. import matplotlib.pyplot as plt. import pandas as pd. import gseapy as gp. from matplotlib import pyplot as plt. from matplotlib_venn import venn2. import mygene. import seaborn as sns. from gseapy import barplot, dotplot. import random. import matplotlib.pyplot as plt. import numpy as np. import decoupler as dc. from numpy.random import default_rng. from copy import deepcopy. import scanpy as sc. import squidpy as sq. import time. from neighborhood_enrichment import neighborhood_enrichment. import schist as scs. with open('TNBC_41patients_KerenEtAl.pkl', 'rb') as f:. pickle_= pickle.load(f). pickle_=dict(list(pickle_.items())[7:8]). for i in pickle_:. cnt+=1. adata=pickle_[i]. sc.pp.neighbors(adata, n_neighbors=10, n_pcs=3). sc.tl.leiden(adata). scs.inference.planted_model(adata). sc.pp.scale(adata). sc.tl.rank_genes_groups(adata, groupby='ppbm', pts=True, method='wilcoxon'). adata_rankGenes_names, adata_rankGenes_scores, adata_rankGenes_logfoldchanges, adata_rankGenes_pvals, adata_rankGenes_pvals_adj, adata_rankGenes_pts, adata_rankGenes_pts_rest=pd.DataFrame(adata.uns['rank_genes_groups']['names']), pd.DataFrame(adata.uns['rank_genes_groups']['scores']), pd.DataFrame(adata.uns['rank_genes_groups']['logfoldchanges']), pd.DataFrame(adata.uns['rank_genes_groups']['pvals']), pd.DataFrame(adata.uns['rank_genes_groups']['pvals_adj']), pd.DataFrame(adata.uns['rank_genes_groups']['pts']), pd.DataFrame(adata.uns['rank_genes_groups']['pts_rest']). ```. ### Error output. ```pytb. It's not really an error, it's just unexpected output. 1. https://github.com/scverse/scanpy/issu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:3418,performance,scale,scale,3418,"mport scanpy as sc. import networkx as nx. import pandas as pd. import numpy as np. import itertools. import random. from scipy.stats import mannwhitneyu. import os. import warnings. import pickle. import matplotlib.pyplot as plt. import pandas as pd. import gseapy as gp. from matplotlib import pyplot as plt. from matplotlib_venn import venn2. import mygene. import seaborn as sns. from gseapy import barplot, dotplot. import random. import matplotlib.pyplot as plt. import numpy as np. import decoupler as dc. from numpy.random import default_rng. from copy import deepcopy. import scanpy as sc. import squidpy as sq. import time. from neighborhood_enrichment import neighborhood_enrichment. import schist as scs. with open('TNBC_41patients_KerenEtAl.pkl', 'rb') as f:. pickle_= pickle.load(f). pickle_=dict(list(pickle_.items())[7:8]). for i in pickle_:. cnt+=1. adata=pickle_[i]. sc.pp.neighbors(adata, n_neighbors=10, n_pcs=3). sc.tl.leiden(adata). scs.inference.planted_model(adata). sc.pp.scale(adata). sc.tl.rank_genes_groups(adata, groupby='ppbm', pts=True, method='wilcoxon'). adata_rankGenes_names, adata_rankGenes_scores, adata_rankGenes_logfoldchanges, adata_rankGenes_pvals, adata_rankGenes_pvals_adj, adata_rankGenes_pts, adata_rankGenes_pts_rest=pd.DataFrame(adata.uns['rank_genes_groups']['names']), pd.DataFrame(adata.uns['rank_genes_groups']['scores']), pd.DataFrame(adata.uns['rank_genes_groups']['logfoldchanges']), pd.DataFrame(adata.uns['rank_genes_groups']['pvals']), pd.DataFrame(adata.uns['rank_genes_groups']['pvals_adj']), pd.DataFrame(adata.uns['rank_genes_groups']['pts']), pd.DataFrame(adata.uns['rank_genes_groups']['pts_rest']). ```. ### Error output. ```pytb. It's not really an error, it's just unexpected output. 1. https://github.com/scverse/scanpy/issues/701 says the p-values should always decrease as we go down the rows of the output matrix, which would make sense because the scores are ordered in decreasing order. 2. https://github.com/scverse/scanpy/issu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:4093,performance,Error,Error,4093,"borhood_enrichment. import schist as scs. with open('TNBC_41patients_KerenEtAl.pkl', 'rb') as f:. pickle_= pickle.load(f). pickle_=dict(list(pickle_.items())[7:8]). for i in pickle_:. cnt+=1. adata=pickle_[i]. sc.pp.neighbors(adata, n_neighbors=10, n_pcs=3). sc.tl.leiden(adata). scs.inference.planted_model(adata). sc.pp.scale(adata). sc.tl.rank_genes_groups(adata, groupby='ppbm', pts=True, method='wilcoxon'). adata_rankGenes_names, adata_rankGenes_scores, adata_rankGenes_logfoldchanges, adata_rankGenes_pvals, adata_rankGenes_pvals_adj, adata_rankGenes_pts, adata_rankGenes_pts_rest=pd.DataFrame(adata.uns['rank_genes_groups']['names']), pd.DataFrame(adata.uns['rank_genes_groups']['scores']), pd.DataFrame(adata.uns['rank_genes_groups']['logfoldchanges']), pd.DataFrame(adata.uns['rank_genes_groups']['pvals']), pd.DataFrame(adata.uns['rank_genes_groups']['pvals_adj']), pd.DataFrame(adata.uns['rank_genes_groups']['pts']), pd.DataFrame(adata.uns['rank_genes_groups']['pts_rest']). ```. ### Error output. ```pytb. It's not really an error, it's just unexpected output. 1. https://github.com/scverse/scanpy/issues/701 says the p-values should always decrease as we go down the rows of the output matrix, which would make sense because the scores are ordered in decreasing order. 2. https://github.com/scverse/scanpy/issues/1688 says that higher score should always correspond to lower p-value, but that's not what's happening. 3. The official documentation says: ""Structured array to be indexed by group id storing the z-score underlying the computation of a p-value for each gene for each group. Ordered according to scores."" --> But if that's the case, why are the scores and p-values not perfectly opposite (that is lower score always equals higher p-value. Also, aren't you generating the U-value corresponding to the p-value?). ```. ### Versions. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 9.0.1. PyQt5 NA. appdirs 1.4.4. appnope 0.1.2. asciitree NA. asttokens NA. atomi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:4135,performance,error,error,4135,"with open('TNBC_41patients_KerenEtAl.pkl', 'rb') as f:. pickle_= pickle.load(f). pickle_=dict(list(pickle_.items())[7:8]). for i in pickle_:. cnt+=1. adata=pickle_[i]. sc.pp.neighbors(adata, n_neighbors=10, n_pcs=3). sc.tl.leiden(adata). scs.inference.planted_model(adata). sc.pp.scale(adata). sc.tl.rank_genes_groups(adata, groupby='ppbm', pts=True, method='wilcoxon'). adata_rankGenes_names, adata_rankGenes_scores, adata_rankGenes_logfoldchanges, adata_rankGenes_pvals, adata_rankGenes_pvals_adj, adata_rankGenes_pts, adata_rankGenes_pts_rest=pd.DataFrame(adata.uns['rank_genes_groups']['names']), pd.DataFrame(adata.uns['rank_genes_groups']['scores']), pd.DataFrame(adata.uns['rank_genes_groups']['logfoldchanges']), pd.DataFrame(adata.uns['rank_genes_groups']['pvals']), pd.DataFrame(adata.uns['rank_genes_groups']['pvals_adj']), pd.DataFrame(adata.uns['rank_genes_groups']['pts']), pd.DataFrame(adata.uns['rank_genes_groups']['pts_rest']). ```. ### Error output. ```pytb. It's not really an error, it's just unexpected output. 1. https://github.com/scverse/scanpy/issues/701 says the p-values should always decrease as we go down the rows of the output matrix, which would make sense because the scores are ordered in decreasing order. 2. https://github.com/scverse/scanpy/issues/1688 says that higher score should always correspond to lower p-value, but that's not what's happening. 3. The official documentation says: ""Structured array to be indexed by group id storing the z-score underlying the computation of a p-value for each gene for each group. Ordered according to scores."" --> But if that's the case, why are the scores and p-values not perfectly opposite (that is lower score always equals higher p-value. Also, aren't you generating the U-value corresponding to the p-value?). ```. ### Versions. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 9.0.1. PyQt5 NA. appdirs 1.4.4. appnope 0.1.2. asciitree NA. asttokens NA. atomicwrites 1.4.0. attr 22.1.0. backcall 0.2.0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:6263,performance,network,networkx,6263," charset_normalizer 2.0.4. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.2. cv2 4.8.0. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dask_image 2023.03.0. datashader 0.15.1. datashape 0.5.2. datatree 0.0.12. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. decoupler 1.4.0. defusedxml 0.7.1. docrep 0.3.2. entrypoints 0.4. et_xmlfile 1.1.0. executing 0.8.3. fasteners 0.18. fontTools 4.25.0. fsspec 2023.4.0. geopandas 0.13.2. graph_tool 2.43 (commit , ). gseapy 1.0.5. h5py 3.7.0. hypergeom_ufunc NA. idna 3.4. igraph 0.10.6. imagecodecs 2021.8.26. imageio 2.31.1. ipykernel 6.19.2. jedi 0.18.1. jinja2 3.1.2. joblib 1.2.0. kiwisolver 1.4.4. lazy_loader NA. leidenalg 0.10.1. llvmlite 0.40.0. markupsafe 2.1.1. matplotlib 3.5.3. matplotlib_inline 0.1.6. matplotlib_scalebar 0.8.1. matplotlib_venn 0.11.9. mkl 2.4.0. mpl_toolkits NA. msgpack 1.0.5. multipledispatch 0.6.0. multiscale_spatial_image 0.11.2. mygene 3.2.2. natsort 7.1.1. nbinom_ufunc NA. neighborhood_enrichment NA. networkx 3.1. numba 0.57.0. numcodecs 0.11.0. numpy 1.24.3. ome_zarr NA. openpyxl 3.1.2. packaging 23.0. pandas 2.0.3. param 1.13.0. parso 0.8.3. patsy 0.5.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 2.5.2. pooch v1.4.0. prompt_toolkit 3.0.36. psutil 5.9.0. ptyprocess 0.7.0. pure_eval 0.2.2. pyarrow 12.0.1. pycparser 2.21. pyct 0.5.0. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygeos 0.14. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pyproj 3.6.0. pytz 2022.7. pywt 1.4.1. requests 2.31.0. rich NA. rtree 1.0.1. schist v0.7.16. scipy 1.8.0. seaborn 0.12.2. session_info 1.0.0. setuptools 68.0.0. shapely 2.0.1. sip NA. six 1.16.0. skimage 0.20.0. sklearn 1.2.2. socks 1.7.1. spatial_image 0.3.0. spatialdata 0.0.12. sphinxcontrib NA. spyder 5.4.3. spyder_kernels 2.4.3. spydercustomize NA. squidpy 1.3.0. stack_data 0.2.0. statsmodels 0.14.0. tblib 2.0.0. texttable 1.6.7. threadpoolctl 2.2.0. tifffil",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:1512,reliability,doe,doesn,1512,"ords, with decreasing pvals, we don't always have increasing score. I did check out the description on the main documentations page, and they say they're calculating the zscores underlying the distribution, however if that's the case shouldn't it always be higher with decreasing pval? Also, I went through the code: it looks like they're calculating the scores on the absolute values instead of the real values--why is this? Are the scores basically U1 values corresponding to the pvalues, in whcih case once again lower pvalues should always have higher scores right? ares calculated from the p-values? What's the relation between the two. I have ran sc.rank_genes_groups() on my gene expression data, and I have generated the matrix for cluster-1 versus the rest, for reference. You can see that one, the pavlues don't increase as we go down the rows; and two, the scores seem kinda arbitrary to the p-values. What am I missing here? Thanks a lot, and sorry for the wordy question. [NOTE: This doesn't just depend on a specific type of clustering technique. Irrespective of whether I try the in-built Leiden clustering technique, or the Schit library, or even cluster the cells randomly, this issue keeps occuring--which makes me think it's ether a bug, or I don't quite understand how the score generation works, maybe both. I've provided a little code snipper below.]. [ClusterOneVsRest.csv](https://github.com/scverse/scanpy/files/12243486/ClusterOneVsRest.csv). ### Minimal code sample. ```python. import pickle. import numpy as np. import pandas as pd. from PIL import Image. import glob. import matplotlib.pyplot as plt. from skimage.morphology import convex_hull_image. from skimage import data, img_as_float. from skimage.util import invert. from scipy.spatial import ConvexHull, convex_hull_plot_2d. from multiprocessing import Pool. import time. import math. from collections import Counter. import scanpy as sc. import networkx as nx. import pandas as pd. import numpy as np. import ite",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:1525,safety,depend,depend,1525,"creasing pvals, we don't always have increasing score. I did check out the description on the main documentations page, and they say they're calculating the zscores underlying the distribution, however if that's the case shouldn't it always be higher with decreasing pval? Also, I went through the code: it looks like they're calculating the scores on the absolute values instead of the real values--why is this? Are the scores basically U1 values corresponding to the pvalues, in whcih case once again lower pvalues should always have higher scores right? ares calculated from the p-values? What's the relation between the two. I have ran sc.rank_genes_groups() on my gene expression data, and I have generated the matrix for cluster-1 versus the rest, for reference. You can see that one, the pavlues don't increase as we go down the rows; and two, the scores seem kinda arbitrary to the p-values. What am I missing here? Thanks a lot, and sorry for the wordy question. [NOTE: This doesn't just depend on a specific type of clustering technique. Irrespective of whether I try the in-built Leiden clustering technique, or the Schit library, or even cluster the cells randomly, this issue keeps occuring--which makes me think it's ether a bug, or I don't quite understand how the score generation works, maybe both. I've provided a little code snipper below.]. [ClusterOneVsRest.csv](https://github.com/scverse/scanpy/files/12243486/ClusterOneVsRest.csv). ### Minimal code sample. ```python. import pickle. import numpy as np. import pandas as pd. from PIL import Image. import glob. import matplotlib.pyplot as plt. from skimage.morphology import convex_hull_image. from skimage import data, img_as_float. from skimage.util import invert. from scipy.spatial import ConvexHull, convex_hull_plot_2d. from multiprocessing import Pool. import time. import math. from collections import Counter. import scanpy as sc. import networkx as nx. import pandas as pd. import numpy as np. import itertools. impor",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:3840,safety,log,logfoldchanges,3840,"t random. import matplotlib.pyplot as plt. import numpy as np. import decoupler as dc. from numpy.random import default_rng. from copy import deepcopy. import scanpy as sc. import squidpy as sq. import time. from neighborhood_enrichment import neighborhood_enrichment. import schist as scs. with open('TNBC_41patients_KerenEtAl.pkl', 'rb') as f:. pickle_= pickle.load(f). pickle_=dict(list(pickle_.items())[7:8]). for i in pickle_:. cnt+=1. adata=pickle_[i]. sc.pp.neighbors(adata, n_neighbors=10, n_pcs=3). sc.tl.leiden(adata). scs.inference.planted_model(adata). sc.pp.scale(adata). sc.tl.rank_genes_groups(adata, groupby='ppbm', pts=True, method='wilcoxon'). adata_rankGenes_names, adata_rankGenes_scores, adata_rankGenes_logfoldchanges, adata_rankGenes_pvals, adata_rankGenes_pvals_adj, adata_rankGenes_pts, adata_rankGenes_pts_rest=pd.DataFrame(adata.uns['rank_genes_groups']['names']), pd.DataFrame(adata.uns['rank_genes_groups']['scores']), pd.DataFrame(adata.uns['rank_genes_groups']['logfoldchanges']), pd.DataFrame(adata.uns['rank_genes_groups']['pvals']), pd.DataFrame(adata.uns['rank_genes_groups']['pvals_adj']), pd.DataFrame(adata.uns['rank_genes_groups']['pts']), pd.DataFrame(adata.uns['rank_genes_groups']['pts_rest']). ```. ### Error output. ```pytb. It's not really an error, it's just unexpected output. 1. https://github.com/scverse/scanpy/issues/701 says the p-values should always decrease as we go down the rows of the output matrix, which would make sense because the scores are ordered in decreasing order. 2. https://github.com/scverse/scanpy/issues/1688 says that higher score should always correspond to lower p-value, but that's not what's happening. 3. The official documentation says: ""Structured array to be indexed by group id storing the z-score underlying the computation of a p-value for each gene for each group. Ordered according to scores."" --> But if that's the case, why are the scores and p-values not perfectly opposite (that is lower score always equals h",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:4093,safety,Error,Error,4093,"borhood_enrichment. import schist as scs. with open('TNBC_41patients_KerenEtAl.pkl', 'rb') as f:. pickle_= pickle.load(f). pickle_=dict(list(pickle_.items())[7:8]). for i in pickle_:. cnt+=1. adata=pickle_[i]. sc.pp.neighbors(adata, n_neighbors=10, n_pcs=3). sc.tl.leiden(adata). scs.inference.planted_model(adata). sc.pp.scale(adata). sc.tl.rank_genes_groups(adata, groupby='ppbm', pts=True, method='wilcoxon'). adata_rankGenes_names, adata_rankGenes_scores, adata_rankGenes_logfoldchanges, adata_rankGenes_pvals, adata_rankGenes_pvals_adj, adata_rankGenes_pts, adata_rankGenes_pts_rest=pd.DataFrame(adata.uns['rank_genes_groups']['names']), pd.DataFrame(adata.uns['rank_genes_groups']['scores']), pd.DataFrame(adata.uns['rank_genes_groups']['logfoldchanges']), pd.DataFrame(adata.uns['rank_genes_groups']['pvals']), pd.DataFrame(adata.uns['rank_genes_groups']['pvals_adj']), pd.DataFrame(adata.uns['rank_genes_groups']['pts']), pd.DataFrame(adata.uns['rank_genes_groups']['pts_rest']). ```. ### Error output. ```pytb. It's not really an error, it's just unexpected output. 1. https://github.com/scverse/scanpy/issues/701 says the p-values should always decrease as we go down the rows of the output matrix, which would make sense because the scores are ordered in decreasing order. 2. https://github.com/scverse/scanpy/issues/1688 says that higher score should always correspond to lower p-value, but that's not what's happening. 3. The official documentation says: ""Structured array to be indexed by group id storing the z-score underlying the computation of a p-value for each gene for each group. Ordered according to scores."" --> But if that's the case, why are the scores and p-values not perfectly opposite (that is lower score always equals higher p-value. Also, aren't you generating the U-value corresponding to the p-value?). ```. ### Versions. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 9.0.1. PyQt5 NA. appdirs 1.4.4. appnope 0.1.2. asciitree NA. asttokens NA. atomi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:4135,safety,error,error,4135,"with open('TNBC_41patients_KerenEtAl.pkl', 'rb') as f:. pickle_= pickle.load(f). pickle_=dict(list(pickle_.items())[7:8]). for i in pickle_:. cnt+=1. adata=pickle_[i]. sc.pp.neighbors(adata, n_neighbors=10, n_pcs=3). sc.tl.leiden(adata). scs.inference.planted_model(adata). sc.pp.scale(adata). sc.tl.rank_genes_groups(adata, groupby='ppbm', pts=True, method='wilcoxon'). adata_rankGenes_names, adata_rankGenes_scores, adata_rankGenes_logfoldchanges, adata_rankGenes_pvals, adata_rankGenes_pvals_adj, adata_rankGenes_pts, adata_rankGenes_pts_rest=pd.DataFrame(adata.uns['rank_genes_groups']['names']), pd.DataFrame(adata.uns['rank_genes_groups']['scores']), pd.DataFrame(adata.uns['rank_genes_groups']['logfoldchanges']), pd.DataFrame(adata.uns['rank_genes_groups']['pvals']), pd.DataFrame(adata.uns['rank_genes_groups']['pvals_adj']), pd.DataFrame(adata.uns['rank_genes_groups']['pts']), pd.DataFrame(adata.uns['rank_genes_groups']['pts_rest']). ```. ### Error output. ```pytb. It's not really an error, it's just unexpected output. 1. https://github.com/scverse/scanpy/issues/701 says the p-values should always decrease as we go down the rows of the output matrix, which would make sense because the scores are ordered in decreasing order. 2. https://github.com/scverse/scanpy/issues/1688 says that higher score should always correspond to lower p-value, but that's not what's happening. 3. The official documentation says: ""Structured array to be indexed by group id storing the z-score underlying the computation of a p-value for each gene for each group. Ordered according to scores."" --> But if that's the case, why are the scores and p-values not perfectly opposite (that is lower score always equals higher p-value. Also, aren't you generating the U-value corresponding to the p-value?). ```. ### Versions. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 9.0.1. PyQt5 NA. appdirs 1.4.4. appnope 0.1.2. asciitree NA. asttokens NA. atomicwrites 1.4.0. attr 22.1.0. backcall 0.2.0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:7401,safety,valid,validators,7401,"0.18.1. jinja2 3.1.2. joblib 1.2.0. kiwisolver 1.4.4. lazy_loader NA. leidenalg 0.10.1. llvmlite 0.40.0. markupsafe 2.1.1. matplotlib 3.5.3. matplotlib_inline 0.1.6. matplotlib_scalebar 0.8.1. matplotlib_venn 0.11.9. mkl 2.4.0. mpl_toolkits NA. msgpack 1.0.5. multipledispatch 0.6.0. multiscale_spatial_image 0.11.2. mygene 3.2.2. natsort 7.1.1. nbinom_ufunc NA. neighborhood_enrichment NA. networkx 3.1. numba 0.57.0. numcodecs 0.11.0. numpy 1.24.3. ome_zarr NA. openpyxl 3.1.2. packaging 23.0. pandas 2.0.3. param 1.13.0. parso 0.8.3. patsy 0.5.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 2.5.2. pooch v1.4.0. prompt_toolkit 3.0.36. psutil 5.9.0. ptyprocess 0.7.0. pure_eval 0.2.2. pyarrow 12.0.1. pycparser 2.21. pyct 0.5.0. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygeos 0.14. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pyproj 3.6.0. pytz 2022.7. pywt 1.4.1. requests 2.31.0. rich NA. rtree 1.0.1. schist v0.7.16. scipy 1.8.0. seaborn 0.12.2. session_info 1.0.0. setuptools 68.0.0. shapely 2.0.1. sip NA. six 1.16.0. skimage 0.20.0. sklearn 1.2.2. socks 1.7.1. spatial_image 0.3.0. spatialdata 0.0.12. sphinxcontrib NA. spyder 5.4.3. spyder_kernels 2.4.3. spydercustomize NA. squidpy 1.3.0. stack_data 0.2.0. statsmodels 0.14.0. tblib 2.0.0. texttable 1.6.7. threadpoolctl 2.2.0. tifffile 2021.7.2. tlz 0.12.0. toolz 0.12.0. tornado 6.3.2. tqdm 4.65.0. traitlets 5.7.1. typing_extensions NA. umap 0.5.3. urllib3 1.26.16. validators 0.20.0. wcwidth 0.2.5. wurlitzer 3.0.2. xarray 2022.12.0. xarray_dataclasses 1.6.0. xarray_schema 0.0.3. xrspatial 0.3.7. yaml 6.0. zarr 2.16.0. zipp NA. zmq 25.1.0. zoneinfo NA. zstandard 0.19.0. -----. IPython 8.12.0. jupyter_client 8.1.0. jupyter_core 5.3.0. -----. Python 3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:50:29) [Clang 14.0.6 ]. macOS-11.6-x86_64-i386-64bit. -----. Session information updated at 2023-08-02 20:20. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:7827,safety,updat,updated,7827,"0.18.1. jinja2 3.1.2. joblib 1.2.0. kiwisolver 1.4.4. lazy_loader NA. leidenalg 0.10.1. llvmlite 0.40.0. markupsafe 2.1.1. matplotlib 3.5.3. matplotlib_inline 0.1.6. matplotlib_scalebar 0.8.1. matplotlib_venn 0.11.9. mkl 2.4.0. mpl_toolkits NA. msgpack 1.0.5. multipledispatch 0.6.0. multiscale_spatial_image 0.11.2. mygene 3.2.2. natsort 7.1.1. nbinom_ufunc NA. neighborhood_enrichment NA. networkx 3.1. numba 0.57.0. numcodecs 0.11.0. numpy 1.24.3. ome_zarr NA. openpyxl 3.1.2. packaging 23.0. pandas 2.0.3. param 1.13.0. parso 0.8.3. patsy 0.5.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 2.5.2. pooch v1.4.0. prompt_toolkit 3.0.36. psutil 5.9.0. ptyprocess 0.7.0. pure_eval 0.2.2. pyarrow 12.0.1. pycparser 2.21. pyct 0.5.0. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygeos 0.14. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pyproj 3.6.0. pytz 2022.7. pywt 1.4.1. requests 2.31.0. rich NA. rtree 1.0.1. schist v0.7.16. scipy 1.8.0. seaborn 0.12.2. session_info 1.0.0. setuptools 68.0.0. shapely 2.0.1. sip NA. six 1.16.0. skimage 0.20.0. sklearn 1.2.2. socks 1.7.1. spatial_image 0.3.0. spatialdata 0.0.12. sphinxcontrib NA. spyder 5.4.3. spyder_kernels 2.4.3. spydercustomize NA. squidpy 1.3.0. stack_data 0.2.0. statsmodels 0.14.0. tblib 2.0.0. texttable 1.6.7. threadpoolctl 2.2.0. tifffile 2021.7.2. tlz 0.12.0. toolz 0.12.0. tornado 6.3.2. tqdm 4.65.0. traitlets 5.7.1. typing_extensions NA. umap 0.5.3. urllib3 1.26.16. validators 0.20.0. wcwidth 0.2.5. wurlitzer 3.0.2. xarray 2022.12.0. xarray_dataclasses 1.6.0. xarray_schema 0.0.3. xrspatial 0.3.7. yaml 6.0. zarr 2.16.0. zipp NA. zmq 25.1.0. zoneinfo NA. zstandard 0.19.0. -----. IPython 8.12.0. jupyter_client 8.1.0. jupyter_core 5.3.0. -----. Python 3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:50:29) [Clang 14.0.6 ]. macOS-11.6-x86_64-i386-64bit. -----. Session information updated at 2023-08-02 20:20. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:2448,security,network,networkx,2448,"Thanks a lot, and sorry for the wordy question. [NOTE: This doesn't just depend on a specific type of clustering technique. Irrespective of whether I try the in-built Leiden clustering technique, or the Schit library, or even cluster the cells randomly, this issue keeps occuring--which makes me think it's ether a bug, or I don't quite understand how the score generation works, maybe both. I've provided a little code snipper below.]. [ClusterOneVsRest.csv](https://github.com/scverse/scanpy/files/12243486/ClusterOneVsRest.csv). ### Minimal code sample. ```python. import pickle. import numpy as np. import pandas as pd. from PIL import Image. import glob. import matplotlib.pyplot as plt. from skimage.morphology import convex_hull_image. from skimage import data, img_as_float. from skimage.util import invert. from scipy.spatial import ConvexHull, convex_hull_plot_2d. from multiprocessing import Pool. import time. import math. from collections import Counter. import scanpy as sc. import networkx as nx. import pandas as pd. import numpy as np. import itertools. import random. from scipy.stats import mannwhitneyu. import os. import warnings. import pickle. import matplotlib.pyplot as plt. import pandas as pd. import gseapy as gp. from matplotlib import pyplot as plt. from matplotlib_venn import venn2. import mygene. import seaborn as sns. from gseapy import barplot, dotplot. import random. import matplotlib.pyplot as plt. import numpy as np. import decoupler as dc. from numpy.random import default_rng. from copy import deepcopy. import scanpy as sc. import squidpy as sq. import time. from neighborhood_enrichment import neighborhood_enrichment. import schist as scs. with open('TNBC_41patients_KerenEtAl.pkl', 'rb') as f:. pickle_= pickle.load(f). pickle_=dict(list(pickle_.items())[7:8]). for i in pickle_:. cnt+=1. adata=pickle_[i]. sc.pp.neighbors(adata, n_neighbors=10, n_pcs=3). sc.tl.leiden(adata). scs.inference.planted_model(adata). sc.pp.scale(adata). sc.tl.rank_genes_gro",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:3840,security,log,logfoldchanges,3840,"t random. import matplotlib.pyplot as plt. import numpy as np. import decoupler as dc. from numpy.random import default_rng. from copy import deepcopy. import scanpy as sc. import squidpy as sq. import time. from neighborhood_enrichment import neighborhood_enrichment. import schist as scs. with open('TNBC_41patients_KerenEtAl.pkl', 'rb') as f:. pickle_= pickle.load(f). pickle_=dict(list(pickle_.items())[7:8]). for i in pickle_:. cnt+=1. adata=pickle_[i]. sc.pp.neighbors(adata, n_neighbors=10, n_pcs=3). sc.tl.leiden(adata). scs.inference.planted_model(adata). sc.pp.scale(adata). sc.tl.rank_genes_groups(adata, groupby='ppbm', pts=True, method='wilcoxon'). adata_rankGenes_names, adata_rankGenes_scores, adata_rankGenes_logfoldchanges, adata_rankGenes_pvals, adata_rankGenes_pvals_adj, adata_rankGenes_pts, adata_rankGenes_pts_rest=pd.DataFrame(adata.uns['rank_genes_groups']['names']), pd.DataFrame(adata.uns['rank_genes_groups']['scores']), pd.DataFrame(adata.uns['rank_genes_groups']['logfoldchanges']), pd.DataFrame(adata.uns['rank_genes_groups']['pvals']), pd.DataFrame(adata.uns['rank_genes_groups']['pvals_adj']), pd.DataFrame(adata.uns['rank_genes_groups']['pts']), pd.DataFrame(adata.uns['rank_genes_groups']['pts_rest']). ```. ### Error output. ```pytb. It's not really an error, it's just unexpected output. 1. https://github.com/scverse/scanpy/issues/701 says the p-values should always decrease as we go down the rows of the output matrix, which would make sense because the scores are ordered in decreasing order. 2. https://github.com/scverse/scanpy/issues/1688 says that higher score should always correspond to lower p-value, but that's not what's happening. 3. The official documentation says: ""Structured array to be indexed by group id storing the z-score underlying the computation of a p-value for each gene for each group. Ordered according to scores."" --> But if that's the case, why are the scores and p-values not perfectly opposite (that is lower score always equals h",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:5220,security,certif,certifi,5220,"the p-values should always decrease as we go down the rows of the output matrix, which would make sense because the scores are ordered in decreasing order. 2. https://github.com/scverse/scanpy/issues/1688 says that higher score should always correspond to lower p-value, but that's not what's happening. 3. The official documentation says: ""Structured array to be indexed by group id storing the z-score underlying the computation of a p-value for each gene for each group. Ordered according to scores."" --> But if that's the case, why are the scores and p-values not perfectly opposite (that is lower score always equals higher p-value. Also, aren't you generating the U-value corresponding to the p-value?). ```. ### Versions. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 9.0.1. PyQt5 NA. appdirs 1.4.4. appnope 0.1.2. asciitree NA. asttokens NA. atomicwrites 1.4.0. attr 22.1.0. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. biothings_client 0.3.0. brotli NA. cairo 1.23.0. certifi 2023.07.22. cffi 1.15.1. chardet 4.0.0. charset_normalizer 2.0.4. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.2. cv2 4.8.0. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dask_image 2023.03.0. datashader 0.15.1. datashape 0.5.2. datatree 0.0.12. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. decoupler 1.4.0. defusedxml 0.7.1. docrep 0.3.2. entrypoints 0.4. et_xmlfile 1.1.0. executing 0.8.3. fasteners 0.18. fontTools 4.25.0. fsspec 2023.4.0. geopandas 0.13.2. graph_tool 2.43 (commit , ). gseapy 1.0.5. h5py 3.7.0. hypergeom_ufunc NA. idna 3.4. igraph 0.10.6. imagecodecs 2021.8.26. imageio 2.31.1. ipykernel 6.19.2. jedi 0.18.1. jinja2 3.1.2. joblib 1.2.0. kiwisolver 1.4.4. lazy_loader NA. leidenalg 0.10.1. llvmlite 0.40.0. markupsafe 2.1.1. matplotlib 3.5.3. matplotlib_inline 0.1.6. matplotlib_scalebar 0.8.1. matplotlib_venn 0.11.9. mkl 2.4.0. mpl_toolkits NA. msgpack 1.0.5. multipledispatch 0.6.0. multiscale_spatial_image 0.11.2. mygene 3.2.2. natsort 7.1.1. nbinom",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:6263,security,network,networkx,6263," charset_normalizer 2.0.4. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.2. cv2 4.8.0. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dask_image 2023.03.0. datashader 0.15.1. datashape 0.5.2. datatree 0.0.12. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. decoupler 1.4.0. defusedxml 0.7.1. docrep 0.3.2. entrypoints 0.4. et_xmlfile 1.1.0. executing 0.8.3. fasteners 0.18. fontTools 4.25.0. fsspec 2023.4.0. geopandas 0.13.2. graph_tool 2.43 (commit , ). gseapy 1.0.5. h5py 3.7.0. hypergeom_ufunc NA. idna 3.4. igraph 0.10.6. imagecodecs 2021.8.26. imageio 2.31.1. ipykernel 6.19.2. jedi 0.18.1. jinja2 3.1.2. joblib 1.2.0. kiwisolver 1.4.4. lazy_loader NA. leidenalg 0.10.1. llvmlite 0.40.0. markupsafe 2.1.1. matplotlib 3.5.3. matplotlib_inline 0.1.6. matplotlib_scalebar 0.8.1. matplotlib_venn 0.11.9. mkl 2.4.0. mpl_toolkits NA. msgpack 1.0.5. multipledispatch 0.6.0. multiscale_spatial_image 0.11.2. mygene 3.2.2. natsort 7.1.1. nbinom_ufunc NA. neighborhood_enrichment NA. networkx 3.1. numba 0.57.0. numcodecs 0.11.0. numpy 1.24.3. ome_zarr NA. openpyxl 3.1.2. packaging 23.0. pandas 2.0.3. param 1.13.0. parso 0.8.3. patsy 0.5.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 2.5.2. pooch v1.4.0. prompt_toolkit 3.0.36. psutil 5.9.0. ptyprocess 0.7.0. pure_eval 0.2.2. pyarrow 12.0.1. pycparser 2.21. pyct 0.5.0. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygeos 0.14. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pyproj 3.6.0. pytz 2022.7. pywt 1.4.1. requests 2.31.0. rich NA. rtree 1.0.1. schist v0.7.16. scipy 1.8.0. seaborn 0.12.2. session_info 1.0.0. setuptools 68.0.0. shapely 2.0.1. sip NA. six 1.16.0. skimage 0.20.0. sklearn 1.2.2. socks 1.7.1. spatial_image 0.3.0. spatialdata 0.0.12. sphinxcontrib NA. spyder 5.4.3. spyder_kernels 2.4.3. spydercustomize NA. squidpy 1.3.0. stack_data 0.2.0. statsmodels 0.14.0. tblib 2.0.0. texttable 1.6.7. threadpoolctl 2.2.0. tifffil",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:7028,security,soc,socks,7028,"0.18.1. jinja2 3.1.2. joblib 1.2.0. kiwisolver 1.4.4. lazy_loader NA. leidenalg 0.10.1. llvmlite 0.40.0. markupsafe 2.1.1. matplotlib 3.5.3. matplotlib_inline 0.1.6. matplotlib_scalebar 0.8.1. matplotlib_venn 0.11.9. mkl 2.4.0. mpl_toolkits NA. msgpack 1.0.5. multipledispatch 0.6.0. multiscale_spatial_image 0.11.2. mygene 3.2.2. natsort 7.1.1. nbinom_ufunc NA. neighborhood_enrichment NA. networkx 3.1. numba 0.57.0. numcodecs 0.11.0. numpy 1.24.3. ome_zarr NA. openpyxl 3.1.2. packaging 23.0. pandas 2.0.3. param 1.13.0. parso 0.8.3. patsy 0.5.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 2.5.2. pooch v1.4.0. prompt_toolkit 3.0.36. psutil 5.9.0. ptyprocess 0.7.0. pure_eval 0.2.2. pyarrow 12.0.1. pycparser 2.21. pyct 0.5.0. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygeos 0.14. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pyproj 3.6.0. pytz 2022.7. pywt 1.4.1. requests 2.31.0. rich NA. rtree 1.0.1. schist v0.7.16. scipy 1.8.0. seaborn 0.12.2. session_info 1.0.0. setuptools 68.0.0. shapely 2.0.1. sip NA. six 1.16.0. skimage 0.20.0. sklearn 1.2.2. socks 1.7.1. spatial_image 0.3.0. spatialdata 0.0.12. sphinxcontrib NA. spyder 5.4.3. spyder_kernels 2.4.3. spydercustomize NA. squidpy 1.3.0. stack_data 0.2.0. statsmodels 0.14.0. tblib 2.0.0. texttable 1.6.7. threadpoolctl 2.2.0. tifffile 2021.7.2. tlz 0.12.0. toolz 0.12.0. tornado 6.3.2. tqdm 4.65.0. traitlets 5.7.1. typing_extensions NA. umap 0.5.3. urllib3 1.26.16. validators 0.20.0. wcwidth 0.2.5. wurlitzer 3.0.2. xarray 2022.12.0. xarray_dataclasses 1.6.0. xarray_schema 0.0.3. xrspatial 0.3.7. yaml 6.0. zarr 2.16.0. zipp NA. zmq 25.1.0. zoneinfo NA. zstandard 0.19.0. -----. IPython 8.12.0. jupyter_client 8.1.0. jupyter_core 5.3.0. -----. Python 3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:50:29) [Clang 14.0.6 ]. macOS-11.6-x86_64-i386-64bit. -----. Session information updated at 2023-08-02 20:20. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:7401,security,validat,validators,7401,"0.18.1. jinja2 3.1.2. joblib 1.2.0. kiwisolver 1.4.4. lazy_loader NA. leidenalg 0.10.1. llvmlite 0.40.0. markupsafe 2.1.1. matplotlib 3.5.3. matplotlib_inline 0.1.6. matplotlib_scalebar 0.8.1. matplotlib_venn 0.11.9. mkl 2.4.0. mpl_toolkits NA. msgpack 1.0.5. multipledispatch 0.6.0. multiscale_spatial_image 0.11.2. mygene 3.2.2. natsort 7.1.1. nbinom_ufunc NA. neighborhood_enrichment NA. networkx 3.1. numba 0.57.0. numcodecs 0.11.0. numpy 1.24.3. ome_zarr NA. openpyxl 3.1.2. packaging 23.0. pandas 2.0.3. param 1.13.0. parso 0.8.3. patsy 0.5.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 2.5.2. pooch v1.4.0. prompt_toolkit 3.0.36. psutil 5.9.0. ptyprocess 0.7.0. pure_eval 0.2.2. pyarrow 12.0.1. pycparser 2.21. pyct 0.5.0. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygeos 0.14. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pyproj 3.6.0. pytz 2022.7. pywt 1.4.1. requests 2.31.0. rich NA. rtree 1.0.1. schist v0.7.16. scipy 1.8.0. seaborn 0.12.2. session_info 1.0.0. setuptools 68.0.0. shapely 2.0.1. sip NA. six 1.16.0. skimage 0.20.0. sklearn 1.2.2. socks 1.7.1. spatial_image 0.3.0. spatialdata 0.0.12. sphinxcontrib NA. spyder 5.4.3. spyder_kernels 2.4.3. spydercustomize NA. squidpy 1.3.0. stack_data 0.2.0. statsmodels 0.14.0. tblib 2.0.0. texttable 1.6.7. threadpoolctl 2.2.0. tifffile 2021.7.2. tlz 0.12.0. toolz 0.12.0. tornado 6.3.2. tqdm 4.65.0. traitlets 5.7.1. typing_extensions NA. umap 0.5.3. urllib3 1.26.16. validators 0.20.0. wcwidth 0.2.5. wurlitzer 3.0.2. xarray 2022.12.0. xarray_dataclasses 1.6.0. xarray_schema 0.0.3. xrspatial 0.3.7. yaml 6.0. zarr 2.16.0. zipp NA. zmq 25.1.0. zoneinfo NA. zstandard 0.19.0. -----. IPython 8.12.0. jupyter_client 8.1.0. jupyter_core 5.3.0. -----. Python 3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:50:29) [Clang 14.0.6 ]. macOS-11.6-x86_64-i386-64bit. -----. Session information updated at 2023-08-02 20:20. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:7807,security,Session,Session,7807,"0.18.1. jinja2 3.1.2. joblib 1.2.0. kiwisolver 1.4.4. lazy_loader NA. leidenalg 0.10.1. llvmlite 0.40.0. markupsafe 2.1.1. matplotlib 3.5.3. matplotlib_inline 0.1.6. matplotlib_scalebar 0.8.1. matplotlib_venn 0.11.9. mkl 2.4.0. mpl_toolkits NA. msgpack 1.0.5. multipledispatch 0.6.0. multiscale_spatial_image 0.11.2. mygene 3.2.2. natsort 7.1.1. nbinom_ufunc NA. neighborhood_enrichment NA. networkx 3.1. numba 0.57.0. numcodecs 0.11.0. numpy 1.24.3. ome_zarr NA. openpyxl 3.1.2. packaging 23.0. pandas 2.0.3. param 1.13.0. parso 0.8.3. patsy 0.5.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 2.5.2. pooch v1.4.0. prompt_toolkit 3.0.36. psutil 5.9.0. ptyprocess 0.7.0. pure_eval 0.2.2. pyarrow 12.0.1. pycparser 2.21. pyct 0.5.0. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygeos 0.14. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pyproj 3.6.0. pytz 2022.7. pywt 1.4.1. requests 2.31.0. rich NA. rtree 1.0.1. schist v0.7.16. scipy 1.8.0. seaborn 0.12.2. session_info 1.0.0. setuptools 68.0.0. shapely 2.0.1. sip NA. six 1.16.0. skimage 0.20.0. sklearn 1.2.2. socks 1.7.1. spatial_image 0.3.0. spatialdata 0.0.12. sphinxcontrib NA. spyder 5.4.3. spyder_kernels 2.4.3. spydercustomize NA. squidpy 1.3.0. stack_data 0.2.0. statsmodels 0.14.0. tblib 2.0.0. texttable 1.6.7. threadpoolctl 2.2.0. tifffile 2021.7.2. tlz 0.12.0. toolz 0.12.0. tornado 6.3.2. tqdm 4.65.0. traitlets 5.7.1. typing_extensions NA. umap 0.5.3. urllib3 1.26.16. validators 0.20.0. wcwidth 0.2.5. wurlitzer 3.0.2. xarray 2022.12.0. xarray_dataclasses 1.6.0. xarray_schema 0.0.3. xrspatial 0.3.7. yaml 6.0. zarr 2.16.0. zipp NA. zmq 25.1.0. zoneinfo NA. zstandard 0.19.0. -----. IPython 8.12.0. jupyter_client 8.1.0. jupyter_core 5.3.0. -----. Python 3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:50:29) [Clang 14.0.6 ]. macOS-11.6-x86_64-i386-64bit. -----. Session information updated at 2023-08-02 20:20. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:7827,security,updat,updated,7827,"0.18.1. jinja2 3.1.2. joblib 1.2.0. kiwisolver 1.4.4. lazy_loader NA. leidenalg 0.10.1. llvmlite 0.40.0. markupsafe 2.1.1. matplotlib 3.5.3. matplotlib_inline 0.1.6. matplotlib_scalebar 0.8.1. matplotlib_venn 0.11.9. mkl 2.4.0. mpl_toolkits NA. msgpack 1.0.5. multipledispatch 0.6.0. multiscale_spatial_image 0.11.2. mygene 3.2.2. natsort 7.1.1. nbinom_ufunc NA. neighborhood_enrichment NA. networkx 3.1. numba 0.57.0. numcodecs 0.11.0. numpy 1.24.3. ome_zarr NA. openpyxl 3.1.2. packaging 23.0. pandas 2.0.3. param 1.13.0. parso 0.8.3. patsy 0.5.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 2.5.2. pooch v1.4.0. prompt_toolkit 3.0.36. psutil 5.9.0. ptyprocess 0.7.0. pure_eval 0.2.2. pyarrow 12.0.1. pycparser 2.21. pyct 0.5.0. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygeos 0.14. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pyproj 3.6.0. pytz 2022.7. pywt 1.4.1. requests 2.31.0. rich NA. rtree 1.0.1. schist v0.7.16. scipy 1.8.0. seaborn 0.12.2. session_info 1.0.0. setuptools 68.0.0. shapely 2.0.1. sip NA. six 1.16.0. skimage 0.20.0. sklearn 1.2.2. socks 1.7.1. spatial_image 0.3.0. spatialdata 0.0.12. sphinxcontrib NA. spyder 5.4.3. spyder_kernels 2.4.3. spydercustomize NA. squidpy 1.3.0. stack_data 0.2.0. statsmodels 0.14.0. tblib 2.0.0. texttable 1.6.7. threadpoolctl 2.2.0. tifffile 2021.7.2. tlz 0.12.0. toolz 0.12.0. tornado 6.3.2. tqdm 4.65.0. traitlets 5.7.1. typing_extensions NA. umap 0.5.3. urllib3 1.26.16. validators 0.20.0. wcwidth 0.2.5. wurlitzer 3.0.2. xarray 2022.12.0. xarray_dataclasses 1.6.0. xarray_schema 0.0.3. xrspatial 0.3.7. yaml 6.0. zarr 2.16.0. zipp NA. zmq 25.1.0. zoneinfo NA. zstandard 0.19.0. -----. IPython 8.12.0. jupyter_client 8.1.0. jupyter_core 5.3.0. -----. Python 3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:50:29) [Clang 14.0.6 ]. macOS-11.6-x86_64-i386-64bit. -----. Session information updated at 2023-08-02 20:20. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:1525,testability,depend,depend,1525,"creasing pvals, we don't always have increasing score. I did check out the description on the main documentations page, and they say they're calculating the zscores underlying the distribution, however if that's the case shouldn't it always be higher with decreasing pval? Also, I went through the code: it looks like they're calculating the scores on the absolute values instead of the real values--why is this? Are the scores basically U1 values corresponding to the pvalues, in whcih case once again lower pvalues should always have higher scores right? ares calculated from the p-values? What's the relation between the two. I have ran sc.rank_genes_groups() on my gene expression data, and I have generated the matrix for cluster-1 versus the rest, for reference. You can see that one, the pavlues don't increase as we go down the rows; and two, the scores seem kinda arbitrary to the p-values. What am I missing here? Thanks a lot, and sorry for the wordy question. [NOTE: This doesn't just depend on a specific type of clustering technique. Irrespective of whether I try the in-built Leiden clustering technique, or the Schit library, or even cluster the cells randomly, this issue keeps occuring--which makes me think it's ether a bug, or I don't quite understand how the score generation works, maybe both. I've provided a little code snipper below.]. [ClusterOneVsRest.csv](https://github.com/scverse/scanpy/files/12243486/ClusterOneVsRest.csv). ### Minimal code sample. ```python. import pickle. import numpy as np. import pandas as pd. from PIL import Image. import glob. import matplotlib.pyplot as plt. from skimage.morphology import convex_hull_image. from skimage import data, img_as_float. from skimage.util import invert. from scipy.spatial import ConvexHull, convex_hull_plot_2d. from multiprocessing import Pool. import time. import math. from collections import Counter. import scanpy as sc. import networkx as nx. import pandas as pd. import numpy as np. import itertools. impor",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:1789,testability,understand,understand,1789," pval? Also, I went through the code: it looks like they're calculating the scores on the absolute values instead of the real values--why is this? Are the scores basically U1 values corresponding to the pvalues, in whcih case once again lower pvalues should always have higher scores right? ares calculated from the p-values? What's the relation between the two. I have ran sc.rank_genes_groups() on my gene expression data, and I have generated the matrix for cluster-1 versus the rest, for reference. You can see that one, the pavlues don't increase as we go down the rows; and two, the scores seem kinda arbitrary to the p-values. What am I missing here? Thanks a lot, and sorry for the wordy question. [NOTE: This doesn't just depend on a specific type of clustering technique. Irrespective of whether I try the in-built Leiden clustering technique, or the Schit library, or even cluster the cells randomly, this issue keeps occuring--which makes me think it's ether a bug, or I don't quite understand how the score generation works, maybe both. I've provided a little code snipper below.]. [ClusterOneVsRest.csv](https://github.com/scverse/scanpy/files/12243486/ClusterOneVsRest.csv). ### Minimal code sample. ```python. import pickle. import numpy as np. import pandas as pd. from PIL import Image. import glob. import matplotlib.pyplot as plt. from skimage.morphology import convex_hull_image. from skimage import data, img_as_float. from skimage.util import invert. from scipy.spatial import ConvexHull, convex_hull_plot_2d. from multiprocessing import Pool. import time. import math. from collections import Counter. import scanpy as sc. import networkx as nx. import pandas as pd. import numpy as np. import itertools. import random. from scipy.stats import mannwhitneyu. import os. import warnings. import pickle. import matplotlib.pyplot as plt. import pandas as pd. import gseapy as gp. from matplotlib import pyplot as plt. from matplotlib_venn import venn2. import mygene. import seabo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:3840,testability,log,logfoldchanges,3840,"t random. import matplotlib.pyplot as plt. import numpy as np. import decoupler as dc. from numpy.random import default_rng. from copy import deepcopy. import scanpy as sc. import squidpy as sq. import time. from neighborhood_enrichment import neighborhood_enrichment. import schist as scs. with open('TNBC_41patients_KerenEtAl.pkl', 'rb') as f:. pickle_= pickle.load(f). pickle_=dict(list(pickle_.items())[7:8]). for i in pickle_:. cnt+=1. adata=pickle_[i]. sc.pp.neighbors(adata, n_neighbors=10, n_pcs=3). sc.tl.leiden(adata). scs.inference.planted_model(adata). sc.pp.scale(adata). sc.tl.rank_genes_groups(adata, groupby='ppbm', pts=True, method='wilcoxon'). adata_rankGenes_names, adata_rankGenes_scores, adata_rankGenes_logfoldchanges, adata_rankGenes_pvals, adata_rankGenes_pvals_adj, adata_rankGenes_pts, adata_rankGenes_pts_rest=pd.DataFrame(adata.uns['rank_genes_groups']['names']), pd.DataFrame(adata.uns['rank_genes_groups']['scores']), pd.DataFrame(adata.uns['rank_genes_groups']['logfoldchanges']), pd.DataFrame(adata.uns['rank_genes_groups']['pvals']), pd.DataFrame(adata.uns['rank_genes_groups']['pvals_adj']), pd.DataFrame(adata.uns['rank_genes_groups']['pts']), pd.DataFrame(adata.uns['rank_genes_groups']['pts_rest']). ```. ### Error output. ```pytb. It's not really an error, it's just unexpected output. 1. https://github.com/scverse/scanpy/issues/701 says the p-values should always decrease as we go down the rows of the output matrix, which would make sense because the scores are ordered in decreasing order. 2. https://github.com/scverse/scanpy/issues/1688 says that higher score should always correspond to lower p-value, but that's not what's happening. 3. The official documentation says: ""Structured array to be indexed by group id storing the z-score underlying the computation of a p-value for each gene for each group. Ordered according to scores."" --> But if that's the case, why are the scores and p-values not perfectly opposite (that is lower score always equals h",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:7100,testability,spy,spyder,7100,"0.18.1. jinja2 3.1.2. joblib 1.2.0. kiwisolver 1.4.4. lazy_loader NA. leidenalg 0.10.1. llvmlite 0.40.0. markupsafe 2.1.1. matplotlib 3.5.3. matplotlib_inline 0.1.6. matplotlib_scalebar 0.8.1. matplotlib_venn 0.11.9. mkl 2.4.0. mpl_toolkits NA. msgpack 1.0.5. multipledispatch 0.6.0. multiscale_spatial_image 0.11.2. mygene 3.2.2. natsort 7.1.1. nbinom_ufunc NA. neighborhood_enrichment NA. networkx 3.1. numba 0.57.0. numcodecs 0.11.0. numpy 1.24.3. ome_zarr NA. openpyxl 3.1.2. packaging 23.0. pandas 2.0.3. param 1.13.0. parso 0.8.3. patsy 0.5.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 2.5.2. pooch v1.4.0. prompt_toolkit 3.0.36. psutil 5.9.0. ptyprocess 0.7.0. pure_eval 0.2.2. pyarrow 12.0.1. pycparser 2.21. pyct 0.5.0. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygeos 0.14. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pyproj 3.6.0. pytz 2022.7. pywt 1.4.1. requests 2.31.0. rich NA. rtree 1.0.1. schist v0.7.16. scipy 1.8.0. seaborn 0.12.2. session_info 1.0.0. setuptools 68.0.0. shapely 2.0.1. sip NA. six 1.16.0. skimage 0.20.0. sklearn 1.2.2. socks 1.7.1. spatial_image 0.3.0. spatialdata 0.0.12. sphinxcontrib NA. spyder 5.4.3. spyder_kernels 2.4.3. spydercustomize NA. squidpy 1.3.0. stack_data 0.2.0. statsmodels 0.14.0. tblib 2.0.0. texttable 1.6.7. threadpoolctl 2.2.0. tifffile 2021.7.2. tlz 0.12.0. toolz 0.12.0. tornado 6.3.2. tqdm 4.65.0. traitlets 5.7.1. typing_extensions NA. umap 0.5.3. urllib3 1.26.16. validators 0.20.0. wcwidth 0.2.5. wurlitzer 3.0.2. xarray 2022.12.0. xarray_dataclasses 1.6.0. xarray_schema 0.0.3. xrspatial 0.3.7. yaml 6.0. zarr 2.16.0. zipp NA. zmq 25.1.0. zoneinfo NA. zstandard 0.19.0. -----. IPython 8.12.0. jupyter_client 8.1.0. jupyter_core 5.3.0. -----. Python 3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:50:29) [Clang 14.0.6 ]. macOS-11.6-x86_64-i386-64bit. -----. Session information updated at 2023-08-02 20:20. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:7136,testability,spy,spydercustomize,7136,"0.18.1. jinja2 3.1.2. joblib 1.2.0. kiwisolver 1.4.4. lazy_loader NA. leidenalg 0.10.1. llvmlite 0.40.0. markupsafe 2.1.1. matplotlib 3.5.3. matplotlib_inline 0.1.6. matplotlib_scalebar 0.8.1. matplotlib_venn 0.11.9. mkl 2.4.0. mpl_toolkits NA. msgpack 1.0.5. multipledispatch 0.6.0. multiscale_spatial_image 0.11.2. mygene 3.2.2. natsort 7.1.1. nbinom_ufunc NA. neighborhood_enrichment NA. networkx 3.1. numba 0.57.0. numcodecs 0.11.0. numpy 1.24.3. ome_zarr NA. openpyxl 3.1.2. packaging 23.0. pandas 2.0.3. param 1.13.0. parso 0.8.3. patsy 0.5.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 2.5.2. pooch v1.4.0. prompt_toolkit 3.0.36. psutil 5.9.0. ptyprocess 0.7.0. pure_eval 0.2.2. pyarrow 12.0.1. pycparser 2.21. pyct 0.5.0. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygeos 0.14. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pyproj 3.6.0. pytz 2022.7. pywt 1.4.1. requests 2.31.0. rich NA. rtree 1.0.1. schist v0.7.16. scipy 1.8.0. seaborn 0.12.2. session_info 1.0.0. setuptools 68.0.0. shapely 2.0.1. sip NA. six 1.16.0. skimage 0.20.0. sklearn 1.2.2. socks 1.7.1. spatial_image 0.3.0. spatialdata 0.0.12. sphinxcontrib NA. spyder 5.4.3. spyder_kernels 2.4.3. spydercustomize NA. squidpy 1.3.0. stack_data 0.2.0. statsmodels 0.14.0. tblib 2.0.0. texttable 1.6.7. threadpoolctl 2.2.0. tifffile 2021.7.2. tlz 0.12.0. toolz 0.12.0. tornado 6.3.2. tqdm 4.65.0. traitlets 5.7.1. typing_extensions NA. umap 0.5.3. urllib3 1.26.16. validators 0.20.0. wcwidth 0.2.5. wurlitzer 3.0.2. xarray 2022.12.0. xarray_dataclasses 1.6.0. xarray_schema 0.0.3. xrspatial 0.3.7. yaml 6.0. zarr 2.16.0. zipp NA. zmq 25.1.0. zoneinfo NA. zstandard 0.19.0. -----. IPython 8.12.0. jupyter_client 8.1.0. jupyter_core 5.3.0. -----. Python 3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:50:29) [Clang 14.0.6 ]. macOS-11.6-x86_64-i386-64bit. -----. Session information updated at 2023-08-02 20:20. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:172,usability,confirm,confirmed,172,"Question regarding: sc.rank_genes_groups(); ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When I run sc.rank_genes_groups() on my gene expression data, I do get separate matrices for names, scores, pvals, and pvals_adj. However, the scores don't match the pvals. In other words, with decreasing pvals, we don't always have increasing score. I did check out the description on the main documentations page, and they say they're calculating the zscores underlying the distribution, however if that's the case shouldn't it always be higher with decreasing pval? Also, I went through the code: it looks like they're calculating the scores on the absolute values instead of the real values--why is this? Are the scores basically U1 values corresponding to the pvalues, in whcih case once again lower pvalues should always have higher scores right? ares calculated from the p-values? What's the relation between the two. I have ran sc.rank_genes_groups() on my gene expression data, and I have generated the matrix for cluster-1 versus the rest, for reference. You can see that one, the pavlues don't increase as we go down the rows; and two, the scores seem kinda arbitrary to the p-values. What am I missing here? Thanks a lot, and sorry for the wordy question. [NOTE: This doesn't just depend on a specific type of clustering technique. Irrespective of whether I try the in-built Leiden clustering technique, or the Schit library, or even cluster the cells randomly, this issue keeps occuring--which makes me think it's ether a bug, or I don't quite understand how the score generation works, maybe both. I've provided a little code snipper below.]. [ClusterOneVsRest.csv](https://github.com/scverse/scanpy/files/12243486/ClusterOneVsRest.csv). ### Minimal code",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:255,usability,confirm,confirmed,255,"Question regarding: sc.rank_genes_groups(); ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When I run sc.rank_genes_groups() on my gene expression data, I do get separate matrices for names, scores, pvals, and pvals_adj. However, the scores don't match the pvals. In other words, with decreasing pvals, we don't always have increasing score. I did check out the description on the main documentations page, and they say they're calculating the zscores underlying the distribution, however if that's the case shouldn't it always be higher with decreasing pval? Also, I went through the code: it looks like they're calculating the scores on the absolute values instead of the real values--why is this? Are the scores basically U1 values corresponding to the pvalues, in whcih case once again lower pvalues should always have higher scores right? ares calculated from the p-values? What's the relation between the two. I have ran sc.rank_genes_groups() on my gene expression data, and I have generated the matrix for cluster-1 versus the rest, for reference. You can see that one, the pavlues don't increase as we go down the rows; and two, the scores seem kinda arbitrary to the p-values. What am I missing here? Thanks a lot, and sorry for the wordy question. [NOTE: This doesn't just depend on a specific type of clustering technique. Irrespective of whether I try the in-built Leiden clustering technique, or the Schit library, or even cluster the cells randomly, this issue keeps occuring--which makes me think it's ether a bug, or I don't quite understand how the score generation works, maybe both. I've provided a little code snipper below.]. [ClusterOneVsRest.csv](https://github.com/scverse/scanpy/files/12243486/ClusterOneVsRest.csv). ### Minimal code",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:627,usability,document,documentations,627,"Question regarding: sc.rank_genes_groups(); ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? When I run sc.rank_genes_groups() on my gene expression data, I do get separate matrices for names, scores, pvals, and pvals_adj. However, the scores don't match the pvals. In other words, with decreasing pvals, we don't always have increasing score. I did check out the description on the main documentations page, and they say they're calculating the zscores underlying the distribution, however if that's the case shouldn't it always be higher with decreasing pval? Also, I went through the code: it looks like they're calculating the scores on the absolute values instead of the real values--why is this? Are the scores basically U1 values corresponding to the pvalues, in whcih case once again lower pvalues should always have higher scores right? ares calculated from the p-values? What's the relation between the two. I have ran sc.rank_genes_groups() on my gene expression data, and I have generated the matrix for cluster-1 versus the rest, for reference. You can see that one, the pavlues don't increase as we go down the rows; and two, the scores seem kinda arbitrary to the p-values. What am I missing here? Thanks a lot, and sorry for the wordy question. [NOTE: This doesn't just depend on a specific type of clustering technique. Irrespective of whether I try the in-built Leiden clustering technique, or the Schit library, or even cluster the cells randomly, this issue keeps occuring--which makes me think it's ether a bug, or I don't quite understand how the score generation works, maybe both. I've provided a little code snipper below.]. [ClusterOneVsRest.csv](https://github.com/scverse/scanpy/files/12243486/ClusterOneVsRest.csv). ### Minimal code",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:1988,usability,Minim,Minimal,1988," the pvalues, in whcih case once again lower pvalues should always have higher scores right? ares calculated from the p-values? What's the relation between the two. I have ran sc.rank_genes_groups() on my gene expression data, and I have generated the matrix for cluster-1 versus the rest, for reference. You can see that one, the pavlues don't increase as we go down the rows; and two, the scores seem kinda arbitrary to the p-values. What am I missing here? Thanks a lot, and sorry for the wordy question. [NOTE: This doesn't just depend on a specific type of clustering technique. Irrespective of whether I try the in-built Leiden clustering technique, or the Schit library, or even cluster the cells randomly, this issue keeps occuring--which makes me think it's ether a bug, or I don't quite understand how the score generation works, maybe both. I've provided a little code snipper below.]. [ClusterOneVsRest.csv](https://github.com/scverse/scanpy/files/12243486/ClusterOneVsRest.csv). ### Minimal code sample. ```python. import pickle. import numpy as np. import pandas as pd. from PIL import Image. import glob. import matplotlib.pyplot as plt. from skimage.morphology import convex_hull_image. from skimage import data, img_as_float. from skimage.util import invert. from scipy.spatial import ConvexHull, convex_hull_plot_2d. from multiprocessing import Pool. import time. import math. from collections import Counter. import scanpy as sc. import networkx as nx. import pandas as pd. import numpy as np. import itertools. import random. from scipy.stats import mannwhitneyu. import os. import warnings. import pickle. import matplotlib.pyplot as plt. import pandas as pd. import gseapy as gp. from matplotlib import pyplot as plt. from matplotlib_venn import venn2. import mygene. import seaborn as sns. from gseapy import barplot, dotplot. import random. import matplotlib.pyplot as plt. import numpy as np. import decoupler as dc. from numpy.random import default_rng. from copy import dee",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:4093,usability,Error,Error,4093,"borhood_enrichment. import schist as scs. with open('TNBC_41patients_KerenEtAl.pkl', 'rb') as f:. pickle_= pickle.load(f). pickle_=dict(list(pickle_.items())[7:8]). for i in pickle_:. cnt+=1. adata=pickle_[i]. sc.pp.neighbors(adata, n_neighbors=10, n_pcs=3). sc.tl.leiden(adata). scs.inference.planted_model(adata). sc.pp.scale(adata). sc.tl.rank_genes_groups(adata, groupby='ppbm', pts=True, method='wilcoxon'). adata_rankGenes_names, adata_rankGenes_scores, adata_rankGenes_logfoldchanges, adata_rankGenes_pvals, adata_rankGenes_pvals_adj, adata_rankGenes_pts, adata_rankGenes_pts_rest=pd.DataFrame(adata.uns['rank_genes_groups']['names']), pd.DataFrame(adata.uns['rank_genes_groups']['scores']), pd.DataFrame(adata.uns['rank_genes_groups']['logfoldchanges']), pd.DataFrame(adata.uns['rank_genes_groups']['pvals']), pd.DataFrame(adata.uns['rank_genes_groups']['pvals_adj']), pd.DataFrame(adata.uns['rank_genes_groups']['pts']), pd.DataFrame(adata.uns['rank_genes_groups']['pts_rest']). ```. ### Error output. ```pytb. It's not really an error, it's just unexpected output. 1. https://github.com/scverse/scanpy/issues/701 says the p-values should always decrease as we go down the rows of the output matrix, which would make sense because the scores are ordered in decreasing order. 2. https://github.com/scverse/scanpy/issues/1688 says that higher score should always correspond to lower p-value, but that's not what's happening. 3. The official documentation says: ""Structured array to be indexed by group id storing the z-score underlying the computation of a p-value for each gene for each group. Ordered according to scores."" --> But if that's the case, why are the scores and p-values not perfectly opposite (that is lower score always equals higher p-value. Also, aren't you generating the U-value corresponding to the p-value?). ```. ### Versions. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 9.0.1. PyQt5 NA. appdirs 1.4.4. appnope 0.1.2. asciitree NA. asttokens NA. atomi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:4135,usability,error,error,4135,"with open('TNBC_41patients_KerenEtAl.pkl', 'rb') as f:. pickle_= pickle.load(f). pickle_=dict(list(pickle_.items())[7:8]). for i in pickle_:. cnt+=1. adata=pickle_[i]. sc.pp.neighbors(adata, n_neighbors=10, n_pcs=3). sc.tl.leiden(adata). scs.inference.planted_model(adata). sc.pp.scale(adata). sc.tl.rank_genes_groups(adata, groupby='ppbm', pts=True, method='wilcoxon'). adata_rankGenes_names, adata_rankGenes_scores, adata_rankGenes_logfoldchanges, adata_rankGenes_pvals, adata_rankGenes_pvals_adj, adata_rankGenes_pts, adata_rankGenes_pts_rest=pd.DataFrame(adata.uns['rank_genes_groups']['names']), pd.DataFrame(adata.uns['rank_genes_groups']['scores']), pd.DataFrame(adata.uns['rank_genes_groups']['logfoldchanges']), pd.DataFrame(adata.uns['rank_genes_groups']['pvals']), pd.DataFrame(adata.uns['rank_genes_groups']['pvals_adj']), pd.DataFrame(adata.uns['rank_genes_groups']['pts']), pd.DataFrame(adata.uns['rank_genes_groups']['pts_rest']). ```. ### Error output. ```pytb. It's not really an error, it's just unexpected output. 1. https://github.com/scverse/scanpy/issues/701 says the p-values should always decrease as we go down the rows of the output matrix, which would make sense because the scores are ordered in decreasing order. 2. https://github.com/scverse/scanpy/issues/1688 says that higher score should always correspond to lower p-value, but that's not what's happening. 3. The official documentation says: ""Structured array to be indexed by group id storing the z-score underlying the computation of a p-value for each gene for each group. Ordered according to scores."" --> But if that's the case, why are the scores and p-values not perfectly opposite (that is lower score always equals higher p-value. Also, aren't you generating the U-value corresponding to the p-value?). ```. ### Versions. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 9.0.1. PyQt5 NA. appdirs 1.4.4. appnope 0.1.2. asciitree NA. asttokens NA. atomicwrites 1.4.0. attr 22.1.0. backcall 0.2.0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:4544,usability,document,documentation,4544,"res, adata_rankGenes_logfoldchanges, adata_rankGenes_pvals, adata_rankGenes_pvals_adj, adata_rankGenes_pts, adata_rankGenes_pts_rest=pd.DataFrame(adata.uns['rank_genes_groups']['names']), pd.DataFrame(adata.uns['rank_genes_groups']['scores']), pd.DataFrame(adata.uns['rank_genes_groups']['logfoldchanges']), pd.DataFrame(adata.uns['rank_genes_groups']['pvals']), pd.DataFrame(adata.uns['rank_genes_groups']['pvals_adj']), pd.DataFrame(adata.uns['rank_genes_groups']['pts']), pd.DataFrame(adata.uns['rank_genes_groups']['pts_rest']). ```. ### Error output. ```pytb. It's not really an error, it's just unexpected output. 1. https://github.com/scverse/scanpy/issues/701 says the p-values should always decrease as we go down the rows of the output matrix, which would make sense because the scores are ordered in decreasing order. 2. https://github.com/scverse/scanpy/issues/1688 says that higher score should always correspond to lower p-value, but that's not what's happening. 3. The official documentation says: ""Structured array to be indexed by group id storing the z-score underlying the computation of a p-value for each gene for each group. Ordered according to scores."" --> But if that's the case, why are the scores and p-values not perfectly opposite (that is lower score always equals higher p-value. Also, aren't you generating the U-value corresponding to the p-value?). ```. ### Versions. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 9.0.1. PyQt5 NA. appdirs 1.4.4. appnope 0.1.2. asciitree NA. asttokens NA. atomicwrites 1.4.0. attr 22.1.0. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. biothings_client 0.3.0. brotli NA. cairo 1.23.0. certifi 2023.07.22. cffi 1.15.1. chardet 4.0.0. charset_normalizer 2.0.4. cloudpickle 2.2.1. colorama 0.4.6. comm 0.1.2. cv2 4.8.0. cycler 0.10.0. cython_runtime NA. cytoolz 0.12.0. dask 2023.6.0. dask_image 2023.03.0. datashader 0.15.1. datashape 0.5.2. datatree 0.0.12. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. decoupler 1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2586:7291,usability,tool,toolz,7291,"0.18.1. jinja2 3.1.2. joblib 1.2.0. kiwisolver 1.4.4. lazy_loader NA. leidenalg 0.10.1. llvmlite 0.40.0. markupsafe 2.1.1. matplotlib 3.5.3. matplotlib_inline 0.1.6. matplotlib_scalebar 0.8.1. matplotlib_venn 0.11.9. mkl 2.4.0. mpl_toolkits NA. msgpack 1.0.5. multipledispatch 0.6.0. multiscale_spatial_image 0.11.2. mygene 3.2.2. natsort 7.1.1. nbinom_ufunc NA. neighborhood_enrichment NA. networkx 3.1. numba 0.57.0. numcodecs 0.11.0. numpy 1.24.3. ome_zarr NA. openpyxl 3.1.2. packaging 23.0. pandas 2.0.3. param 1.13.0. parso 0.8.3. patsy 0.5.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 2.5.2. pooch v1.4.0. prompt_toolkit 3.0.36. psutil 5.9.0. ptyprocess 0.7.0. pure_eval 0.2.2. pyarrow 12.0.1. pycparser 2.21. pyct 0.5.0. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygeos 0.14. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pyproj 3.6.0. pytz 2022.7. pywt 1.4.1. requests 2.31.0. rich NA. rtree 1.0.1. schist v0.7.16. scipy 1.8.0. seaborn 0.12.2. session_info 1.0.0. setuptools 68.0.0. shapely 2.0.1. sip NA. six 1.16.0. skimage 0.20.0. sklearn 1.2.2. socks 1.7.1. spatial_image 0.3.0. spatialdata 0.0.12. sphinxcontrib NA. spyder 5.4.3. spyder_kernels 2.4.3. spydercustomize NA. squidpy 1.3.0. stack_data 0.2.0. statsmodels 0.14.0. tblib 2.0.0. texttable 1.6.7. threadpoolctl 2.2.0. tifffile 2021.7.2. tlz 0.12.0. toolz 0.12.0. tornado 6.3.2. tqdm 4.65.0. traitlets 5.7.1. typing_extensions NA. umap 0.5.3. urllib3 1.26.16. validators 0.20.0. wcwidth 0.2.5. wurlitzer 3.0.2. xarray 2022.12.0. xarray_dataclasses 1.6.0. xarray_schema 0.0.3. xrspatial 0.3.7. yaml 6.0. zarr 2.16.0. zipp NA. zmq 25.1.0. zoneinfo NA. zstandard 0.19.0. -----. IPython 8.12.0. jupyter_client 8.1.0. jupyter_core 5.3.0. -----. Python 3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:50:29) [Clang 14.0.6 ]. macOS-11.6-x86_64-i386-64bit. -----. Session information updated at 2023-08-02 20:20. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586
https://github.com/scverse/scanpy/issues/2587:809,availability,state,stated,809,"Unexpected behavior of `sc.pp.neighbors`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hej! Thanks for maintaining such a great package! This issue relates another [issue](https://github.com/scverse/squidpy/issues/735) posted (by me) in the `squidpy` repo, but I think it might be worth bringing up here as well. . The issue in question is how the `sc.pp.neighbors` function returns an inconsistent number of neighbors even when `knn=True`. In the [documentation](https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.neighbors.html) of `sc.pp.neighbors` it's stated that :. >n_neighbors: The size of local neighborhood (in terms of number of neighboring data points) used for manifold approximation. Larger values result in more global views of the manifold, while smaller values result in more local data being preserved. In general values should be in the range 2 to 100. If knn is True, number of nearest neighbors to be searched. If knn is False, a Gaussian kernel width is set to the distance of the n_neighbors neighbor. and. > knn: If True, use a hard threshold to restrict the number of neighbors to n_neighbors, that is, consider a knn graph. Otherwise, use a Gaussian Kernel to assign low weights to neighbors more distant than the n_neighbors nearest neighbor. as well as . > Connectivities: Weighted adjacency matrix of the neighborhood graph of data points. Weights should be interpreted as connectivities. Hence I would expect that the number of non-zero elements in `adata.obsp['connectivities']` in an object to which `sc.pp.neighbors(adata, n_neighbors = k, knn = True)` have been applied, would sum to `k` for each row. However, when inspecting these results, it is not true. The number of non-zero elements in a row varies between ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2587
https://github.com/scverse/scanpy/issues/2587:2879,availability,Error,Error,2879," row. However, when inspecting these results, it is not true. The number of non-zero elements in a row varies between both higher as well as lower values than the specified `n_neighbors` (obviously, sometimes it's also the expected `n_neighbors` value). Perhaps I'm misunderstanding something, but this behavior is somewhat counterintuitive to me and not what I expect; happy to be corrected though! /Alma. ### Minimal code sample. ```python. # Import packages. import scanpy as sc. import anndata as ad. import numpy as np. # set random seed. np.random.seed(42). # create dummy data. adata = ad.AnnData(shape=(1000,1)). adata.obsm['rep'] = np.random.random(size = (1000,2)). # get spatial connectivities. k = 10. sc.pp.neighbors(adata, n_neighbors=k, use_rep = 'rep', knn = True). # get and count connectivities for each cell. gr = adata.obsp['connectivities']. nn = (np.array(gr.todense()) > 0).sum(axis=1).flatten(). # check if neighbors are equal to k. np.testing.assert_equal(nn,k). ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. anyio NA. arrow 1.2.3. asciitree NA. asttokens NA. attr 23.1.0. babel 2.12.1. backcall 0.2.0. brotli NA. certifi 2022.12.07. cffi 1.15.1. charset_normalizer 2.0.4. cloudpickle 2.2.1. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dask 2023.5.1. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. entrypoints 0.4. executing 1.2.0. fasteners 0.18. fastjsonschema NA. fqdn NA. h5py 3.8.0. idna 3.4. igraph 0.10.4. ipykernel 6.23.1. isoduration NA. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonpointer 2.3. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.22.1. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.1. markupsafe 2.1.2. matplotlib 3.7.1. mpl_toolkits NA. msgpack 1.0.5. natsort 8.3.1. nbformat 5.8.0. numba 0.57.1. numcodecs 0.11.0. numpy 1.23.4. overrides NA. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickl",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2587
https://github.com/scverse/scanpy/issues/2587:210,deployability,version,version,210,"Unexpected behavior of `sc.pp.neighbors`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hej! Thanks for maintaining such a great package! This issue relates another [issue](https://github.com/scverse/squidpy/issues/735) posted (by me) in the `squidpy` repo, but I think it might be worth bringing up here as well. . The issue in question is how the `sc.pp.neighbors` function returns an inconsistent number of neighbors even when `knn=True`. In the [documentation](https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.neighbors.html) of `sc.pp.neighbors` it's stated that :. >n_neighbors: The size of local neighborhood (in terms of number of neighboring data points) used for manifold approximation. Larger values result in more global views of the manifold, while smaller values result in more local data being preserved. In general values should be in the range 2 to 100. If knn is True, number of nearest neighbors to be searched. If knn is False, a Gaussian kernel width is set to the distance of the n_neighbors neighbor. and. > knn: If True, use a hard threshold to restrict the number of neighbors to n_neighbors, that is, consider a knn graph. Otherwise, use a Gaussian Kernel to assign low weights to neighbors more distant than the n_neighbors nearest neighbor. as well as . > Connectivities: Weighted adjacency matrix of the neighborhood graph of data points. Weights should be interpreted as connectivities. Hence I would expect that the number of non-zero elements in `adata.obsp['connectivities']` in an object to which `sc.pp.neighbors(adata, n_neighbors = k, knn = True)` have been applied, would sum to `k` for each row. However, when inspecting these results, it is not true. The number of non-zero elements in a row varies between ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2587
https://github.com/scverse/scanpy/issues/2587:2912,deployability,Version,Versions,2912,"se results, it is not true. The number of non-zero elements in a row varies between both higher as well as lower values than the specified `n_neighbors` (obviously, sometimes it's also the expected `n_neighbors` value). Perhaps I'm misunderstanding something, but this behavior is somewhat counterintuitive to me and not what I expect; happy to be corrected though! /Alma. ### Minimal code sample. ```python. # Import packages. import scanpy as sc. import anndata as ad. import numpy as np. # set random seed. np.random.seed(42). # create dummy data. adata = ad.AnnData(shape=(1000,1)). adata.obsm['rep'] = np.random.random(size = (1000,2)). # get spatial connectivities. k = 10. sc.pp.neighbors(adata, n_neighbors=k, use_rep = 'rep', knn = True). # get and count connectivities for each cell. gr = adata.obsp['connectivities']. nn = (np.array(gr.todense()) > 0).sum(axis=1).flatten(). # check if neighbors are equal to k. np.testing.assert_equal(nn,k). ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. anyio NA. arrow 1.2.3. asciitree NA. asttokens NA. attr 23.1.0. babel 2.12.1. backcall 0.2.0. brotli NA. certifi 2022.12.07. cffi 1.15.1. charset_normalizer 2.0.4. cloudpickle 2.2.1. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dask 2023.5.1. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. entrypoints 0.4. executing 1.2.0. fasteners 0.18. fastjsonschema NA. fqdn NA. h5py 3.8.0. idna 3.4. igraph 0.10.4. ipykernel 6.23.1. isoduration NA. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonpointer 2.3. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.22.1. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.1. markupsafe 2.1.2. matplotlib 3.7.1. mpl_toolkits NA. msgpack 1.0.5. natsort 8.3.1. nbformat 5.8.0. numba 0.57.1. numcodecs 0.11.0. numpy 1.23.4. overrides NA. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. pl",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2587
https://github.com/scverse/scanpy/issues/2587:5022,deployability,updat,updated,5022,"1. backcall 0.2.0. brotli NA. certifi 2022.12.07. cffi 1.15.1. charset_normalizer 2.0.4. cloudpickle 2.2.1. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dask 2023.5.1. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. entrypoints 0.4. executing 1.2.0. fasteners 0.18. fastjsonschema NA. fqdn NA. h5py 3.8.0. idna 3.4. igraph 0.10.4. ipykernel 6.23.1. isoduration NA. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonpointer 2.3. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.22.1. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.1. markupsafe 2.1.2. matplotlib 3.7.1. mpl_toolkits NA. msgpack 1.0.5. natsort 8.3.1. nbformat 5.8.0. numba 0.57.1. numcodecs 0.11.0. numpy 1.23.4. overrides NA. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.1. prometheus_client NA. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pyarrow 12.0.1. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.15.1. pynndescent 0.5.10. pyparsing 3.0.9. pyrsistent NA. pythonjsonlogger NA. pytz 2023.3. requests 2.28.1. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. scipy 1.10.1. send2trash NA. session_info 1.0.0. setuptools 66.0.0. six 1.16.0. sklearn 1.2.2. sniffio 1.3.0. socks 1.7.1. stack_data 0.6.2. tblib 2.0.0. texttable 1.6.7. threadpoolctl 3.1.0. tlz 0.12.0. toolz 0.12.0. torch 1.13.1. tornado 6.3.2. tqdm 4.65.0. traitlets 5.9.0. typing_extensions NA. umap 0.5.3. uri_template NA. urllib3 1.26.15. wcwidth 0.2.6. webcolors 1.13. websocket 1.5.2. yaml 6.0. zarr 2.14.2. zipp NA. zmq 25.1.0. zoneinfo NA. -----. IPython 8.13.2. jupyter_client 8.2.0. jupyter_core 5.3.0. jupyterlab 4.0.1. -----. Python 3.10.11 (main, Apr 20 2023, 19:02:41) [GCC 11.2.0]. Linux-3.10.0-1160.66.1.el7.x86_64-x86_64-with-glibc2.17. -----. Session information updated at 2023-08-02 14:21. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2587
https://github.com/scverse/scanpy/issues/2587:3156,energy efficiency,cloud,cloudpickle,3156,"ng something, but this behavior is somewhat counterintuitive to me and not what I expect; happy to be corrected though! /Alma. ### Minimal code sample. ```python. # Import packages. import scanpy as sc. import anndata as ad. import numpy as np. # set random seed. np.random.seed(42). # create dummy data. adata = ad.AnnData(shape=(1000,1)). adata.obsm['rep'] = np.random.random(size = (1000,2)). # get spatial connectivities. k = 10. sc.pp.neighbors(adata, n_neighbors=k, use_rep = 'rep', knn = True). # get and count connectivities for each cell. gr = adata.obsp['connectivities']. nn = (np.array(gr.todense()) > 0).sum(axis=1).flatten(). # check if neighbors are equal to k. np.testing.assert_equal(nn,k). ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. anyio NA. arrow 1.2.3. asciitree NA. asttokens NA. attr 23.1.0. babel 2.12.1. backcall 0.2.0. brotli NA. certifi 2022.12.07. cffi 1.15.1. charset_normalizer 2.0.4. cloudpickle 2.2.1. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dask 2023.5.1. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. entrypoints 0.4. executing 1.2.0. fasteners 0.18. fastjsonschema NA. fqdn NA. h5py 3.8.0. idna 3.4. igraph 0.10.4. ipykernel 6.23.1. isoduration NA. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonpointer 2.3. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.22.1. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.1. markupsafe 2.1.2. matplotlib 3.7.1. mpl_toolkits NA. msgpack 1.0.5. natsort 8.3.1. nbformat 5.8.0. numba 0.57.1. numcodecs 0.11.0. numpy 1.23.4. overrides NA. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.1. prometheus_client NA. prompt_toolkit 3.0.38. psutil 5.9.5. ptyprocess 0.7.0. pure_eval 0.2.2. pyarrow 12.0.1. pycparser 2.21. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_traci",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2587
https://github.com/scverse/scanpy/issues/2587:210,integrability,version,version,210,"Unexpected behavior of `sc.pp.neighbors`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hej! Thanks for maintaining such a great package! This issue relates another [issue](https://github.com/scverse/squidpy/issues/735) posted (by me) in the `squidpy` repo, but I think it might be worth bringing up here as well. . The issue in question is how the `sc.pp.neighbors` function returns an inconsistent number of neighbors even when `knn=True`. In the [documentation](https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.neighbors.html) of `sc.pp.neighbors` it's stated that :. >n_neighbors: The size of local neighborhood (in terms of number of neighboring data points) used for manifold approximation. Larger values result in more global views of the manifold, while smaller values result in more local data being preserved. In general values should be in the range 2 to 100. If knn is True, number of nearest neighbors to be searched. If knn is False, a Gaussian kernel width is set to the distance of the n_neighbors neighbor. and. > knn: If True, use a hard threshold to restrict the number of neighbors to n_neighbors, that is, consider a knn graph. Otherwise, use a Gaussian Kernel to assign low weights to neighbors more distant than the n_neighbors nearest neighbor. as well as . > Connectivities: Weighted adjacency matrix of the neighborhood graph of data points. Weights should be interpreted as connectivities. Hence I would expect that the number of non-zero elements in `adata.obsp['connectivities']` in an object to which `sc.pp.neighbors(adata, n_neighbors = k, knn = True)` have been applied, would sum to `k` for each row. However, when inspecting these results, it is not true. The number of non-zero elements in a row varies between ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2587
https://github.com/scverse/scanpy/issues/2587:809,integrability,state,stated,809,"Unexpected behavior of `sc.pp.neighbors`; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened? Hej! Thanks for maintaining such a great package! This issue relates another [issue](https://github.com/scverse/squidpy/issues/735) posted (by me) in the `squidpy` repo, but I think it might be worth bringing up here as well. . The issue in question is how the `sc.pp.neighbors` function returns an inconsistent number of neighbors even when `knn=True`. In the [documentation](https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.neighbors.html) of `sc.pp.neighbors` it's stated that :. >n_neighbors: The size of local neighborhood (in terms of number of neighboring data points) used for manifold approximation. Larger values result in more global views of the manifold, while smaller values result in more local data being preserved. In general values should be in the range 2 to 100. If knn is True, number of nearest neighbors to be searched. If knn is False, a Gaussian kernel width is set to the distance of the n_neighbors neighbor. and. > knn: If True, use a hard threshold to restrict the number of neighbors to n_neighbors, that is, consider a knn graph. Otherwise, use a Gaussian Kernel to assign low weights to neighbors more distant than the n_neighbors nearest neighbor. as well as . > Connectivities: Weighted adjacency matrix of the neighborhood graph of data points. Weights should be interpreted as connectivities. Hence I would expect that the number of non-zero elements in `adata.obsp['connectivities']` in an object to which `sc.pp.neighbors(adata, n_neighbors = k, knn = True)` have been applied, would sum to `k` for each row. However, when inspecting these results, it is not true. The number of non-zero elements in a row varies between ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2587
https://github.com/scverse/scanpy/issues/2587:2912,integrability,Version,Versions,2912,"se results, it is not true. The number of non-zero elements in a row varies between both higher as well as lower values than the specified `n_neighbors` (obviously, sometimes it's also the expected `n_neighbors` value). Perhaps I'm misunderstanding something, but this behavior is somewhat counterintuitive to me and not what I expect; happy to be corrected though! /Alma. ### Minimal code sample. ```python. # Import packages. import scanpy as sc. import anndata as ad. import numpy as np. # set random seed. np.random.seed(42). # create dummy data. adata = ad.AnnData(shape=(1000,1)). adata.obsm['rep'] = np.random.random(size = (1000,2)). # get spatial connectivities. k = 10. sc.pp.neighbors(adata, n_neighbors=k, use_rep = 'rep', knn = True). # get and count connectivities for each cell. gr = adata.obsp['connectivities']. nn = (np.array(gr.todense()) > 0).sum(axis=1).flatten(). # check if neighbors are equal to k. np.testing.assert_equal(nn,k). ```. ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.9.1. scanpy 1.9.3. -----. PIL 9.4.0. anyio NA. arrow 1.2.3. asciitree NA. asttokens NA. attr 23.1.0. babel 2.12.1. backcall 0.2.0. brotli NA. certifi 2022.12.07. cffi 1.15.1. charset_normalizer 2.0.4. cloudpickle 2.2.1. comm 0.1.3. cycler 0.10.0. cython_runtime NA. dask 2023.5.1. dateutil 2.8.2. debugpy 1.6.7. decorator 5.1.1. defusedxml 0.7.1. entrypoints 0.4. executing 1.2.0. fasteners 0.18. fastjsonschema NA. fqdn NA. h5py 3.8.0. idna 3.4. igraph 0.10.4. ipykernel 6.23.1. isoduration NA. jedi 0.18.2. jinja2 3.1.2. joblib 1.2.0. json5 NA. jsonpointer 2.3. jsonschema 4.17.3. jupyter_events 0.6.3. jupyter_server 2.6.0. jupyterlab_server 2.22.1. kiwisolver 1.4.4. leidenalg 0.9.1. llvmlite 0.40.1. markupsafe 2.1.2. matplotlib 3.7.1. mpl_toolkits NA. msgpack 1.0.5. natsort 8.3.1. nbformat 5.8.0. numba 0.57.1. numcodecs 0.11.0. numpy 1.23.4. overrides NA. packaging 23.1. pandas 2.0.1. parso 0.8.3. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. pl",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2587
