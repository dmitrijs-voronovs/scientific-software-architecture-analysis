id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/root-project/root/pull/8626:781,integrability,API,API,781,"<img src=""https://www.deepcode.ai/icons/green_check.svg"" width= ""50px"" align= ""left""/> Congratulations :tada:. DeepCode [analyzed](https://www.deepcode.ai/app/gh/root-project/root/fb72c0ef18b8d5e3d7d037195dd58ed2bfafc02e/root-project/root/80df257929e2686b7264e63284b6d701a11c0c09/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) your code in 4.639 seconds and we found no issues. Enjoy a moment of no bugs :sunny:. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/fb72c0ef18b8d5e3d7d037195dd58ed2bfafc02e/root-project/root/80df257929e2686b7264e63284b6d701a11c0c09/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:781,interoperability,API,API,781,"<img src=""https://www.deepcode.ai/icons/green_check.svg"" width= ""50px"" align= ""left""/> Congratulations :tada:. DeepCode [analyzed](https://www.deepcode.ai/app/gh/root-project/root/fb72c0ef18b8d5e3d7d037195dd58ed2bfafc02e/root-project/root/80df257929e2686b7264e63284b6d701a11c0c09/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) your code in 4.639 seconds and we found no issues. Enjoy a moment of no bugs :sunny:. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/fb72c0ef18b8d5e3d7d037195dd58ed2bfafc02e/root-project/root/80df257929e2686b7264e63284b6d701a11c0c09/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:971,interoperability,plug,plugins,971,"<img src=""https://www.deepcode.ai/icons/green_check.svg"" width= ""50px"" align= ""left""/> Congratulations :tada:. DeepCode [analyzed](https://www.deepcode.ai/app/gh/root-project/root/fb72c0ef18b8d5e3d7d037195dd58ed2bfafc02e/root-project/root/80df257929e2686b7264e63284b6d701a11c0c09/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) your code in 4.639 seconds and we found no issues. Enjoy a moment of no bugs :sunny:. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/fb72c0ef18b8d5e3d7d037195dd58ed2bfafc02e/root-project/root/80df257929e2686b7264e63284b6d701a11c0c09/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:1043,interoperability,plug,plugin,1043,"<img src=""https://www.deepcode.ai/icons/green_check.svg"" width= ""50px"" align= ""left""/> Congratulations :tada:. DeepCode [analyzed](https://www.deepcode.ai/app/gh/root-project/root/fb72c0ef18b8d5e3d7d037195dd58ed2bfafc02e/root-project/root/80df257929e2686b7264e63284b6d701a11c0c09/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) your code in 4.639 seconds and we found no issues. Enjoy a moment of no bugs :sunny:. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/fb72c0ef18b8d5e3d7d037195dd58ed2bfafc02e/root-project/root/80df257929e2686b7264e63284b6d701a11c0c09/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:1094,interoperability,plug,plugin,1094,"<img src=""https://www.deepcode.ai/icons/green_check.svg"" width= ""50px"" align= ""left""/> Congratulations :tada:. DeepCode [analyzed](https://www.deepcode.ai/app/gh/root-project/root/fb72c0ef18b8d5e3d7d037195dd58ed2bfafc02e/root-project/root/80df257929e2686b7264e63284b6d701a11c0c09/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) your code in 4.639 seconds and we found no issues. Enjoy a moment of no bugs :sunny:. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/fb72c0ef18b8d5e3d7d037195dd58ed2bfafc02e/root-project/root/80df257929e2686b7264e63284b6d701a11c0c09/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:769,modifiability,servic,service,769,"<img src=""https://www.deepcode.ai/icons/green_check.svg"" width= ""50px"" align= ""left""/> Congratulations :tada:. DeepCode [analyzed](https://www.deepcode.ai/app/gh/root-project/root/fb72c0ef18b8d5e3d7d037195dd58ed2bfafc02e/root-project/root/80df257929e2686b7264e63284b6d701a11c0c09/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) your code in 4.639 seconds and we found no issues. Enjoy a moment of no bugs :sunny:. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/fb72c0ef18b8d5e3d7d037195dd58ed2bfafc02e/root-project/root/80df257929e2686b7264e63284b6d701a11c0c09/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:126,deployability,integr,integrated,126,"Thanks, @egpbos and @jspaaks ! Validation succeeds now. > suggest to add a GitHub Actions workflow. AFAICT we need this to be integrated with our release workflow. Once that's a GH Action we can also migrate the `CITATION.cff` generation and validation there. I *think* it's fairly hard to break the file given that it's generated by a script; I also dislike the pile of dependencies of `cffconvert` for a simple thing like validation, IMO it's too heavy to add it to the script producing `CITATION.cff`.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:146,deployability,releas,release,146,"Thanks, @egpbos and @jspaaks ! Validation succeeds now. > suggest to add a GitHub Actions workflow. AFAICT we need this to be integrated with our release workflow. Once that's a GH Action we can also migrate the `CITATION.cff` generation and validation there. I *think* it's fairly hard to break the file given that it's generated by a script; I also dislike the pile of dependencies of `cffconvert` for a simple thing like validation, IMO it's too heavy to add it to the script producing `CITATION.cff`.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:371,deployability,depend,dependencies,371,"Thanks, @egpbos and @jspaaks ! Validation succeeds now. > suggest to add a GitHub Actions workflow. AFAICT we need this to be integrated with our release workflow. Once that's a GH Action we can also migrate the `CITATION.cff` generation and validation there. I *think* it's fairly hard to break the file given that it's generated by a script; I also dislike the pile of dependencies of `cffconvert` for a simple thing like validation, IMO it's too heavy to add it to the script producing `CITATION.cff`.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:126,integrability,integr,integrated,126,"Thanks, @egpbos and @jspaaks ! Validation succeeds now. > suggest to add a GitHub Actions workflow. AFAICT we need this to be integrated with our release workflow. Once that's a GH Action we can also migrate the `CITATION.cff` generation and validation there. I *think* it's fairly hard to break the file given that it's generated by a script; I also dislike the pile of dependencies of `cffconvert` for a simple thing like validation, IMO it's too heavy to add it to the script producing `CITATION.cff`.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:371,integrability,depend,dependencies,371,"Thanks, @egpbos and @jspaaks ! Validation succeeds now. > suggest to add a GitHub Actions workflow. AFAICT we need this to be integrated with our release workflow. Once that's a GH Action we can also migrate the `CITATION.cff` generation and validation there. I *think* it's fairly hard to break the file given that it's generated by a script; I also dislike the pile of dependencies of `cffconvert` for a simple thing like validation, IMO it's too heavy to add it to the script producing `CITATION.cff`.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:126,interoperability,integr,integrated,126,"Thanks, @egpbos and @jspaaks ! Validation succeeds now. > suggest to add a GitHub Actions workflow. AFAICT we need this to be integrated with our release workflow. Once that's a GH Action we can also migrate the `CITATION.cff` generation and validation there. I *think* it's fairly hard to break the file given that it's generated by a script; I also dislike the pile of dependencies of `cffconvert` for a simple thing like validation, IMO it's too heavy to add it to the script producing `CITATION.cff`.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:126,modifiability,integr,integrated,126,"Thanks, @egpbos and @jspaaks ! Validation succeeds now. > suggest to add a GitHub Actions workflow. AFAICT we need this to be integrated with our release workflow. Once that's a GH Action we can also migrate the `CITATION.cff` generation and validation there. I *think* it's fairly hard to break the file given that it's generated by a script; I also dislike the pile of dependencies of `cffconvert` for a simple thing like validation, IMO it's too heavy to add it to the script producing `CITATION.cff`.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:371,modifiability,depend,dependencies,371,"Thanks, @egpbos and @jspaaks ! Validation succeeds now. > suggest to add a GitHub Actions workflow. AFAICT we need this to be integrated with our release workflow. Once that's a GH Action we can also migrate the `CITATION.cff` generation and validation there. I *think* it's fairly hard to break the file given that it's generated by a script; I also dislike the pile of dependencies of `cffconvert` for a simple thing like validation, IMO it's too heavy to add it to the script producing `CITATION.cff`.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:126,reliability,integr,integrated,126,"Thanks, @egpbos and @jspaaks ! Validation succeeds now. > suggest to add a GitHub Actions workflow. AFAICT we need this to be integrated with our release workflow. Once that's a GH Action we can also migrate the `CITATION.cff` generation and validation there. I *think* it's fairly hard to break the file given that it's generated by a script; I also dislike the pile of dependencies of `cffconvert` for a simple thing like validation, IMO it's too heavy to add it to the script producing `CITATION.cff`.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:31,safety,Valid,Validation,31,"Thanks, @egpbos and @jspaaks ! Validation succeeds now. > suggest to add a GitHub Actions workflow. AFAICT we need this to be integrated with our release workflow. Once that's a GH Action we can also migrate the `CITATION.cff` generation and validation there. I *think* it's fairly hard to break the file given that it's generated by a script; I also dislike the pile of dependencies of `cffconvert` for a simple thing like validation, IMO it's too heavy to add it to the script producing `CITATION.cff`.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:242,safety,valid,validation,242,"Thanks, @egpbos and @jspaaks ! Validation succeeds now. > suggest to add a GitHub Actions workflow. AFAICT we need this to be integrated with our release workflow. Once that's a GH Action we can also migrate the `CITATION.cff` generation and validation there. I *think* it's fairly hard to break the file given that it's generated by a script; I also dislike the pile of dependencies of `cffconvert` for a simple thing like validation, IMO it's too heavy to add it to the script producing `CITATION.cff`.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:371,safety,depend,dependencies,371,"Thanks, @egpbos and @jspaaks ! Validation succeeds now. > suggest to add a GitHub Actions workflow. AFAICT we need this to be integrated with our release workflow. Once that's a GH Action we can also migrate the `CITATION.cff` generation and validation there. I *think* it's fairly hard to break the file given that it's generated by a script; I also dislike the pile of dependencies of `cffconvert` for a simple thing like validation, IMO it's too heavy to add it to the script producing `CITATION.cff`.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:424,safety,valid,validation,424,"Thanks, @egpbos and @jspaaks ! Validation succeeds now. > suggest to add a GitHub Actions workflow. AFAICT we need this to be integrated with our release workflow. Once that's a GH Action we can also migrate the `CITATION.cff` generation and validation there. I *think* it's fairly hard to break the file given that it's generated by a script; I also dislike the pile of dependencies of `cffconvert` for a simple thing like validation, IMO it's too heavy to add it to the script producing `CITATION.cff`.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:31,security,Validat,Validation,31,"Thanks, @egpbos and @jspaaks ! Validation succeeds now. > suggest to add a GitHub Actions workflow. AFAICT we need this to be integrated with our release workflow. Once that's a GH Action we can also migrate the `CITATION.cff` generation and validation there. I *think* it's fairly hard to break the file given that it's generated by a script; I also dislike the pile of dependencies of `cffconvert` for a simple thing like validation, IMO it's too heavy to add it to the script producing `CITATION.cff`.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:126,security,integr,integrated,126,"Thanks, @egpbos and @jspaaks ! Validation succeeds now. > suggest to add a GitHub Actions workflow. AFAICT we need this to be integrated with our release workflow. Once that's a GH Action we can also migrate the `CITATION.cff` generation and validation there. I *think* it's fairly hard to break the file given that it's generated by a script; I also dislike the pile of dependencies of `cffconvert` for a simple thing like validation, IMO it's too heavy to add it to the script producing `CITATION.cff`.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:242,security,validat,validation,242,"Thanks, @egpbos and @jspaaks ! Validation succeeds now. > suggest to add a GitHub Actions workflow. AFAICT we need this to be integrated with our release workflow. Once that's a GH Action we can also migrate the `CITATION.cff` generation and validation there. I *think* it's fairly hard to break the file given that it's generated by a script; I also dislike the pile of dependencies of `cffconvert` for a simple thing like validation, IMO it's too heavy to add it to the script producing `CITATION.cff`.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:424,security,validat,validation,424,"Thanks, @egpbos and @jspaaks ! Validation succeeds now. > suggest to add a GitHub Actions workflow. AFAICT we need this to be integrated with our release workflow. Once that's a GH Action we can also migrate the `CITATION.cff` generation and validation there. I *think* it's fairly hard to break the file given that it's generated by a script; I also dislike the pile of dependencies of `cffconvert` for a simple thing like validation, IMO it's too heavy to add it to the script producing `CITATION.cff`.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:126,testability,integr,integrated,126,"Thanks, @egpbos and @jspaaks ! Validation succeeds now. > suggest to add a GitHub Actions workflow. AFAICT we need this to be integrated with our release workflow. Once that's a GH Action we can also migrate the `CITATION.cff` generation and validation there. I *think* it's fairly hard to break the file given that it's generated by a script; I also dislike the pile of dependencies of `cffconvert` for a simple thing like validation, IMO it's too heavy to add it to the script producing `CITATION.cff`.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:371,testability,depend,dependencies,371,"Thanks, @egpbos and @jspaaks ! Validation succeeds now. > suggest to add a GitHub Actions workflow. AFAICT we need this to be integrated with our release workflow. Once that's a GH Action we can also migrate the `CITATION.cff` generation and validation there. I *think* it's fairly hard to break the file given that it's generated by a script; I also dislike the pile of dependencies of `cffconvert` for a simple thing like validation, IMO it's too heavy to add it to the script producing `CITATION.cff`.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:406,testability,simpl,simple,406,"Thanks, @egpbos and @jspaaks ! Validation succeeds now. > suggest to add a GitHub Actions workflow. AFAICT we need this to be integrated with our release workflow. Once that's a GH Action we can also migrate the `CITATION.cff` generation and validation there. I *think* it's fairly hard to break the file given that it's generated by a script; I also dislike the pile of dependencies of `cffconvert` for a simple thing like validation, IMO it's too heavy to add it to the script producing `CITATION.cff`.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:90,usability,workflow,workflow,90,"Thanks, @egpbos and @jspaaks ! Validation succeeds now. > suggest to add a GitHub Actions workflow. AFAICT we need this to be integrated with our release workflow. Once that's a GH Action we can also migrate the `CITATION.cff` generation and validation there. I *think* it's fairly hard to break the file given that it's generated by a script; I also dislike the pile of dependencies of `cffconvert` for a simple thing like validation, IMO it's too heavy to add it to the script producing `CITATION.cff`.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:154,usability,workflow,workflow,154,"Thanks, @egpbos and @jspaaks ! Validation succeeds now. > suggest to add a GitHub Actions workflow. AFAICT we need this to be integrated with our release workflow. Once that's a GH Action we can also migrate the `CITATION.cff` generation and validation there. I *think* it's fairly hard to break the file given that it's generated by a script; I also dislike the pile of dependencies of `cffconvert` for a simple thing like validation, IMO it's too heavy to add it to the script producing `CITATION.cff`.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:406,usability,simpl,simple,406,"Thanks, @egpbos and @jspaaks ! Validation succeeds now. > suggest to add a GitHub Actions workflow. AFAICT we need this to be integrated with our release workflow. Once that's a GH Action we can also migrate the `CITATION.cff` generation and validation there. I *think* it's fairly hard to break the file given that it's generated by a script; I also dislike the pile of dependencies of `cffconvert` for a simple thing like validation, IMO it's too heavy to add it to the script producing `CITATION.cff`.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:167,deployability,instal,install,167,"> IMO it's too heavy to add it to the script producing CITATION.cff. Makes sense. What I would do in the GH Actions workflow would probably be something like:. 1. pip install and run the validation script there (so that it doesn't have to be included in the script, but can still be used to check PRs). 2. Maybe also run `makeCITATION.py` and compare the output to the existing CITATION.cff file, just to make sure both files were updated. But, again, can be added later. I'm not sure how much you want to depend on GitHub Actions anyway, perhaps it makes more sense to automate things elsewhere.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:431,deployability,updat,updated,431,"> IMO it's too heavy to add it to the script producing CITATION.cff. Makes sense. What I would do in the GH Actions workflow would probably be something like:. 1. pip install and run the validation script there (so that it doesn't have to be included in the script, but can still be used to check PRs). 2. Maybe also run `makeCITATION.py` and compare the output to the existing CITATION.cff file, just to make sure both files were updated. But, again, can be added later. I'm not sure how much you want to depend on GitHub Actions anyway, perhaps it makes more sense to automate things elsewhere.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:506,deployability,depend,depend,506,"> IMO it's too heavy to add it to the script producing CITATION.cff. Makes sense. What I would do in the GH Actions workflow would probably be something like:. 1. pip install and run the validation script there (so that it doesn't have to be included in the script, but can still be used to check PRs). 2. Maybe also run `makeCITATION.py` and compare the output to the existing CITATION.cff file, just to make sure both files were updated. But, again, can be added later. I'm not sure how much you want to depend on GitHub Actions anyway, perhaps it makes more sense to automate things elsewhere.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:570,deployability,automat,automate,570,"> IMO it's too heavy to add it to the script producing CITATION.cff. Makes sense. What I would do in the GH Actions workflow would probably be something like:. 1. pip install and run the validation script there (so that it doesn't have to be included in the script, but can still be used to check PRs). 2. Maybe also run `makeCITATION.py` and compare the output to the existing CITATION.cff file, just to make sure both files were updated. But, again, can be added later. I'm not sure how much you want to depend on GitHub Actions anyway, perhaps it makes more sense to automate things elsewhere.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:506,integrability,depend,depend,506,"> IMO it's too heavy to add it to the script producing CITATION.cff. Makes sense. What I would do in the GH Actions workflow would probably be something like:. 1. pip install and run the validation script there (so that it doesn't have to be included in the script, but can still be used to check PRs). 2. Maybe also run `makeCITATION.py` and compare the output to the existing CITATION.cff file, just to make sure both files were updated. But, again, can be added later. I'm not sure how much you want to depend on GitHub Actions anyway, perhaps it makes more sense to automate things elsewhere.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:506,modifiability,depend,depend,506,"> IMO it's too heavy to add it to the script producing CITATION.cff. Makes sense. What I would do in the GH Actions workflow would probably be something like:. 1. pip install and run the validation script there (so that it doesn't have to be included in the script, but can still be used to check PRs). 2. Maybe also run `makeCITATION.py` and compare the output to the existing CITATION.cff file, just to make sure both files were updated. But, again, can be added later. I'm not sure how much you want to depend on GitHub Actions anyway, perhaps it makes more sense to automate things elsewhere.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:223,reliability,doe,doesn,223,"> IMO it's too heavy to add it to the script producing CITATION.cff. Makes sense. What I would do in the GH Actions workflow would probably be something like:. 1. pip install and run the validation script there (so that it doesn't have to be included in the script, but can still be used to check PRs). 2. Maybe also run `makeCITATION.py` and compare the output to the existing CITATION.cff file, just to make sure both files were updated. But, again, can be added later. I'm not sure how much you want to depend on GitHub Actions anyway, perhaps it makes more sense to automate things elsewhere.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:187,safety,valid,validation,187,"> IMO it's too heavy to add it to the script producing CITATION.cff. Makes sense. What I would do in the GH Actions workflow would probably be something like:. 1. pip install and run the validation script there (so that it doesn't have to be included in the script, but can still be used to check PRs). 2. Maybe also run `makeCITATION.py` and compare the output to the existing CITATION.cff file, just to make sure both files were updated. But, again, can be added later. I'm not sure how much you want to depend on GitHub Actions anyway, perhaps it makes more sense to automate things elsewhere.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:431,safety,updat,updated,431,"> IMO it's too heavy to add it to the script producing CITATION.cff. Makes sense. What I would do in the GH Actions workflow would probably be something like:. 1. pip install and run the validation script there (so that it doesn't have to be included in the script, but can still be used to check PRs). 2. Maybe also run `makeCITATION.py` and compare the output to the existing CITATION.cff file, just to make sure both files were updated. But, again, can be added later. I'm not sure how much you want to depend on GitHub Actions anyway, perhaps it makes more sense to automate things elsewhere.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:506,safety,depend,depend,506,"> IMO it's too heavy to add it to the script producing CITATION.cff. Makes sense. What I would do in the GH Actions workflow would probably be something like:. 1. pip install and run the validation script there (so that it doesn't have to be included in the script, but can still be used to check PRs). 2. Maybe also run `makeCITATION.py` and compare the output to the existing CITATION.cff file, just to make sure both files were updated. But, again, can be added later. I'm not sure how much you want to depend on GitHub Actions anyway, perhaps it makes more sense to automate things elsewhere.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:187,security,validat,validation,187,"> IMO it's too heavy to add it to the script producing CITATION.cff. Makes sense. What I would do in the GH Actions workflow would probably be something like:. 1. pip install and run the validation script there (so that it doesn't have to be included in the script, but can still be used to check PRs). 2. Maybe also run `makeCITATION.py` and compare the output to the existing CITATION.cff file, just to make sure both files were updated. But, again, can be added later. I'm not sure how much you want to depend on GitHub Actions anyway, perhaps it makes more sense to automate things elsewhere.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:431,security,updat,updated,431,"> IMO it's too heavy to add it to the script producing CITATION.cff. Makes sense. What I would do in the GH Actions workflow would probably be something like:. 1. pip install and run the validation script there (so that it doesn't have to be included in the script, but can still be used to check PRs). 2. Maybe also run `makeCITATION.py` and compare the output to the existing CITATION.cff file, just to make sure both files were updated. But, again, can be added later. I'm not sure how much you want to depend on GitHub Actions anyway, perhaps it makes more sense to automate things elsewhere.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:506,testability,depend,depend,506,"> IMO it's too heavy to add it to the script producing CITATION.cff. Makes sense. What I would do in the GH Actions workflow would probably be something like:. 1. pip install and run the validation script there (so that it doesn't have to be included in the script, but can still be used to check PRs). 2. Maybe also run `makeCITATION.py` and compare the output to the existing CITATION.cff file, just to make sure both files were updated. But, again, can be added later. I'm not sure how much you want to depend on GitHub Actions anyway, perhaps it makes more sense to automate things elsewhere.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:570,testability,automat,automate,570,"> IMO it's too heavy to add it to the script producing CITATION.cff. Makes sense. What I would do in the GH Actions workflow would probably be something like:. 1. pip install and run the validation script there (so that it doesn't have to be included in the script, but can still be used to check PRs). 2. Maybe also run `makeCITATION.py` and compare the output to the existing CITATION.cff file, just to make sure both files were updated. But, again, can be added later. I'm not sure how much you want to depend on GitHub Actions anyway, perhaps it makes more sense to automate things elsewhere.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:116,usability,workflow,workflow,116,"> IMO it's too heavy to add it to the script producing CITATION.cff. Makes sense. What I would do in the GH Actions workflow would probably be something like:. 1. pip install and run the validation script there (so that it doesn't have to be included in the script, but can still be used to check PRs). 2. Maybe also run `makeCITATION.py` and compare the output to the existing CITATION.cff file, just to make sure both files were updated. But, again, can be added later. I'm not sure how much you want to depend on GitHub Actions anyway, perhaps it makes more sense to automate things elsewhere.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:54,deployability,releas,release,54,"Topics to discuss:. - generating the author list from release notes is wrong, as I just realized. Those are just those people who contributed to that version, but generally don't include earlier authors. Can we use ""ROOT team"" as author? Or should we take everyone from `README/CREDITS`? Or should we skip it, at the cost of a lack of (computer-) identifiable attributions? - we want people who dislike using Zenodo to [cite the paper](https://github.com/root-project/root/#cite). Actually, very few cite Zenodo. How do we embed that paper in `CITATION.cff`, with [references](https://github.com/citation-file-format/citation-file-format/blob/main/README.md#references-optional)? What does the BibTeX file looks like for that? Happy to hear @jspaaks 's comments here - I do realize that some of these issues are non-technical, but I bet you've encountered them before :-)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:150,deployability,version,version,150,"Topics to discuss:. - generating the author list from release notes is wrong, as I just realized. Those are just those people who contributed to that version, but generally don't include earlier authors. Can we use ""ROOT team"" as author? Or should we take everyone from `README/CREDITS`? Or should we skip it, at the cost of a lack of (computer-) identifiable attributions? - we want people who dislike using Zenodo to [cite the paper](https://github.com/root-project/root/#cite). Actually, very few cite Zenodo. How do we embed that paper in `CITATION.cff`, with [references](https://github.com/citation-file-format/citation-file-format/blob/main/README.md#references-optional)? What does the BibTeX file looks like for that? Happy to hear @jspaaks 's comments here - I do realize that some of these issues are non-technical, but I bet you've encountered them before :-)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:0,integrability,Topic,Topics,0,"Topics to discuss:. - generating the author list from release notes is wrong, as I just realized. Those are just those people who contributed to that version, but generally don't include earlier authors. Can we use ""ROOT team"" as author? Or should we take everyone from `README/CREDITS`? Or should we skip it, at the cost of a lack of (computer-) identifiable attributions? - we want people who dislike using Zenodo to [cite the paper](https://github.com/root-project/root/#cite). Actually, very few cite Zenodo. How do we embed that paper in `CITATION.cff`, with [references](https://github.com/citation-file-format/citation-file-format/blob/main/README.md#references-optional)? What does the BibTeX file looks like for that? Happy to hear @jspaaks 's comments here - I do realize that some of these issues are non-technical, but I bet you've encountered them before :-)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:150,integrability,version,version,150,"Topics to discuss:. - generating the author list from release notes is wrong, as I just realized. Those are just those people who contributed to that version, but generally don't include earlier authors. Can we use ""ROOT team"" as author? Or should we take everyone from `README/CREDITS`? Or should we skip it, at the cost of a lack of (computer-) identifiable attributions? - we want people who dislike using Zenodo to [cite the paper](https://github.com/root-project/root/#cite). Actually, very few cite Zenodo. How do we embed that paper in `CITATION.cff`, with [references](https://github.com/citation-file-format/citation-file-format/blob/main/README.md#references-optional)? What does the BibTeX file looks like for that? Happy to hear @jspaaks 's comments here - I do realize that some of these issues are non-technical, but I bet you've encountered them before :-)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:610,interoperability,format,format,610,"Topics to discuss:. - generating the author list from release notes is wrong, as I just realized. Those are just those people who contributed to that version, but generally don't include earlier authors. Can we use ""ROOT team"" as author? Or should we take everyone from `README/CREDITS`? Or should we skip it, at the cost of a lack of (computer-) identifiable attributions? - we want people who dislike using Zenodo to [cite the paper](https://github.com/root-project/root/#cite). Actually, very few cite Zenodo. How do we embed that paper in `CITATION.cff`, with [references](https://github.com/citation-file-format/citation-file-format/blob/main/README.md#references-optional)? What does the BibTeX file looks like for that? Happy to hear @jspaaks 's comments here - I do realize that some of these issues are non-technical, but I bet you've encountered them before :-)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:631,interoperability,format,format,631,"Topics to discuss:. - generating the author list from release notes is wrong, as I just realized. Those are just those people who contributed to that version, but generally don't include earlier authors. Can we use ""ROOT team"" as author? Or should we take everyone from `README/CREDITS`? Or should we skip it, at the cost of a lack of (computer-) identifiable attributions? - we want people who dislike using Zenodo to [cite the paper](https://github.com/root-project/root/#cite). Actually, very few cite Zenodo. How do we embed that paper in `CITATION.cff`, with [references](https://github.com/citation-file-format/citation-file-format/blob/main/README.md#references-optional)? What does the BibTeX file looks like for that? Happy to hear @jspaaks 's comments here - I do realize that some of these issues are non-technical, but I bet you've encountered them before :-)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:150,modifiability,version,version,150,"Topics to discuss:. - generating the author list from release notes is wrong, as I just realized. Those are just those people who contributed to that version, but generally don't include earlier authors. Can we use ""ROOT team"" as author? Or should we take everyone from `README/CREDITS`? Or should we skip it, at the cost of a lack of (computer-) identifiable attributions? - we want people who dislike using Zenodo to [cite the paper](https://github.com/root-project/root/#cite). Actually, very few cite Zenodo. How do we embed that paper in `CITATION.cff`, with [references](https://github.com/citation-file-format/citation-file-format/blob/main/README.md#references-optional)? What does the BibTeX file looks like for that? Happy to hear @jspaaks 's comments here - I do realize that some of these issues are non-technical, but I bet you've encountered them before :-)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:685,reliability,doe,does,685,"Topics to discuss:. - generating the author list from release notes is wrong, as I just realized. Those are just those people who contributed to that version, but generally don't include earlier authors. Can we use ""ROOT team"" as author? Or should we take everyone from `README/CREDITS`? Or should we skip it, at the cost of a lack of (computer-) identifiable attributions? - we want people who dislike using Zenodo to [cite the paper](https://github.com/root-project/root/#cite). Actually, very few cite Zenodo. How do we embed that paper in `CITATION.cff`, with [references](https://github.com/citation-file-format/citation-file-format/blob/main/README.md#references-optional)? What does the BibTeX file looks like for that? Happy to hear @jspaaks 's comments here - I do realize that some of these issues are non-technical, but I bet you've encountered them before :-)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:37,security,auth,author,37,"Topics to discuss:. - generating the author list from release notes is wrong, as I just realized. Those are just those people who contributed to that version, but generally don't include earlier authors. Can we use ""ROOT team"" as author? Or should we take everyone from `README/CREDITS`? Or should we skip it, at the cost of a lack of (computer-) identifiable attributions? - we want people who dislike using Zenodo to [cite the paper](https://github.com/root-project/root/#cite). Actually, very few cite Zenodo. How do we embed that paper in `CITATION.cff`, with [references](https://github.com/citation-file-format/citation-file-format/blob/main/README.md#references-optional)? What does the BibTeX file looks like for that? Happy to hear @jspaaks 's comments here - I do realize that some of these issues are non-technical, but I bet you've encountered them before :-)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:195,security,auth,authors,195,"Topics to discuss:. - generating the author list from release notes is wrong, as I just realized. Those are just those people who contributed to that version, but generally don't include earlier authors. Can we use ""ROOT team"" as author? Or should we take everyone from `README/CREDITS`? Or should we skip it, at the cost of a lack of (computer-) identifiable attributions? - we want people who dislike using Zenodo to [cite the paper](https://github.com/root-project/root/#cite). Actually, very few cite Zenodo. How do we embed that paper in `CITATION.cff`, with [references](https://github.com/citation-file-format/citation-file-format/blob/main/README.md#references-optional)? What does the BibTeX file looks like for that? Happy to hear @jspaaks 's comments here - I do realize that some of these issues are non-technical, but I bet you've encountered them before :-)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:221,security,team,team,221,"Topics to discuss:. - generating the author list from release notes is wrong, as I just realized. Those are just those people who contributed to that version, but generally don't include earlier authors. Can we use ""ROOT team"" as author? Or should we take everyone from `README/CREDITS`? Or should we skip it, at the cost of a lack of (computer-) identifiable attributions? - we want people who dislike using Zenodo to [cite the paper](https://github.com/root-project/root/#cite). Actually, very few cite Zenodo. How do we embed that paper in `CITATION.cff`, with [references](https://github.com/citation-file-format/citation-file-format/blob/main/README.md#references-optional)? What does the BibTeX file looks like for that? Happy to hear @jspaaks 's comments here - I do realize that some of these issues are non-technical, but I bet you've encountered them before :-)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:230,security,auth,author,230,"Topics to discuss:. - generating the author list from release notes is wrong, as I just realized. Those are just those people who contributed to that version, but generally don't include earlier authors. Can we use ""ROOT team"" as author? Or should we take everyone from `README/CREDITS`? Or should we skip it, at the cost of a lack of (computer-) identifiable attributions? - we want people who dislike using Zenodo to [cite the paper](https://github.com/root-project/root/#cite). Actually, very few cite Zenodo. How do we embed that paper in `CITATION.cff`, with [references](https://github.com/citation-file-format/citation-file-format/blob/main/README.md#references-optional)? What does the BibTeX file looks like for that? Happy to hear @jspaaks 's comments here - I do realize that some of these issues are non-technical, but I bet you've encountered them before :-)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8626:347,security,ident,identifiable,347,"Topics to discuss:. - generating the author list from release notes is wrong, as I just realized. Those are just those people who contributed to that version, but generally don't include earlier authors. Can we use ""ROOT team"" as author? Or should we take everyone from `README/CREDITS`? Or should we skip it, at the cost of a lack of (computer-) identifiable attributions? - we want people who dislike using Zenodo to [cite the paper](https://github.com/root-project/root/#cite). Actually, very few cite Zenodo. How do we embed that paper in `CITATION.cff`, with [references](https://github.com/citation-file-format/citation-file-format/blob/main/README.md#references-optional)? What does the BibTeX file looks like for that? Happy to hear @jspaaks 's comments here - I do realize that some of these issues are non-technical, but I bet you've encountered them before :-)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8626
https://github.com/root-project/root/pull/8629:68,usability,user,user-images,68,Finally I implement all new marker styles 35..50. ![Canvas](https://user-images.githubusercontent.com/4936580/124938127-52e48400-e008-11eb-9621-029148b8c61e.png).,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8629
https://github.com/root-project/root/issues/8633:18,availability,down,downside,18,"What would be the downside of renaming ROOT's `find_package` (say to `ROOT_FIND_PACKAGE`) and using that throughout ROOT? Would there be e.g. any side-effects for say package X used by ROOT which depends on package Y, which ROOT has already find, but now package X will find a different one? And what about users configuring against ROOT, what would be the side-effect they see? pulling @amadio in for opinions and expertise!",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:196,deployability,depend,depends,196,"What would be the downside of renaming ROOT's `find_package` (say to `ROOT_FIND_PACKAGE`) and using that throughout ROOT? Would there be e.g. any side-effects for say package X used by ROOT which depends on package Y, which ROOT has already find, but now package X will find a different one? And what about users configuring against ROOT, what would be the side-effect they see? pulling @amadio in for opinions and expertise!",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:196,integrability,depend,depends,196,"What would be the downside of renaming ROOT's `find_package` (say to `ROOT_FIND_PACKAGE`) and using that throughout ROOT? Would there be e.g. any side-effects for say package X used by ROOT which depends on package Y, which ROOT has already find, but now package X will find a different one? And what about users configuring against ROOT, what would be the side-effect they see? pulling @amadio in for opinions and expertise!",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:313,integrability,configur,configuring,313,"What would be the downside of renaming ROOT's `find_package` (say to `ROOT_FIND_PACKAGE`) and using that throughout ROOT? Would there be e.g. any side-effects for say package X used by ROOT which depends on package Y, which ROOT has already find, but now package X will find a different one? And what about users configuring against ROOT, what would be the side-effect they see? pulling @amadio in for opinions and expertise!",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:167,modifiability,pac,package,167,"What would be the downside of renaming ROOT's `find_package` (say to `ROOT_FIND_PACKAGE`) and using that throughout ROOT? Would there be e.g. any side-effects for say package X used by ROOT which depends on package Y, which ROOT has already find, but now package X will find a different one? And what about users configuring against ROOT, what would be the side-effect they see? pulling @amadio in for opinions and expertise!",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:196,modifiability,depend,depends,196,"What would be the downside of renaming ROOT's `find_package` (say to `ROOT_FIND_PACKAGE`) and using that throughout ROOT? Would there be e.g. any side-effects for say package X used by ROOT which depends on package Y, which ROOT has already find, but now package X will find a different one? And what about users configuring against ROOT, what would be the side-effect they see? pulling @amadio in for opinions and expertise!",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:207,modifiability,pac,package,207,"What would be the downside of renaming ROOT's `find_package` (say to `ROOT_FIND_PACKAGE`) and using that throughout ROOT? Would there be e.g. any side-effects for say package X used by ROOT which depends on package Y, which ROOT has already find, but now package X will find a different one? And what about users configuring against ROOT, what would be the side-effect they see? pulling @amadio in for opinions and expertise!",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:255,modifiability,pac,package,255,"What would be the downside of renaming ROOT's `find_package` (say to `ROOT_FIND_PACKAGE`) and using that throughout ROOT? Would there be e.g. any side-effects for say package X used by ROOT which depends on package Y, which ROOT has already find, but now package X will find a different one? And what about users configuring against ROOT, what would be the side-effect they see? pulling @amadio in for opinions and expertise!",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:313,modifiability,configur,configuring,313,"What would be the downside of renaming ROOT's `find_package` (say to `ROOT_FIND_PACKAGE`) and using that throughout ROOT? Would there be e.g. any side-effects for say package X used by ROOT which depends on package Y, which ROOT has already find, but now package X will find a different one? And what about users configuring against ROOT, what would be the side-effect they see? pulling @amadio in for opinions and expertise!",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:196,safety,depend,depends,196,"What would be the downside of renaming ROOT's `find_package` (say to `ROOT_FIND_PACKAGE`) and using that throughout ROOT? Would there be e.g. any side-effects for say package X used by ROOT which depends on package Y, which ROOT has already find, but now package X will find a different one? And what about users configuring against ROOT, what would be the side-effect they see? pulling @amadio in for opinions and expertise!",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:313,security,configur,configuring,313,"What would be the downside of renaming ROOT's `find_package` (say to `ROOT_FIND_PACKAGE`) and using that throughout ROOT? Would there be e.g. any side-effects for say package X used by ROOT which depends on package Y, which ROOT has already find, but now package X will find a different one? And what about users configuring against ROOT, what would be the side-effect they see? pulling @amadio in for opinions and expertise!",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:196,testability,depend,depends,196,"What would be the downside of renaming ROOT's `find_package` (say to `ROOT_FIND_PACKAGE`) and using that throughout ROOT? Would there be e.g. any side-effects for say package X used by ROOT which depends on package Y, which ROOT has already find, but now package X will find a different one? And what about users configuring against ROOT, what would be the side-effect they see? pulling @amadio in for opinions and expertise!",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:307,usability,user,users,307,"What would be the downside of renaming ROOT's `find_package` (say to `ROOT_FIND_PACKAGE`) and using that throughout ROOT? Would there be e.g. any side-effects for say package X used by ROOT which depends on package Y, which ROOT has already find, but now package X will find a different one? And what about users configuring against ROOT, what would be the side-effect they see? pulling @amadio in for opinions and expertise!",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:333,availability,fault,fault,333,"ROOT's approach of turning `find_package` to a no-op was based on the [talk](https://youtu.be/bsXLMQ6WgIk) ""Effective CMake"". It only works with subprojects of ROOT (i.e. LLVM) if we override `find_package` without renaming it, otherwise LLVM will not use the builtin zlib, for example, when that's enabled. I don't think ROOT is at fault here for relying on this feature of CMake.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:333,energy efficiency,fault,fault,333,"ROOT's approach of turning `find_package` to a no-op was based on the [talk](https://youtu.be/bsXLMQ6WgIk) ""Effective CMake"". It only works with subprojects of ROOT (i.e. LLVM) if we override `find_package` without renaming it, otherwise LLVM will not use the builtin zlib, for example, when that's enabled. I don't think ROOT is at fault here for relying on this feature of CMake.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:145,integrability,sub,subprojects,145,"ROOT's approach of turning `find_package` to a no-op was based on the [talk](https://youtu.be/bsXLMQ6WgIk) ""Effective CMake"". It only works with subprojects of ROOT (i.e. LLVM) if we override `find_package` without renaming it, otherwise LLVM will not use the builtin zlib, for example, when that's enabled. I don't think ROOT is at fault here for relying on this feature of CMake.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:333,performance,fault,fault,333,"ROOT's approach of turning `find_package` to a no-op was based on the [talk](https://youtu.be/bsXLMQ6WgIk) ""Effective CMake"". It only works with subprojects of ROOT (i.e. LLVM) if we override `find_package` without renaming it, otherwise LLVM will not use the builtin zlib, for example, when that's enabled. I don't think ROOT is at fault here for relying on this feature of CMake.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:333,reliability,fault,fault,333,"ROOT's approach of turning `find_package` to a no-op was based on the [talk](https://youtu.be/bsXLMQ6WgIk) ""Effective CMake"". It only works with subprojects of ROOT (i.e. LLVM) if we override `find_package` without renaming it, otherwise LLVM will not use the builtin zlib, for example, when that's enabled. I don't think ROOT is at fault here for relying on this feature of CMake.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:333,safety,fault,fault,333,"ROOT's approach of turning `find_package` to a no-op was based on the [talk](https://youtu.be/bsXLMQ6WgIk) ""Effective CMake"". It only works with subprojects of ROOT (i.e. LLVM) if we override `find_package` without renaming it, otherwise LLVM will not use the builtin zlib, for example, when that's enabled. I don't think ROOT is at fault here for relying on this feature of CMake.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:108,usability,Effectiv,Effective,108,"ROOT's approach of turning `find_package` to a no-op was based on the [talk](https://youtu.be/bsXLMQ6WgIk) ""Effective CMake"". It only works with subprojects of ROOT (i.e. LLVM) if we override `find_package` without renaming it, otherwise LLVM will not use the builtin zlib, for example, when that's enabled. I don't think ROOT is at fault here for relying on this feature of CMake.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:27,availability,fault,fault,27,"> I don't think ROOT is at fault here for relying on this feature of CMake. As described in the blob post by a CMake maintainer (emphasis is mine):. > Even if find_package() were only redefined once though, it would still be relying on **undocumented** CMake behavior which may be modified or removed completely in a future version. Reliance on such behavior should be discouraged and as the above discussion shows, the technique is not safe to use in general. It's mostly sad that vcpkg did the same trick, making ROOT incompatible with it. Given the circumstances I think this cannot be fixed at the moment, since neither ROOT nor vcpkg can easily change and I also don't expect CMake to make such behavior defined and allow overriding build-ins multiple times. So I guess we need to close this is won't-fix?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:324,deployability,version,version,324,"> I don't think ROOT is at fault here for relying on this feature of CMake. As described in the blob post by a CMake maintainer (emphasis is mine):. > Even if find_package() were only redefined once though, it would still be relying on **undocumented** CMake behavior which may be modified or removed completely in a future version. Reliance on such behavior should be discouraged and as the above discussion shows, the technique is not safe to use in general. It's mostly sad that vcpkg did the same trick, making ROOT incompatible with it. Given the circumstances I think this cannot be fixed at the moment, since neither ROOT nor vcpkg can easily change and I also don't expect CMake to make such behavior defined and allow overriding build-ins multiple times. So I guess we need to close this is won't-fix?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:738,deployability,build,build-ins,738,"> I don't think ROOT is at fault here for relying on this feature of CMake. As described in the blob post by a CMake maintainer (emphasis is mine):. > Even if find_package() were only redefined once though, it would still be relying on **undocumented** CMake behavior which may be modified or removed completely in a future version. Reliance on such behavior should be discouraged and as the above discussion shows, the technique is not safe to use in general. It's mostly sad that vcpkg did the same trick, making ROOT incompatible with it. Given the circumstances I think this cannot be fixed at the moment, since neither ROOT nor vcpkg can easily change and I also don't expect CMake to make such behavior defined and allow overriding build-ins multiple times. So I guess we need to close this is won't-fix?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:27,energy efficiency,fault,fault,27,"> I don't think ROOT is at fault here for relying on this feature of CMake. As described in the blob post by a CMake maintainer (emphasis is mine):. > Even if find_package() were only redefined once though, it would still be relying on **undocumented** CMake behavior which may be modified or removed completely in a future version. Reliance on such behavior should be discouraged and as the above discussion shows, the technique is not safe to use in general. It's mostly sad that vcpkg did the same trick, making ROOT incompatible with it. Given the circumstances I think this cannot be fixed at the moment, since neither ROOT nor vcpkg can easily change and I also don't expect CMake to make such behavior defined and allow overriding build-ins multiple times. So I guess we need to close this is won't-fix?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:324,integrability,version,version,324,"> I don't think ROOT is at fault here for relying on this feature of CMake. As described in the blob post by a CMake maintainer (emphasis is mine):. > Even if find_package() were only redefined once though, it would still be relying on **undocumented** CMake behavior which may be modified or removed completely in a future version. Reliance on such behavior should be discouraged and as the above discussion shows, the technique is not safe to use in general. It's mostly sad that vcpkg did the same trick, making ROOT incompatible with it. Given the circumstances I think this cannot be fixed at the moment, since neither ROOT nor vcpkg can easily change and I also don't expect CMake to make such behavior defined and allow overriding build-ins multiple times. So I guess we need to close this is won't-fix?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:520,interoperability,incompatib,incompatible,520,"> I don't think ROOT is at fault here for relying on this feature of CMake. As described in the blob post by a CMake maintainer (emphasis is mine):. > Even if find_package() were only redefined once though, it would still be relying on **undocumented** CMake behavior which may be modified or removed completely in a future version. Reliance on such behavior should be discouraged and as the above discussion shows, the technique is not safe to use in general. It's mostly sad that vcpkg did the same trick, making ROOT incompatible with it. Given the circumstances I think this cannot be fixed at the moment, since neither ROOT nor vcpkg can easily change and I also don't expect CMake to make such behavior defined and allow overriding build-ins multiple times. So I guess we need to close this is won't-fix?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:117,modifiability,maintain,maintainer,117,"> I don't think ROOT is at fault here for relying on this feature of CMake. As described in the blob post by a CMake maintainer (emphasis is mine):. > Even if find_package() were only redefined once though, it would still be relying on **undocumented** CMake behavior which may be modified or removed completely in a future version. Reliance on such behavior should be discouraged and as the above discussion shows, the technique is not safe to use in general. It's mostly sad that vcpkg did the same trick, making ROOT incompatible with it. Given the circumstances I think this cannot be fixed at the moment, since neither ROOT nor vcpkg can easily change and I also don't expect CMake to make such behavior defined and allow overriding build-ins multiple times. So I guess we need to close this is won't-fix?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:324,modifiability,version,version,324,"> I don't think ROOT is at fault here for relying on this feature of CMake. As described in the blob post by a CMake maintainer (emphasis is mine):. > Even if find_package() were only redefined once though, it would still be relying on **undocumented** CMake behavior which may be modified or removed completely in a future version. Reliance on such behavior should be discouraged and as the above discussion shows, the technique is not safe to use in general. It's mostly sad that vcpkg did the same trick, making ROOT incompatible with it. Given the circumstances I think this cannot be fixed at the moment, since neither ROOT nor vcpkg can easily change and I also don't expect CMake to make such behavior defined and allow overriding build-ins multiple times. So I guess we need to close this is won't-fix?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:27,performance,fault,fault,27,"> I don't think ROOT is at fault here for relying on this feature of CMake. As described in the blob post by a CMake maintainer (emphasis is mine):. > Even if find_package() were only redefined once though, it would still be relying on **undocumented** CMake behavior which may be modified or removed completely in a future version. Reliance on such behavior should be discouraged and as the above discussion shows, the technique is not safe to use in general. It's mostly sad that vcpkg did the same trick, making ROOT incompatible with it. Given the circumstances I think this cannot be fixed at the moment, since neither ROOT nor vcpkg can easily change and I also don't expect CMake to make such behavior defined and allow overriding build-ins multiple times. So I guess we need to close this is won't-fix?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:757,performance,time,times,757,"> I don't think ROOT is at fault here for relying on this feature of CMake. As described in the blob post by a CMake maintainer (emphasis is mine):. > Even if find_package() were only redefined once though, it would still be relying on **undocumented** CMake behavior which may be modified or removed completely in a future version. Reliance on such behavior should be discouraged and as the above discussion shows, the technique is not safe to use in general. It's mostly sad that vcpkg did the same trick, making ROOT incompatible with it. Given the circumstances I think this cannot be fixed at the moment, since neither ROOT nor vcpkg can easily change and I also don't expect CMake to make such behavior defined and allow overriding build-ins multiple times. So I guess we need to close this is won't-fix?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:27,reliability,fault,fault,27,"> I don't think ROOT is at fault here for relying on this feature of CMake. As described in the blob post by a CMake maintainer (emphasis is mine):. > Even if find_package() were only redefined once though, it would still be relying on **undocumented** CMake behavior which may be modified or removed completely in a future version. Reliance on such behavior should be discouraged and as the above discussion shows, the technique is not safe to use in general. It's mostly sad that vcpkg did the same trick, making ROOT incompatible with it. Given the circumstances I think this cannot be fixed at the moment, since neither ROOT nor vcpkg can easily change and I also don't expect CMake to make such behavior defined and allow overriding build-ins multiple times. So I guess we need to close this is won't-fix?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:27,safety,fault,fault,27,"> I don't think ROOT is at fault here for relying on this feature of CMake. As described in the blob post by a CMake maintainer (emphasis is mine):. > Even if find_package() were only redefined once though, it would still be relying on **undocumented** CMake behavior which may be modified or removed completely in a future version. Reliance on such behavior should be discouraged and as the above discussion shows, the technique is not safe to use in general. It's mostly sad that vcpkg did the same trick, making ROOT incompatible with it. Given the circumstances I think this cannot be fixed at the moment, since neither ROOT nor vcpkg can easily change and I also don't expect CMake to make such behavior defined and allow overriding build-ins multiple times. So I guess we need to close this is won't-fix?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:117,safety,maintain,maintainer,117,"> I don't think ROOT is at fault here for relying on this feature of CMake. As described in the blob post by a CMake maintainer (emphasis is mine):. > Even if find_package() were only redefined once though, it would still be relying on **undocumented** CMake behavior which may be modified or removed completely in a future version. Reliance on such behavior should be discouraged and as the above discussion shows, the technique is not safe to use in general. It's mostly sad that vcpkg did the same trick, making ROOT incompatible with it. Given the circumstances I think this cannot be fixed at the moment, since neither ROOT nor vcpkg can easily change and I also don't expect CMake to make such behavior defined and allow overriding build-ins multiple times. So I guess we need to close this is won't-fix?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:301,safety,compl,completely,301,"> I don't think ROOT is at fault here for relying on this feature of CMake. As described in the blob post by a CMake maintainer (emphasis is mine):. > Even if find_package() were only redefined once though, it would still be relying on **undocumented** CMake behavior which may be modified or removed completely in a future version. Reliance on such behavior should be discouraged and as the above discussion shows, the technique is not safe to use in general. It's mostly sad that vcpkg did the same trick, making ROOT incompatible with it. Given the circumstances I think this cannot be fixed at the moment, since neither ROOT nor vcpkg can easily change and I also don't expect CMake to make such behavior defined and allow overriding build-ins multiple times. So I guess we need to close this is won't-fix?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:437,safety,safe,safe,437,"> I don't think ROOT is at fault here for relying on this feature of CMake. As described in the blob post by a CMake maintainer (emphasis is mine):. > Even if find_package() were only redefined once though, it would still be relying on **undocumented** CMake behavior which may be modified or removed completely in a future version. Reliance on such behavior should be discouraged and as the above discussion shows, the technique is not safe to use in general. It's mostly sad that vcpkg did the same trick, making ROOT incompatible with it. Given the circumstances I think this cannot be fixed at the moment, since neither ROOT nor vcpkg can easily change and I also don't expect CMake to make such behavior defined and allow overriding build-ins multiple times. So I guess we need to close this is won't-fix?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:281,security,modif,modified,281,"> I don't think ROOT is at fault here for relying on this feature of CMake. As described in the blob post by a CMake maintainer (emphasis is mine):. > Even if find_package() were only redefined once though, it would still be relying on **undocumented** CMake behavior which may be modified or removed completely in a future version. Reliance on such behavior should be discouraged and as the above discussion shows, the technique is not safe to use in general. It's mostly sad that vcpkg did the same trick, making ROOT incompatible with it. Given the circumstances I think this cannot be fixed at the moment, since neither ROOT nor vcpkg can easily change and I also don't expect CMake to make such behavior defined and allow overriding build-ins multiple times. So I guess we need to close this is won't-fix?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:301,security,compl,completely,301,"> I don't think ROOT is at fault here for relying on this feature of CMake. As described in the blob post by a CMake maintainer (emphasis is mine):. > Even if find_package() were only redefined once though, it would still be relying on **undocumented** CMake behavior which may be modified or removed completely in a future version. Reliance on such behavior should be discouraged and as the above discussion shows, the technique is not safe to use in general. It's mostly sad that vcpkg did the same trick, making ROOT incompatible with it. Given the circumstances I think this cannot be fixed at the moment, since neither ROOT nor vcpkg can easily change and I also don't expect CMake to make such behavior defined and allow overriding build-ins multiple times. So I guess we need to close this is won't-fix?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:238,usability,undo,undocumented,238,"> I don't think ROOT is at fault here for relying on this feature of CMake. As described in the blob post by a CMake maintainer (emphasis is mine):. > Even if find_package() were only redefined once though, it would still be relying on **undocumented** CMake behavior which may be modified or removed completely in a future version. Reliance on such behavior should be discouraged and as the above discussion shows, the technique is not safe to use in general. It's mostly sad that vcpkg did the same trick, making ROOT incompatible with it. Given the circumstances I think this cannot be fixed at the moment, since neither ROOT nor vcpkg can easily change and I also don't expect CMake to make such behavior defined and allow overriding build-ins multiple times. So I guess we need to close this is won't-fix?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:259,usability,behavi,behavior,259,"> I don't think ROOT is at fault here for relying on this feature of CMake. As described in the blob post by a CMake maintainer (emphasis is mine):. > Even if find_package() were only redefined once though, it would still be relying on **undocumented** CMake behavior which may be modified or removed completely in a future version. Reliance on such behavior should be discouraged and as the above discussion shows, the technique is not safe to use in general. It's mostly sad that vcpkg did the same trick, making ROOT incompatible with it. Given the circumstances I think this cannot be fixed at the moment, since neither ROOT nor vcpkg can easily change and I also don't expect CMake to make such behavior defined and allow overriding build-ins multiple times. So I guess we need to close this is won't-fix?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:350,usability,behavi,behavior,350,"> I don't think ROOT is at fault here for relying on this feature of CMake. As described in the blob post by a CMake maintainer (emphasis is mine):. > Even if find_package() were only redefined once though, it would still be relying on **undocumented** CMake behavior which may be modified or removed completely in a future version. Reliance on such behavior should be discouraged and as the above discussion shows, the technique is not safe to use in general. It's mostly sad that vcpkg did the same trick, making ROOT incompatible with it. Given the circumstances I think this cannot be fixed at the moment, since neither ROOT nor vcpkg can easily change and I also don't expect CMake to make such behavior defined and allow overriding build-ins multiple times. So I guess we need to close this is won't-fix?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:700,usability,behavi,behavior,700,"> I don't think ROOT is at fault here for relying on this feature of CMake. As described in the blob post by a CMake maintainer (emphasis is mine):. > Even if find_package() were only redefined once though, it would still be relying on **undocumented** CMake behavior which may be modified or removed completely in a future version. Reliance on such behavior should be discouraged and as the above discussion shows, the technique is not safe to use in general. It's mostly sad that vcpkg did the same trick, making ROOT incompatible with it. Given the circumstances I think this cannot be fixed at the moment, since neither ROOT nor vcpkg can easily change and I also don't expect CMake to make such behavior defined and allow overriding build-ins multiple times. So I guess we need to close this is won't-fix?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:786,usability,close,close,786,"> I don't think ROOT is at fault here for relying on this feature of CMake. As described in the blob post by a CMake maintainer (emphasis is mine):. > Even if find_package() were only redefined once though, it would still be relying on **undocumented** CMake behavior which may be modified or removed completely in a future version. Reliance on such behavior should be discouraged and as the above discussion shows, the technique is not safe to use in general. It's mostly sad that vcpkg did the same trick, making ROOT incompatible with it. Given the circumstances I think this cannot be fixed at the moment, since neither ROOT nor vcpkg can easily change and I also don't expect CMake to make such behavior defined and allow overriding build-ins multiple times. So I guess we need to close this is won't-fix?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:576,deployability,version,version,576,"Here is a relevant [issue](https://gitlab.kitware.com/cmake/cmake/-/issues/17735) where this is discussed more in depth. One of the [comments](https://gitlab.kitware.com/cmake/cmake/-/issues/17735#note_487572) in this issue explains the rationale behind overriding `find_package` quite well. If there is a better solution that can work the way the current solution does (i.e. works also for LLVM without having to change its calls to `find_package(ZLIB)`, for example), we can implement it. However, so far I have not found a another way to do it. Maybe now that the required version of CMake is newer than 3.11, we may be able to improve things by using the [FetchContent](https://cmake.org/cmake/help/latest/module/FetchContent.html) module. It may also be possible to use [CMAKE_DISABLE_FIND_\<PackageName\>](https://cmake.org/cmake/help/v3.10/variable/CMAKE_DISABLE_FIND_PACKAGE_PackageName.html) to skip checking for packages if that doesn't force packages to be considered not found when `<PackageName_FOUND>` is then also set by hand. The reality, though, is that CMake still kinda sucks for managing optionally bundled dependencies like ROOT wants to support. As for the undocumented nature of the feature, it's sad but, like the [CDash test measurements](https://cmake.org/cmake/help/latest/command/ctest_test.html?#additional-test-measurements) that were undocumented for a long time, if you offer it, people will try to use it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:710,deployability,modul,module,710,"Here is a relevant [issue](https://gitlab.kitware.com/cmake/cmake/-/issues/17735) where this is discussed more in depth. One of the [comments](https://gitlab.kitware.com/cmake/cmake/-/issues/17735#note_487572) in this issue explains the rationale behind overriding `find_package` quite well. If there is a better solution that can work the way the current solution does (i.e. works also for LLVM without having to change its calls to `find_package(ZLIB)`, for example), we can implement it. However, so far I have not found a another way to do it. Maybe now that the required version of CMake is newer than 3.11, we may be able to improve things by using the [FetchContent](https://cmake.org/cmake/help/latest/module/FetchContent.html) module. It may also be possible to use [CMAKE_DISABLE_FIND_\<PackageName\>](https://cmake.org/cmake/help/v3.10/variable/CMAKE_DISABLE_FIND_PACKAGE_PackageName.html) to skip checking for packages if that doesn't force packages to be considered not found when `<PackageName_FOUND>` is then also set by hand. The reality, though, is that CMake still kinda sucks for managing optionally bundled dependencies like ROOT wants to support. As for the undocumented nature of the feature, it's sad but, like the [CDash test measurements](https://cmake.org/cmake/help/latest/command/ctest_test.html?#additional-test-measurements) that were undocumented for a long time, if you offer it, people will try to use it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:736,deployability,modul,module,736,"Here is a relevant [issue](https://gitlab.kitware.com/cmake/cmake/-/issues/17735) where this is discussed more in depth. One of the [comments](https://gitlab.kitware.com/cmake/cmake/-/issues/17735#note_487572) in this issue explains the rationale behind overriding `find_package` quite well. If there is a better solution that can work the way the current solution does (i.e. works also for LLVM without having to change its calls to `find_package(ZLIB)`, for example), we can implement it. However, so far I have not found a another way to do it. Maybe now that the required version of CMake is newer than 3.11, we may be able to improve things by using the [FetchContent](https://cmake.org/cmake/help/latest/module/FetchContent.html) module. It may also be possible to use [CMAKE_DISABLE_FIND_\<PackageName\>](https://cmake.org/cmake/help/v3.10/variable/CMAKE_DISABLE_FIND_PACKAGE_PackageName.html) to skip checking for packages if that doesn't force packages to be considered not found when `<PackageName_FOUND>` is then also set by hand. The reality, though, is that CMake still kinda sucks for managing optionally bundled dependencies like ROOT wants to support. As for the undocumented nature of the feature, it's sad but, like the [CDash test measurements](https://cmake.org/cmake/help/latest/command/ctest_test.html?#additional-test-measurements) that were undocumented for a long time, if you offer it, people will try to use it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:1099,deployability,manag,managing,1099,"Here is a relevant [issue](https://gitlab.kitware.com/cmake/cmake/-/issues/17735) where this is discussed more in depth. One of the [comments](https://gitlab.kitware.com/cmake/cmake/-/issues/17735#note_487572) in this issue explains the rationale behind overriding `find_package` quite well. If there is a better solution that can work the way the current solution does (i.e. works also for LLVM without having to change its calls to `find_package(ZLIB)`, for example), we can implement it. However, so far I have not found a another way to do it. Maybe now that the required version of CMake is newer than 3.11, we may be able to improve things by using the [FetchContent](https://cmake.org/cmake/help/latest/module/FetchContent.html) module. It may also be possible to use [CMAKE_DISABLE_FIND_\<PackageName\>](https://cmake.org/cmake/help/v3.10/variable/CMAKE_DISABLE_FIND_PACKAGE_PackageName.html) to skip checking for packages if that doesn't force packages to be considered not found when `<PackageName_FOUND>` is then also set by hand. The reality, though, is that CMake still kinda sucks for managing optionally bundled dependencies like ROOT wants to support. As for the undocumented nature of the feature, it's sad but, like the [CDash test measurements](https://cmake.org/cmake/help/latest/command/ctest_test.html?#additional-test-measurements) that were undocumented for a long time, if you offer it, people will try to use it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:1127,deployability,depend,dependencies,1127,"Here is a relevant [issue](https://gitlab.kitware.com/cmake/cmake/-/issues/17735) where this is discussed more in depth. One of the [comments](https://gitlab.kitware.com/cmake/cmake/-/issues/17735#note_487572) in this issue explains the rationale behind overriding `find_package` quite well. If there is a better solution that can work the way the current solution does (i.e. works also for LLVM without having to change its calls to `find_package(ZLIB)`, for example), we can implement it. However, so far I have not found a another way to do it. Maybe now that the required version of CMake is newer than 3.11, we may be able to improve things by using the [FetchContent](https://cmake.org/cmake/help/latest/module/FetchContent.html) module. It may also be possible to use [CMAKE_DISABLE_FIND_\<PackageName\>](https://cmake.org/cmake/help/v3.10/variable/CMAKE_DISABLE_FIND_PACKAGE_PackageName.html) to skip checking for packages if that doesn't force packages to be considered not found when `<PackageName_FOUND>` is then also set by hand. The reality, though, is that CMake still kinda sucks for managing optionally bundled dependencies like ROOT wants to support. As for the undocumented nature of the feature, it's sad but, like the [CDash test measurements](https://cmake.org/cmake/help/latest/command/ctest_test.html?#additional-test-measurements) that were undocumented for a long time, if you offer it, people will try to use it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:348,energy efficiency,current,current,348,"Here is a relevant [issue](https://gitlab.kitware.com/cmake/cmake/-/issues/17735) where this is discussed more in depth. One of the [comments](https://gitlab.kitware.com/cmake/cmake/-/issues/17735#note_487572) in this issue explains the rationale behind overriding `find_package` quite well. If there is a better solution that can work the way the current solution does (i.e. works also for LLVM without having to change its calls to `find_package(ZLIB)`, for example), we can implement it. However, so far I have not found a another way to do it. Maybe now that the required version of CMake is newer than 3.11, we may be able to improve things by using the [FetchContent](https://cmake.org/cmake/help/latest/module/FetchContent.html) module. It may also be possible to use [CMAKE_DISABLE_FIND_\<PackageName\>](https://cmake.org/cmake/help/v3.10/variable/CMAKE_DISABLE_FIND_PACKAGE_PackageName.html) to skip checking for packages if that doesn't force packages to be considered not found when `<PackageName_FOUND>` is then also set by hand. The reality, though, is that CMake still kinda sucks for managing optionally bundled dependencies like ROOT wants to support. As for the undocumented nature of the feature, it's sad but, like the [CDash test measurements](https://cmake.org/cmake/help/latest/command/ctest_test.html?#additional-test-measurements) that were undocumented for a long time, if you offer it, people will try to use it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:1099,energy efficiency,manag,managing,1099,"Here is a relevant [issue](https://gitlab.kitware.com/cmake/cmake/-/issues/17735) where this is discussed more in depth. One of the [comments](https://gitlab.kitware.com/cmake/cmake/-/issues/17735#note_487572) in this issue explains the rationale behind overriding `find_package` quite well. If there is a better solution that can work the way the current solution does (i.e. works also for LLVM without having to change its calls to `find_package(ZLIB)`, for example), we can implement it. However, so far I have not found a another way to do it. Maybe now that the required version of CMake is newer than 3.11, we may be able to improve things by using the [FetchContent](https://cmake.org/cmake/help/latest/module/FetchContent.html) module. It may also be possible to use [CMAKE_DISABLE_FIND_\<PackageName\>](https://cmake.org/cmake/help/v3.10/variable/CMAKE_DISABLE_FIND_PACKAGE_PackageName.html) to skip checking for packages if that doesn't force packages to be considered not found when `<PackageName_FOUND>` is then also set by hand. The reality, though, is that CMake still kinda sucks for managing optionally bundled dependencies like ROOT wants to support. As for the undocumented nature of the feature, it's sad but, like the [CDash test measurements](https://cmake.org/cmake/help/latest/command/ctest_test.html?#additional-test-measurements) that were undocumented for a long time, if you offer it, people will try to use it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:1250,energy efficiency,measur,measurements,1250,"Here is a relevant [issue](https://gitlab.kitware.com/cmake/cmake/-/issues/17735) where this is discussed more in depth. One of the [comments](https://gitlab.kitware.com/cmake/cmake/-/issues/17735#note_487572) in this issue explains the rationale behind overriding `find_package` quite well. If there is a better solution that can work the way the current solution does (i.e. works also for LLVM without having to change its calls to `find_package(ZLIB)`, for example), we can implement it. However, so far I have not found a another way to do it. Maybe now that the required version of CMake is newer than 3.11, we may be able to improve things by using the [FetchContent](https://cmake.org/cmake/help/latest/module/FetchContent.html) module. It may also be possible to use [CMAKE_DISABLE_FIND_\<PackageName\>](https://cmake.org/cmake/help/v3.10/variable/CMAKE_DISABLE_FIND_PACKAGE_PackageName.html) to skip checking for packages if that doesn't force packages to be considered not found when `<PackageName_FOUND>` is then also set by hand. The reality, though, is that CMake still kinda sucks for managing optionally bundled dependencies like ROOT wants to support. As for the undocumented nature of the feature, it's sad but, like the [CDash test measurements](https://cmake.org/cmake/help/latest/command/ctest_test.html?#additional-test-measurements) that were undocumented for a long time, if you offer it, people will try to use it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:1341,energy efficiency,measur,measurements,1341,"Here is a relevant [issue](https://gitlab.kitware.com/cmake/cmake/-/issues/17735) where this is discussed more in depth. One of the [comments](https://gitlab.kitware.com/cmake/cmake/-/issues/17735#note_487572) in this issue explains the rationale behind overriding `find_package` quite well. If there is a better solution that can work the way the current solution does (i.e. works also for LLVM without having to change its calls to `find_package(ZLIB)`, for example), we can implement it. However, so far I have not found a another way to do it. Maybe now that the required version of CMake is newer than 3.11, we may be able to improve things by using the [FetchContent](https://cmake.org/cmake/help/latest/module/FetchContent.html) module. It may also be possible to use [CMAKE_DISABLE_FIND_\<PackageName\>](https://cmake.org/cmake/help/v3.10/variable/CMAKE_DISABLE_FIND_PACKAGE_PackageName.html) to skip checking for packages if that doesn't force packages to be considered not found when `<PackageName_FOUND>` is then also set by hand. The reality, though, is that CMake still kinda sucks for managing optionally bundled dependencies like ROOT wants to support. As for the undocumented nature of the feature, it's sad but, like the [CDash test measurements](https://cmake.org/cmake/help/latest/command/ctest_test.html?#additional-test-measurements) that were undocumented for a long time, if you offer it, people will try to use it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:576,integrability,version,version,576,"Here is a relevant [issue](https://gitlab.kitware.com/cmake/cmake/-/issues/17735) where this is discussed more in depth. One of the [comments](https://gitlab.kitware.com/cmake/cmake/-/issues/17735#note_487572) in this issue explains the rationale behind overriding `find_package` quite well. If there is a better solution that can work the way the current solution does (i.e. works also for LLVM without having to change its calls to `find_package(ZLIB)`, for example), we can implement it. However, so far I have not found a another way to do it. Maybe now that the required version of CMake is newer than 3.11, we may be able to improve things by using the [FetchContent](https://cmake.org/cmake/help/latest/module/FetchContent.html) module. It may also be possible to use [CMAKE_DISABLE_FIND_\<PackageName\>](https://cmake.org/cmake/help/v3.10/variable/CMAKE_DISABLE_FIND_PACKAGE_PackageName.html) to skip checking for packages if that doesn't force packages to be considered not found when `<PackageName_FOUND>` is then also set by hand. The reality, though, is that CMake still kinda sucks for managing optionally bundled dependencies like ROOT wants to support. As for the undocumented nature of the feature, it's sad but, like the [CDash test measurements](https://cmake.org/cmake/help/latest/command/ctest_test.html?#additional-test-measurements) that were undocumented for a long time, if you offer it, people will try to use it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:1127,integrability,depend,dependencies,1127,"Here is a relevant [issue](https://gitlab.kitware.com/cmake/cmake/-/issues/17735) where this is discussed more in depth. One of the [comments](https://gitlab.kitware.com/cmake/cmake/-/issues/17735#note_487572) in this issue explains the rationale behind overriding `find_package` quite well. If there is a better solution that can work the way the current solution does (i.e. works also for LLVM without having to change its calls to `find_package(ZLIB)`, for example), we can implement it. However, so far I have not found a another way to do it. Maybe now that the required version of CMake is newer than 3.11, we may be able to improve things by using the [FetchContent](https://cmake.org/cmake/help/latest/module/FetchContent.html) module. It may also be possible to use [CMAKE_DISABLE_FIND_\<PackageName\>](https://cmake.org/cmake/help/v3.10/variable/CMAKE_DISABLE_FIND_PACKAGE_PackageName.html) to skip checking for packages if that doesn't force packages to be considered not found when `<PackageName_FOUND>` is then also set by hand. The reality, though, is that CMake still kinda sucks for managing optionally bundled dependencies like ROOT wants to support. As for the undocumented nature of the feature, it's sad but, like the [CDash test measurements](https://cmake.org/cmake/help/latest/command/ctest_test.html?#additional-test-measurements) that were undocumented for a long time, if you offer it, people will try to use it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:576,modifiability,version,version,576,"Here is a relevant [issue](https://gitlab.kitware.com/cmake/cmake/-/issues/17735) where this is discussed more in depth. One of the [comments](https://gitlab.kitware.com/cmake/cmake/-/issues/17735#note_487572) in this issue explains the rationale behind overriding `find_package` quite well. If there is a better solution that can work the way the current solution does (i.e. works also for LLVM without having to change its calls to `find_package(ZLIB)`, for example), we can implement it. However, so far I have not found a another way to do it. Maybe now that the required version of CMake is newer than 3.11, we may be able to improve things by using the [FetchContent](https://cmake.org/cmake/help/latest/module/FetchContent.html) module. It may also be possible to use [CMAKE_DISABLE_FIND_\<PackageName\>](https://cmake.org/cmake/help/v3.10/variable/CMAKE_DISABLE_FIND_PACKAGE_PackageName.html) to skip checking for packages if that doesn't force packages to be considered not found when `<PackageName_FOUND>` is then also set by hand. The reality, though, is that CMake still kinda sucks for managing optionally bundled dependencies like ROOT wants to support. As for the undocumented nature of the feature, it's sad but, like the [CDash test measurements](https://cmake.org/cmake/help/latest/command/ctest_test.html?#additional-test-measurements) that were undocumented for a long time, if you offer it, people will try to use it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:710,modifiability,modul,module,710,"Here is a relevant [issue](https://gitlab.kitware.com/cmake/cmake/-/issues/17735) where this is discussed more in depth. One of the [comments](https://gitlab.kitware.com/cmake/cmake/-/issues/17735#note_487572) in this issue explains the rationale behind overriding `find_package` quite well. If there is a better solution that can work the way the current solution does (i.e. works also for LLVM without having to change its calls to `find_package(ZLIB)`, for example), we can implement it. However, so far I have not found a another way to do it. Maybe now that the required version of CMake is newer than 3.11, we may be able to improve things by using the [FetchContent](https://cmake.org/cmake/help/latest/module/FetchContent.html) module. It may also be possible to use [CMAKE_DISABLE_FIND_\<PackageName\>](https://cmake.org/cmake/help/v3.10/variable/CMAKE_DISABLE_FIND_PACKAGE_PackageName.html) to skip checking for packages if that doesn't force packages to be considered not found when `<PackageName_FOUND>` is then also set by hand. The reality, though, is that CMake still kinda sucks for managing optionally bundled dependencies like ROOT wants to support. As for the undocumented nature of the feature, it's sad but, like the [CDash test measurements](https://cmake.org/cmake/help/latest/command/ctest_test.html?#additional-test-measurements) that were undocumented for a long time, if you offer it, people will try to use it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:736,modifiability,modul,module,736,"Here is a relevant [issue](https://gitlab.kitware.com/cmake/cmake/-/issues/17735) where this is discussed more in depth. One of the [comments](https://gitlab.kitware.com/cmake/cmake/-/issues/17735#note_487572) in this issue explains the rationale behind overriding `find_package` quite well. If there is a better solution that can work the way the current solution does (i.e. works also for LLVM without having to change its calls to `find_package(ZLIB)`, for example), we can implement it. However, so far I have not found a another way to do it. Maybe now that the required version of CMake is newer than 3.11, we may be able to improve things by using the [FetchContent](https://cmake.org/cmake/help/latest/module/FetchContent.html) module. It may also be possible to use [CMAKE_DISABLE_FIND_\<PackageName\>](https://cmake.org/cmake/help/v3.10/variable/CMAKE_DISABLE_FIND_PACKAGE_PackageName.html) to skip checking for packages if that doesn't force packages to be considered not found when `<PackageName_FOUND>` is then also set by hand. The reality, though, is that CMake still kinda sucks for managing optionally bundled dependencies like ROOT wants to support. As for the undocumented nature of the feature, it's sad but, like the [CDash test measurements](https://cmake.org/cmake/help/latest/command/ctest_test.html?#additional-test-measurements) that were undocumented for a long time, if you offer it, people will try to use it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:797,modifiability,Pac,PackageName,797,"Here is a relevant [issue](https://gitlab.kitware.com/cmake/cmake/-/issues/17735) where this is discussed more in depth. One of the [comments](https://gitlab.kitware.com/cmake/cmake/-/issues/17735#note_487572) in this issue explains the rationale behind overriding `find_package` quite well. If there is a better solution that can work the way the current solution does (i.e. works also for LLVM without having to change its calls to `find_package(ZLIB)`, for example), we can implement it. However, so far I have not found a another way to do it. Maybe now that the required version of CMake is newer than 3.11, we may be able to improve things by using the [FetchContent](https://cmake.org/cmake/help/latest/module/FetchContent.html) module. It may also be possible to use [CMAKE_DISABLE_FIND_\<PackageName\>](https://cmake.org/cmake/help/v3.10/variable/CMAKE_DISABLE_FIND_PACKAGE_PackageName.html) to skip checking for packages if that doesn't force packages to be considered not found when `<PackageName_FOUND>` is then also set by hand. The reality, though, is that CMake still kinda sucks for managing optionally bundled dependencies like ROOT wants to support. As for the undocumented nature of the feature, it's sad but, like the [CDash test measurements](https://cmake.org/cmake/help/latest/command/ctest_test.html?#additional-test-measurements) that were undocumented for a long time, if you offer it, people will try to use it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:847,modifiability,variab,variable,847,"Here is a relevant [issue](https://gitlab.kitware.com/cmake/cmake/-/issues/17735) where this is discussed more in depth. One of the [comments](https://gitlab.kitware.com/cmake/cmake/-/issues/17735#note_487572) in this issue explains the rationale behind overriding `find_package` quite well. If there is a better solution that can work the way the current solution does (i.e. works also for LLVM without having to change its calls to `find_package(ZLIB)`, for example), we can implement it. However, so far I have not found a another way to do it. Maybe now that the required version of CMake is newer than 3.11, we may be able to improve things by using the [FetchContent](https://cmake.org/cmake/help/latest/module/FetchContent.html) module. It may also be possible to use [CMAKE_DISABLE_FIND_\<PackageName\>](https://cmake.org/cmake/help/v3.10/variable/CMAKE_DISABLE_FIND_PACKAGE_PackageName.html) to skip checking for packages if that doesn't force packages to be considered not found when `<PackageName_FOUND>` is then also set by hand. The reality, though, is that CMake still kinda sucks for managing optionally bundled dependencies like ROOT wants to support. As for the undocumented nature of the feature, it's sad but, like the [CDash test measurements](https://cmake.org/cmake/help/latest/command/ctest_test.html?#additional-test-measurements) that were undocumented for a long time, if you offer it, people will try to use it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:922,modifiability,pac,packages,922,"Here is a relevant [issue](https://gitlab.kitware.com/cmake/cmake/-/issues/17735) where this is discussed more in depth. One of the [comments](https://gitlab.kitware.com/cmake/cmake/-/issues/17735#note_487572) in this issue explains the rationale behind overriding `find_package` quite well. If there is a better solution that can work the way the current solution does (i.e. works also for LLVM without having to change its calls to `find_package(ZLIB)`, for example), we can implement it. However, so far I have not found a another way to do it. Maybe now that the required version of CMake is newer than 3.11, we may be able to improve things by using the [FetchContent](https://cmake.org/cmake/help/latest/module/FetchContent.html) module. It may also be possible to use [CMAKE_DISABLE_FIND_\<PackageName\>](https://cmake.org/cmake/help/v3.10/variable/CMAKE_DISABLE_FIND_PACKAGE_PackageName.html) to skip checking for packages if that doesn't force packages to be considered not found when `<PackageName_FOUND>` is then also set by hand. The reality, though, is that CMake still kinda sucks for managing optionally bundled dependencies like ROOT wants to support. As for the undocumented nature of the feature, it's sad but, like the [CDash test measurements](https://cmake.org/cmake/help/latest/command/ctest_test.html?#additional-test-measurements) that were undocumented for a long time, if you offer it, people will try to use it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:953,modifiability,pac,packages,953,"Here is a relevant [issue](https://gitlab.kitware.com/cmake/cmake/-/issues/17735) where this is discussed more in depth. One of the [comments](https://gitlab.kitware.com/cmake/cmake/-/issues/17735#note_487572) in this issue explains the rationale behind overriding `find_package` quite well. If there is a better solution that can work the way the current solution does (i.e. works also for LLVM without having to change its calls to `find_package(ZLIB)`, for example), we can implement it. However, so far I have not found a another way to do it. Maybe now that the required version of CMake is newer than 3.11, we may be able to improve things by using the [FetchContent](https://cmake.org/cmake/help/latest/module/FetchContent.html) module. It may also be possible to use [CMAKE_DISABLE_FIND_\<PackageName\>](https://cmake.org/cmake/help/v3.10/variable/CMAKE_DISABLE_FIND_PACKAGE_PackageName.html) to skip checking for packages if that doesn't force packages to be considered not found when `<PackageName_FOUND>` is then also set by hand. The reality, though, is that CMake still kinda sucks for managing optionally bundled dependencies like ROOT wants to support. As for the undocumented nature of the feature, it's sad but, like the [CDash test measurements](https://cmake.org/cmake/help/latest/command/ctest_test.html?#additional-test-measurements) that were undocumented for a long time, if you offer it, people will try to use it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:1127,modifiability,depend,dependencies,1127,"Here is a relevant [issue](https://gitlab.kitware.com/cmake/cmake/-/issues/17735) where this is discussed more in depth. One of the [comments](https://gitlab.kitware.com/cmake/cmake/-/issues/17735#note_487572) in this issue explains the rationale behind overriding `find_package` quite well. If there is a better solution that can work the way the current solution does (i.e. works also for LLVM without having to change its calls to `find_package(ZLIB)`, for example), we can implement it. However, so far I have not found a another way to do it. Maybe now that the required version of CMake is newer than 3.11, we may be able to improve things by using the [FetchContent](https://cmake.org/cmake/help/latest/module/FetchContent.html) module. It may also be possible to use [CMAKE_DISABLE_FIND_\<PackageName\>](https://cmake.org/cmake/help/v3.10/variable/CMAKE_DISABLE_FIND_PACKAGE_PackageName.html) to skip checking for packages if that doesn't force packages to be considered not found when `<PackageName_FOUND>` is then also set by hand. The reality, though, is that CMake still kinda sucks for managing optionally bundled dependencies like ROOT wants to support. As for the undocumented nature of the feature, it's sad but, like the [CDash test measurements](https://cmake.org/cmake/help/latest/command/ctest_test.html?#additional-test-measurements) that were undocumented for a long time, if you offer it, people will try to use it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:1389,performance,time,time,1389,"Here is a relevant [issue](https://gitlab.kitware.com/cmake/cmake/-/issues/17735) where this is discussed more in depth. One of the [comments](https://gitlab.kitware.com/cmake/cmake/-/issues/17735#note_487572) in this issue explains the rationale behind overriding `find_package` quite well. If there is a better solution that can work the way the current solution does (i.e. works also for LLVM without having to change its calls to `find_package(ZLIB)`, for example), we can implement it. However, so far I have not found a another way to do it. Maybe now that the required version of CMake is newer than 3.11, we may be able to improve things by using the [FetchContent](https://cmake.org/cmake/help/latest/module/FetchContent.html) module. It may also be possible to use [CMAKE_DISABLE_FIND_\<PackageName\>](https://cmake.org/cmake/help/v3.10/variable/CMAKE_DISABLE_FIND_PACKAGE_PackageName.html) to skip checking for packages if that doesn't force packages to be considered not found when `<PackageName_FOUND>` is then also set by hand. The reality, though, is that CMake still kinda sucks for managing optionally bundled dependencies like ROOT wants to support. As for the undocumented nature of the feature, it's sad but, like the [CDash test measurements](https://cmake.org/cmake/help/latest/command/ctest_test.html?#additional-test-measurements) that were undocumented for a long time, if you offer it, people will try to use it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:365,reliability,doe,does,365,"Here is a relevant [issue](https://gitlab.kitware.com/cmake/cmake/-/issues/17735) where this is discussed more in depth. One of the [comments](https://gitlab.kitware.com/cmake/cmake/-/issues/17735#note_487572) in this issue explains the rationale behind overriding `find_package` quite well. If there is a better solution that can work the way the current solution does (i.e. works also for LLVM without having to change its calls to `find_package(ZLIB)`, for example), we can implement it. However, so far I have not found a another way to do it. Maybe now that the required version of CMake is newer than 3.11, we may be able to improve things by using the [FetchContent](https://cmake.org/cmake/help/latest/module/FetchContent.html) module. It may also be possible to use [CMAKE_DISABLE_FIND_\<PackageName\>](https://cmake.org/cmake/help/v3.10/variable/CMAKE_DISABLE_FIND_PACKAGE_PackageName.html) to skip checking for packages if that doesn't force packages to be considered not found when `<PackageName_FOUND>` is then also set by hand. The reality, though, is that CMake still kinda sucks for managing optionally bundled dependencies like ROOT wants to support. As for the undocumented nature of the feature, it's sad but, like the [CDash test measurements](https://cmake.org/cmake/help/latest/command/ctest_test.html?#additional-test-measurements) that were undocumented for a long time, if you offer it, people will try to use it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:939,reliability,doe,doesn,939,"Here is a relevant [issue](https://gitlab.kitware.com/cmake/cmake/-/issues/17735) where this is discussed more in depth. One of the [comments](https://gitlab.kitware.com/cmake/cmake/-/issues/17735#note_487572) in this issue explains the rationale behind overriding `find_package` quite well. If there is a better solution that can work the way the current solution does (i.e. works also for LLVM without having to change its calls to `find_package(ZLIB)`, for example), we can implement it. However, so far I have not found a another way to do it. Maybe now that the required version of CMake is newer than 3.11, we may be able to improve things by using the [FetchContent](https://cmake.org/cmake/help/latest/module/FetchContent.html) module. It may also be possible to use [CMAKE_DISABLE_FIND_\<PackageName\>](https://cmake.org/cmake/help/v3.10/variable/CMAKE_DISABLE_FIND_PACKAGE_PackageName.html) to skip checking for packages if that doesn't force packages to be considered not found when `<PackageName_FOUND>` is then also set by hand. The reality, though, is that CMake still kinda sucks for managing optionally bundled dependencies like ROOT wants to support. As for the undocumented nature of the feature, it's sad but, like the [CDash test measurements](https://cmake.org/cmake/help/latest/command/ctest_test.html?#additional-test-measurements) that were undocumented for a long time, if you offer it, people will try to use it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:710,safety,modul,module,710,"Here is a relevant [issue](https://gitlab.kitware.com/cmake/cmake/-/issues/17735) where this is discussed more in depth. One of the [comments](https://gitlab.kitware.com/cmake/cmake/-/issues/17735#note_487572) in this issue explains the rationale behind overriding `find_package` quite well. If there is a better solution that can work the way the current solution does (i.e. works also for LLVM without having to change its calls to `find_package(ZLIB)`, for example), we can implement it. However, so far I have not found a another way to do it. Maybe now that the required version of CMake is newer than 3.11, we may be able to improve things by using the [FetchContent](https://cmake.org/cmake/help/latest/module/FetchContent.html) module. It may also be possible to use [CMAKE_DISABLE_FIND_\<PackageName\>](https://cmake.org/cmake/help/v3.10/variable/CMAKE_DISABLE_FIND_PACKAGE_PackageName.html) to skip checking for packages if that doesn't force packages to be considered not found when `<PackageName_FOUND>` is then also set by hand. The reality, though, is that CMake still kinda sucks for managing optionally bundled dependencies like ROOT wants to support. As for the undocumented nature of the feature, it's sad but, like the [CDash test measurements](https://cmake.org/cmake/help/latest/command/ctest_test.html?#additional-test-measurements) that were undocumented for a long time, if you offer it, people will try to use it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:736,safety,modul,module,736,"Here is a relevant [issue](https://gitlab.kitware.com/cmake/cmake/-/issues/17735) where this is discussed more in depth. One of the [comments](https://gitlab.kitware.com/cmake/cmake/-/issues/17735#note_487572) in this issue explains the rationale behind overriding `find_package` quite well. If there is a better solution that can work the way the current solution does (i.e. works also for LLVM without having to change its calls to `find_package(ZLIB)`, for example), we can implement it. However, so far I have not found a another way to do it. Maybe now that the required version of CMake is newer than 3.11, we may be able to improve things by using the [FetchContent](https://cmake.org/cmake/help/latest/module/FetchContent.html) module. It may also be possible to use [CMAKE_DISABLE_FIND_\<PackageName\>](https://cmake.org/cmake/help/v3.10/variable/CMAKE_DISABLE_FIND_PACKAGE_PackageName.html) to skip checking for packages if that doesn't force packages to be considered not found when `<PackageName_FOUND>` is then also set by hand. The reality, though, is that CMake still kinda sucks for managing optionally bundled dependencies like ROOT wants to support. As for the undocumented nature of the feature, it's sad but, like the [CDash test measurements](https://cmake.org/cmake/help/latest/command/ctest_test.html?#additional-test-measurements) that were undocumented for a long time, if you offer it, people will try to use it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:1099,safety,manag,managing,1099,"Here is a relevant [issue](https://gitlab.kitware.com/cmake/cmake/-/issues/17735) where this is discussed more in depth. One of the [comments](https://gitlab.kitware.com/cmake/cmake/-/issues/17735#note_487572) in this issue explains the rationale behind overriding `find_package` quite well. If there is a better solution that can work the way the current solution does (i.e. works also for LLVM without having to change its calls to `find_package(ZLIB)`, for example), we can implement it. However, so far I have not found a another way to do it. Maybe now that the required version of CMake is newer than 3.11, we may be able to improve things by using the [FetchContent](https://cmake.org/cmake/help/latest/module/FetchContent.html) module. It may also be possible to use [CMAKE_DISABLE_FIND_\<PackageName\>](https://cmake.org/cmake/help/v3.10/variable/CMAKE_DISABLE_FIND_PACKAGE_PackageName.html) to skip checking for packages if that doesn't force packages to be considered not found when `<PackageName_FOUND>` is then also set by hand. The reality, though, is that CMake still kinda sucks for managing optionally bundled dependencies like ROOT wants to support. As for the undocumented nature of the feature, it's sad but, like the [CDash test measurements](https://cmake.org/cmake/help/latest/command/ctest_test.html?#additional-test-measurements) that were undocumented for a long time, if you offer it, people will try to use it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:1127,safety,depend,dependencies,1127,"Here is a relevant [issue](https://gitlab.kitware.com/cmake/cmake/-/issues/17735) where this is discussed more in depth. One of the [comments](https://gitlab.kitware.com/cmake/cmake/-/issues/17735#note_487572) in this issue explains the rationale behind overriding `find_package` quite well. If there is a better solution that can work the way the current solution does (i.e. works also for LLVM without having to change its calls to `find_package(ZLIB)`, for example), we can implement it. However, so far I have not found a another way to do it. Maybe now that the required version of CMake is newer than 3.11, we may be able to improve things by using the [FetchContent](https://cmake.org/cmake/help/latest/module/FetchContent.html) module. It may also be possible to use [CMAKE_DISABLE_FIND_\<PackageName\>](https://cmake.org/cmake/help/v3.10/variable/CMAKE_DISABLE_FIND_PACKAGE_PackageName.html) to skip checking for packages if that doesn't force packages to be considered not found when `<PackageName_FOUND>` is then also set by hand. The reality, though, is that CMake still kinda sucks for managing optionally bundled dependencies like ROOT wants to support. As for the undocumented nature of the feature, it's sad but, like the [CDash test measurements](https://cmake.org/cmake/help/latest/command/ctest_test.html?#additional-test-measurements) that were undocumented for a long time, if you offer it, people will try to use it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:1245,safety,test,test,1245,"Here is a relevant [issue](https://gitlab.kitware.com/cmake/cmake/-/issues/17735) where this is discussed more in depth. One of the [comments](https://gitlab.kitware.com/cmake/cmake/-/issues/17735#note_487572) in this issue explains the rationale behind overriding `find_package` quite well. If there is a better solution that can work the way the current solution does (i.e. works also for LLVM without having to change its calls to `find_package(ZLIB)`, for example), we can implement it. However, so far I have not found a another way to do it. Maybe now that the required version of CMake is newer than 3.11, we may be able to improve things by using the [FetchContent](https://cmake.org/cmake/help/latest/module/FetchContent.html) module. It may also be possible to use [CMAKE_DISABLE_FIND_\<PackageName\>](https://cmake.org/cmake/help/v3.10/variable/CMAKE_DISABLE_FIND_PACKAGE_PackageName.html) to skip checking for packages if that doesn't force packages to be considered not found when `<PackageName_FOUND>` is then also set by hand. The reality, though, is that CMake still kinda sucks for managing optionally bundled dependencies like ROOT wants to support. As for the undocumented nature of the feature, it's sad but, like the [CDash test measurements](https://cmake.org/cmake/help/latest/command/ctest_test.html?#additional-test-measurements) that were undocumented for a long time, if you offer it, people will try to use it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:1336,safety,test,test-measurements,1336,"Here is a relevant [issue](https://gitlab.kitware.com/cmake/cmake/-/issues/17735) where this is discussed more in depth. One of the [comments](https://gitlab.kitware.com/cmake/cmake/-/issues/17735#note_487572) in this issue explains the rationale behind overriding `find_package` quite well. If there is a better solution that can work the way the current solution does (i.e. works also for LLVM without having to change its calls to `find_package(ZLIB)`, for example), we can implement it. However, so far I have not found a another way to do it. Maybe now that the required version of CMake is newer than 3.11, we may be able to improve things by using the [FetchContent](https://cmake.org/cmake/help/latest/module/FetchContent.html) module. It may also be possible to use [CMAKE_DISABLE_FIND_\<PackageName\>](https://cmake.org/cmake/help/v3.10/variable/CMAKE_DISABLE_FIND_PACKAGE_PackageName.html) to skip checking for packages if that doesn't force packages to be considered not found when `<PackageName_FOUND>` is then also set by hand. The reality, though, is that CMake still kinda sucks for managing optionally bundled dependencies like ROOT wants to support. As for the undocumented nature of the feature, it's sad but, like the [CDash test measurements](https://cmake.org/cmake/help/latest/command/ctest_test.html?#additional-test-measurements) that were undocumented for a long time, if you offer it, people will try to use it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:1127,testability,depend,dependencies,1127,"Here is a relevant [issue](https://gitlab.kitware.com/cmake/cmake/-/issues/17735) where this is discussed more in depth. One of the [comments](https://gitlab.kitware.com/cmake/cmake/-/issues/17735#note_487572) in this issue explains the rationale behind overriding `find_package` quite well. If there is a better solution that can work the way the current solution does (i.e. works also for LLVM without having to change its calls to `find_package(ZLIB)`, for example), we can implement it. However, so far I have not found a another way to do it. Maybe now that the required version of CMake is newer than 3.11, we may be able to improve things by using the [FetchContent](https://cmake.org/cmake/help/latest/module/FetchContent.html) module. It may also be possible to use [CMAKE_DISABLE_FIND_\<PackageName\>](https://cmake.org/cmake/help/v3.10/variable/CMAKE_DISABLE_FIND_PACKAGE_PackageName.html) to skip checking for packages if that doesn't force packages to be considered not found when `<PackageName_FOUND>` is then also set by hand. The reality, though, is that CMake still kinda sucks for managing optionally bundled dependencies like ROOT wants to support. As for the undocumented nature of the feature, it's sad but, like the [CDash test measurements](https://cmake.org/cmake/help/latest/command/ctest_test.html?#additional-test-measurements) that were undocumented for a long time, if you offer it, people will try to use it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:1245,testability,test,test,1245,"Here is a relevant [issue](https://gitlab.kitware.com/cmake/cmake/-/issues/17735) where this is discussed more in depth. One of the [comments](https://gitlab.kitware.com/cmake/cmake/-/issues/17735#note_487572) in this issue explains the rationale behind overriding `find_package` quite well. If there is a better solution that can work the way the current solution does (i.e. works also for LLVM without having to change its calls to `find_package(ZLIB)`, for example), we can implement it. However, so far I have not found a another way to do it. Maybe now that the required version of CMake is newer than 3.11, we may be able to improve things by using the [FetchContent](https://cmake.org/cmake/help/latest/module/FetchContent.html) module. It may also be possible to use [CMAKE_DISABLE_FIND_\<PackageName\>](https://cmake.org/cmake/help/v3.10/variable/CMAKE_DISABLE_FIND_PACKAGE_PackageName.html) to skip checking for packages if that doesn't force packages to be considered not found when `<PackageName_FOUND>` is then also set by hand. The reality, though, is that CMake still kinda sucks for managing optionally bundled dependencies like ROOT wants to support. As for the undocumented nature of the feature, it's sad but, like the [CDash test measurements](https://cmake.org/cmake/help/latest/command/ctest_test.html?#additional-test-measurements) that were undocumented for a long time, if you offer it, people will try to use it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:1336,testability,test,test-measurements,1336,"Here is a relevant [issue](https://gitlab.kitware.com/cmake/cmake/-/issues/17735) where this is discussed more in depth. One of the [comments](https://gitlab.kitware.com/cmake/cmake/-/issues/17735#note_487572) in this issue explains the rationale behind overriding `find_package` quite well. If there is a better solution that can work the way the current solution does (i.e. works also for LLVM without having to change its calls to `find_package(ZLIB)`, for example), we can implement it. However, so far I have not found a another way to do it. Maybe now that the required version of CMake is newer than 3.11, we may be able to improve things by using the [FetchContent](https://cmake.org/cmake/help/latest/module/FetchContent.html) module. It may also be possible to use [CMAKE_DISABLE_FIND_\<PackageName\>](https://cmake.org/cmake/help/v3.10/variable/CMAKE_DISABLE_FIND_PACKAGE_PackageName.html) to skip checking for packages if that doesn't force packages to be considered not found when `<PackageName_FOUND>` is then also set by hand. The reality, though, is that CMake still kinda sucks for managing optionally bundled dependencies like ROOT wants to support. As for the undocumented nature of the feature, it's sad but, like the [CDash test measurements](https://cmake.org/cmake/help/latest/command/ctest_test.html?#additional-test-measurements) that were undocumented for a long time, if you offer it, people will try to use it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:698,usability,help,help,698,"Here is a relevant [issue](https://gitlab.kitware.com/cmake/cmake/-/issues/17735) where this is discussed more in depth. One of the [comments](https://gitlab.kitware.com/cmake/cmake/-/issues/17735#note_487572) in this issue explains the rationale behind overriding `find_package` quite well. If there is a better solution that can work the way the current solution does (i.e. works also for LLVM without having to change its calls to `find_package(ZLIB)`, for example), we can implement it. However, so far I have not found a another way to do it. Maybe now that the required version of CMake is newer than 3.11, we may be able to improve things by using the [FetchContent](https://cmake.org/cmake/help/latest/module/FetchContent.html) module. It may also be possible to use [CMAKE_DISABLE_FIND_\<PackageName\>](https://cmake.org/cmake/help/v3.10/variable/CMAKE_DISABLE_FIND_PACKAGE_PackageName.html) to skip checking for packages if that doesn't force packages to be considered not found when `<PackageName_FOUND>` is then also set by hand. The reality, though, is that CMake still kinda sucks for managing optionally bundled dependencies like ROOT wants to support. As for the undocumented nature of the feature, it's sad but, like the [CDash test measurements](https://cmake.org/cmake/help/latest/command/ctest_test.html?#additional-test-measurements) that were undocumented for a long time, if you offer it, people will try to use it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:836,usability,help,help,836,"Here is a relevant [issue](https://gitlab.kitware.com/cmake/cmake/-/issues/17735) where this is discussed more in depth. One of the [comments](https://gitlab.kitware.com/cmake/cmake/-/issues/17735#note_487572) in this issue explains the rationale behind overriding `find_package` quite well. If there is a better solution that can work the way the current solution does (i.e. works also for LLVM without having to change its calls to `find_package(ZLIB)`, for example), we can implement it. However, so far I have not found a another way to do it. Maybe now that the required version of CMake is newer than 3.11, we may be able to improve things by using the [FetchContent](https://cmake.org/cmake/help/latest/module/FetchContent.html) module. It may also be possible to use [CMAKE_DISABLE_FIND_\<PackageName\>](https://cmake.org/cmake/help/v3.10/variable/CMAKE_DISABLE_FIND_PACKAGE_PackageName.html) to skip checking for packages if that doesn't force packages to be considered not found when `<PackageName_FOUND>` is then also set by hand. The reality, though, is that CMake still kinda sucks for managing optionally bundled dependencies like ROOT wants to support. As for the undocumented nature of the feature, it's sad but, like the [CDash test measurements](https://cmake.org/cmake/help/latest/command/ctest_test.html?#additional-test-measurements) that were undocumented for a long time, if you offer it, people will try to use it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:1159,usability,support,support,1159,"Here is a relevant [issue](https://gitlab.kitware.com/cmake/cmake/-/issues/17735) where this is discussed more in depth. One of the [comments](https://gitlab.kitware.com/cmake/cmake/-/issues/17735#note_487572) in this issue explains the rationale behind overriding `find_package` quite well. If there is a better solution that can work the way the current solution does (i.e. works also for LLVM without having to change its calls to `find_package(ZLIB)`, for example), we can implement it. However, so far I have not found a another way to do it. Maybe now that the required version of CMake is newer than 3.11, we may be able to improve things by using the [FetchContent](https://cmake.org/cmake/help/latest/module/FetchContent.html) module. It may also be possible to use [CMAKE_DISABLE_FIND_\<PackageName\>](https://cmake.org/cmake/help/v3.10/variable/CMAKE_DISABLE_FIND_PACKAGE_PackageName.html) to skip checking for packages if that doesn't force packages to be considered not found when `<PackageName_FOUND>` is then also set by hand. The reality, though, is that CMake still kinda sucks for managing optionally bundled dependencies like ROOT wants to support. As for the undocumented nature of the feature, it's sad but, like the [CDash test measurements](https://cmake.org/cmake/help/latest/command/ctest_test.html?#additional-test-measurements) that were undocumented for a long time, if you offer it, people will try to use it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:1179,usability,undo,undocumented,1179,"Here is a relevant [issue](https://gitlab.kitware.com/cmake/cmake/-/issues/17735) where this is discussed more in depth. One of the [comments](https://gitlab.kitware.com/cmake/cmake/-/issues/17735#note_487572) in this issue explains the rationale behind overriding `find_package` quite well. If there is a better solution that can work the way the current solution does (i.e. works also for LLVM without having to change its calls to `find_package(ZLIB)`, for example), we can implement it. However, so far I have not found a another way to do it. Maybe now that the required version of CMake is newer than 3.11, we may be able to improve things by using the [FetchContent](https://cmake.org/cmake/help/latest/module/FetchContent.html) module. It may also be possible to use [CMAKE_DISABLE_FIND_\<PackageName\>](https://cmake.org/cmake/help/v3.10/variable/CMAKE_DISABLE_FIND_PACKAGE_PackageName.html) to skip checking for packages if that doesn't force packages to be considered not found when `<PackageName_FOUND>` is then also set by hand. The reality, though, is that CMake still kinda sucks for managing optionally bundled dependencies like ROOT wants to support. As for the undocumented nature of the feature, it's sad but, like the [CDash test measurements](https://cmake.org/cmake/help/latest/command/ctest_test.html?#additional-test-measurements) that were undocumented for a long time, if you offer it, people will try to use it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:1288,usability,help,help,1288,"Here is a relevant [issue](https://gitlab.kitware.com/cmake/cmake/-/issues/17735) where this is discussed more in depth. One of the [comments](https://gitlab.kitware.com/cmake/cmake/-/issues/17735#note_487572) in this issue explains the rationale behind overriding `find_package` quite well. If there is a better solution that can work the way the current solution does (i.e. works also for LLVM without having to change its calls to `find_package(ZLIB)`, for example), we can implement it. However, so far I have not found a another way to do it. Maybe now that the required version of CMake is newer than 3.11, we may be able to improve things by using the [FetchContent](https://cmake.org/cmake/help/latest/module/FetchContent.html) module. It may also be possible to use [CMAKE_DISABLE_FIND_\<PackageName\>](https://cmake.org/cmake/help/v3.10/variable/CMAKE_DISABLE_FIND_PACKAGE_PackageName.html) to skip checking for packages if that doesn't force packages to be considered not found when `<PackageName_FOUND>` is then also set by hand. The reality, though, is that CMake still kinda sucks for managing optionally bundled dependencies like ROOT wants to support. As for the undocumented nature of the feature, it's sad but, like the [CDash test measurements](https://cmake.org/cmake/help/latest/command/ctest_test.html?#additional-test-measurements) that were undocumented for a long time, if you offer it, people will try to use it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:1300,usability,command,command,1300,"Here is a relevant [issue](https://gitlab.kitware.com/cmake/cmake/-/issues/17735) where this is discussed more in depth. One of the [comments](https://gitlab.kitware.com/cmake/cmake/-/issues/17735#note_487572) in this issue explains the rationale behind overriding `find_package` quite well. If there is a better solution that can work the way the current solution does (i.e. works also for LLVM without having to change its calls to `find_package(ZLIB)`, for example), we can implement it. However, so far I have not found a another way to do it. Maybe now that the required version of CMake is newer than 3.11, we may be able to improve things by using the [FetchContent](https://cmake.org/cmake/help/latest/module/FetchContent.html) module. It may also be possible to use [CMAKE_DISABLE_FIND_\<PackageName\>](https://cmake.org/cmake/help/v3.10/variable/CMAKE_DISABLE_FIND_PACKAGE_PackageName.html) to skip checking for packages if that doesn't force packages to be considered not found when `<PackageName_FOUND>` is then also set by hand. The reality, though, is that CMake still kinda sucks for managing optionally bundled dependencies like ROOT wants to support. As for the undocumented nature of the feature, it's sad but, like the [CDash test measurements](https://cmake.org/cmake/help/latest/command/ctest_test.html?#additional-test-measurements) that were undocumented for a long time, if you offer it, people will try to use it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/issues/8633:1365,usability,undo,undocumented,1365,"Here is a relevant [issue](https://gitlab.kitware.com/cmake/cmake/-/issues/17735) where this is discussed more in depth. One of the [comments](https://gitlab.kitware.com/cmake/cmake/-/issues/17735#note_487572) in this issue explains the rationale behind overriding `find_package` quite well. If there is a better solution that can work the way the current solution does (i.e. works also for LLVM without having to change its calls to `find_package(ZLIB)`, for example), we can implement it. However, so far I have not found a another way to do it. Maybe now that the required version of CMake is newer than 3.11, we may be able to improve things by using the [FetchContent](https://cmake.org/cmake/help/latest/module/FetchContent.html) module. It may also be possible to use [CMAKE_DISABLE_FIND_\<PackageName\>](https://cmake.org/cmake/help/v3.10/variable/CMAKE_DISABLE_FIND_PACKAGE_PackageName.html) to skip checking for packages if that doesn't force packages to be considered not found when `<PackageName_FOUND>` is then also set by hand. The reality, though, is that CMake still kinda sucks for managing optionally bundled dependencies like ROOT wants to support. As for the undocumented nature of the feature, it's sad but, like the [CDash test measurements](https://cmake.org/cmake/help/latest/command/ctest_test.html?#additional-test-measurements) that were undocumented for a long time, if you offer it, people will try to use it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8633
https://github.com/root-project/root/pull/8634:29,performance,I/O,I/O,29,"@mxxo, as I said in the ROOT I/O meeting -and if you don't mind-, I can take this :-). Therefore, I have self-assigned this PR.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8634
https://github.com/root-project/root/pull/8634:47,testability,plan,plans,47,Hey @mxxo; I did not forget about this! ;-) My plans are to retake this in mid-February or so...,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8634
https://github.com/root-project/root/pull/8634:271,availability,error,error,271,"Hey @jalopezg-r00t, that's great! It will be cool to see how it turns out :). For what it's worth I seem to remember not seeing the same improvement on spinning disk while benchmarking. But I have no idea of why that would be the case and it might have just been a setup error by me.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8634
https://github.com/root-project/root/pull/8634:45,energy efficiency,cool,cool,45,"Hey @jalopezg-r00t, that's great! It will be cool to see how it turns out :). For what it's worth I seem to remember not seeing the same improvement on spinning disk while benchmarking. But I have no idea of why that would be the case and it might have just been a setup error by me.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8634
https://github.com/root-project/root/pull/8634:161,performance,disk,disk,161,"Hey @jalopezg-r00t, that's great! It will be cool to see how it turns out :). For what it's worth I seem to remember not seeing the same improvement on spinning disk while benchmarking. But I have no idea of why that would be the case and it might have just been a setup error by me.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8634
https://github.com/root-project/root/pull/8634:271,performance,error,error,271,"Hey @jalopezg-r00t, that's great! It will be cool to see how it turns out :). For what it's worth I seem to remember not seeing the same improvement on spinning disk while benchmarking. But I have no idea of why that would be the case and it might have just been a setup error by me.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8634
https://github.com/root-project/root/pull/8634:108,safety,reme,remember,108,"Hey @jalopezg-r00t, that's great! It will be cool to see how it turns out :). For what it's worth I seem to remember not seeing the same improvement on spinning disk while benchmarking. But I have no idea of why that would be the case and it might have just been a setup error by me.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8634
https://github.com/root-project/root/pull/8634:271,safety,error,error,271,"Hey @jalopezg-r00t, that's great! It will be cool to see how it turns out :). For what it's worth I seem to remember not seeing the same improvement on spinning disk while benchmarking. But I have no idea of why that would be the case and it might have just been a setup error by me.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8634
https://github.com/root-project/root/pull/8634:271,usability,error,error,271,"Hey @jalopezg-r00t, that's great! It will be cool to see how it turns out :). For what it's worth I seem to remember not seeing the same improvement on spinning disk while benchmarking. But I have no idea of why that would be the case and it might have just been a setup error by me.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8634
https://github.com/root-project/root/pull/8634:442,deployability,updat,update,442,"> Profiles of RNTuple benchmarks (`iotools/cms, lhcb`) showed ~10-20% of. > total runtime is due to allocations in `RPageSource::UnsealPage`. @mxxo @jblomer This work will be resumed soon as we suspect that not reusing addresses is affecting RDMA data transfers (which has an impact on the measured throughput in the DAOS backend). Specifically, we need to confirm that not reusing addresses that appear in the IOVs array in `daos_obj_{fetch,update}()` is related to a higher overhead due to RDMA MR registration. Therefore, this PR might actually have some other side benefits besides reducing memory allocations and heap fragmentation.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8634
https://github.com/root-project/root/pull/8634:2,energy efficiency,Profil,Profiles,2,"> Profiles of RNTuple benchmarks (`iotools/cms, lhcb`) showed ~10-20% of. > total runtime is due to allocations in `RPageSource::UnsealPage`. @mxxo @jblomer This work will be resumed soon as we suspect that not reusing addresses is affecting RDMA data transfers (which has an impact on the measured throughput in the DAOS backend). Specifically, we need to confirm that not reusing addresses that appear in the IOVs array in `daos_obj_{fetch,update}()` is related to a higher overhead due to RDMA MR registration. Therefore, this PR might actually have some other side benefits besides reducing memory allocations and heap fragmentation.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8634
https://github.com/root-project/root/pull/8634:100,energy efficiency,alloc,allocations,100,"> Profiles of RNTuple benchmarks (`iotools/cms, lhcb`) showed ~10-20% of. > total runtime is due to allocations in `RPageSource::UnsealPage`. @mxxo @jblomer This work will be resumed soon as we suspect that not reusing addresses is affecting RDMA data transfers (which has an impact on the measured throughput in the DAOS backend). Specifically, we need to confirm that not reusing addresses that appear in the IOVs array in `daos_obj_{fetch,update}()` is related to a higher overhead due to RDMA MR registration. Therefore, this PR might actually have some other side benefits besides reducing memory allocations and heap fragmentation.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8634
https://github.com/root-project/root/pull/8634:290,energy efficiency,measur,measured,290,"> Profiles of RNTuple benchmarks (`iotools/cms, lhcb`) showed ~10-20% of. > total runtime is due to allocations in `RPageSource::UnsealPage`. @mxxo @jblomer This work will be resumed soon as we suspect that not reusing addresses is affecting RDMA data transfers (which has an impact on the measured throughput in the DAOS backend). Specifically, we need to confirm that not reusing addresses that appear in the IOVs array in `daos_obj_{fetch,update}()` is related to a higher overhead due to RDMA MR registration. Therefore, this PR might actually have some other side benefits besides reducing memory allocations and heap fragmentation.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8634
https://github.com/root-project/root/pull/8634:586,energy efficiency,reduc,reducing,586,"> Profiles of RNTuple benchmarks (`iotools/cms, lhcb`) showed ~10-20% of. > total runtime is due to allocations in `RPageSource::UnsealPage`. @mxxo @jblomer This work will be resumed soon as we suspect that not reusing addresses is affecting RDMA data transfers (which has an impact on the measured throughput in the DAOS backend). Specifically, we need to confirm that not reusing addresses that appear in the IOVs array in `daos_obj_{fetch,update}()` is related to a higher overhead due to RDMA MR registration. Therefore, this PR might actually have some other side benefits besides reducing memory allocations and heap fragmentation.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8634
https://github.com/root-project/root/pull/8634:602,energy efficiency,alloc,allocations,602,"> Profiles of RNTuple benchmarks (`iotools/cms, lhcb`) showed ~10-20% of. > total runtime is due to allocations in `RPageSource::UnsealPage`. @mxxo @jblomer This work will be resumed soon as we suspect that not reusing addresses is affecting RDMA data transfers (which has an impact on the measured throughput in the DAOS backend). Specifically, we need to confirm that not reusing addresses that appear in the IOVs array in `daos_obj_{fetch,update}()` is related to a higher overhead due to RDMA MR registration. Therefore, this PR might actually have some other side benefits besides reducing memory allocations and heap fragmentation.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8634
https://github.com/root-project/root/pull/8634:332,interoperability,Specif,Specifically,332,"> Profiles of RNTuple benchmarks (`iotools/cms, lhcb`) showed ~10-20% of. > total runtime is due to allocations in `RPageSource::UnsealPage`. @mxxo @jblomer This work will be resumed soon as we suspect that not reusing addresses is affecting RDMA data transfers (which has an impact on the measured throughput in the DAOS backend). Specifically, we need to confirm that not reusing addresses that appear in the IOVs array in `daos_obj_{fetch,update}()` is related to a higher overhead due to RDMA MR registration. Therefore, this PR might actually have some other side benefits besides reducing memory allocations and heap fragmentation.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8634
https://github.com/root-project/root/pull/8634:500,interoperability,registr,registration,500,"> Profiles of RNTuple benchmarks (`iotools/cms, lhcb`) showed ~10-20% of. > total runtime is due to allocations in `RPageSource::UnsealPage`. @mxxo @jblomer This work will be resumed soon as we suspect that not reusing addresses is affecting RDMA data transfers (which has an impact on the measured throughput in the DAOS backend). Specifically, we need to confirm that not reusing addresses that appear in the IOVs array in `daos_obj_{fetch,update}()` is related to a higher overhead due to RDMA MR registration. Therefore, this PR might actually have some other side benefits besides reducing memory allocations and heap fragmentation.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8634
https://github.com/root-project/root/pull/8634:211,modifiability,reu,reusing,211,"> Profiles of RNTuple benchmarks (`iotools/cms, lhcb`) showed ~10-20% of. > total runtime is due to allocations in `RPageSource::UnsealPage`. @mxxo @jblomer This work will be resumed soon as we suspect that not reusing addresses is affecting RDMA data transfers (which has an impact on the measured throughput in the DAOS backend). Specifically, we need to confirm that not reusing addresses that appear in the IOVs array in `daos_obj_{fetch,update}()` is related to a higher overhead due to RDMA MR registration. Therefore, this PR might actually have some other side benefits besides reducing memory allocations and heap fragmentation.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8634
https://github.com/root-project/root/pull/8634:374,modifiability,reu,reusing,374,"> Profiles of RNTuple benchmarks (`iotools/cms, lhcb`) showed ~10-20% of. > total runtime is due to allocations in `RPageSource::UnsealPage`. @mxxo @jblomer This work will be resumed soon as we suspect that not reusing addresses is affecting RDMA data transfers (which has an impact on the measured throughput in the DAOS backend). Specifically, we need to confirm that not reusing addresses that appear in the IOVs array in `daos_obj_{fetch,update}()` is related to a higher overhead due to RDMA MR registration. Therefore, this PR might actually have some other side benefits besides reducing memory allocations and heap fragmentation.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8634
https://github.com/root-project/root/pull/8634:2,performance,Profil,Profiles,2,"> Profiles of RNTuple benchmarks (`iotools/cms, lhcb`) showed ~10-20% of. > total runtime is due to allocations in `RPageSource::UnsealPage`. @mxxo @jblomer This work will be resumed soon as we suspect that not reusing addresses is affecting RDMA data transfers (which has an impact on the measured throughput in the DAOS backend). Specifically, we need to confirm that not reusing addresses that appear in the IOVs array in `daos_obj_{fetch,update}()` is related to a higher overhead due to RDMA MR registration. Therefore, this PR might actually have some other side benefits besides reducing memory allocations and heap fragmentation.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8634
https://github.com/root-project/root/pull/8634:299,performance,throughput,throughput,299,"> Profiles of RNTuple benchmarks (`iotools/cms, lhcb`) showed ~10-20% of. > total runtime is due to allocations in `RPageSource::UnsealPage`. @mxxo @jblomer This work will be resumed soon as we suspect that not reusing addresses is affecting RDMA data transfers (which has an impact on the measured throughput in the DAOS backend). Specifically, we need to confirm that not reusing addresses that appear in the IOVs array in `daos_obj_{fetch,update}()` is related to a higher overhead due to RDMA MR registration. Therefore, this PR might actually have some other side benefits besides reducing memory allocations and heap fragmentation.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8634
https://github.com/root-project/root/pull/8634:476,performance,overhead,overhead,476,"> Profiles of RNTuple benchmarks (`iotools/cms, lhcb`) showed ~10-20% of. > total runtime is due to allocations in `RPageSource::UnsealPage`. @mxxo @jblomer This work will be resumed soon as we suspect that not reusing addresses is affecting RDMA data transfers (which has an impact on the measured throughput in the DAOS backend). Specifically, we need to confirm that not reusing addresses that appear in the IOVs array in `daos_obj_{fetch,update}()` is related to a higher overhead due to RDMA MR registration. Therefore, this PR might actually have some other side benefits besides reducing memory allocations and heap fragmentation.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8634
https://github.com/root-project/root/pull/8634:595,performance,memor,memory,595,"> Profiles of RNTuple benchmarks (`iotools/cms, lhcb`) showed ~10-20% of. > total runtime is due to allocations in `RPageSource::UnsealPage`. @mxxo @jblomer This work will be resumed soon as we suspect that not reusing addresses is affecting RDMA data transfers (which has an impact on the measured throughput in the DAOS backend). Specifically, we need to confirm that not reusing addresses that appear in the IOVs array in `daos_obj_{fetch,update}()` is related to a higher overhead due to RDMA MR registration. Therefore, this PR might actually have some other side benefits besides reducing memory allocations and heap fragmentation.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8634
https://github.com/root-project/root/pull/8634:442,safety,updat,update,442,"> Profiles of RNTuple benchmarks (`iotools/cms, lhcb`) showed ~10-20% of. > total runtime is due to allocations in `RPageSource::UnsealPage`. @mxxo @jblomer This work will be resumed soon as we suspect that not reusing addresses is affecting RDMA data transfers (which has an impact on the measured throughput in the DAOS backend). Specifically, we need to confirm that not reusing addresses that appear in the IOVs array in `daos_obj_{fetch,update}()` is related to a higher overhead due to RDMA MR registration. Therefore, this PR might actually have some other side benefits besides reducing memory allocations and heap fragmentation.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8634
https://github.com/root-project/root/pull/8634:442,security,updat,update,442,"> Profiles of RNTuple benchmarks (`iotools/cms, lhcb`) showed ~10-20% of. > total runtime is due to allocations in `RPageSource::UnsealPage`. @mxxo @jblomer This work will be resumed soon as we suspect that not reusing addresses is affecting RDMA data transfers (which has an impact on the measured throughput in the DAOS backend). Specifically, we need to confirm that not reusing addresses that appear in the IOVs array in `daos_obj_{fetch,update}()` is related to a higher overhead due to RDMA MR registration. Therefore, this PR might actually have some other side benefits besides reducing memory allocations and heap fragmentation.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8634
https://github.com/root-project/root/pull/8634:175,usability,resum,resumed,175,"> Profiles of RNTuple benchmarks (`iotools/cms, lhcb`) showed ~10-20% of. > total runtime is due to allocations in `RPageSource::UnsealPage`. @mxxo @jblomer This work will be resumed soon as we suspect that not reusing addresses is affecting RDMA data transfers (which has an impact on the measured throughput in the DAOS backend). Specifically, we need to confirm that not reusing addresses that appear in the IOVs array in `daos_obj_{fetch,update}()` is related to a higher overhead due to RDMA MR registration. Therefore, this PR might actually have some other side benefits besides reducing memory allocations and heap fragmentation.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8634
https://github.com/root-project/root/pull/8634:357,usability,confirm,confirm,357,"> Profiles of RNTuple benchmarks (`iotools/cms, lhcb`) showed ~10-20% of. > total runtime is due to allocations in `RPageSource::UnsealPage`. @mxxo @jblomer This work will be resumed soon as we suspect that not reusing addresses is affecting RDMA data transfers (which has an impact on the measured throughput in the DAOS backend). Specifically, we need to confirm that not reusing addresses that appear in the IOVs array in `daos_obj_{fetch,update}()` is related to a higher overhead due to RDMA MR registration. Therefore, this PR might actually have some other side benefits besides reducing memory allocations and heap fragmentation.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8634
https://github.com/root-project/root/pull/8634:595,usability,memor,memory,595,"> Profiles of RNTuple benchmarks (`iotools/cms, lhcb`) showed ~10-20% of. > total runtime is due to allocations in `RPageSource::UnsealPage`. @mxxo @jblomer This work will be resumed soon as we suspect that not reusing addresses is affecting RDMA data transfers (which has an impact on the measured throughput in the DAOS backend). Specifically, we need to confirm that not reusing addresses that appear in the IOVs array in `daos_obj_{fetch,update}()` is related to a higher overhead due to RDMA MR registration. Therefore, this PR might actually have some other side benefits besides reducing memory allocations and heap fragmentation.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8634
https://github.com/root-project/root/pull/8634:302,deployability,updat,update,302,"> @mxxo @jblomer This work will be resumed soon as we suspect that not reusing addresses is affecting RDMA data transfers (which has an impact on the measured throughput in the DAOS backend). Specifically, we need to confirm that not reusing addresses that appear in the IOVs array in `daos_obj_{fetch,update}()` is related to a higher overhead due to RDMA MR registration. Also, as discussed with @jblomer, this is a separate problem that will be addressed in a separate PR if need be (CC: also FYI, @glmiotto :-)).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8634
https://github.com/root-project/root/pull/8634:150,energy efficiency,measur,measured,150,"> @mxxo @jblomer This work will be resumed soon as we suspect that not reusing addresses is affecting RDMA data transfers (which has an impact on the measured throughput in the DAOS backend). Specifically, we need to confirm that not reusing addresses that appear in the IOVs array in `daos_obj_{fetch,update}()` is related to a higher overhead due to RDMA MR registration. Also, as discussed with @jblomer, this is a separate problem that will be addressed in a separate PR if need be (CC: also FYI, @glmiotto :-)).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8634
https://github.com/root-project/root/pull/8634:192,interoperability,Specif,Specifically,192,"> @mxxo @jblomer This work will be resumed soon as we suspect that not reusing addresses is affecting RDMA data transfers (which has an impact on the measured throughput in the DAOS backend). Specifically, we need to confirm that not reusing addresses that appear in the IOVs array in `daos_obj_{fetch,update}()` is related to a higher overhead due to RDMA MR registration. Also, as discussed with @jblomer, this is a separate problem that will be addressed in a separate PR if need be (CC: also FYI, @glmiotto :-)).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8634
https://github.com/root-project/root/pull/8634:360,interoperability,registr,registration,360,"> @mxxo @jblomer This work will be resumed soon as we suspect that not reusing addresses is affecting RDMA data transfers (which has an impact on the measured throughput in the DAOS backend). Specifically, we need to confirm that not reusing addresses that appear in the IOVs array in `daos_obj_{fetch,update}()` is related to a higher overhead due to RDMA MR registration. Also, as discussed with @jblomer, this is a separate problem that will be addressed in a separate PR if need be (CC: also FYI, @glmiotto :-)).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8634
https://github.com/root-project/root/pull/8634:71,modifiability,reu,reusing,71,"> @mxxo @jblomer This work will be resumed soon as we suspect that not reusing addresses is affecting RDMA data transfers (which has an impact on the measured throughput in the DAOS backend). Specifically, we need to confirm that not reusing addresses that appear in the IOVs array in `daos_obj_{fetch,update}()` is related to a higher overhead due to RDMA MR registration. Also, as discussed with @jblomer, this is a separate problem that will be addressed in a separate PR if need be (CC: also FYI, @glmiotto :-)).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8634
https://github.com/root-project/root/pull/8634:234,modifiability,reu,reusing,234,"> @mxxo @jblomer This work will be resumed soon as we suspect that not reusing addresses is affecting RDMA data transfers (which has an impact on the measured throughput in the DAOS backend). Specifically, we need to confirm that not reusing addresses that appear in the IOVs array in `daos_obj_{fetch,update}()` is related to a higher overhead due to RDMA MR registration. Also, as discussed with @jblomer, this is a separate problem that will be addressed in a separate PR if need be (CC: also FYI, @glmiotto :-)).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8634
https://github.com/root-project/root/pull/8634:159,performance,throughput,throughput,159,"> @mxxo @jblomer This work will be resumed soon as we suspect that not reusing addresses is affecting RDMA data transfers (which has an impact on the measured throughput in the DAOS backend). Specifically, we need to confirm that not reusing addresses that appear in the IOVs array in `daos_obj_{fetch,update}()` is related to a higher overhead due to RDMA MR registration. Also, as discussed with @jblomer, this is a separate problem that will be addressed in a separate PR if need be (CC: also FYI, @glmiotto :-)).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8634
https://github.com/root-project/root/pull/8634:336,performance,overhead,overhead,336,"> @mxxo @jblomer This work will be resumed soon as we suspect that not reusing addresses is affecting RDMA data transfers (which has an impact on the measured throughput in the DAOS backend). Specifically, we need to confirm that not reusing addresses that appear in the IOVs array in `daos_obj_{fetch,update}()` is related to a higher overhead due to RDMA MR registration. Also, as discussed with @jblomer, this is a separate problem that will be addressed in a separate PR if need be (CC: also FYI, @glmiotto :-)).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8634
https://github.com/root-project/root/pull/8634:302,safety,updat,update,302,"> @mxxo @jblomer This work will be resumed soon as we suspect that not reusing addresses is affecting RDMA data transfers (which has an impact on the measured throughput in the DAOS backend). Specifically, we need to confirm that not reusing addresses that appear in the IOVs array in `daos_obj_{fetch,update}()` is related to a higher overhead due to RDMA MR registration. Also, as discussed with @jblomer, this is a separate problem that will be addressed in a separate PR if need be (CC: also FYI, @glmiotto :-)).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8634
https://github.com/root-project/root/pull/8634:302,security,updat,update,302,"> @mxxo @jblomer This work will be resumed soon as we suspect that not reusing addresses is affecting RDMA data transfers (which has an impact on the measured throughput in the DAOS backend). Specifically, we need to confirm that not reusing addresses that appear in the IOVs array in `daos_obj_{fetch,update}()` is related to a higher overhead due to RDMA MR registration. Also, as discussed with @jblomer, this is a separate problem that will be addressed in a separate PR if need be (CC: also FYI, @glmiotto :-)).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8634
https://github.com/root-project/root/pull/8634:35,usability,resum,resumed,35,"> @mxxo @jblomer This work will be resumed soon as we suspect that not reusing addresses is affecting RDMA data transfers (which has an impact on the measured throughput in the DAOS backend). Specifically, we need to confirm that not reusing addresses that appear in the IOVs array in `daos_obj_{fetch,update}()` is related to a higher overhead due to RDMA MR registration. Also, as discussed with @jblomer, this is a separate problem that will be addressed in a separate PR if need be (CC: also FYI, @glmiotto :-)).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8634
https://github.com/root-project/root/pull/8634:217,usability,confirm,confirm,217,"> @mxxo @jblomer This work will be resumed soon as we suspect that not reusing addresses is affecting RDMA data transfers (which has an impact on the measured throughput in the DAOS backend). Specifically, we need to confirm that not reusing addresses that appear in the IOVs array in `daos_obj_{fetch,update}()` is related to a higher overhead due to RDMA MR registration. Also, as discussed with @jblomer, this is a separate problem that will be addressed in a separate PR if need be (CC: also FYI, @glmiotto :-)).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8634
https://github.com/root-project/root/pull/8637:1658,availability,servic,service,1658,"analysis on [#b494dc](https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) found:. - :information_source: **1** minor issue. :point_down:. ## Top issues. <table>. <thead>. <tr>. <th align=""left"">Description</th>. <th align=""left"">Example fixes</th>. </tr>. </thead>. <tbody>. <tr>. <td width=""77%"">Use comprehensions instead of list(map(lambda: x ...)) Occurrences: <ul><li><a href=""https://github.com/root-project/root/blob/b494dc5668d0afbd988d4a57515308159492f146/bindings/experimental/distrdf/test/test_buildranges.py#L9"">test_buildranges.py:9</a></li> <li><a href=""https://github.com/root-project/root/blob/b494dc5668d0afbd988d4a57515308159492f146/bindings/experimental/distrdf/test/test_buildranges.py#L13"">test_buildranges.py:13</a></li></ul></td>. <td width=""23%""><a href=""https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2Fbindings%2Fexperimental%2Fdistrdf%2Ftest%2Ftest_buildranges.py/python%2Fdc_interfile_project%2FuseCompehensions%2Ftest/code/?utm_source=gh_review&"">:wrench: Example fixes</a></td>. </tr>. </tbody>. </table>. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8637
https://github.com/root-project/root/pull/8637:1658,deployability,servic,service,1658,"analysis on [#b494dc](https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) found:. - :information_source: **1** minor issue. :point_down:. ## Top issues. <table>. <thead>. <tr>. <th align=""left"">Description</th>. <th align=""left"">Example fixes</th>. </tr>. </thead>. <tbody>. <tr>. <td width=""77%"">Use comprehensions instead of list(map(lambda: x ...)) Occurrences: <ul><li><a href=""https://github.com/root-project/root/blob/b494dc5668d0afbd988d4a57515308159492f146/bindings/experimental/distrdf/test/test_buildranges.py#L9"">test_buildranges.py:9</a></li> <li><a href=""https://github.com/root-project/root/blob/b494dc5668d0afbd988d4a57515308159492f146/bindings/experimental/distrdf/test/test_buildranges.py#L13"">test_buildranges.py:13</a></li></ul></td>. <td width=""23%""><a href=""https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2Fbindings%2Fexperimental%2Fdistrdf%2Ftest%2Ftest_buildranges.py/python%2Fdc_interfile_project%2FuseCompehensions%2Ftest/code/?utm_source=gh_review&"">:wrench: Example fixes</a></td>. </tr>. </tbody>. </table>. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8637
https://github.com/root-project/root/pull/8637:1670,deployability,API,API,1670,"analysis on [#b494dc](https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) found:. - :information_source: **1** minor issue. :point_down:. ## Top issues. <table>. <thead>. <tr>. <th align=""left"">Description</th>. <th align=""left"">Example fixes</th>. </tr>. </thead>. <tbody>. <tr>. <td width=""77%"">Use comprehensions instead of list(map(lambda: x ...)) Occurrences: <ul><li><a href=""https://github.com/root-project/root/blob/b494dc5668d0afbd988d4a57515308159492f146/bindings/experimental/distrdf/test/test_buildranges.py#L9"">test_buildranges.py:9</a></li> <li><a href=""https://github.com/root-project/root/blob/b494dc5668d0afbd988d4a57515308159492f146/bindings/experimental/distrdf/test/test_buildranges.py#L13"">test_buildranges.py:13</a></li></ul></td>. <td width=""23%""><a href=""https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2Fbindings%2Fexperimental%2Fdistrdf%2Ftest%2Ftest_buildranges.py/python%2Fdc_interfile_project%2FuseCompehensions%2Ftest/code/?utm_source=gh_review&"">:wrench: Example fixes</a></td>. </tr>. </tbody>. </table>. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8637
https://github.com/root-project/root/pull/8637:1658,integrability,servic,service,1658,"analysis on [#b494dc](https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) found:. - :information_source: **1** minor issue. :point_down:. ## Top issues. <table>. <thead>. <tr>. <th align=""left"">Description</th>. <th align=""left"">Example fixes</th>. </tr>. </thead>. <tbody>. <tr>. <td width=""77%"">Use comprehensions instead of list(map(lambda: x ...)) Occurrences: <ul><li><a href=""https://github.com/root-project/root/blob/b494dc5668d0afbd988d4a57515308159492f146/bindings/experimental/distrdf/test/test_buildranges.py#L9"">test_buildranges.py:9</a></li> <li><a href=""https://github.com/root-project/root/blob/b494dc5668d0afbd988d4a57515308159492f146/bindings/experimental/distrdf/test/test_buildranges.py#L13"">test_buildranges.py:13</a></li></ul></td>. <td width=""23%""><a href=""https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2Fbindings%2Fexperimental%2Fdistrdf%2Ftest%2Ftest_buildranges.py/python%2Fdc_interfile_project%2FuseCompehensions%2Ftest/code/?utm_source=gh_review&"">:wrench: Example fixes</a></td>. </tr>. </tbody>. </table>. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8637
https://github.com/root-project/root/pull/8637:1670,integrability,API,API,1670,"analysis on [#b494dc](https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) found:. - :information_source: **1** minor issue. :point_down:. ## Top issues. <table>. <thead>. <tr>. <th align=""left"">Description</th>. <th align=""left"">Example fixes</th>. </tr>. </thead>. <tbody>. <tr>. <td width=""77%"">Use comprehensions instead of list(map(lambda: x ...)) Occurrences: <ul><li><a href=""https://github.com/root-project/root/blob/b494dc5668d0afbd988d4a57515308159492f146/bindings/experimental/distrdf/test/test_buildranges.py#L9"">test_buildranges.py:9</a></li> <li><a href=""https://github.com/root-project/root/blob/b494dc5668d0afbd988d4a57515308159492f146/bindings/experimental/distrdf/test/test_buildranges.py#L13"">test_buildranges.py:13</a></li></ul></td>. <td width=""23%""><a href=""https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2Fbindings%2Fexperimental%2Fdistrdf%2Ftest%2Ftest_buildranges.py/python%2Fdc_interfile_project%2FuseCompehensions%2Ftest/code/?utm_source=gh_review&"">:wrench: Example fixes</a></td>. </tr>. </tbody>. </table>. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8637
https://github.com/root-project/root/pull/8637:626,interoperability,bind,bindings,626,"## DeepCode's analysis on [#b494dc](https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) found:. - :information_source: **1** minor issue. :point_down:. ## Top issues. <table>. <thead>. <tr>. <th align=""left"">Description</th>. <th align=""left"">Example fixes</th>. </tr>. </thead>. <tbody>. <tr>. <td width=""77%"">Use comprehensions instead of list(map(lambda: x ...)) Occurrences: <ul><li><a href=""https://github.com/root-project/root/blob/b494dc5668d0afbd988d4a57515308159492f146/bindings/experimental/distrdf/test/test_buildranges.py#L9"">test_buildranges.py:9</a></li> <li><a href=""https://github.com/root-project/root/blob/b494dc5668d0afbd988d4a57515308159492f146/bindings/experimental/distrdf/test/test_buildranges.py#L13"">test_buildranges.py:13</a></li></ul></td>. <td width=""23%""><a href=""https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2Fbindings%2Fexperimental%2Fdistrdf%2Ftest%2Ftest_buildranges.py/python%2Fdc_interfile_project%2FuseCompehensions%2Ftest/code/?utm_source=gh_review&"">:wrench: Example fixes</a></td>. </tr>. </tbody>. </table>. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://s",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8637
https://github.com/root-project/root/pull/8637:812,interoperability,bind,bindings,812,"## DeepCode's analysis on [#b494dc](https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) found:. - :information_source: **1** minor issue. :point_down:. ## Top issues. <table>. <thead>. <tr>. <th align=""left"">Description</th>. <th align=""left"">Example fixes</th>. </tr>. </thead>. <tbody>. <tr>. <td width=""77%"">Use comprehensions instead of list(map(lambda: x ...)) Occurrences: <ul><li><a href=""https://github.com/root-project/root/blob/b494dc5668d0afbd988d4a57515308159492f146/bindings/experimental/distrdf/test/test_buildranges.py#L9"">test_buildranges.py:9</a></li> <li><a href=""https://github.com/root-project/root/blob/b494dc5668d0afbd988d4a57515308159492f146/bindings/experimental/distrdf/test/test_buildranges.py#L13"">test_buildranges.py:13</a></li></ul></td>. <td width=""23%""><a href=""https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2Fbindings%2Fexperimental%2Fdistrdf%2Ftest%2Ftest_buildranges.py/python%2Fdc_interfile_project%2FuseCompehensions%2Ftest/code/?utm_source=gh_review&"">:wrench: Example fixes</a></td>. </tr>. </tbody>. </table>. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://s",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8637
https://github.com/root-project/root/pull/8637:1670,interoperability,API,API,1670,"analysis on [#b494dc](https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) found:. - :information_source: **1** minor issue. :point_down:. ## Top issues. <table>. <thead>. <tr>. <th align=""left"">Description</th>. <th align=""left"">Example fixes</th>. </tr>. </thead>. <tbody>. <tr>. <td width=""77%"">Use comprehensions instead of list(map(lambda: x ...)) Occurrences: <ul><li><a href=""https://github.com/root-project/root/blob/b494dc5668d0afbd988d4a57515308159492f146/bindings/experimental/distrdf/test/test_buildranges.py#L9"">test_buildranges.py:9</a></li> <li><a href=""https://github.com/root-project/root/blob/b494dc5668d0afbd988d4a57515308159492f146/bindings/experimental/distrdf/test/test_buildranges.py#L13"">test_buildranges.py:13</a></li></ul></td>. <td width=""23%""><a href=""https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2Fbindings%2Fexperimental%2Fdistrdf%2Ftest%2Ftest_buildranges.py/python%2Fdc_interfile_project%2FuseCompehensions%2Ftest/code/?utm_source=gh_review&"">:wrench: Example fixes</a></td>. </tr>. </tbody>. </table>. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8637
https://github.com/root-project/root/pull/8637:1860,interoperability,plug,plugins,1860,"analysis on [#b494dc](https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) found:. - :information_source: **1** minor issue. :point_down:. ## Top issues. <table>. <thead>. <tr>. <th align=""left"">Description</th>. <th align=""left"">Example fixes</th>. </tr>. </thead>. <tbody>. <tr>. <td width=""77%"">Use comprehensions instead of list(map(lambda: x ...)) Occurrences: <ul><li><a href=""https://github.com/root-project/root/blob/b494dc5668d0afbd988d4a57515308159492f146/bindings/experimental/distrdf/test/test_buildranges.py#L9"">test_buildranges.py:9</a></li> <li><a href=""https://github.com/root-project/root/blob/b494dc5668d0afbd988d4a57515308159492f146/bindings/experimental/distrdf/test/test_buildranges.py#L13"">test_buildranges.py:13</a></li></ul></td>. <td width=""23%""><a href=""https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2Fbindings%2Fexperimental%2Fdistrdf%2Ftest%2Ftest_buildranges.py/python%2Fdc_interfile_project%2FuseCompehensions%2Ftest/code/?utm_source=gh_review&"">:wrench: Example fixes</a></td>. </tr>. </tbody>. </table>. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8637
https://github.com/root-project/root/pull/8637:1932,interoperability,plug,plugin,1932,"analysis on [#b494dc](https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) found:. - :information_source: **1** minor issue. :point_down:. ## Top issues. <table>. <thead>. <tr>. <th align=""left"">Description</th>. <th align=""left"">Example fixes</th>. </tr>. </thead>. <tbody>. <tr>. <td width=""77%"">Use comprehensions instead of list(map(lambda: x ...)) Occurrences: <ul><li><a href=""https://github.com/root-project/root/blob/b494dc5668d0afbd988d4a57515308159492f146/bindings/experimental/distrdf/test/test_buildranges.py#L9"">test_buildranges.py:9</a></li> <li><a href=""https://github.com/root-project/root/blob/b494dc5668d0afbd988d4a57515308159492f146/bindings/experimental/distrdf/test/test_buildranges.py#L13"">test_buildranges.py:13</a></li></ul></td>. <td width=""23%""><a href=""https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2Fbindings%2Fexperimental%2Fdistrdf%2Ftest%2Ftest_buildranges.py/python%2Fdc_interfile_project%2FuseCompehensions%2Ftest/code/?utm_source=gh_review&"">:wrench: Example fixes</a></td>. </tr>. </tbody>. </table>. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8637
https://github.com/root-project/root/pull/8637:1983,interoperability,plug,plugin,1983,"analysis on [#b494dc](https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) found:. - :information_source: **1** minor issue. :point_down:. ## Top issues. <table>. <thead>. <tr>. <th align=""left"">Description</th>. <th align=""left"">Example fixes</th>. </tr>. </thead>. <tbody>. <tr>. <td width=""77%"">Use comprehensions instead of list(map(lambda: x ...)) Occurrences: <ul><li><a href=""https://github.com/root-project/root/blob/b494dc5668d0afbd988d4a57515308159492f146/bindings/experimental/distrdf/test/test_buildranges.py#L9"">test_buildranges.py:9</a></li> <li><a href=""https://github.com/root-project/root/blob/b494dc5668d0afbd988d4a57515308159492f146/bindings/experimental/distrdf/test/test_buildranges.py#L13"">test_buildranges.py:13</a></li></ul></td>. <td width=""23%""><a href=""https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2Fbindings%2Fexperimental%2Fdistrdf%2Ftest%2Ftest_buildranges.py/python%2Fdc_interfile_project%2FuseCompehensions%2Ftest/code/?utm_source=gh_review&"">:wrench: Example fixes</a></td>. </tr>. </tbody>. </table>. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8637
https://github.com/root-project/root/pull/8637:626,modifiability,bind,bindings,626,"## DeepCode's analysis on [#b494dc](https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) found:. - :information_source: **1** minor issue. :point_down:. ## Top issues. <table>. <thead>. <tr>. <th align=""left"">Description</th>. <th align=""left"">Example fixes</th>. </tr>. </thead>. <tbody>. <tr>. <td width=""77%"">Use comprehensions instead of list(map(lambda: x ...)) Occurrences: <ul><li><a href=""https://github.com/root-project/root/blob/b494dc5668d0afbd988d4a57515308159492f146/bindings/experimental/distrdf/test/test_buildranges.py#L9"">test_buildranges.py:9</a></li> <li><a href=""https://github.com/root-project/root/blob/b494dc5668d0afbd988d4a57515308159492f146/bindings/experimental/distrdf/test/test_buildranges.py#L13"">test_buildranges.py:13</a></li></ul></td>. <td width=""23%""><a href=""https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2Fbindings%2Fexperimental%2Fdistrdf%2Ftest%2Ftest_buildranges.py/python%2Fdc_interfile_project%2FuseCompehensions%2Ftest/code/?utm_source=gh_review&"">:wrench: Example fixes</a></td>. </tr>. </tbody>. </table>. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://s",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8637
https://github.com/root-project/root/pull/8637:812,modifiability,bind,bindings,812,"## DeepCode's analysis on [#b494dc](https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) found:. - :information_source: **1** minor issue. :point_down:. ## Top issues. <table>. <thead>. <tr>. <th align=""left"">Description</th>. <th align=""left"">Example fixes</th>. </tr>. </thead>. <tbody>. <tr>. <td width=""77%"">Use comprehensions instead of list(map(lambda: x ...)) Occurrences: <ul><li><a href=""https://github.com/root-project/root/blob/b494dc5668d0afbd988d4a57515308159492f146/bindings/experimental/distrdf/test/test_buildranges.py#L9"">test_buildranges.py:9</a></li> <li><a href=""https://github.com/root-project/root/blob/b494dc5668d0afbd988d4a57515308159492f146/bindings/experimental/distrdf/test/test_buildranges.py#L13"">test_buildranges.py:13</a></li></ul></td>. <td width=""23%""><a href=""https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2Fbindings%2Fexperimental%2Fdistrdf%2Ftest%2Ftest_buildranges.py/python%2Fdc_interfile_project%2FuseCompehensions%2Ftest/code/?utm_source=gh_review&"">:wrench: Example fixes</a></td>. </tr>. </tbody>. </table>. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://s",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8637
https://github.com/root-project/root/pull/8637:1658,modifiability,servic,service,1658,"analysis on [#b494dc](https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) found:. - :information_source: **1** minor issue. :point_down:. ## Top issues. <table>. <thead>. <tr>. <th align=""left"">Description</th>. <th align=""left"">Example fixes</th>. </tr>. </thead>. <tbody>. <tr>. <td width=""77%"">Use comprehensions instead of list(map(lambda: x ...)) Occurrences: <ul><li><a href=""https://github.com/root-project/root/blob/b494dc5668d0afbd988d4a57515308159492f146/bindings/experimental/distrdf/test/test_buildranges.py#L9"">test_buildranges.py:9</a></li> <li><a href=""https://github.com/root-project/root/blob/b494dc5668d0afbd988d4a57515308159492f146/bindings/experimental/distrdf/test/test_buildranges.py#L13"">test_buildranges.py:13</a></li></ul></td>. <td width=""23%""><a href=""https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2Fbindings%2Fexperimental%2Fdistrdf%2Ftest%2Ftest_buildranges.py/python%2Fdc_interfile_project%2FuseCompehensions%2Ftest/code/?utm_source=gh_review&"">:wrench: Example fixes</a></td>. </tr>. </tbody>. </table>. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8637
https://github.com/root-project/root/pull/8637:656,safety,test,test,656,"## DeepCode's analysis on [#b494dc](https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) found:. - :information_source: **1** minor issue. :point_down:. ## Top issues. <table>. <thead>. <tr>. <th align=""left"">Description</th>. <th align=""left"">Example fixes</th>. </tr>. </thead>. <tbody>. <tr>. <td width=""77%"">Use comprehensions instead of list(map(lambda: x ...)) Occurrences: <ul><li><a href=""https://github.com/root-project/root/blob/b494dc5668d0afbd988d4a57515308159492f146/bindings/experimental/distrdf/test/test_buildranges.py#L9"">test_buildranges.py:9</a></li> <li><a href=""https://github.com/root-project/root/blob/b494dc5668d0afbd988d4a57515308159492f146/bindings/experimental/distrdf/test/test_buildranges.py#L13"">test_buildranges.py:13</a></li></ul></td>. <td width=""23%""><a href=""https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2Fbindings%2Fexperimental%2Fdistrdf%2Ftest%2Ftest_buildranges.py/python%2Fdc_interfile_project%2FuseCompehensions%2Ftest/code/?utm_source=gh_review&"">:wrench: Example fixes</a></td>. </tr>. </tbody>. </table>. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://s",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8637
https://github.com/root-project/root/pull/8637:842,safety,test,test,842,"## DeepCode's analysis on [#b494dc](https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) found:. - :information_source: **1** minor issue. :point_down:. ## Top issues. <table>. <thead>. <tr>. <th align=""left"">Description</th>. <th align=""left"">Example fixes</th>. </tr>. </thead>. <tbody>. <tr>. <td width=""77%"">Use comprehensions instead of list(map(lambda: x ...)) Occurrences: <ul><li><a href=""https://github.com/root-project/root/blob/b494dc5668d0afbd988d4a57515308159492f146/bindings/experimental/distrdf/test/test_buildranges.py#L9"">test_buildranges.py:9</a></li> <li><a href=""https://github.com/root-project/root/blob/b494dc5668d0afbd988d4a57515308159492f146/bindings/experimental/distrdf/test/test_buildranges.py#L13"">test_buildranges.py:13</a></li></ul></td>. <td width=""23%""><a href=""https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2Fbindings%2Fexperimental%2Fdistrdf%2Ftest%2Ftest_buildranges.py/python%2Fdc_interfile_project%2FuseCompehensions%2Ftest/code/?utm_source=gh_review&"">:wrench: Example fixes</a></td>. </tr>. </tbody>. </table>. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://s",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8637
https://github.com/root-project/root/pull/8637:656,testability,test,test,656,"## DeepCode's analysis on [#b494dc](https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) found:. - :information_source: **1** minor issue. :point_down:. ## Top issues. <table>. <thead>. <tr>. <th align=""left"">Description</th>. <th align=""left"">Example fixes</th>. </tr>. </thead>. <tbody>. <tr>. <td width=""77%"">Use comprehensions instead of list(map(lambda: x ...)) Occurrences: <ul><li><a href=""https://github.com/root-project/root/blob/b494dc5668d0afbd988d4a57515308159492f146/bindings/experimental/distrdf/test/test_buildranges.py#L9"">test_buildranges.py:9</a></li> <li><a href=""https://github.com/root-project/root/blob/b494dc5668d0afbd988d4a57515308159492f146/bindings/experimental/distrdf/test/test_buildranges.py#L13"">test_buildranges.py:13</a></li></ul></td>. <td width=""23%""><a href=""https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2Fbindings%2Fexperimental%2Fdistrdf%2Ftest%2Ftest_buildranges.py/python%2Fdc_interfile_project%2FuseCompehensions%2Ftest/code/?utm_source=gh_review&"">:wrench: Example fixes</a></td>. </tr>. </tbody>. </table>. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://s",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8637
https://github.com/root-project/root/pull/8637:842,testability,test,test,842,"## DeepCode's analysis on [#b494dc](https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) found:. - :information_source: **1** minor issue. :point_down:. ## Top issues. <table>. <thead>. <tr>. <th align=""left"">Description</th>. <th align=""left"">Example fixes</th>. </tr>. </thead>. <tbody>. <tr>. <td width=""77%"">Use comprehensions instead of list(map(lambda: x ...)) Occurrences: <ul><li><a href=""https://github.com/root-project/root/blob/b494dc5668d0afbd988d4a57515308159492f146/bindings/experimental/distrdf/test/test_buildranges.py#L9"">test_buildranges.py:9</a></li> <li><a href=""https://github.com/root-project/root/blob/b494dc5668d0afbd988d4a57515308159492f146/bindings/experimental/distrdf/test/test_buildranges.py#L13"">test_buildranges.py:13</a></li></ul></td>. <td width=""23%""><a href=""https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2Fbindings%2Fexperimental%2Fdistrdf%2Ftest%2Ftest_buildranges.py/python%2Fdc_interfile_project%2FuseCompehensions%2Ftest/code/?utm_source=gh_review&"">:wrench: Example fixes</a></td>. </tr>. </tbody>. </table>. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/8341e59956b08e78aea43bdcdb0ada478be7018b/root-project/root/b494dc5668d0afbd988d4a57515308159492f146/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://s",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8637
https://github.com/root-project/root/pull/8637:86,availability,operat,operation,86,"I found out about this while doing some tests for the PR that will remove the `Range` operation. We actually have tests for the machinery that creates the ranges. Unfortunately, they were all passing through the `rangesToTuples` function that effectively removed information from the `TreeRange` objects so the filelist was never checked.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8637
https://github.com/root-project/root/pull/8637:40,safety,test,tests,40,"I found out about this while doing some tests for the PR that will remove the `Range` operation. We actually have tests for the machinery that creates the ranges. Unfortunately, they were all passing through the `rangesToTuples` function that effectively removed information from the `TreeRange` objects so the filelist was never checked.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8637
https://github.com/root-project/root/pull/8637:114,safety,test,tests,114,"I found out about this while doing some tests for the PR that will remove the `Range` operation. We actually have tests for the machinery that creates the ranges. Unfortunately, they were all passing through the `rangesToTuples` function that effectively removed information from the `TreeRange` objects so the filelist was never checked.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8637
https://github.com/root-project/root/pull/8637:40,testability,test,tests,40,"I found out about this while doing some tests for the PR that will remove the `Range` operation. We actually have tests for the machinery that creates the ranges. Unfortunately, they were all passing through the `rangesToTuples` function that effectively removed information from the `TreeRange` objects so the filelist was never checked.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8637
https://github.com/root-project/root/pull/8637:114,testability,test,tests,114,"I found out about this while doing some tests for the PR that will remove the `Range` operation. We actually have tests for the machinery that creates the ranges. Unfortunately, they were all passing through the `rangesToTuples` function that effectively removed information from the `TreeRange` objects so the filelist was never checked.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8637
https://github.com/root-project/root/pull/8637:243,usability,effectiv,effectively,243,"I found out about this while doing some tests for the PR that will remove the `Range` operation. We actually have tests for the machinery that creates the ranges. Unfortunately, they were all passing through the `rangesToTuples` function that effectively removed information from the `TreeRange` objects so the filelist was never checked.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8637
https://github.com/root-project/root/pull/8637:5,performance,time,time,5,"This time deepcode might be right with its warning:. ```diff. def treeranges_to_tuples(ranges):. """"""Convert TreeRange objects to tuples with the shape (start, end, filelist)"""""". - return list(map(lambda r: (r.start, r.end, r.filelist), ranges)). + return [(r.start, r.end, r.filelist) for r in ranges]. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8637
https://github.com/root-project/root/pull/8637:0,reliability,doe,does,0,does this warrant a test?,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8637
https://github.com/root-project/root/pull/8637:20,safety,test,test,20,does this warrant a test?,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8637
https://github.com/root-project/root/pull/8637:20,testability,test,test,20,does this warrant a test?,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8637
https://github.com/root-project/root/pull/8637:67,availability,avail,available,67,"> does this warrant a test? Yes definitely, I already modified the available tests to check the filelist. I can also add a new one to check a filelist with more than one file in it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8637
https://github.com/root-project/root/pull/8637:2,reliability,doe,does,2,"> does this warrant a test? Yes definitely, I already modified the available tests to check the filelist. I can also add a new one to check a filelist with more than one file in it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8637
https://github.com/root-project/root/pull/8637:67,reliability,availab,available,67,"> does this warrant a test? Yes definitely, I already modified the available tests to check the filelist. I can also add a new one to check a filelist with more than one file in it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8637
https://github.com/root-project/root/pull/8637:22,safety,test,test,22,"> does this warrant a test? Yes definitely, I already modified the available tests to check the filelist. I can also add a new one to check a filelist with more than one file in it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8637
https://github.com/root-project/root/pull/8637:67,safety,avail,available,67,"> does this warrant a test? Yes definitely, I already modified the available tests to check the filelist. I can also add a new one to check a filelist with more than one file in it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8637
https://github.com/root-project/root/pull/8637:77,safety,test,tests,77,"> does this warrant a test? Yes definitely, I already modified the available tests to check the filelist. I can also add a new one to check a filelist with more than one file in it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8637
https://github.com/root-project/root/pull/8637:54,security,modif,modified,54,"> does this warrant a test? Yes definitely, I already modified the available tests to check the filelist. I can also add a new one to check a filelist with more than one file in it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8637
https://github.com/root-project/root/pull/8637:67,security,availab,available,67,"> does this warrant a test? Yes definitely, I already modified the available tests to check the filelist. I can also add a new one to check a filelist with more than one file in it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8637
https://github.com/root-project/root/pull/8637:22,testability,test,test,22,"> does this warrant a test? Yes definitely, I already modified the available tests to check the filelist. I can also add a new one to check a filelist with more than one file in it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8637
https://github.com/root-project/root/pull/8637:77,testability,test,tests,77,"> does this warrant a test? Yes definitely, I already modified the available tests to check the filelist. I can also add a new one to check a filelist with more than one file in it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8637
https://github.com/root-project/root/pull/8637:158,energy efficiency,adapt,adaptation,158,"I rebased commits, reverting the change that removed a test that is repeated twice (will do that in another PR). I also split the change of the class and the adaptation of tests in two commits. If the CI reports green I will merge",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8637
https://github.com/root-project/root/pull/8637:212,energy efficiency,green,green,212,"I rebased commits, reverting the change that removed a test that is repeated twice (will do that in another PR). I also split the change of the class and the adaptation of tests in two commits. If the CI reports green I will merge",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8637
https://github.com/root-project/root/pull/8637:158,integrability,adapt,adaptation,158,"I rebased commits, reverting the change that removed a test that is repeated twice (will do that in another PR). I also split the change of the class and the adaptation of tests in two commits. If the CI reports green I will merge",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8637
https://github.com/root-project/root/pull/8637:158,interoperability,adapt,adaptation,158,"I rebased commits, reverting the change that removed a test that is repeated twice (will do that in another PR). I also split the change of the class and the adaptation of tests in two commits. If the CI reports green I will merge",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8637
https://github.com/root-project/root/pull/8637:158,modifiability,adapt,adaptation,158,"I rebased commits, reverting the change that removed a test that is repeated twice (will do that in another PR). I also split the change of the class and the adaptation of tests in two commits. If the CI reports green I will merge",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8637
https://github.com/root-project/root/pull/8637:55,safety,test,test,55,"I rebased commits, reverting the change that removed a test that is repeated twice (will do that in another PR). I also split the change of the class and the adaptation of tests in two commits. If the CI reports green I will merge",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8637
https://github.com/root-project/root/pull/8637:172,safety,test,tests,172,"I rebased commits, reverting the change that removed a test that is repeated twice (will do that in another PR). I also split the change of the class and the adaptation of tests in two commits. If the CI reports green I will merge",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8637
https://github.com/root-project/root/pull/8637:55,testability,test,test,55,"I rebased commits, reverting the change that removed a test that is repeated twice (will do that in another PR). I also split the change of the class and the adaptation of tests in two commits. If the CI reports green I will merge",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8637
https://github.com/root-project/root/pull/8637:172,testability,test,tests,172,"I rebased commits, reverting the change that removed a test that is repeated twice (will do that in another PR). I also split the change of the class and the adaptation of tests in two commits. If the CI reports green I will merge",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8637
https://github.com/root-project/root/pull/8637:16,safety,test,test,16,Small typo in a test :smile: should be good now,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8637
https://github.com/root-project/root/pull/8637:16,testability,test,test,16,Small typo in a test :smile: should be good now,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8637
https://github.com/root-project/root/issues/8641:141,deployability,releas,release,141,"Hi @aemerman, thank you for reporting this. I will fix this in ROOT master and also make backports to 6.24 and 6.22 since you are using this release. Do you need this fix in any older release?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8641
https://github.com/root-project/root/issues/8641:184,deployability,releas,release,184,"Hi @aemerman, thank you for reporting this. I will fix this in ROOT master and also make backports to 6.24 and 6.22 since you are using this release. Do you need this fix in any older release?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8641
https://github.com/root-project/root/issues/8641:38,deployability,releas,release,38,"Thank you! No, I don't need any older release.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8641
https://github.com/root-project/root/issues/8642:229,deployability,build,build,229,"Ok, sorry for much of the noise here. The issue only happens when *switching* `CMAKE_CXX_STANDARD` which doesn't rebuild `std.pcm`, directly setting the right value works fine. Assigning to Oksana because it would be nice if the build system could rebuild `std.pcm` if the value of `-std=c++XY` changed, but feel free to close if not feasible.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:105,reliability,doe,doesn,105,"Ok, sorry for much of the noise here. The issue only happens when *switching* `CMAKE_CXX_STANDARD` which doesn't rebuild `std.pcm`, directly setting the right value works fine. Assigning to Oksana because it would be nice if the build system could rebuild `std.pcm` if the value of `-std=c++XY` changed, but feel free to close if not feasible.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:321,usability,close,close,321,"Ok, sorry for much of the noise here. The issue only happens when *switching* `CMAKE_CXX_STANDARD` which doesn't rebuild `std.pcm`, directly setting the right value works fine. Assigning to Oksana because it would be nice if the build system could rebuild `std.pcm` if the value of `-std=c++XY` changed, but feel free to close if not feasible.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:0,deployability,Build,Build,0,Build system ownership went to Bertrand; reassigned.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:162,energy efficiency,current,currently-supported,162,"Yes, let's maybe close this issue. We don't support C++ 14 anymore, and this problem occurred when changing from C++14 to C++17. If there are new issues with the currently-supported C++ standards that you want to get fixed, feel free to open a new issue!",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:186,interoperability,standard,standards,186,"Yes, let's maybe close this issue. We don't support C++ 14 anymore, and this problem occurred when changing from C++14 to C++17. If there are new issues with the currently-supported C++ standards that you want to get fixed, feel free to open a new issue!",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:17,usability,close,close,17,"Yes, let's maybe close this issue. We don't support C++ 14 anymore, and this problem occurred when changing from C++14 to C++17. If there are new issues with the currently-supported C++ standards that you want to get fixed, feel free to open a new issue!",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:44,usability,support,support,44,"Yes, let's maybe close this issue. We don't support C++ 14 anymore, and this problem occurred when changing from C++14 to C++17. If there are new issues with the currently-supported C++ standards that you want to get fixed, feel free to open a new issue!",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:172,usability,support,supported,172,"Yes, let's maybe close this issue. We don't support C++ 14 anymore, and this problem occurred when changing from C++14 to C++17. If there are new issues with the currently-supported C++ standards that you want to get fixed, feel free to open a new issue!",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:274,availability,error,error,274,"FWIW the same problem can be seen when going from `CMAKE_CXX_STANDARD=17` to `CMAKE_CXX_STANDARD=20` with GCC 13.2.0:. ```. In module 'std' imported from input_line_1:1:. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/type_traits:1101:21: error: static assertion expression is not an integral constant expression. static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),. ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/bits/unique_ptr.h:237:13: note: in instantiation of template class 'std::is_move_constructible<std::default_delete<ROO. T::Internal::RHashMap>>' requested here. bool = is_move_constructible<_Dp>::value,. ^. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/bits/unique_ptr.h:283:7: note: in instantiation of default argument for '__uniq_ptr_data<ROOT::Internal::RHashMap, std. ::default_delete<ROOT::Internal::RHashMap>>' required here. __uniq_ptr_data<_Tp, _Dp> _M_t;. ^~~~~~~~~~~~~~~~~~~~~~~~~. /home/jhahnfel/ROOT/build-asserts-gcc13/include/ROOT/RConcurrentHashColl.hxx:32:38: note: in instantiation of template class 'std::unique_ptr<ROOT::Internal::RHashMap>' requested here. mutable std::unique_ptr<RHashMap> fHashMap;. ^. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/type_traits:1101:21: note: non-literal type 'true_type' (aka 'integral_constant<bool, true>') cannot be used in a cons. tant expression. static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),. ^. ```. I understand the desire to reduce the number of issues, but closing fully valid reports IMHO sends the wrong signal...",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:127,deployability,modul,module,127,"FWIW the same problem can be seen when going from `CMAKE_CXX_STANDARD=17` to `CMAKE_CXX_STANDARD=20` with GCC 13.2.0:. ```. In module 'std' imported from input_line_1:1:. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/type_traits:1101:21: error: static assertion expression is not an integral constant expression. static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),. ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/bits/unique_ptr.h:237:13: note: in instantiation of template class 'std::is_move_constructible<std::default_delete<ROO. T::Internal::RHashMap>>' requested here. bool = is_move_constructible<_Dp>::value,. ^. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/bits/unique_ptr.h:283:7: note: in instantiation of default argument for '__uniq_ptr_data<ROOT::Internal::RHashMap, std. ::default_delete<ROOT::Internal::RHashMap>>' required here. __uniq_ptr_data<_Tp, _Dp> _M_t;. ^~~~~~~~~~~~~~~~~~~~~~~~~. /home/jhahnfel/ROOT/build-asserts-gcc13/include/ROOT/RConcurrentHashColl.hxx:32:38: note: in instantiation of template class 'std::unique_ptr<ROOT::Internal::RHashMap>' requested here. mutable std::unique_ptr<RHashMap> fHashMap;. ^. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/type_traits:1101:21: note: non-literal type 'true_type' (aka 'integral_constant<bool, true>') cannot be used in a cons. tant expression. static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),. ^. ```. I understand the desire to reduce the number of issues, but closing fully valid reports IMHO sends the wrong signal...",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:319,deployability,integr,integral,319,"FWIW the same problem can be seen when going from `CMAKE_CXX_STANDARD=17` to `CMAKE_CXX_STANDARD=20` with GCC 13.2.0:. ```. In module 'std' imported from input_line_1:1:. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/type_traits:1101:21: error: static assertion expression is not an integral constant expression. static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),. ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/bits/unique_ptr.h:237:13: note: in instantiation of template class 'std::is_move_constructible<std::default_delete<ROO. T::Internal::RHashMap>>' requested here. bool = is_move_constructible<_Dp>::value,. ^. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/bits/unique_ptr.h:283:7: note: in instantiation of default argument for '__uniq_ptr_data<ROOT::Internal::RHashMap, std. ::default_delete<ROOT::Internal::RHashMap>>' required here. __uniq_ptr_data<_Tp, _Dp> _M_t;. ^~~~~~~~~~~~~~~~~~~~~~~~~. /home/jhahnfel/ROOT/build-asserts-gcc13/include/ROOT/RConcurrentHashColl.hxx:32:38: note: in instantiation of template class 'std::unique_ptr<ROOT::Internal::RHashMap>' requested here. mutable std::unique_ptr<RHashMap> fHashMap;. ^. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/type_traits:1101:21: note: non-literal type 'true_type' (aka 'integral_constant<bool, true>') cannot be used in a cons. tant expression. static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),. ^. ```. I understand the desire to reduce the number of issues, but closing fully valid reports IMHO sends the wrong signal...",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:1109,deployability,build,build-asserts-,1109,"FWIW the same problem can be seen when going from `CMAKE_CXX_STANDARD=17` to `CMAKE_CXX_STANDARD=20` with GCC 13.2.0:. ```. In module 'std' imported from input_line_1:1:. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/type_traits:1101:21: error: static assertion expression is not an integral constant expression. static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),. ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/bits/unique_ptr.h:237:13: note: in instantiation of template class 'std::is_move_constructible<std::default_delete<ROO. T::Internal::RHashMap>>' requested here. bool = is_move_constructible<_Dp>::value,. ^. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/bits/unique_ptr.h:283:7: note: in instantiation of default argument for '__uniq_ptr_data<ROOT::Internal::RHashMap, std. ::default_delete<ROOT::Internal::RHashMap>>' required here. __uniq_ptr_data<_Tp, _Dp> _M_t;. ^~~~~~~~~~~~~~~~~~~~~~~~~. /home/jhahnfel/ROOT/build-asserts-gcc13/include/ROOT/RConcurrentHashColl.hxx:32:38: note: in instantiation of template class 'std::unique_ptr<ROOT::Internal::RHashMap>' requested here. mutable std::unique_ptr<RHashMap> fHashMap;. ^. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/type_traits:1101:21: note: non-literal type 'true_type' (aka 'integral_constant<bool, true>') cannot be used in a cons. tant expression. static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),. ^. ```. I understand the desire to reduce the number of issues, but closing fully valid reports IMHO sends the wrong signal...",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:1648,energy efficiency,reduc,reduce,1648,"FWIW the same problem can be seen when going from `CMAKE_CXX_STANDARD=17` to `CMAKE_CXX_STANDARD=20` with GCC 13.2.0:. ```. In module 'std' imported from input_line_1:1:. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/type_traits:1101:21: error: static assertion expression is not an integral constant expression. static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),. ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/bits/unique_ptr.h:237:13: note: in instantiation of template class 'std::is_move_constructible<std::default_delete<ROO. T::Internal::RHashMap>>' requested here. bool = is_move_constructible<_Dp>::value,. ^. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/bits/unique_ptr.h:283:7: note: in instantiation of default argument for '__uniq_ptr_data<ROOT::Internal::RHashMap, std. ::default_delete<ROOT::Internal::RHashMap>>' required here. __uniq_ptr_data<_Tp, _Dp> _M_t;. ^~~~~~~~~~~~~~~~~~~~~~~~~. /home/jhahnfel/ROOT/build-asserts-gcc13/include/ROOT/RConcurrentHashColl.hxx:32:38: note: in instantiation of template class 'std::unique_ptr<ROOT::Internal::RHashMap>' requested here. mutable std::unique_ptr<RHashMap> fHashMap;. ^. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/type_traits:1101:21: note: non-literal type 'true_type' (aka 'integral_constant<bool, true>') cannot be used in a cons. tant expression. static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),. ^. ```. I understand the desire to reduce the number of issues, but closing fully valid reports IMHO sends the wrong signal...",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:319,integrability,integr,integral,319,"FWIW the same problem can be seen when going from `CMAKE_CXX_STANDARD=17` to `CMAKE_CXX_STANDARD=20` with GCC 13.2.0:. ```. In module 'std' imported from input_line_1:1:. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/type_traits:1101:21: error: static assertion expression is not an integral constant expression. static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),. ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/bits/unique_ptr.h:237:13: note: in instantiation of template class 'std::is_move_constructible<std::default_delete<ROO. T::Internal::RHashMap>>' requested here. bool = is_move_constructible<_Dp>::value,. ^. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/bits/unique_ptr.h:283:7: note: in instantiation of default argument for '__uniq_ptr_data<ROOT::Internal::RHashMap, std. ::default_delete<ROOT::Internal::RHashMap>>' required here. __uniq_ptr_data<_Tp, _Dp> _M_t;. ^~~~~~~~~~~~~~~~~~~~~~~~~. /home/jhahnfel/ROOT/build-asserts-gcc13/include/ROOT/RConcurrentHashColl.hxx:32:38: note: in instantiation of template class 'std::unique_ptr<ROOT::Internal::RHashMap>' requested here. mutable std::unique_ptr<RHashMap> fHashMap;. ^. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/type_traits:1101:21: note: non-literal type 'true_type' (aka 'integral_constant<bool, true>') cannot be used in a cons. tant expression. static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),. ^. ```. I understand the desire to reduce the number of issues, but closing fully valid reports IMHO sends the wrong signal...",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:319,interoperability,integr,integral,319,"FWIW the same problem can be seen when going from `CMAKE_CXX_STANDARD=17` to `CMAKE_CXX_STANDARD=20` with GCC 13.2.0:. ```. In module 'std' imported from input_line_1:1:. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/type_traits:1101:21: error: static assertion expression is not an integral constant expression. static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),. ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/bits/unique_ptr.h:237:13: note: in instantiation of template class 'std::is_move_constructible<std::default_delete<ROO. T::Internal::RHashMap>>' requested here. bool = is_move_constructible<_Dp>::value,. ^. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/bits/unique_ptr.h:283:7: note: in instantiation of default argument for '__uniq_ptr_data<ROOT::Internal::RHashMap, std. ::default_delete<ROOT::Internal::RHashMap>>' required here. __uniq_ptr_data<_Tp, _Dp> _M_t;. ^~~~~~~~~~~~~~~~~~~~~~~~~. /home/jhahnfel/ROOT/build-asserts-gcc13/include/ROOT/RConcurrentHashColl.hxx:32:38: note: in instantiation of template class 'std::unique_ptr<ROOT::Internal::RHashMap>' requested here. mutable std::unique_ptr<RHashMap> fHashMap;. ^. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/type_traits:1101:21: note: non-literal type 'true_type' (aka 'integral_constant<bool, true>') cannot be used in a cons. tant expression. static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),. ^. ```. I understand the desire to reduce the number of issues, but closing fully valid reports IMHO sends the wrong signal...",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:127,modifiability,modul,module,127,"FWIW the same problem can be seen when going from `CMAKE_CXX_STANDARD=17` to `CMAKE_CXX_STANDARD=20` with GCC 13.2.0:. ```. In module 'std' imported from input_line_1:1:. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/type_traits:1101:21: error: static assertion expression is not an integral constant expression. static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),. ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/bits/unique_ptr.h:237:13: note: in instantiation of template class 'std::is_move_constructible<std::default_delete<ROO. T::Internal::RHashMap>>' requested here. bool = is_move_constructible<_Dp>::value,. ^. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/bits/unique_ptr.h:283:7: note: in instantiation of default argument for '__uniq_ptr_data<ROOT::Internal::RHashMap, std. ::default_delete<ROOT::Internal::RHashMap>>' required here. __uniq_ptr_data<_Tp, _Dp> _M_t;. ^~~~~~~~~~~~~~~~~~~~~~~~~. /home/jhahnfel/ROOT/build-asserts-gcc13/include/ROOT/RConcurrentHashColl.hxx:32:38: note: in instantiation of template class 'std::unique_ptr<ROOT::Internal::RHashMap>' requested here. mutable std::unique_ptr<RHashMap> fHashMap;. ^. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/type_traits:1101:21: note: non-literal type 'true_type' (aka 'integral_constant<bool, true>') cannot be used in a cons. tant expression. static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),. ^. ```. I understand the desire to reduce the number of issues, but closing fully valid reports IMHO sends the wrong signal...",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:319,modifiability,integr,integral,319,"FWIW the same problem can be seen when going from `CMAKE_CXX_STANDARD=17` to `CMAKE_CXX_STANDARD=20` with GCC 13.2.0:. ```. In module 'std' imported from input_line_1:1:. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/type_traits:1101:21: error: static assertion expression is not an integral constant expression. static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),. ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/bits/unique_ptr.h:237:13: note: in instantiation of template class 'std::is_move_constructible<std::default_delete<ROO. T::Internal::RHashMap>>' requested here. bool = is_move_constructible<_Dp>::value,. ^. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/bits/unique_ptr.h:283:7: note: in instantiation of default argument for '__uniq_ptr_data<ROOT::Internal::RHashMap, std. ::default_delete<ROOT::Internal::RHashMap>>' required here. __uniq_ptr_data<_Tp, _Dp> _M_t;. ^~~~~~~~~~~~~~~~~~~~~~~~~. /home/jhahnfel/ROOT/build-asserts-gcc13/include/ROOT/RConcurrentHashColl.hxx:32:38: note: in instantiation of template class 'std::unique_ptr<ROOT::Internal::RHashMap>' requested here. mutable std::unique_ptr<RHashMap> fHashMap;. ^. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/type_traits:1101:21: note: non-literal type 'true_type' (aka 'integral_constant<bool, true>') cannot be used in a cons. tant expression. static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),. ^. ```. I understand the desire to reduce the number of issues, but closing fully valid reports IMHO sends the wrong signal...",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:274,performance,error,error,274,"FWIW the same problem can be seen when going from `CMAKE_CXX_STANDARD=17` to `CMAKE_CXX_STANDARD=20` with GCC 13.2.0:. ```. In module 'std' imported from input_line_1:1:. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/type_traits:1101:21: error: static assertion expression is not an integral constant expression. static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),. ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/bits/unique_ptr.h:237:13: note: in instantiation of template class 'std::is_move_constructible<std::default_delete<ROO. T::Internal::RHashMap>>' requested here. bool = is_move_constructible<_Dp>::value,. ^. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/bits/unique_ptr.h:283:7: note: in instantiation of default argument for '__uniq_ptr_data<ROOT::Internal::RHashMap, std. ::default_delete<ROOT::Internal::RHashMap>>' required here. __uniq_ptr_data<_Tp, _Dp> _M_t;. ^~~~~~~~~~~~~~~~~~~~~~~~~. /home/jhahnfel/ROOT/build-asserts-gcc13/include/ROOT/RConcurrentHashColl.hxx:32:38: note: in instantiation of template class 'std::unique_ptr<ROOT::Internal::RHashMap>' requested here. mutable std::unique_ptr<RHashMap> fHashMap;. ^. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/type_traits:1101:21: note: non-literal type 'true_type' (aka 'integral_constant<bool, true>') cannot be used in a cons. tant expression. static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),. ^. ```. I understand the desire to reduce the number of issues, but closing fully valid reports IMHO sends the wrong signal...",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:319,reliability,integr,integral,319,"FWIW the same problem can be seen when going from `CMAKE_CXX_STANDARD=17` to `CMAKE_CXX_STANDARD=20` with GCC 13.2.0:. ```. In module 'std' imported from input_line_1:1:. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/type_traits:1101:21: error: static assertion expression is not an integral constant expression. static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),. ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/bits/unique_ptr.h:237:13: note: in instantiation of template class 'std::is_move_constructible<std::default_delete<ROO. T::Internal::RHashMap>>' requested here. bool = is_move_constructible<_Dp>::value,. ^. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/bits/unique_ptr.h:283:7: note: in instantiation of default argument for '__uniq_ptr_data<ROOT::Internal::RHashMap, std. ::default_delete<ROOT::Internal::RHashMap>>' required here. __uniq_ptr_data<_Tp, _Dp> _M_t;. ^~~~~~~~~~~~~~~~~~~~~~~~~. /home/jhahnfel/ROOT/build-asserts-gcc13/include/ROOT/RConcurrentHashColl.hxx:32:38: note: in instantiation of template class 'std::unique_ptr<ROOT::Internal::RHashMap>' requested here. mutable std::unique_ptr<RHashMap> fHashMap;. ^. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/type_traits:1101:21: note: non-literal type 'true_type' (aka 'integral_constant<bool, true>') cannot be used in a cons. tant expression. static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),. ^. ```. I understand the desire to reduce the number of issues, but closing fully valid reports IMHO sends the wrong signal...",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:127,safety,modul,module,127,"FWIW the same problem can be seen when going from `CMAKE_CXX_STANDARD=17` to `CMAKE_CXX_STANDARD=20` with GCC 13.2.0:. ```. In module 'std' imported from input_line_1:1:. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/type_traits:1101:21: error: static assertion expression is not an integral constant expression. static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),. ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/bits/unique_ptr.h:237:13: note: in instantiation of template class 'std::is_move_constructible<std::default_delete<ROO. T::Internal::RHashMap>>' requested here. bool = is_move_constructible<_Dp>::value,. ^. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/bits/unique_ptr.h:283:7: note: in instantiation of default argument for '__uniq_ptr_data<ROOT::Internal::RHashMap, std. ::default_delete<ROOT::Internal::RHashMap>>' required here. __uniq_ptr_data<_Tp, _Dp> _M_t;. ^~~~~~~~~~~~~~~~~~~~~~~~~. /home/jhahnfel/ROOT/build-asserts-gcc13/include/ROOT/RConcurrentHashColl.hxx:32:38: note: in instantiation of template class 'std::unique_ptr<ROOT::Internal::RHashMap>' requested here. mutable std::unique_ptr<RHashMap> fHashMap;. ^. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/type_traits:1101:21: note: non-literal type 'true_type' (aka 'integral_constant<bool, true>') cannot be used in a cons. tant expression. static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),. ^. ```. I understand the desire to reduce the number of issues, but closing fully valid reports IMHO sends the wrong signal...",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:274,safety,error,error,274,"FWIW the same problem can be seen when going from `CMAKE_CXX_STANDARD=17` to `CMAKE_CXX_STANDARD=20` with GCC 13.2.0:. ```. In module 'std' imported from input_line_1:1:. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/type_traits:1101:21: error: static assertion expression is not an integral constant expression. static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),. ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/bits/unique_ptr.h:237:13: note: in instantiation of template class 'std::is_move_constructible<std::default_delete<ROO. T::Internal::RHashMap>>' requested here. bool = is_move_constructible<_Dp>::value,. ^. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/bits/unique_ptr.h:283:7: note: in instantiation of default argument for '__uniq_ptr_data<ROOT::Internal::RHashMap, std. ::default_delete<ROOT::Internal::RHashMap>>' required here. __uniq_ptr_data<_Tp, _Dp> _M_t;. ^~~~~~~~~~~~~~~~~~~~~~~~~. /home/jhahnfel/ROOT/build-asserts-gcc13/include/ROOT/RConcurrentHashColl.hxx:32:38: note: in instantiation of template class 'std::unique_ptr<ROOT::Internal::RHashMap>' requested here. mutable std::unique_ptr<RHashMap> fHashMap;. ^. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/type_traits:1101:21: note: non-literal type 'true_type' (aka 'integral_constant<bool, true>') cannot be used in a cons. tant expression. static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),. ^. ```. I understand the desire to reduce the number of issues, but closing fully valid reports IMHO sends the wrong signal...",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:1695,safety,valid,valid,1695,"FWIW the same problem can be seen when going from `CMAKE_CXX_STANDARD=17` to `CMAKE_CXX_STANDARD=20` with GCC 13.2.0:. ```. In module 'std' imported from input_line_1:1:. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/type_traits:1101:21: error: static assertion expression is not an integral constant expression. static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),. ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/bits/unique_ptr.h:237:13: note: in instantiation of template class 'std::is_move_constructible<std::default_delete<ROO. T::Internal::RHashMap>>' requested here. bool = is_move_constructible<_Dp>::value,. ^. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/bits/unique_ptr.h:283:7: note: in instantiation of default argument for '__uniq_ptr_data<ROOT::Internal::RHashMap, std. ::default_delete<ROOT::Internal::RHashMap>>' required here. __uniq_ptr_data<_Tp, _Dp> _M_t;. ^~~~~~~~~~~~~~~~~~~~~~~~~. /home/jhahnfel/ROOT/build-asserts-gcc13/include/ROOT/RConcurrentHashColl.hxx:32:38: note: in instantiation of template class 'std::unique_ptr<ROOT::Internal::RHashMap>' requested here. mutable std::unique_ptr<RHashMap> fHashMap;. ^. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/type_traits:1101:21: note: non-literal type 'true_type' (aka 'integral_constant<bool, true>') cannot be used in a cons. tant expression. static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),. ^. ```. I understand the desire to reduce the number of issues, but closing fully valid reports IMHO sends the wrong signal...",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:319,security,integr,integral,319,"FWIW the same problem can be seen when going from `CMAKE_CXX_STANDARD=17` to `CMAKE_CXX_STANDARD=20` with GCC 13.2.0:. ```. In module 'std' imported from input_line_1:1:. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/type_traits:1101:21: error: static assertion expression is not an integral constant expression. static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),. ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/bits/unique_ptr.h:237:13: note: in instantiation of template class 'std::is_move_constructible<std::default_delete<ROO. T::Internal::RHashMap>>' requested here. bool = is_move_constructible<_Dp>::value,. ^. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/bits/unique_ptr.h:283:7: note: in instantiation of default argument for '__uniq_ptr_data<ROOT::Internal::RHashMap, std. ::default_delete<ROOT::Internal::RHashMap>>' required here. __uniq_ptr_data<_Tp, _Dp> _M_t;. ^~~~~~~~~~~~~~~~~~~~~~~~~. /home/jhahnfel/ROOT/build-asserts-gcc13/include/ROOT/RConcurrentHashColl.hxx:32:38: note: in instantiation of template class 'std::unique_ptr<ROOT::Internal::RHashMap>' requested here. mutable std::unique_ptr<RHashMap> fHashMap;. ^. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/type_traits:1101:21: note: non-literal type 'true_type' (aka 'integral_constant<bool, true>') cannot be used in a cons. tant expression. static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),. ^. ```. I understand the desire to reduce the number of issues, but closing fully valid reports IMHO sends the wrong signal...",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:1730,security,sign,signal,1730,"FWIW the same problem can be seen when going from `CMAKE_CXX_STANDARD=17` to `CMAKE_CXX_STANDARD=20` with GCC 13.2.0:. ```. In module 'std' imported from input_line_1:1:. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/type_traits:1101:21: error: static assertion expression is not an integral constant expression. static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),. ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/bits/unique_ptr.h:237:13: note: in instantiation of template class 'std::is_move_constructible<std::default_delete<ROO. T::Internal::RHashMap>>' requested here. bool = is_move_constructible<_Dp>::value,. ^. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/bits/unique_ptr.h:283:7: note: in instantiation of default argument for '__uniq_ptr_data<ROOT::Internal::RHashMap, std. ::default_delete<ROOT::Internal::RHashMap>>' required here. __uniq_ptr_data<_Tp, _Dp> _M_t;. ^~~~~~~~~~~~~~~~~~~~~~~~~. /home/jhahnfel/ROOT/build-asserts-gcc13/include/ROOT/RConcurrentHashColl.hxx:32:38: note: in instantiation of template class 'std::unique_ptr<ROOT::Internal::RHashMap>' requested here. mutable std::unique_ptr<RHashMap> fHashMap;. ^. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/type_traits:1101:21: note: non-literal type 'true_type' (aka 'integral_constant<bool, true>') cannot be used in a cons. tant expression. static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),. ^. ```. I understand the desire to reduce the number of issues, but closing fully valid reports IMHO sends the wrong signal...",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:288,testability,assert,assertion,288,"FWIW the same problem can be seen when going from `CMAKE_CXX_STANDARD=17` to `CMAKE_CXX_STANDARD=20` with GCC 13.2.0:. ```. In module 'std' imported from input_line_1:1:. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/type_traits:1101:21: error: static assertion expression is not an integral constant expression. static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),. ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/bits/unique_ptr.h:237:13: note: in instantiation of template class 'std::is_move_constructible<std::default_delete<ROO. T::Internal::RHashMap>>' requested here. bool = is_move_constructible<_Dp>::value,. ^. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/bits/unique_ptr.h:283:7: note: in instantiation of default argument for '__uniq_ptr_data<ROOT::Internal::RHashMap, std. ::default_delete<ROOT::Internal::RHashMap>>' required here. __uniq_ptr_data<_Tp, _Dp> _M_t;. ^~~~~~~~~~~~~~~~~~~~~~~~~. /home/jhahnfel/ROOT/build-asserts-gcc13/include/ROOT/RConcurrentHashColl.hxx:32:38: note: in instantiation of template class 'std::unique_ptr<ROOT::Internal::RHashMap>' requested here. mutable std::unique_ptr<RHashMap> fHashMap;. ^. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/type_traits:1101:21: note: non-literal type 'true_type' (aka 'integral_constant<bool, true>') cannot be used in a cons. tant expression. static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),. ^. ```. I understand the desire to reduce the number of issues, but closing fully valid reports IMHO sends the wrong signal...",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:319,testability,integr,integral,319,"FWIW the same problem can be seen when going from `CMAKE_CXX_STANDARD=17` to `CMAKE_CXX_STANDARD=20` with GCC 13.2.0:. ```. In module 'std' imported from input_line_1:1:. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/type_traits:1101:21: error: static assertion expression is not an integral constant expression. static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),. ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/bits/unique_ptr.h:237:13: note: in instantiation of template class 'std::is_move_constructible<std::default_delete<ROO. T::Internal::RHashMap>>' requested here. bool = is_move_constructible<_Dp>::value,. ^. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/bits/unique_ptr.h:283:7: note: in instantiation of default argument for '__uniq_ptr_data<ROOT::Internal::RHashMap, std. ::default_delete<ROOT::Internal::RHashMap>>' required here. __uniq_ptr_data<_Tp, _Dp> _M_t;. ^~~~~~~~~~~~~~~~~~~~~~~~~. /home/jhahnfel/ROOT/build-asserts-gcc13/include/ROOT/RConcurrentHashColl.hxx:32:38: note: in instantiation of template class 'std::unique_ptr<ROOT::Internal::RHashMap>' requested here. mutable std::unique_ptr<RHashMap> fHashMap;. ^. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/type_traits:1101:21: note: non-literal type 'true_type' (aka 'integral_constant<bool, true>') cannot be used in a cons. tant expression. static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),. ^. ```. I understand the desire to reduce the number of issues, but closing fully valid reports IMHO sends the wrong signal...",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:1115,testability,assert,asserts-,1115,"FWIW the same problem can be seen when going from `CMAKE_CXX_STANDARD=17` to `CMAKE_CXX_STANDARD=20` with GCC 13.2.0:. ```. In module 'std' imported from input_line_1:1:. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/type_traits:1101:21: error: static assertion expression is not an integral constant expression. static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),. ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/bits/unique_ptr.h:237:13: note: in instantiation of template class 'std::is_move_constructible<std::default_delete<ROO. T::Internal::RHashMap>>' requested here. bool = is_move_constructible<_Dp>::value,. ^. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/bits/unique_ptr.h:283:7: note: in instantiation of default argument for '__uniq_ptr_data<ROOT::Internal::RHashMap, std. ::default_delete<ROOT::Internal::RHashMap>>' required here. __uniq_ptr_data<_Tp, _Dp> _M_t;. ^~~~~~~~~~~~~~~~~~~~~~~~~. /home/jhahnfel/ROOT/build-asserts-gcc13/include/ROOT/RConcurrentHashColl.hxx:32:38: note: in instantiation of template class 'std::unique_ptr<ROOT::Internal::RHashMap>' requested here. mutable std::unique_ptr<RHashMap> fHashMap;. ^. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/type_traits:1101:21: note: non-literal type 'true_type' (aka 'integral_constant<bool, true>') cannot be used in a cons. tant expression. static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),. ^. ```. I understand the desire to reduce the number of issues, but closing fully valid reports IMHO sends the wrong signal...",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:1623,testability,understand,understand,1623,"FWIW the same problem can be seen when going from `CMAKE_CXX_STANDARD=17` to `CMAKE_CXX_STANDARD=20` with GCC 13.2.0:. ```. In module 'std' imported from input_line_1:1:. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/type_traits:1101:21: error: static assertion expression is not an integral constant expression. static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),. ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/bits/unique_ptr.h:237:13: note: in instantiation of template class 'std::is_move_constructible<std::default_delete<ROO. T::Internal::RHashMap>>' requested here. bool = is_move_constructible<_Dp>::value,. ^. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/bits/unique_ptr.h:283:7: note: in instantiation of default argument for '__uniq_ptr_data<ROOT::Internal::RHashMap, std. ::default_delete<ROOT::Internal::RHashMap>>' required here. __uniq_ptr_data<_Tp, _Dp> _M_t;. ^~~~~~~~~~~~~~~~~~~~~~~~~. /home/jhahnfel/ROOT/build-asserts-gcc13/include/ROOT/RConcurrentHashColl.hxx:32:38: note: in instantiation of template class 'std::unique_ptr<ROOT::Internal::RHashMap>' requested here. mutable std::unique_ptr<RHashMap> fHashMap;. ^. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/type_traits:1101:21: note: non-literal type 'true_type' (aka 'integral_constant<bool, true>') cannot be used in a cons. tant expression. static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),. ^. ```. I understand the desire to reduce the number of issues, but closing fully valid reports IMHO sends the wrong signal...",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:274,usability,error,error,274,"FWIW the same problem can be seen when going from `CMAKE_CXX_STANDARD=17` to `CMAKE_CXX_STANDARD=20` with GCC 13.2.0:. ```. In module 'std' imported from input_line_1:1:. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/type_traits:1101:21: error: static assertion expression is not an integral constant expression. static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),. ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/bits/unique_ptr.h:237:13: note: in instantiation of template class 'std::is_move_constructible<std::default_delete<ROO. T::Internal::RHashMap>>' requested here. bool = is_move_constructible<_Dp>::value,. ^. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/bits/unique_ptr.h:283:7: note: in instantiation of default argument for '__uniq_ptr_data<ROOT::Internal::RHashMap, std. ::default_delete<ROOT::Internal::RHashMap>>' required here. __uniq_ptr_data<_Tp, _Dp> _M_t;. ^~~~~~~~~~~~~~~~~~~~~~~~~. /home/jhahnfel/ROOT/build-asserts-gcc13/include/ROOT/RConcurrentHashColl.hxx:32:38: note: in instantiation of template class 'std::unique_ptr<ROOT::Internal::RHashMap>' requested here. mutable std::unique_ptr<RHashMap> fHashMap;. ^. /opt/gcc/13.2.0/lib/gcc/x86_64-pc-linux-gnu/13.2.0/../../../../include/c++/13.2.0/type_traits:1101:21: note: non-literal type 'true_type' (aka 'integral_constant<bool, true>') cannot be used in a cons. tant expression. static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),. ^. ```. I understand the desire to reduce the number of issues, but closing fully valid reports IMHO sends the wrong signal...",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:515,availability,error,errors,515,"@hahnjo, you were opening this issue as ""Build broken with C++17 on CentOS 8"", and then only renamed it later to just because you understood the build was not actually broken ""Switching CMAKE_CXX_STANDARD breaks build"". This kind of re-interpretation biases the relevance of the issue, which is why I'm not a big fan of it and was trigger happy to close it. When reinterpreting issues like this, you have to ask yourself: would I have also opened an original issue for this too? Especially because of:. > The build errors when switching from CMAKE_CXX_STANDARD=14 (the default with GCC 8.4.1 on CentOS 8) to CMAKE_CXX_STANDARD=17 (for example to get ROOT7). ROOT 7 has now the same CMAKE_CXX_STANDARD requirements, so please give me a new example of why someone would do this. Anyone who reads this issue to consider fixing it will have this context in mind, which brings them to the conclusion that it's not relevant to fix. So why keep it open?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:41,deployability,Build,Build,41,"@hahnjo, you were opening this issue as ""Build broken with C++17 on CentOS 8"", and then only renamed it later to just because you understood the build was not actually broken ""Switching CMAKE_CXX_STANDARD breaks build"". This kind of re-interpretation biases the relevance of the issue, which is why I'm not a big fan of it and was trigger happy to close it. When reinterpreting issues like this, you have to ask yourself: would I have also opened an original issue for this too? Especially because of:. > The build errors when switching from CMAKE_CXX_STANDARD=14 (the default with GCC 8.4.1 on CentOS 8) to CMAKE_CXX_STANDARD=17 (for example to get ROOT7). ROOT 7 has now the same CMAKE_CXX_STANDARD requirements, so please give me a new example of why someone would do this. Anyone who reads this issue to consider fixing it will have this context in mind, which brings them to the conclusion that it's not relevant to fix. So why keep it open?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:145,deployability,build,build,145,"@hahnjo, you were opening this issue as ""Build broken with C++17 on CentOS 8"", and then only renamed it later to just because you understood the build was not actually broken ""Switching CMAKE_CXX_STANDARD breaks build"". This kind of re-interpretation biases the relevance of the issue, which is why I'm not a big fan of it and was trigger happy to close it. When reinterpreting issues like this, you have to ask yourself: would I have also opened an original issue for this too? Especially because of:. > The build errors when switching from CMAKE_CXX_STANDARD=14 (the default with GCC 8.4.1 on CentOS 8) to CMAKE_CXX_STANDARD=17 (for example to get ROOT7). ROOT 7 has now the same CMAKE_CXX_STANDARD requirements, so please give me a new example of why someone would do this. Anyone who reads this issue to consider fixing it will have this context in mind, which brings them to the conclusion that it's not relevant to fix. So why keep it open?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:212,deployability,build,build,212,"@hahnjo, you were opening this issue as ""Build broken with C++17 on CentOS 8"", and then only renamed it later to just because you understood the build was not actually broken ""Switching CMAKE_CXX_STANDARD breaks build"". This kind of re-interpretation biases the relevance of the issue, which is why I'm not a big fan of it and was trigger happy to close it. When reinterpreting issues like this, you have to ask yourself: would I have also opened an original issue for this too? Especially because of:. > The build errors when switching from CMAKE_CXX_STANDARD=14 (the default with GCC 8.4.1 on CentOS 8) to CMAKE_CXX_STANDARD=17 (for example to get ROOT7). ROOT 7 has now the same CMAKE_CXX_STANDARD requirements, so please give me a new example of why someone would do this. Anyone who reads this issue to consider fixing it will have this context in mind, which brings them to the conclusion that it's not relevant to fix. So why keep it open?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:509,deployability,build,build,509,"@hahnjo, you were opening this issue as ""Build broken with C++17 on CentOS 8"", and then only renamed it later to just because you understood the build was not actually broken ""Switching CMAKE_CXX_STANDARD breaks build"". This kind of re-interpretation biases the relevance of the issue, which is why I'm not a big fan of it and was trigger happy to close it. When reinterpreting issues like this, you have to ask yourself: would I have also opened an original issue for this too? Especially because of:. > The build errors when switching from CMAKE_CXX_STANDARD=14 (the default with GCC 8.4.1 on CentOS 8) to CMAKE_CXX_STANDARD=17 (for example to get ROOT7). ROOT 7 has now the same CMAKE_CXX_STANDARD requirements, so please give me a new example of why someone would do this. Anyone who reads this issue to consider fixing it will have this context in mind, which brings them to the conclusion that it's not relevant to fix. So why keep it open?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:515,performance,error,errors,515,"@hahnjo, you were opening this issue as ""Build broken with C++17 on CentOS 8"", and then only renamed it later to just because you understood the build was not actually broken ""Switching CMAKE_CXX_STANDARD breaks build"". This kind of re-interpretation biases the relevance of the issue, which is why I'm not a big fan of it and was trigger happy to close it. When reinterpreting issues like this, you have to ask yourself: would I have also opened an original issue for this too? Especially because of:. > The build errors when switching from CMAKE_CXX_STANDARD=14 (the default with GCC 8.4.1 on CentOS 8) to CMAKE_CXX_STANDARD=17 (for example to get ROOT7). ROOT 7 has now the same CMAKE_CXX_STANDARD requirements, so please give me a new example of why someone would do this. Anyone who reads this issue to consider fixing it will have this context in mind, which brings them to the conclusion that it's not relevant to fix. So why keep it open?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:515,safety,error,errors,515,"@hahnjo, you were opening this issue as ""Build broken with C++17 on CentOS 8"", and then only renamed it later to just because you understood the build was not actually broken ""Switching CMAKE_CXX_STANDARD breaks build"". This kind of re-interpretation biases the relevance of the issue, which is why I'm not a big fan of it and was trigger happy to close it. When reinterpreting issues like this, you have to ask yourself: would I have also opened an original issue for this too? Especially because of:. > The build errors when switching from CMAKE_CXX_STANDARD=14 (the default with GCC 8.4.1 on CentOS 8) to CMAKE_CXX_STANDARD=17 (for example to get ROOT7). ROOT 7 has now the same CMAKE_CXX_STANDARD requirements, so please give me a new example of why someone would do this. Anyone who reads this issue to consider fixing it will have this context in mind, which brings them to the conclusion that it's not relevant to fix. So why keep it open?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:842,testability,context,context,842,"@hahnjo, you were opening this issue as ""Build broken with C++17 on CentOS 8"", and then only renamed it later to just because you understood the build was not actually broken ""Switching CMAKE_CXX_STANDARD breaks build"". This kind of re-interpretation biases the relevance of the issue, which is why I'm not a big fan of it and was trigger happy to close it. When reinterpreting issues like this, you have to ask yourself: would I have also opened an original issue for this too? Especially because of:. > The build errors when switching from CMAKE_CXX_STANDARD=14 (the default with GCC 8.4.1 on CentOS 8) to CMAKE_CXX_STANDARD=17 (for example to get ROOT7). ROOT 7 has now the same CMAKE_CXX_STANDARD requirements, so please give me a new example of why someone would do this. Anyone who reads this issue to consider fixing it will have this context in mind, which brings them to the conclusion that it's not relevant to fix. So why keep it open?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:348,usability,close,close,348,"@hahnjo, you were opening this issue as ""Build broken with C++17 on CentOS 8"", and then only renamed it later to just because you understood the build was not actually broken ""Switching CMAKE_CXX_STANDARD breaks build"". This kind of re-interpretation biases the relevance of the issue, which is why I'm not a big fan of it and was trigger happy to close it. When reinterpreting issues like this, you have to ask yourself: would I have also opened an original issue for this too? Especially because of:. > The build errors when switching from CMAKE_CXX_STANDARD=14 (the default with GCC 8.4.1 on CentOS 8) to CMAKE_CXX_STANDARD=17 (for example to get ROOT7). ROOT 7 has now the same CMAKE_CXX_STANDARD requirements, so please give me a new example of why someone would do this. Anyone who reads this issue to consider fixing it will have this context in mind, which brings them to the conclusion that it's not relevant to fix. So why keep it open?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:515,usability,error,errors,515,"@hahnjo, you were opening this issue as ""Build broken with C++17 on CentOS 8"", and then only renamed it later to just because you understood the build was not actually broken ""Switching CMAKE_CXX_STANDARD breaks build"". This kind of re-interpretation biases the relevance of the issue, which is why I'm not a big fan of it and was trigger happy to close it. When reinterpreting issues like this, you have to ask yourself: would I have also opened an original issue for this too? Especially because of:. > The build errors when switching from CMAKE_CXX_STANDARD=14 (the default with GCC 8.4.1 on CentOS 8) to CMAKE_CXX_STANDARD=17 (for example to get ROOT7). ROOT 7 has now the same CMAKE_CXX_STANDARD requirements, so please give me a new example of why someone would do this. Anyone who reads this issue to consider fixing it will have this context in mind, which brings them to the conclusion that it's not relevant to fix. So why keep it open?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:29,energy efficiency,reduc,reduce,29,"> I understand the desire to reduce the number of issues, but closing fully valid reports IMHO sends the wrong signal... I'm very happy to discuss about this by the way and I would like to understand you better and improve my communication when closing issues: what ""wrong signal"" did it send in this case?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:76,safety,valid,valid,76,"> I understand the desire to reduce the number of issues, but closing fully valid reports IMHO sends the wrong signal... I'm very happy to discuss about this by the way and I would like to understand you better and improve my communication when closing issues: what ""wrong signal"" did it send in this case?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:111,security,sign,signal,111,"> I understand the desire to reduce the number of issues, but closing fully valid reports IMHO sends the wrong signal... I'm very happy to discuss about this by the way and I would like to understand you better and improve my communication when closing issues: what ""wrong signal"" did it send in this case?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:273,security,sign,signal,273,"> I understand the desire to reduce the number of issues, but closing fully valid reports IMHO sends the wrong signal... I'm very happy to discuss about this by the way and I would like to understand you better and improve my communication when closing issues: what ""wrong signal"" did it send in this case?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:4,testability,understand,understand,4,"> I understand the desire to reduce the number of issues, but closing fully valid reports IMHO sends the wrong signal... I'm very happy to discuss about this by the way and I would like to understand you better and improve my communication when closing issues: what ""wrong signal"" did it send in this case?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:189,testability,understand,understand,189,"> I understand the desire to reduce the number of issues, but closing fully valid reports IMHO sends the wrong signal... I'm very happy to discuss about this by the way and I would like to understand you better and improve my communication when closing issues: what ""wrong signal"" did it send in this case?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:1127,availability,error,error,1127,"ce of the issue, which is why I'm not a big fan of it and was trigger happy to close it. Acknowledged that re-interpretation of issues is difficult; in this case I would argue that I better understood the underlying reason and changed the issue to correct the description. Just closing and dismissing this analysis is what I'm against. > When reinterpreting issues like this, you have to ask yourself: would I have also opened an original issue for this too? Especially because of:. Yes, I would have opened the same issue if I had understood the underlying reason before; that's why I updated it. > ROOT 7 has now the same CMAKE_CXX_STANDARD requirements, so please give me a new example of why someone would do this. I already gave you an example in https://github.com/root-project/root/issues/8642#issuecomment-2039323384:. > FWIW the same problem can be seen when going from `CMAKE_CXX_STANDARD=17` to `CMAKE_CXX_STANDARD=20` with GCC 13.2.0:. C++17 is now the default, but if somebody tries to switch an existing build directory to C++20, they will be faced with obscure error messages. > Anyone who reads this issue to consider fixing it will have this context in mind, which brings them to the conclusion that it's not relevant to fix. So why keep it open? Anyone looking at this issue should have a look at the entire discussion, including https://github.com/root-project/root/issues/8642#issuecomment-2039323384 and this comment. This should explain why I think it's still relevant. > I'm very happy to discuss about this by the way and I would like to understand you better and improve my communication when closing issues: what ""wrong signal"" did it send in this case? The ""wrong signal"" that this sends is exactly what you describe: A valid report of something not working is thought ""not relevant to fix"". IMHO this is a disastrous attitude and will just discourage people from reporting issue - it just risks being deemed ""not relevant"" by the core developers, so why put in the effort?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:637,deployability,updat,updated,637,"> This kind of re-interpretation biases the relevance of the issue, which is why I'm not a big fan of it and was trigger happy to close it. Acknowledged that re-interpretation of issues is difficult; in this case I would argue that I better understood the underlying reason and changed the issue to correct the description. Just closing and dismissing this analysis is what I'm against. > When reinterpreting issues like this, you have to ask yourself: would I have also opened an original issue for this too? Especially because of:. Yes, I would have opened the same issue if I had understood the underlying reason before; that's why I updated it. > ROOT 7 has now the same CMAKE_CXX_STANDARD requirements, so please give me a new example of why someone would do this. I already gave you an example in https://github.com/root-project/root/issues/8642#issuecomment-2039323384:. > FWIW the same problem can be seen when going from `CMAKE_CXX_STANDARD=17` to `CMAKE_CXX_STANDARD=20` with GCC 13.2.0:. C++17 is now the default, but if somebody tries to switch an existing build directory to C++20, they will be faced with obscure error messages. > Anyone who reads this issue to consider fixing it will have this context in mind, which brings them to the conclusion that it's not relevant to fix. So why keep it open? Anyone looking at this issue should have a look at the entire discussion, including https://github.com/root-project/root/issues/8642#issuecomment-2039323384 and this comment. This should explain why I think it's still relevant. > I'm very happy to discuss about this by the way and I would like to understand you better and improve my communication when closing issues: what ""wrong signal"" did it send in this case? The ""wrong signal"" that this sends is exactly what you describe: A valid report of something not working is thought ""not relevant to fix"". IMHO this is a disastrous attitude and will just discourage people from reporting issue - it just risks being deemed ""not relevant",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:1069,deployability,build,build,1069,"ce of the issue, which is why I'm not a big fan of it and was trigger happy to close it. Acknowledged that re-interpretation of issues is difficult; in this case I would argue that I better understood the underlying reason and changed the issue to correct the description. Just closing and dismissing this analysis is what I'm against. > When reinterpreting issues like this, you have to ask yourself: would I have also opened an original issue for this too? Especially because of:. Yes, I would have opened the same issue if I had understood the underlying reason before; that's why I updated it. > ROOT 7 has now the same CMAKE_CXX_STANDARD requirements, so please give me a new example of why someone would do this. I already gave you an example in https://github.com/root-project/root/issues/8642#issuecomment-2039323384:. > FWIW the same problem can be seen when going from `CMAKE_CXX_STANDARD=17` to `CMAKE_CXX_STANDARD=20` with GCC 13.2.0:. C++17 is now the default, but if somebody tries to switch an existing build directory to C++20, they will be faced with obscure error messages. > Anyone who reads this issue to consider fixing it will have this context in mind, which brings them to the conclusion that it's not relevant to fix. So why keep it open? Anyone looking at this issue should have a look at the entire discussion, including https://github.com/root-project/root/issues/8642#issuecomment-2039323384 and this comment. This should explain why I think it's still relevant. > I'm very happy to discuss about this by the way and I would like to understand you better and improve my communication when closing issues: what ""wrong signal"" did it send in this case? The ""wrong signal"" that this sends is exactly what you describe: A valid report of something not working is thought ""not relevant to fix"". IMHO this is a disastrous attitude and will just discourage people from reporting issue - it just risks being deemed ""not relevant"" by the core developers, so why put in the effort?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:2009,energy efficiency,core,core,2009,"ce of the issue, which is why I'm not a big fan of it and was trigger happy to close it. Acknowledged that re-interpretation of issues is difficult; in this case I would argue that I better understood the underlying reason and changed the issue to correct the description. Just closing and dismissing this analysis is what I'm against. > When reinterpreting issues like this, you have to ask yourself: would I have also opened an original issue for this too? Especially because of:. Yes, I would have opened the same issue if I had understood the underlying reason before; that's why I updated it. > ROOT 7 has now the same CMAKE_CXX_STANDARD requirements, so please give me a new example of why someone would do this. I already gave you an example in https://github.com/root-project/root/issues/8642#issuecomment-2039323384:. > FWIW the same problem can be seen when going from `CMAKE_CXX_STANDARD=17` to `CMAKE_CXX_STANDARD=20` with GCC 13.2.0:. C++17 is now the default, but if somebody tries to switch an existing build directory to C++20, they will be faced with obscure error messages. > Anyone who reads this issue to consider fixing it will have this context in mind, which brings them to the conclusion that it's not relevant to fix. So why keep it open? Anyone looking at this issue should have a look at the entire discussion, including https://github.com/root-project/root/issues/8642#issuecomment-2039323384 and this comment. This should explain why I think it's still relevant. > I'm very happy to discuss about this by the way and I would like to understand you better and improve my communication when closing issues: what ""wrong signal"" did it send in this case? The ""wrong signal"" that this sends is exactly what you describe: A valid report of something not working is thought ""not relevant to fix"". IMHO this is a disastrous attitude and will just discourage people from reporting issue - it just risks being deemed ""not relevant"" by the core developers, so why put in the effort?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:1133,integrability,messag,messages,1133,"ce of the issue, which is why I'm not a big fan of it and was trigger happy to close it. Acknowledged that re-interpretation of issues is difficult; in this case I would argue that I better understood the underlying reason and changed the issue to correct the description. Just closing and dismissing this analysis is what I'm against. > When reinterpreting issues like this, you have to ask yourself: would I have also opened an original issue for this too? Especially because of:. Yes, I would have opened the same issue if I had understood the underlying reason before; that's why I updated it. > ROOT 7 has now the same CMAKE_CXX_STANDARD requirements, so please give me a new example of why someone would do this. I already gave you an example in https://github.com/root-project/root/issues/8642#issuecomment-2039323384:. > FWIW the same problem can be seen when going from `CMAKE_CXX_STANDARD=17` to `CMAKE_CXX_STANDARD=20` with GCC 13.2.0:. C++17 is now the default, but if somebody tries to switch an existing build directory to C++20, they will be faced with obscure error messages. > Anyone who reads this issue to consider fixing it will have this context in mind, which brings them to the conclusion that it's not relevant to fix. So why keep it open? Anyone looking at this issue should have a look at the entire discussion, including https://github.com/root-project/root/issues/8642#issuecomment-2039323384 and this comment. This should explain why I think it's still relevant. > I'm very happy to discuss about this by the way and I would like to understand you better and improve my communication when closing issues: what ""wrong signal"" did it send in this case? The ""wrong signal"" that this sends is exactly what you describe: A valid report of something not working is thought ""not relevant to fix"". IMHO this is a disastrous attitude and will just discourage people from reporting issue - it just risks being deemed ""not relevant"" by the core developers, so why put in the effort?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:1133,interoperability,messag,messages,1133,"ce of the issue, which is why I'm not a big fan of it and was trigger happy to close it. Acknowledged that re-interpretation of issues is difficult; in this case I would argue that I better understood the underlying reason and changed the issue to correct the description. Just closing and dismissing this analysis is what I'm against. > When reinterpreting issues like this, you have to ask yourself: would I have also opened an original issue for this too? Especially because of:. Yes, I would have opened the same issue if I had understood the underlying reason before; that's why I updated it. > ROOT 7 has now the same CMAKE_CXX_STANDARD requirements, so please give me a new example of why someone would do this. I already gave you an example in https://github.com/root-project/root/issues/8642#issuecomment-2039323384:. > FWIW the same problem can be seen when going from `CMAKE_CXX_STANDARD=17` to `CMAKE_CXX_STANDARD=20` with GCC 13.2.0:. C++17 is now the default, but if somebody tries to switch an existing build directory to C++20, they will be faced with obscure error messages. > Anyone who reads this issue to consider fixing it will have this context in mind, which brings them to the conclusion that it's not relevant to fix. So why keep it open? Anyone looking at this issue should have a look at the entire discussion, including https://github.com/root-project/root/issues/8642#issuecomment-2039323384 and this comment. This should explain why I think it's still relevant. > I'm very happy to discuss about this by the way and I would like to understand you better and improve my communication when closing issues: what ""wrong signal"" did it send in this case? The ""wrong signal"" that this sends is exactly what you describe: A valid report of something not working is thought ""not relevant to fix"". IMHO this is a disastrous attitude and will just discourage people from reporting issue - it just risks being deemed ""not relevant"" by the core developers, so why put in the effort?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:1127,performance,error,error,1127,"ce of the issue, which is why I'm not a big fan of it and was trigger happy to close it. Acknowledged that re-interpretation of issues is difficult; in this case I would argue that I better understood the underlying reason and changed the issue to correct the description. Just closing and dismissing this analysis is what I'm against. > When reinterpreting issues like this, you have to ask yourself: would I have also opened an original issue for this too? Especially because of:. Yes, I would have opened the same issue if I had understood the underlying reason before; that's why I updated it. > ROOT 7 has now the same CMAKE_CXX_STANDARD requirements, so please give me a new example of why someone would do this. I already gave you an example in https://github.com/root-project/root/issues/8642#issuecomment-2039323384:. > FWIW the same problem can be seen when going from `CMAKE_CXX_STANDARD=17` to `CMAKE_CXX_STANDARD=20` with GCC 13.2.0:. C++17 is now the default, but if somebody tries to switch an existing build directory to C++20, they will be faced with obscure error messages. > Anyone who reads this issue to consider fixing it will have this context in mind, which brings them to the conclusion that it's not relevant to fix. So why keep it open? Anyone looking at this issue should have a look at the entire discussion, including https://github.com/root-project/root/issues/8642#issuecomment-2039323384 and this comment. This should explain why I think it's still relevant. > I'm very happy to discuss about this by the way and I would like to understand you better and improve my communication when closing issues: what ""wrong signal"" did it send in this case? The ""wrong signal"" that this sends is exactly what you describe: A valid report of something not working is thought ""not relevant to fix"". IMHO this is a disastrous attitude and will just discourage people from reporting issue - it just risks being deemed ""not relevant"" by the core developers, so why put in the effort?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:637,safety,updat,updated,637,"> This kind of re-interpretation biases the relevance of the issue, which is why I'm not a big fan of it and was trigger happy to close it. Acknowledged that re-interpretation of issues is difficult; in this case I would argue that I better understood the underlying reason and changed the issue to correct the description. Just closing and dismissing this analysis is what I'm against. > When reinterpreting issues like this, you have to ask yourself: would I have also opened an original issue for this too? Especially because of:. Yes, I would have opened the same issue if I had understood the underlying reason before; that's why I updated it. > ROOT 7 has now the same CMAKE_CXX_STANDARD requirements, so please give me a new example of why someone would do this. I already gave you an example in https://github.com/root-project/root/issues/8642#issuecomment-2039323384:. > FWIW the same problem can be seen when going from `CMAKE_CXX_STANDARD=17` to `CMAKE_CXX_STANDARD=20` with GCC 13.2.0:. C++17 is now the default, but if somebody tries to switch an existing build directory to C++20, they will be faced with obscure error messages. > Anyone who reads this issue to consider fixing it will have this context in mind, which brings them to the conclusion that it's not relevant to fix. So why keep it open? Anyone looking at this issue should have a look at the entire discussion, including https://github.com/root-project/root/issues/8642#issuecomment-2039323384 and this comment. This should explain why I think it's still relevant. > I'm very happy to discuss about this by the way and I would like to understand you better and improve my communication when closing issues: what ""wrong signal"" did it send in this case? The ""wrong signal"" that this sends is exactly what you describe: A valid report of something not working is thought ""not relevant to fix"". IMHO this is a disastrous attitude and will just discourage people from reporting issue - it just risks being deemed ""not relevant",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:1127,safety,error,error,1127,"ce of the issue, which is why I'm not a big fan of it and was trigger happy to close it. Acknowledged that re-interpretation of issues is difficult; in this case I would argue that I better understood the underlying reason and changed the issue to correct the description. Just closing and dismissing this analysis is what I'm against. > When reinterpreting issues like this, you have to ask yourself: would I have also opened an original issue for this too? Especially because of:. Yes, I would have opened the same issue if I had understood the underlying reason before; that's why I updated it. > ROOT 7 has now the same CMAKE_CXX_STANDARD requirements, so please give me a new example of why someone would do this. I already gave you an example in https://github.com/root-project/root/issues/8642#issuecomment-2039323384:. > FWIW the same problem can be seen when going from `CMAKE_CXX_STANDARD=17` to `CMAKE_CXX_STANDARD=20` with GCC 13.2.0:. C++17 is now the default, but if somebody tries to switch an existing build directory to C++20, they will be faced with obscure error messages. > Anyone who reads this issue to consider fixing it will have this context in mind, which brings them to the conclusion that it's not relevant to fix. So why keep it open? Anyone looking at this issue should have a look at the entire discussion, including https://github.com/root-project/root/issues/8642#issuecomment-2039323384 and this comment. This should explain why I think it's still relevant. > I'm very happy to discuss about this by the way and I would like to understand you better and improve my communication when closing issues: what ""wrong signal"" did it send in this case? The ""wrong signal"" that this sends is exactly what you describe: A valid report of something not working is thought ""not relevant to fix"". IMHO this is a disastrous attitude and will just discourage people from reporting issue - it just risks being deemed ""not relevant"" by the core developers, so why put in the effort?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:1798,safety,valid,valid,1798,"ce of the issue, which is why I'm not a big fan of it and was trigger happy to close it. Acknowledged that re-interpretation of issues is difficult; in this case I would argue that I better understood the underlying reason and changed the issue to correct the description. Just closing and dismissing this analysis is what I'm against. > When reinterpreting issues like this, you have to ask yourself: would I have also opened an original issue for this too? Especially because of:. Yes, I would have opened the same issue if I had understood the underlying reason before; that's why I updated it. > ROOT 7 has now the same CMAKE_CXX_STANDARD requirements, so please give me a new example of why someone would do this. I already gave you an example in https://github.com/root-project/root/issues/8642#issuecomment-2039323384:. > FWIW the same problem can be seen when going from `CMAKE_CXX_STANDARD=17` to `CMAKE_CXX_STANDARD=20` with GCC 13.2.0:. C++17 is now the default, but if somebody tries to switch an existing build directory to C++20, they will be faced with obscure error messages. > Anyone who reads this issue to consider fixing it will have this context in mind, which brings them to the conclusion that it's not relevant to fix. So why keep it open? Anyone looking at this issue should have a look at the entire discussion, including https://github.com/root-project/root/issues/8642#issuecomment-2039323384 and this comment. This should explain why I think it's still relevant. > I'm very happy to discuss about this by the way and I would like to understand you better and improve my communication when closing issues: what ""wrong signal"" did it send in this case? The ""wrong signal"" that this sends is exactly what you describe: A valid report of something not working is thought ""not relevant to fix"". IMHO this is a disastrous attitude and will just discourage people from reporting issue - it just risks being deemed ""not relevant"" by the core developers, so why put in the effort?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:1968,safety,risk,risks,1968,"ce of the issue, which is why I'm not a big fan of it and was trigger happy to close it. Acknowledged that re-interpretation of issues is difficult; in this case I would argue that I better understood the underlying reason and changed the issue to correct the description. Just closing and dismissing this analysis is what I'm against. > When reinterpreting issues like this, you have to ask yourself: would I have also opened an original issue for this too? Especially because of:. Yes, I would have opened the same issue if I had understood the underlying reason before; that's why I updated it. > ROOT 7 has now the same CMAKE_CXX_STANDARD requirements, so please give me a new example of why someone would do this. I already gave you an example in https://github.com/root-project/root/issues/8642#issuecomment-2039323384:. > FWIW the same problem can be seen when going from `CMAKE_CXX_STANDARD=17` to `CMAKE_CXX_STANDARD=20` with GCC 13.2.0:. C++17 is now the default, but if somebody tries to switch an existing build directory to C++20, they will be faced with obscure error messages. > Anyone who reads this issue to consider fixing it will have this context in mind, which brings them to the conclusion that it's not relevant to fix. So why keep it open? Anyone looking at this issue should have a look at the entire discussion, including https://github.com/root-project/root/issues/8642#issuecomment-2039323384 and this comment. This should explain why I think it's still relevant. > I'm very happy to discuss about this by the way and I would like to understand you better and improve my communication when closing issues: what ""wrong signal"" did it send in this case? The ""wrong signal"" that this sends is exactly what you describe: A valid report of something not working is thought ""not relevant to fix"". IMHO this is a disastrous attitude and will just discourage people from reporting issue - it just risks being deemed ""not relevant"" by the core developers, so why put in the effort?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:637,security,updat,updated,637,"> This kind of re-interpretation biases the relevance of the issue, which is why I'm not a big fan of it and was trigger happy to close it. Acknowledged that re-interpretation of issues is difficult; in this case I would argue that I better understood the underlying reason and changed the issue to correct the description. Just closing and dismissing this analysis is what I'm against. > When reinterpreting issues like this, you have to ask yourself: would I have also opened an original issue for this too? Especially because of:. Yes, I would have opened the same issue if I had understood the underlying reason before; that's why I updated it. > ROOT 7 has now the same CMAKE_CXX_STANDARD requirements, so please give me a new example of why someone would do this. I already gave you an example in https://github.com/root-project/root/issues/8642#issuecomment-2039323384:. > FWIW the same problem can be seen when going from `CMAKE_CXX_STANDARD=17` to `CMAKE_CXX_STANDARD=20` with GCC 13.2.0:. C++17 is now the default, but if somebody tries to switch an existing build directory to C++20, they will be faced with obscure error messages. > Anyone who reads this issue to consider fixing it will have this context in mind, which brings them to the conclusion that it's not relevant to fix. So why keep it open? Anyone looking at this issue should have a look at the entire discussion, including https://github.com/root-project/root/issues/8642#issuecomment-2039323384 and this comment. This should explain why I think it's still relevant. > I'm very happy to discuss about this by the way and I would like to understand you better and improve my communication when closing issues: what ""wrong signal"" did it send in this case? The ""wrong signal"" that this sends is exactly what you describe: A valid report of something not working is thought ""not relevant to fix"". IMHO this is a disastrous attitude and will just discourage people from reporting issue - it just risks being deemed ""not relevant",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:1697,security,sign,signal,1697,"ce of the issue, which is why I'm not a big fan of it and was trigger happy to close it. Acknowledged that re-interpretation of issues is difficult; in this case I would argue that I better understood the underlying reason and changed the issue to correct the description. Just closing and dismissing this analysis is what I'm against. > When reinterpreting issues like this, you have to ask yourself: would I have also opened an original issue for this too? Especially because of:. Yes, I would have opened the same issue if I had understood the underlying reason before; that's why I updated it. > ROOT 7 has now the same CMAKE_CXX_STANDARD requirements, so please give me a new example of why someone would do this. I already gave you an example in https://github.com/root-project/root/issues/8642#issuecomment-2039323384:. > FWIW the same problem can be seen when going from `CMAKE_CXX_STANDARD=17` to `CMAKE_CXX_STANDARD=20` with GCC 13.2.0:. C++17 is now the default, but if somebody tries to switch an existing build directory to C++20, they will be faced with obscure error messages. > Anyone who reads this issue to consider fixing it will have this context in mind, which brings them to the conclusion that it's not relevant to fix. So why keep it open? Anyone looking at this issue should have a look at the entire discussion, including https://github.com/root-project/root/issues/8642#issuecomment-2039323384 and this comment. This should explain why I think it's still relevant. > I'm very happy to discuss about this by the way and I would like to understand you better and improve my communication when closing issues: what ""wrong signal"" did it send in this case? The ""wrong signal"" that this sends is exactly what you describe: A valid report of something not working is thought ""not relevant to fix"". IMHO this is a disastrous attitude and will just discourage people from reporting issue - it just risks being deemed ""not relevant"" by the core developers, so why put in the effort?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:1742,security,sign,signal,1742,"ce of the issue, which is why I'm not a big fan of it and was trigger happy to close it. Acknowledged that re-interpretation of issues is difficult; in this case I would argue that I better understood the underlying reason and changed the issue to correct the description. Just closing and dismissing this analysis is what I'm against. > When reinterpreting issues like this, you have to ask yourself: would I have also opened an original issue for this too? Especially because of:. Yes, I would have opened the same issue if I had understood the underlying reason before; that's why I updated it. > ROOT 7 has now the same CMAKE_CXX_STANDARD requirements, so please give me a new example of why someone would do this. I already gave you an example in https://github.com/root-project/root/issues/8642#issuecomment-2039323384:. > FWIW the same problem can be seen when going from `CMAKE_CXX_STANDARD=17` to `CMAKE_CXX_STANDARD=20` with GCC 13.2.0:. C++17 is now the default, but if somebody tries to switch an existing build directory to C++20, they will be faced with obscure error messages. > Anyone who reads this issue to consider fixing it will have this context in mind, which brings them to the conclusion that it's not relevant to fix. So why keep it open? Anyone looking at this issue should have a look at the entire discussion, including https://github.com/root-project/root/issues/8642#issuecomment-2039323384 and this comment. This should explain why I think it's still relevant. > I'm very happy to discuss about this by the way and I would like to understand you better and improve my communication when closing issues: what ""wrong signal"" did it send in this case? The ""wrong signal"" that this sends is exactly what you describe: A valid report of something not working is thought ""not relevant to fix"". IMHO this is a disastrous attitude and will just discourage people from reporting issue - it just risks being deemed ""not relevant"" by the core developers, so why put in the effort?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:1968,security,risk,risks,1968,"ce of the issue, which is why I'm not a big fan of it and was trigger happy to close it. Acknowledged that re-interpretation of issues is difficult; in this case I would argue that I better understood the underlying reason and changed the issue to correct the description. Just closing and dismissing this analysis is what I'm against. > When reinterpreting issues like this, you have to ask yourself: would I have also opened an original issue for this too? Especially because of:. Yes, I would have opened the same issue if I had understood the underlying reason before; that's why I updated it. > ROOT 7 has now the same CMAKE_CXX_STANDARD requirements, so please give me a new example of why someone would do this. I already gave you an example in https://github.com/root-project/root/issues/8642#issuecomment-2039323384:. > FWIW the same problem can be seen when going from `CMAKE_CXX_STANDARD=17` to `CMAKE_CXX_STANDARD=20` with GCC 13.2.0:. C++17 is now the default, but if somebody tries to switch an existing build directory to C++20, they will be faced with obscure error messages. > Anyone who reads this issue to consider fixing it will have this context in mind, which brings them to the conclusion that it's not relevant to fix. So why keep it open? Anyone looking at this issue should have a look at the entire discussion, including https://github.com/root-project/root/issues/8642#issuecomment-2039323384 and this comment. This should explain why I think it's still relevant. > I'm very happy to discuss about this by the way and I would like to understand you better and improve my communication when closing issues: what ""wrong signal"" did it send in this case? The ""wrong signal"" that this sends is exactly what you describe: A valid report of something not working is thought ""not relevant to fix"". IMHO this is a disastrous attitude and will just discourage people from reporting issue - it just risks being deemed ""not relevant"" by the core developers, so why put in the effort?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:1210,testability,context,context,1210,"ce of the issue, which is why I'm not a big fan of it and was trigger happy to close it. Acknowledged that re-interpretation of issues is difficult; in this case I would argue that I better understood the underlying reason and changed the issue to correct the description. Just closing and dismissing this analysis is what I'm against. > When reinterpreting issues like this, you have to ask yourself: would I have also opened an original issue for this too? Especially because of:. Yes, I would have opened the same issue if I had understood the underlying reason before; that's why I updated it. > ROOT 7 has now the same CMAKE_CXX_STANDARD requirements, so please give me a new example of why someone would do this. I already gave you an example in https://github.com/root-project/root/issues/8642#issuecomment-2039323384:. > FWIW the same problem can be seen when going from `CMAKE_CXX_STANDARD=17` to `CMAKE_CXX_STANDARD=20` with GCC 13.2.0:. C++17 is now the default, but if somebody tries to switch an existing build directory to C++20, they will be faced with obscure error messages. > Anyone who reads this issue to consider fixing it will have this context in mind, which brings them to the conclusion that it's not relevant to fix. So why keep it open? Anyone looking at this issue should have a look at the entire discussion, including https://github.com/root-project/root/issues/8642#issuecomment-2039323384 and this comment. This should explain why I think it's still relevant. > I'm very happy to discuss about this by the way and I would like to understand you better and improve my communication when closing issues: what ""wrong signal"" did it send in this case? The ""wrong signal"" that this sends is exactly what you describe: A valid report of something not working is thought ""not relevant to fix"". IMHO this is a disastrous attitude and will just discourage people from reporting issue - it just risks being deemed ""not relevant"" by the core developers, so why put in the effort?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:1613,testability,understand,understand,1613,"ce of the issue, which is why I'm not a big fan of it and was trigger happy to close it. Acknowledged that re-interpretation of issues is difficult; in this case I would argue that I better understood the underlying reason and changed the issue to correct the description. Just closing and dismissing this analysis is what I'm against. > When reinterpreting issues like this, you have to ask yourself: would I have also opened an original issue for this too? Especially because of:. Yes, I would have opened the same issue if I had understood the underlying reason before; that's why I updated it. > ROOT 7 has now the same CMAKE_CXX_STANDARD requirements, so please give me a new example of why someone would do this. I already gave you an example in https://github.com/root-project/root/issues/8642#issuecomment-2039323384:. > FWIW the same problem can be seen when going from `CMAKE_CXX_STANDARD=17` to `CMAKE_CXX_STANDARD=20` with GCC 13.2.0:. C++17 is now the default, but if somebody tries to switch an existing build directory to C++20, they will be faced with obscure error messages. > Anyone who reads this issue to consider fixing it will have this context in mind, which brings them to the conclusion that it's not relevant to fix. So why keep it open? Anyone looking at this issue should have a look at the entire discussion, including https://github.com/root-project/root/issues/8642#issuecomment-2039323384 and this comment. This should explain why I think it's still relevant. > I'm very happy to discuss about this by the way and I would like to understand you better and improve my communication when closing issues: what ""wrong signal"" did it send in this case? The ""wrong signal"" that this sends is exactly what you describe: A valid report of something not working is thought ""not relevant to fix"". IMHO this is a disastrous attitude and will just discourage people from reporting issue - it just risks being deemed ""not relevant"" by the core developers, so why put in the effort?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:130,usability,close,close,130,"> This kind of re-interpretation biases the relevance of the issue, which is why I'm not a big fan of it and was trigger happy to close it. Acknowledged that re-interpretation of issues is difficult; in this case I would argue that I better understood the underlying reason and changed the issue to correct the description. Just closing and dismissing this analysis is what I'm against. > When reinterpreting issues like this, you have to ask yourself: would I have also opened an original issue for this too? Especially because of:. Yes, I would have opened the same issue if I had understood the underlying reason before; that's why I updated it. > ROOT 7 has now the same CMAKE_CXX_STANDARD requirements, so please give me a new example of why someone would do this. I already gave you an example in https://github.com/root-project/root/issues/8642#issuecomment-2039323384:. > FWIW the same problem can be seen when going from `CMAKE_CXX_STANDARD=17` to `CMAKE_CXX_STANDARD=20` with GCC 13.2.0:. C++17 is now the default, but if somebody tries to switch an existing build directory to C++20, they will be faced with obscure error messages. > Anyone who reads this issue to consider fixing it will have this context in mind, which brings them to the conclusion that it's not relevant to fix. So why keep it open? Anyone looking at this issue should have a look at the entire discussion, including https://github.com/root-project/root/issues/8642#issuecomment-2039323384 and this comment. This should explain why I think it's still relevant. > I'm very happy to discuss about this by the way and I would like to understand you better and improve my communication when closing issues: what ""wrong signal"" did it send in this case? The ""wrong signal"" that this sends is exactly what you describe: A valid report of something not working is thought ""not relevant to fix"". IMHO this is a disastrous attitude and will just discourage people from reporting issue - it just risks being deemed ""not relevant",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:1127,usability,error,error,1127,"ce of the issue, which is why I'm not a big fan of it and was trigger happy to close it. Acknowledged that re-interpretation of issues is difficult; in this case I would argue that I better understood the underlying reason and changed the issue to correct the description. Just closing and dismissing this analysis is what I'm against. > When reinterpreting issues like this, you have to ask yourself: would I have also opened an original issue for this too? Especially because of:. Yes, I would have opened the same issue if I had understood the underlying reason before; that's why I updated it. > ROOT 7 has now the same CMAKE_CXX_STANDARD requirements, so please give me a new example of why someone would do this. I already gave you an example in https://github.com/root-project/root/issues/8642#issuecomment-2039323384:. > FWIW the same problem can be seen when going from `CMAKE_CXX_STANDARD=17` to `CMAKE_CXX_STANDARD=20` with GCC 13.2.0:. C++17 is now the default, but if somebody tries to switch an existing build directory to C++20, they will be faced with obscure error messages. > Anyone who reads this issue to consider fixing it will have this context in mind, which brings them to the conclusion that it's not relevant to fix. So why keep it open? Anyone looking at this issue should have a look at the entire discussion, including https://github.com/root-project/root/issues/8642#issuecomment-2039323384 and this comment. This should explain why I think it's still relevant. > I'm very happy to discuss about this by the way and I would like to understand you better and improve my communication when closing issues: what ""wrong signal"" did it send in this case? The ""wrong signal"" that this sends is exactly what you describe: A valid report of something not working is thought ""not relevant to fix"". IMHO this is a disastrous attitude and will just discourage people from reporting issue - it just risks being deemed ""not relevant"" by the core developers, so why put in the effort?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:388,deployability,updat,updated,388,"> I already gave you an example in https://github.com/root-project/root/issues/8642#issuecomment-2039323384:. I meant not an example of a crash, but an example of a user story. But you provided that now with the switch from the default C++17 to C++20 because they want new features. > Yes, I would have opened the same issue if I had understood the underlying reason before; that's why I updated it. Ok, then I apologize for closing it and let's leave it open. > The ""wrong signal"" that this sends is exactly what you describe: A valid report of something not working is thought ""not relevant to fix"". IMHO this is a disastrous attitude and will just discourage people from reporting issue - it just risks being deemed ""not relevant"" by the core developers, so why put in the effort? Point taken!",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:741,energy efficiency,core,core,741,"> I already gave you an example in https://github.com/root-project/root/issues/8642#issuecomment-2039323384:. I meant not an example of a crash, but an example of a user story. But you provided that now with the switch from the default C++17 to C++20 because they want new features. > Yes, I would have opened the same issue if I had understood the underlying reason before; that's why I updated it. Ok, then I apologize for closing it and let's leave it open. > The ""wrong signal"" that this sends is exactly what you describe: A valid report of something not working is thought ""not relevant to fix"". IMHO this is a disastrous attitude and will just discourage people from reporting issue - it just risks being deemed ""not relevant"" by the core developers, so why put in the effort? Point taken!",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:388,safety,updat,updated,388,"> I already gave you an example in https://github.com/root-project/root/issues/8642#issuecomment-2039323384:. I meant not an example of a crash, but an example of a user story. But you provided that now with the switch from the default C++17 to C++20 because they want new features. > Yes, I would have opened the same issue if I had understood the underlying reason before; that's why I updated it. Ok, then I apologize for closing it and let's leave it open. > The ""wrong signal"" that this sends is exactly what you describe: A valid report of something not working is thought ""not relevant to fix"". IMHO this is a disastrous attitude and will just discourage people from reporting issue - it just risks being deemed ""not relevant"" by the core developers, so why put in the effort? Point taken!",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:530,safety,valid,valid,530,"> I already gave you an example in https://github.com/root-project/root/issues/8642#issuecomment-2039323384:. I meant not an example of a crash, but an example of a user story. But you provided that now with the switch from the default C++17 to C++20 because they want new features. > Yes, I would have opened the same issue if I had understood the underlying reason before; that's why I updated it. Ok, then I apologize for closing it and let's leave it open. > The ""wrong signal"" that this sends is exactly what you describe: A valid report of something not working is thought ""not relevant to fix"". IMHO this is a disastrous attitude and will just discourage people from reporting issue - it just risks being deemed ""not relevant"" by the core developers, so why put in the effort? Point taken!",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:700,safety,risk,risks,700,"> I already gave you an example in https://github.com/root-project/root/issues/8642#issuecomment-2039323384:. I meant not an example of a crash, but an example of a user story. But you provided that now with the switch from the default C++17 to C++20 because they want new features. > Yes, I would have opened the same issue if I had understood the underlying reason before; that's why I updated it. Ok, then I apologize for closing it and let's leave it open. > The ""wrong signal"" that this sends is exactly what you describe: A valid report of something not working is thought ""not relevant to fix"". IMHO this is a disastrous attitude and will just discourage people from reporting issue - it just risks being deemed ""not relevant"" by the core developers, so why put in the effort? Point taken!",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:388,security,updat,updated,388,"> I already gave you an example in https://github.com/root-project/root/issues/8642#issuecomment-2039323384:. I meant not an example of a crash, but an example of a user story. But you provided that now with the switch from the default C++17 to C++20 because they want new features. > Yes, I would have opened the same issue if I had understood the underlying reason before; that's why I updated it. Ok, then I apologize for closing it and let's leave it open. > The ""wrong signal"" that this sends is exactly what you describe: A valid report of something not working is thought ""not relevant to fix"". IMHO this is a disastrous attitude and will just discourage people from reporting issue - it just risks being deemed ""not relevant"" by the core developers, so why put in the effort? Point taken!",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:474,security,sign,signal,474,"> I already gave you an example in https://github.com/root-project/root/issues/8642#issuecomment-2039323384:. I meant not an example of a crash, but an example of a user story. But you provided that now with the switch from the default C++17 to C++20 because they want new features. > Yes, I would have opened the same issue if I had understood the underlying reason before; that's why I updated it. Ok, then I apologize for closing it and let's leave it open. > The ""wrong signal"" that this sends is exactly what you describe: A valid report of something not working is thought ""not relevant to fix"". IMHO this is a disastrous attitude and will just discourage people from reporting issue - it just risks being deemed ""not relevant"" by the core developers, so why put in the effort? Point taken!",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:700,security,risk,risks,700,"> I already gave you an example in https://github.com/root-project/root/issues/8642#issuecomment-2039323384:. I meant not an example of a crash, but an example of a user story. But you provided that now with the switch from the default C++17 to C++20 because they want new features. > Yes, I would have opened the same issue if I had understood the underlying reason before; that's why I updated it. Ok, then I apologize for closing it and let's leave it open. > The ""wrong signal"" that this sends is exactly what you describe: A valid report of something not working is thought ""not relevant to fix"". IMHO this is a disastrous attitude and will just discourage people from reporting issue - it just risks being deemed ""not relevant"" by the core developers, so why put in the effort? Point taken!",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:165,usability,user,user,165,"> I already gave you an example in https://github.com/root-project/root/issues/8642#issuecomment-2039323384:. I meant not an example of a crash, but an example of a user story. But you provided that now with the switch from the default C++17 to C++20 because they want new features. > Yes, I would have opened the same issue if I had understood the underlying reason before; that's why I updated it. Ok, then I apologize for closing it and let's leave it open. > The ""wrong signal"" that this sends is exactly what you describe: A valid report of something not working is thought ""not relevant to fix"". IMHO this is a disastrous attitude and will just discourage people from reporting issue - it just risks being deemed ""not relevant"" by the core developers, so why put in the effort? Point taken!",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:31,deployability,version,versions,31,The issue looks related to old versions of the standard and compiler. Can we close the issue (won't fix or not relevant)?,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:31,integrability,version,versions,31,The issue looks related to old versions of the standard and compiler. Can we close the issue (won't fix or not relevant)?,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:47,interoperability,standard,standard,47,The issue looks related to old versions of the standard and compiler. Can we close the issue (won't fix or not relevant)?,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:31,modifiability,version,versions,31,The issue looks related to old versions of the standard and compiler. Can we close the issue (won't fix or not relevant)?,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8642:77,usability,close,close,77,The issue looks related to old versions of the standard and compiler. Can we close the issue (won't fix or not relevant)?,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8642
https://github.com/root-project/root/issues/8644:29,usability,help,help,29,"Side note: we really need to help Delphes move away from `TClonesArray`... Let's see whether we can cook up a PR over summer, something that works well with today's software world :-)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:186,deployability,version,versions,186,"As a developer and maintainer of Delphes, I am quite happy with TClonesArray. As far as I know, storing TClonesArray in TTree was the fastest and least crash-prone method for all ROOT 5 versions and early ROOT 6 versions. If there is a newer and better method that meets all of the Delphes requirements, I would be interested in seeing and evaluating a few examples. Before doing any pull requests, let's start with something more basic. I think a very basic example showing how to write/read a collection of generated particles to/from a file would be a good starting point. > something that works well with today's software world :-). Newer doesn't always mean better :smiley: Especially in the HEP community where some researchers are still happy with CERNLIB and PAW and do not want to migrate to ROOT :smiley:. At the moment, Delphes maintains compatibility with ROOT 5 and I still receive questions from researches who use Delphes with ROOT 5. So, Delphes is still regularly tested with Ubuntu 14.04, GCC 4.8 and ROOT 5. If the new IO method only works with recent versions of ROOT 6 and requires the change of file format and programming interface, it will be a major change for Delphes.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:212,deployability,version,versions,212,"As a developer and maintainer of Delphes, I am quite happy with TClonesArray. As far as I know, storing TClonesArray in TTree was the fastest and least crash-prone method for all ROOT 5 versions and early ROOT 6 versions. If there is a newer and better method that meets all of the Delphes requirements, I would be interested in seeing and evaluating a few examples. Before doing any pull requests, let's start with something more basic. I think a very basic example showing how to write/read a collection of generated particles to/from a file would be a good starting point. > something that works well with today's software world :-). Newer doesn't always mean better :smiley: Especially in the HEP community where some researchers are still happy with CERNLIB and PAW and do not want to migrate to ROOT :smiley:. At the moment, Delphes maintains compatibility with ROOT 5 and I still receive questions from researches who use Delphes with ROOT 5. So, Delphes is still regularly tested with Ubuntu 14.04, GCC 4.8 and ROOT 5. If the new IO method only works with recent versions of ROOT 6 and requires the change of file format and programming interface, it will be a major change for Delphes.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:1071,deployability,version,versions,1071,"As a developer and maintainer of Delphes, I am quite happy with TClonesArray. As far as I know, storing TClonesArray in TTree was the fastest and least crash-prone method for all ROOT 5 versions and early ROOT 6 versions. If there is a newer and better method that meets all of the Delphes requirements, I would be interested in seeing and evaluating a few examples. Before doing any pull requests, let's start with something more basic. I think a very basic example showing how to write/read a collection of generated particles to/from a file would be a good starting point. > something that works well with today's software world :-). Newer doesn't always mean better :smiley: Especially in the HEP community where some researchers are still happy with CERNLIB and PAW and do not want to migrate to ROOT :smiley:. At the moment, Delphes maintains compatibility with ROOT 5 and I still receive questions from researches who use Delphes with ROOT 5. So, Delphes is still regularly tested with Ubuntu 14.04, GCC 4.8 and ROOT 5. If the new IO method only works with recent versions of ROOT 6 and requires the change of file format and programming interface, it will be a major change for Delphes.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:186,integrability,version,versions,186,"As a developer and maintainer of Delphes, I am quite happy with TClonesArray. As far as I know, storing TClonesArray in TTree was the fastest and least crash-prone method for all ROOT 5 versions and early ROOT 6 versions. If there is a newer and better method that meets all of the Delphes requirements, I would be interested in seeing and evaluating a few examples. Before doing any pull requests, let's start with something more basic. I think a very basic example showing how to write/read a collection of generated particles to/from a file would be a good starting point. > something that works well with today's software world :-). Newer doesn't always mean better :smiley: Especially in the HEP community where some researchers are still happy with CERNLIB and PAW and do not want to migrate to ROOT :smiley:. At the moment, Delphes maintains compatibility with ROOT 5 and I still receive questions from researches who use Delphes with ROOT 5. So, Delphes is still regularly tested with Ubuntu 14.04, GCC 4.8 and ROOT 5. If the new IO method only works with recent versions of ROOT 6 and requires the change of file format and programming interface, it will be a major change for Delphes.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:212,integrability,version,versions,212,"As a developer and maintainer of Delphes, I am quite happy with TClonesArray. As far as I know, storing TClonesArray in TTree was the fastest and least crash-prone method for all ROOT 5 versions and early ROOT 6 versions. If there is a newer and better method that meets all of the Delphes requirements, I would be interested in seeing and evaluating a few examples. Before doing any pull requests, let's start with something more basic. I think a very basic example showing how to write/read a collection of generated particles to/from a file would be a good starting point. > something that works well with today's software world :-). Newer doesn't always mean better :smiley: Especially in the HEP community where some researchers are still happy with CERNLIB and PAW and do not want to migrate to ROOT :smiley:. At the moment, Delphes maintains compatibility with ROOT 5 and I still receive questions from researches who use Delphes with ROOT 5. So, Delphes is still regularly tested with Ubuntu 14.04, GCC 4.8 and ROOT 5. If the new IO method only works with recent versions of ROOT 6 and requires the change of file format and programming interface, it will be a major change for Delphes.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:1071,integrability,version,versions,1071,"As a developer and maintainer of Delphes, I am quite happy with TClonesArray. As far as I know, storing TClonesArray in TTree was the fastest and least crash-prone method for all ROOT 5 versions and early ROOT 6 versions. If there is a newer and better method that meets all of the Delphes requirements, I would be interested in seeing and evaluating a few examples. Before doing any pull requests, let's start with something more basic. I think a very basic example showing how to write/read a collection of generated particles to/from a file would be a good starting point. > something that works well with today's software world :-). Newer doesn't always mean better :smiley: Especially in the HEP community where some researchers are still happy with CERNLIB and PAW and do not want to migrate to ROOT :smiley:. At the moment, Delphes maintains compatibility with ROOT 5 and I still receive questions from researches who use Delphes with ROOT 5. So, Delphes is still regularly tested with Ubuntu 14.04, GCC 4.8 and ROOT 5. If the new IO method only works with recent versions of ROOT 6 and requires the change of file format and programming interface, it will be a major change for Delphes.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:1145,integrability,interfac,interface,1145,"As a developer and maintainer of Delphes, I am quite happy with TClonesArray. As far as I know, storing TClonesArray in TTree was the fastest and least crash-prone method for all ROOT 5 versions and early ROOT 6 versions. If there is a newer and better method that meets all of the Delphes requirements, I would be interested in seeing and evaluating a few examples. Before doing any pull requests, let's start with something more basic. I think a very basic example showing how to write/read a collection of generated particles to/from a file would be a good starting point. > something that works well with today's software world :-). Newer doesn't always mean better :smiley: Especially in the HEP community where some researchers are still happy with CERNLIB and PAW and do not want to migrate to ROOT :smiley:. At the moment, Delphes maintains compatibility with ROOT 5 and I still receive questions from researches who use Delphes with ROOT 5. So, Delphes is still regularly tested with Ubuntu 14.04, GCC 4.8 and ROOT 5. If the new IO method only works with recent versions of ROOT 6 and requires the change of file format and programming interface, it will be a major change for Delphes.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:849,interoperability,compatib,compatibility,849,"As a developer and maintainer of Delphes, I am quite happy with TClonesArray. As far as I know, storing TClonesArray in TTree was the fastest and least crash-prone method for all ROOT 5 versions and early ROOT 6 versions. If there is a newer and better method that meets all of the Delphes requirements, I would be interested in seeing and evaluating a few examples. Before doing any pull requests, let's start with something more basic. I think a very basic example showing how to write/read a collection of generated particles to/from a file would be a good starting point. > something that works well with today's software world :-). Newer doesn't always mean better :smiley: Especially in the HEP community where some researchers are still happy with CERNLIB and PAW and do not want to migrate to ROOT :smiley:. At the moment, Delphes maintains compatibility with ROOT 5 and I still receive questions from researches who use Delphes with ROOT 5. So, Delphes is still regularly tested with Ubuntu 14.04, GCC 4.8 and ROOT 5. If the new IO method only works with recent versions of ROOT 6 and requires the change of file format and programming interface, it will be a major change for Delphes.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:1122,interoperability,format,format,1122,"As a developer and maintainer of Delphes, I am quite happy with TClonesArray. As far as I know, storing TClonesArray in TTree was the fastest and least crash-prone method for all ROOT 5 versions and early ROOT 6 versions. If there is a newer and better method that meets all of the Delphes requirements, I would be interested in seeing and evaluating a few examples. Before doing any pull requests, let's start with something more basic. I think a very basic example showing how to write/read a collection of generated particles to/from a file would be a good starting point. > something that works well with today's software world :-). Newer doesn't always mean better :smiley: Especially in the HEP community where some researchers are still happy with CERNLIB and PAW and do not want to migrate to ROOT :smiley:. At the moment, Delphes maintains compatibility with ROOT 5 and I still receive questions from researches who use Delphes with ROOT 5. So, Delphes is still regularly tested with Ubuntu 14.04, GCC 4.8 and ROOT 5. If the new IO method only works with recent versions of ROOT 6 and requires the change of file format and programming interface, it will be a major change for Delphes.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:1145,interoperability,interfac,interface,1145,"As a developer and maintainer of Delphes, I am quite happy with TClonesArray. As far as I know, storing TClonesArray in TTree was the fastest and least crash-prone method for all ROOT 5 versions and early ROOT 6 versions. If there is a newer and better method that meets all of the Delphes requirements, I would be interested in seeing and evaluating a few examples. Before doing any pull requests, let's start with something more basic. I think a very basic example showing how to write/read a collection of generated particles to/from a file would be a good starting point. > something that works well with today's software world :-). Newer doesn't always mean better :smiley: Especially in the HEP community where some researchers are still happy with CERNLIB and PAW and do not want to migrate to ROOT :smiley:. At the moment, Delphes maintains compatibility with ROOT 5 and I still receive questions from researches who use Delphes with ROOT 5. So, Delphes is still regularly tested with Ubuntu 14.04, GCC 4.8 and ROOT 5. If the new IO method only works with recent versions of ROOT 6 and requires the change of file format and programming interface, it will be a major change for Delphes.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:19,modifiability,maintain,maintainer,19,"As a developer and maintainer of Delphes, I am quite happy with TClonesArray. As far as I know, storing TClonesArray in TTree was the fastest and least crash-prone method for all ROOT 5 versions and early ROOT 6 versions. If there is a newer and better method that meets all of the Delphes requirements, I would be interested in seeing and evaluating a few examples. Before doing any pull requests, let's start with something more basic. I think a very basic example showing how to write/read a collection of generated particles to/from a file would be a good starting point. > something that works well with today's software world :-). Newer doesn't always mean better :smiley: Especially in the HEP community where some researchers are still happy with CERNLIB and PAW and do not want to migrate to ROOT :smiley:. At the moment, Delphes maintains compatibility with ROOT 5 and I still receive questions from researches who use Delphes with ROOT 5. So, Delphes is still regularly tested with Ubuntu 14.04, GCC 4.8 and ROOT 5. If the new IO method only works with recent versions of ROOT 6 and requires the change of file format and programming interface, it will be a major change for Delphes.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:186,modifiability,version,versions,186,"As a developer and maintainer of Delphes, I am quite happy with TClonesArray. As far as I know, storing TClonesArray in TTree was the fastest and least crash-prone method for all ROOT 5 versions and early ROOT 6 versions. If there is a newer and better method that meets all of the Delphes requirements, I would be interested in seeing and evaluating a few examples. Before doing any pull requests, let's start with something more basic. I think a very basic example showing how to write/read a collection of generated particles to/from a file would be a good starting point. > something that works well with today's software world :-). Newer doesn't always mean better :smiley: Especially in the HEP community where some researchers are still happy with CERNLIB and PAW and do not want to migrate to ROOT :smiley:. At the moment, Delphes maintains compatibility with ROOT 5 and I still receive questions from researches who use Delphes with ROOT 5. So, Delphes is still regularly tested with Ubuntu 14.04, GCC 4.8 and ROOT 5. If the new IO method only works with recent versions of ROOT 6 and requires the change of file format and programming interface, it will be a major change for Delphes.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:212,modifiability,version,versions,212,"As a developer and maintainer of Delphes, I am quite happy with TClonesArray. As far as I know, storing TClonesArray in TTree was the fastest and least crash-prone method for all ROOT 5 versions and early ROOT 6 versions. If there is a newer and better method that meets all of the Delphes requirements, I would be interested in seeing and evaluating a few examples. Before doing any pull requests, let's start with something more basic. I think a very basic example showing how to write/read a collection of generated particles to/from a file would be a good starting point. > something that works well with today's software world :-). Newer doesn't always mean better :smiley: Especially in the HEP community where some researchers are still happy with CERNLIB and PAW and do not want to migrate to ROOT :smiley:. At the moment, Delphes maintains compatibility with ROOT 5 and I still receive questions from researches who use Delphes with ROOT 5. So, Delphes is still regularly tested with Ubuntu 14.04, GCC 4.8 and ROOT 5. If the new IO method only works with recent versions of ROOT 6 and requires the change of file format and programming interface, it will be a major change for Delphes.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:839,modifiability,maintain,maintains,839,"As a developer and maintainer of Delphes, I am quite happy with TClonesArray. As far as I know, storing TClonesArray in TTree was the fastest and least crash-prone method for all ROOT 5 versions and early ROOT 6 versions. If there is a newer and better method that meets all of the Delphes requirements, I would be interested in seeing and evaluating a few examples. Before doing any pull requests, let's start with something more basic. I think a very basic example showing how to write/read a collection of generated particles to/from a file would be a good starting point. > something that works well with today's software world :-). Newer doesn't always mean better :smiley: Especially in the HEP community where some researchers are still happy with CERNLIB and PAW and do not want to migrate to ROOT :smiley:. At the moment, Delphes maintains compatibility with ROOT 5 and I still receive questions from researches who use Delphes with ROOT 5. So, Delphes is still regularly tested with Ubuntu 14.04, GCC 4.8 and ROOT 5. If the new IO method only works with recent versions of ROOT 6 and requires the change of file format and programming interface, it will be a major change for Delphes.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:1071,modifiability,version,versions,1071,"As a developer and maintainer of Delphes, I am quite happy with TClonesArray. As far as I know, storing TClonesArray in TTree was the fastest and least crash-prone method for all ROOT 5 versions and early ROOT 6 versions. If there is a newer and better method that meets all of the Delphes requirements, I would be interested in seeing and evaluating a few examples. Before doing any pull requests, let's start with something more basic. I think a very basic example showing how to write/read a collection of generated particles to/from a file would be a good starting point. > something that works well with today's software world :-). Newer doesn't always mean better :smiley: Especially in the HEP community where some researchers are still happy with CERNLIB and PAW and do not want to migrate to ROOT :smiley:. At the moment, Delphes maintains compatibility with ROOT 5 and I still receive questions from researches who use Delphes with ROOT 5. So, Delphes is still regularly tested with Ubuntu 14.04, GCC 4.8 and ROOT 5. If the new IO method only works with recent versions of ROOT 6 and requires the change of file format and programming interface, it will be a major change for Delphes.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:1145,modifiability,interfac,interface,1145,"As a developer and maintainer of Delphes, I am quite happy with TClonesArray. As far as I know, storing TClonesArray in TTree was the fastest and least crash-prone method for all ROOT 5 versions and early ROOT 6 versions. If there is a newer and better method that meets all of the Delphes requirements, I would be interested in seeing and evaluating a few examples. Before doing any pull requests, let's start with something more basic. I think a very basic example showing how to write/read a collection of generated particles to/from a file would be a good starting point. > something that works well with today's software world :-). Newer doesn't always mean better :smiley: Especially in the HEP community where some researchers are still happy with CERNLIB and PAW and do not want to migrate to ROOT :smiley:. At the moment, Delphes maintains compatibility with ROOT 5 and I still receive questions from researches who use Delphes with ROOT 5. So, Delphes is still regularly tested with Ubuntu 14.04, GCC 4.8 and ROOT 5. If the new IO method only works with recent versions of ROOT 6 and requires the change of file format and programming interface, it will be a major change for Delphes.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:643,reliability,doe,doesn,643,"As a developer and maintainer of Delphes, I am quite happy with TClonesArray. As far as I know, storing TClonesArray in TTree was the fastest and least crash-prone method for all ROOT 5 versions and early ROOT 6 versions. If there is a newer and better method that meets all of the Delphes requirements, I would be interested in seeing and evaluating a few examples. Before doing any pull requests, let's start with something more basic. I think a very basic example showing how to write/read a collection of generated particles to/from a file would be a good starting point. > something that works well with today's software world :-). Newer doesn't always mean better :smiley: Especially in the HEP community where some researchers are still happy with CERNLIB and PAW and do not want to migrate to ROOT :smiley:. At the moment, Delphes maintains compatibility with ROOT 5 and I still receive questions from researches who use Delphes with ROOT 5. So, Delphes is still regularly tested with Ubuntu 14.04, GCC 4.8 and ROOT 5. If the new IO method only works with recent versions of ROOT 6 and requires the change of file format and programming interface, it will be a major change for Delphes.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:19,safety,maintain,maintainer,19,"As a developer and maintainer of Delphes, I am quite happy with TClonesArray. As far as I know, storing TClonesArray in TTree was the fastest and least crash-prone method for all ROOT 5 versions and early ROOT 6 versions. If there is a newer and better method that meets all of the Delphes requirements, I would be interested in seeing and evaluating a few examples. Before doing any pull requests, let's start with something more basic. I think a very basic example showing how to write/read a collection of generated particles to/from a file would be a good starting point. > something that works well with today's software world :-). Newer doesn't always mean better :smiley: Especially in the HEP community where some researchers are still happy with CERNLIB and PAW and do not want to migrate to ROOT :smiley:. At the moment, Delphes maintains compatibility with ROOT 5 and I still receive questions from researches who use Delphes with ROOT 5. So, Delphes is still regularly tested with Ubuntu 14.04, GCC 4.8 and ROOT 5. If the new IO method only works with recent versions of ROOT 6 and requires the change of file format and programming interface, it will be a major change for Delphes.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:839,safety,maintain,maintains,839,"As a developer and maintainer of Delphes, I am quite happy with TClonesArray. As far as I know, storing TClonesArray in TTree was the fastest and least crash-prone method for all ROOT 5 versions and early ROOT 6 versions. If there is a newer and better method that meets all of the Delphes requirements, I would be interested in seeing and evaluating a few examples. Before doing any pull requests, let's start with something more basic. I think a very basic example showing how to write/read a collection of generated particles to/from a file would be a good starting point. > something that works well with today's software world :-). Newer doesn't always mean better :smiley: Especially in the HEP community where some researchers are still happy with CERNLIB and PAW and do not want to migrate to ROOT :smiley:. At the moment, Delphes maintains compatibility with ROOT 5 and I still receive questions from researches who use Delphes with ROOT 5. So, Delphes is still regularly tested with Ubuntu 14.04, GCC 4.8 and ROOT 5. If the new IO method only works with recent versions of ROOT 6 and requires the change of file format and programming interface, it will be a major change for Delphes.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:981,safety,test,tested,981,"As a developer and maintainer of Delphes, I am quite happy with TClonesArray. As far as I know, storing TClonesArray in TTree was the fastest and least crash-prone method for all ROOT 5 versions and early ROOT 6 versions. If there is a newer and better method that meets all of the Delphes requirements, I would be interested in seeing and evaluating a few examples. Before doing any pull requests, let's start with something more basic. I think a very basic example showing how to write/read a collection of generated particles to/from a file would be a good starting point. > something that works well with today's software world :-). Newer doesn't always mean better :smiley: Especially in the HEP community where some researchers are still happy with CERNLIB and PAW and do not want to migrate to ROOT :smiley:. At the moment, Delphes maintains compatibility with ROOT 5 and I still receive questions from researches who use Delphes with ROOT 5. So, Delphes is still regularly tested with Ubuntu 14.04, GCC 4.8 and ROOT 5. If the new IO method only works with recent versions of ROOT 6 and requires the change of file format and programming interface, it will be a major change for Delphes.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:981,testability,test,tested,981,"As a developer and maintainer of Delphes, I am quite happy with TClonesArray. As far as I know, storing TClonesArray in TTree was the fastest and least crash-prone method for all ROOT 5 versions and early ROOT 6 versions. If there is a newer and better method that meets all of the Delphes requirements, I would be interested in seeing and evaluating a few examples. Before doing any pull requests, let's start with something more basic. I think a very basic example showing how to write/read a collection of generated particles to/from a file would be a good starting point. > something that works well with today's software world :-). Newer doesn't always mean better :smiley: Especially in the HEP community where some researchers are still happy with CERNLIB and PAW and do not want to migrate to ROOT :smiley:. At the moment, Delphes maintains compatibility with ROOT 5 and I still receive questions from researches who use Delphes with ROOT 5. So, Delphes is still regularly tested with Ubuntu 14.04, GCC 4.8 and ROOT 5. If the new IO method only works with recent versions of ROOT 6 and requires the change of file format and programming interface, it will be a major change for Delphes.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:121,integrability,coupl,couple,121,"Understood, and it's mostly outside this issue anyway. Let me first do my homework: dig out the code that we discussed a couple of years ago (?) and remind myself what Delphes does. I'll contact you then so we can discuss a proposal. And I'll make sure it works just fine for ROOT 5, too. And btw we - on the ROOT side - see several ""lost souls"" due to Delphes and its `TClonesArray`, so we might have some common motivation to evolve ;-) even if you feel ROOT 5 still has a relevant user community. I'll let contact you, @pavel-demin !",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:121,modifiability,coupl,couple,121,"Understood, and it's mostly outside this issue anyway. Let me first do my homework: dig out the code that we discussed a couple of years ago (?) and remind myself what Delphes does. I'll contact you then so we can discuss a proposal. And I'll make sure it works just fine for ROOT 5, too. And btw we - on the ROOT side - see several ""lost souls"" due to Delphes and its `TClonesArray`, so we might have some common motivation to evolve ;-) even if you feel ROOT 5 still has a relevant user community. I'll let contact you, @pavel-demin !",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:428,modifiability,evolv,evolve,428,"Understood, and it's mostly outside this issue anyway. Let me first do my homework: dig out the code that we discussed a couple of years ago (?) and remind myself what Delphes does. I'll contact you then so we can discuss a proposal. And I'll make sure it works just fine for ROOT 5, too. And btw we - on the ROOT side - see several ""lost souls"" due to Delphes and its `TClonesArray`, so we might have some common motivation to evolve ;-) even if you feel ROOT 5 still has a relevant user community. I'll let contact you, @pavel-demin !",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:176,reliability,doe,does,176,"Understood, and it's mostly outside this issue anyway. Let me first do my homework: dig out the code that we discussed a couple of years ago (?) and remind myself what Delphes does. I'll contact you then so we can discuss a proposal. And I'll make sure it works just fine for ROOT 5, too. And btw we - on the ROOT side - see several ""lost souls"" due to Delphes and its `TClonesArray`, so we might have some common motivation to evolve ;-) even if you feel ROOT 5 still has a relevant user community. I'll let contact you, @pavel-demin !",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:121,testability,coupl,couple,121,"Understood, and it's mostly outside this issue anyway. Let me first do my homework: dig out the code that we discussed a couple of years ago (?) and remind myself what Delphes does. I'll contact you then so we can discuss a proposal. And I'll make sure it works just fine for ROOT 5, too. And btw we - on the ROOT side - see several ""lost souls"" due to Delphes and its `TClonesArray`, so we might have some common motivation to evolve ;-) even if you feel ROOT 5 still has a relevant user community. I'll let contact you, @pavel-demin !",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:484,usability,user,user,484,"Understood, and it's mostly outside this issue anyway. Let me first do my homework: dig out the code that we discussed a couple of years ago (?) and remind myself what Delphes does. I'll contact you then so we can discuss a proposal. And I'll make sure it works just fine for ROOT 5, too. And btw we - on the ROOT side - see several ""lost souls"" due to Delphes and its `TClonesArray`, so we might have some common motivation to evolve ;-) even if you feel ROOT 5 still has a relevant user community. I'll let contact you, @pavel-demin !",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:283,availability,degrad,degradation,283,"Regarding the titular issue, I have been thinking about it for a bit and I don't think we can do much better than what @pieterdavid already implemented. Solutions considered:. 1. Implicitly converting `TClonesArrays` to `RVecs` would be a) backward-incompatible and b) a silent perf degradation, as it requires a copy. Moreover, if we did this I don't see a migration path that doesn't require that users revise all their TClonesArrays-related logic. 2. Adding a `TClonesArrays2RVec` helper function: it would require a copy (or it would have to return a clunky `RVec<T*>`). It is also trivial to implement for users that need it and don't mind the downsides. 3. Adding a RDF toggle to tweak ""read TClonesArrays as RVecs"" or not: requires adding a bunch of extra logic to the column-reading mechanism for a feature that might end up being mostly unused (modern data models don't use TClonesArrays), and it would require the extra copy (i.e. silent performance degradation). The best seems to be 3, but feature toggles complicate internals and are typically not easy to discover (might end up mostly unused). It seems simpler to ask users to explicitly do the conversion in a `Redefine` if they need/want to. Sorry I could not come up with something nice :confused: . I will close this in a few days unless people have something against it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:649,availability,down,downsides,649,"Regarding the titular issue, I have been thinking about it for a bit and I don't think we can do much better than what @pieterdavid already implemented. Solutions considered:. 1. Implicitly converting `TClonesArrays` to `RVecs` would be a) backward-incompatible and b) a silent perf degradation, as it requires a copy. Moreover, if we did this I don't see a migration path that doesn't require that users revise all their TClonesArrays-related logic. 2. Adding a `TClonesArrays2RVec` helper function: it would require a copy (or it would have to return a clunky `RVec<T*>`). It is also trivial to implement for users that need it and don't mind the downsides. 3. Adding a RDF toggle to tweak ""read TClonesArrays as RVecs"" or not: requires adding a bunch of extra logic to the column-reading mechanism for a feature that might end up being mostly unused (modern data models don't use TClonesArrays), and it would require the extra copy (i.e. silent performance degradation). The best seems to be 3, but feature toggles complicate internals and are typically not easy to discover (might end up mostly unused). It seems simpler to ask users to explicitly do the conversion in a `Redefine` if they need/want to. Sorry I could not come up with something nice :confused: . I will close this in a few days unless people have something against it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:960,availability,degrad,degradation,960,"Regarding the titular issue, I have been thinking about it for a bit and I don't think we can do much better than what @pieterdavid already implemented. Solutions considered:. 1. Implicitly converting `TClonesArrays` to `RVecs` would be a) backward-incompatible and b) a silent perf degradation, as it requires a copy. Moreover, if we did this I don't see a migration path that doesn't require that users revise all their TClonesArrays-related logic. 2. Adding a `TClonesArrays2RVec` helper function: it would require a copy (or it would have to return a clunky `RVec<T*>`). It is also trivial to implement for users that need it and don't mind the downsides. 3. Adding a RDF toggle to tweak ""read TClonesArrays as RVecs"" or not: requires adding a bunch of extra logic to the column-reading mechanism for a feature that might end up being mostly unused (modern data models don't use TClonesArrays), and it would require the extra copy (i.e. silent performance degradation). The best seems to be 3, but feature toggles complicate internals and are typically not easy to discover (might end up mostly unused). It seems simpler to ask users to explicitly do the conversion in a `Redefine` if they need/want to. Sorry I could not come up with something nice :confused: . I will close this in a few days unless people have something against it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:444,deployability,log,logic,444,"Regarding the titular issue, I have been thinking about it for a bit and I don't think we can do much better than what @pieterdavid already implemented. Solutions considered:. 1. Implicitly converting `TClonesArrays` to `RVecs` would be a) backward-incompatible and b) a silent perf degradation, as it requires a copy. Moreover, if we did this I don't see a migration path that doesn't require that users revise all their TClonesArrays-related logic. 2. Adding a `TClonesArrays2RVec` helper function: it would require a copy (or it would have to return a clunky `RVec<T*>`). It is also trivial to implement for users that need it and don't mind the downsides. 3. Adding a RDF toggle to tweak ""read TClonesArrays as RVecs"" or not: requires adding a bunch of extra logic to the column-reading mechanism for a feature that might end up being mostly unused (modern data models don't use TClonesArrays), and it would require the extra copy (i.e. silent performance degradation). The best seems to be 3, but feature toggles complicate internals and are typically not easy to discover (might end up mostly unused). It seems simpler to ask users to explicitly do the conversion in a `Redefine` if they need/want to. Sorry I could not come up with something nice :confused: . I will close this in a few days unless people have something against it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:676,deployability,toggl,toggle,676,"Regarding the titular issue, I have been thinking about it for a bit and I don't think we can do much better than what @pieterdavid already implemented. Solutions considered:. 1. Implicitly converting `TClonesArrays` to `RVecs` would be a) backward-incompatible and b) a silent perf degradation, as it requires a copy. Moreover, if we did this I don't see a migration path that doesn't require that users revise all their TClonesArrays-related logic. 2. Adding a `TClonesArrays2RVec` helper function: it would require a copy (or it would have to return a clunky `RVec<T*>`). It is also trivial to implement for users that need it and don't mind the downsides. 3. Adding a RDF toggle to tweak ""read TClonesArrays as RVecs"" or not: requires adding a bunch of extra logic to the column-reading mechanism for a feature that might end up being mostly unused (modern data models don't use TClonesArrays), and it would require the extra copy (i.e. silent performance degradation). The best seems to be 3, but feature toggles complicate internals and are typically not easy to discover (might end up mostly unused). It seems simpler to ask users to explicitly do the conversion in a `Redefine` if they need/want to. Sorry I could not come up with something nice :confused: . I will close this in a few days unless people have something against it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:763,deployability,log,logic,763,"Regarding the titular issue, I have been thinking about it for a bit and I don't think we can do much better than what @pieterdavid already implemented. Solutions considered:. 1. Implicitly converting `TClonesArrays` to `RVecs` would be a) backward-incompatible and b) a silent perf degradation, as it requires a copy. Moreover, if we did this I don't see a migration path that doesn't require that users revise all their TClonesArrays-related logic. 2. Adding a `TClonesArrays2RVec` helper function: it would require a copy (or it would have to return a clunky `RVec<T*>`). It is also trivial to implement for users that need it and don't mind the downsides. 3. Adding a RDF toggle to tweak ""read TClonesArrays as RVecs"" or not: requires adding a bunch of extra logic to the column-reading mechanism for a feature that might end up being mostly unused (modern data models don't use TClonesArrays), and it would require the extra copy (i.e. silent performance degradation). The best seems to be 3, but feature toggles complicate internals and are typically not easy to discover (might end up mostly unused). It seems simpler to ask users to explicitly do the conversion in a `Redefine` if they need/want to. Sorry I could not come up with something nice :confused: . I will close this in a few days unless people have something against it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:1010,deployability,toggl,toggles,1010,"Regarding the titular issue, I have been thinking about it for a bit and I don't think we can do much better than what @pieterdavid already implemented. Solutions considered:. 1. Implicitly converting `TClonesArrays` to `RVecs` would be a) backward-incompatible and b) a silent perf degradation, as it requires a copy. Moreover, if we did this I don't see a migration path that doesn't require that users revise all their TClonesArrays-related logic. 2. Adding a `TClonesArrays2RVec` helper function: it would require a copy (or it would have to return a clunky `RVec<T*>`). It is also trivial to implement for users that need it and don't mind the downsides. 3. Adding a RDF toggle to tweak ""read TClonesArrays as RVecs"" or not: requires adding a bunch of extra logic to the column-reading mechanism for a feature that might end up being mostly unused (modern data models don't use TClonesArrays), and it would require the extra copy (i.e. silent performance degradation). The best seems to be 3, but feature toggles complicate internals and are typically not easy to discover (might end up mostly unused). It seems simpler to ask users to explicitly do the conversion in a `Redefine` if they need/want to. Sorry I could not come up with something nice :confused: . I will close this in a few days unless people have something against it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:866,energy efficiency,model,models,866,"Regarding the titular issue, I have been thinking about it for a bit and I don't think we can do much better than what @pieterdavid already implemented. Solutions considered:. 1. Implicitly converting `TClonesArrays` to `RVecs` would be a) backward-incompatible and b) a silent perf degradation, as it requires a copy. Moreover, if we did this I don't see a migration path that doesn't require that users revise all their TClonesArrays-related logic. 2. Adding a `TClonesArrays2RVec` helper function: it would require a copy (or it would have to return a clunky `RVec<T*>`). It is also trivial to implement for users that need it and don't mind the downsides. 3. Adding a RDF toggle to tweak ""read TClonesArrays as RVecs"" or not: requires adding a bunch of extra logic to the column-reading mechanism for a feature that might end up being mostly unused (modern data models don't use TClonesArrays), and it would require the extra copy (i.e. silent performance degradation). The best seems to be 3, but feature toggles complicate internals and are typically not easy to discover (might end up mostly unused). It seems simpler to ask users to explicitly do the conversion in a `Redefine` if they need/want to. Sorry I could not come up with something nice :confused: . I will close this in a few days unless people have something against it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:1069,integrability,discover,discover,1069,"Regarding the titular issue, I have been thinking about it for a bit and I don't think we can do much better than what @pieterdavid already implemented. Solutions considered:. 1. Implicitly converting `TClonesArrays` to `RVecs` would be a) backward-incompatible and b) a silent perf degradation, as it requires a copy. Moreover, if we did this I don't see a migration path that doesn't require that users revise all their TClonesArrays-related logic. 2. Adding a `TClonesArrays2RVec` helper function: it would require a copy (or it would have to return a clunky `RVec<T*>`). It is also trivial to implement for users that need it and don't mind the downsides. 3. Adding a RDF toggle to tweak ""read TClonesArrays as RVecs"" or not: requires adding a bunch of extra logic to the column-reading mechanism for a feature that might end up being mostly unused (modern data models don't use TClonesArrays), and it would require the extra copy (i.e. silent performance degradation). The best seems to be 3, but feature toggles complicate internals and are typically not easy to discover (might end up mostly unused). It seems simpler to ask users to explicitly do the conversion in a `Redefine` if they need/want to. Sorry I could not come up with something nice :confused: . I will close this in a few days unless people have something against it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:249,interoperability,incompatib,incompatible,249,"Regarding the titular issue, I have been thinking about it for a bit and I don't think we can do much better than what @pieterdavid already implemented. Solutions considered:. 1. Implicitly converting `TClonesArrays` to `RVecs` would be a) backward-incompatible and b) a silent perf degradation, as it requires a copy. Moreover, if we did this I don't see a migration path that doesn't require that users revise all their TClonesArrays-related logic. 2. Adding a `TClonesArrays2RVec` helper function: it would require a copy (or it would have to return a clunky `RVec<T*>`). It is also trivial to implement for users that need it and don't mind the downsides. 3. Adding a RDF toggle to tweak ""read TClonesArrays as RVecs"" or not: requires adding a bunch of extra logic to the column-reading mechanism for a feature that might end up being mostly unused (modern data models don't use TClonesArrays), and it would require the extra copy (i.e. silent performance degradation). The best seems to be 3, but feature toggles complicate internals and are typically not easy to discover (might end up mostly unused). It seems simpler to ask users to explicitly do the conversion in a `Redefine` if they need/want to. Sorry I could not come up with something nice :confused: . I will close this in a few days unless people have something against it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:1069,interoperability,discover,discover,1069,"Regarding the titular issue, I have been thinking about it for a bit and I don't think we can do much better than what @pieterdavid already implemented. Solutions considered:. 1. Implicitly converting `TClonesArrays` to `RVecs` would be a) backward-incompatible and b) a silent perf degradation, as it requires a copy. Moreover, if we did this I don't see a migration path that doesn't require that users revise all their TClonesArrays-related logic. 2. Adding a `TClonesArrays2RVec` helper function: it would require a copy (or it would have to return a clunky `RVec<T*>`). It is also trivial to implement for users that need it and don't mind the downsides. 3. Adding a RDF toggle to tweak ""read TClonesArrays as RVecs"" or not: requires adding a bunch of extra logic to the column-reading mechanism for a feature that might end up being mostly unused (modern data models don't use TClonesArrays), and it would require the extra copy (i.e. silent performance degradation). The best seems to be 3, but feature toggles complicate internals and are typically not easy to discover (might end up mostly unused). It seems simpler to ask users to explicitly do the conversion in a `Redefine` if they need/want to. Sorry I could not come up with something nice :confused: . I will close this in a few days unless people have something against it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:1159,interoperability,convers,conversion,1159,"Regarding the titular issue, I have been thinking about it for a bit and I don't think we can do much better than what @pieterdavid already implemented. Solutions considered:. 1. Implicitly converting `TClonesArrays` to `RVecs` would be a) backward-incompatible and b) a silent perf degradation, as it requires a copy. Moreover, if we did this I don't see a migration path that doesn't require that users revise all their TClonesArrays-related logic. 2. Adding a `TClonesArrays2RVec` helper function: it would require a copy (or it would have to return a clunky `RVec<T*>`). It is also trivial to implement for users that need it and don't mind the downsides. 3. Adding a RDF toggle to tweak ""read TClonesArrays as RVecs"" or not: requires adding a bunch of extra logic to the column-reading mechanism for a feature that might end up being mostly unused (modern data models don't use TClonesArrays), and it would require the extra copy (i.e. silent performance degradation). The best seems to be 3, but feature toggles complicate internals and are typically not easy to discover (might end up mostly unused). It seems simpler to ask users to explicitly do the conversion in a `Redefine` if they need/want to. Sorry I could not come up with something nice :confused: . I will close this in a few days unless people have something against it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:948,performance,perform,performance,948,"Regarding the titular issue, I have been thinking about it for a bit and I don't think we can do much better than what @pieterdavid already implemented. Solutions considered:. 1. Implicitly converting `TClonesArrays` to `RVecs` would be a) backward-incompatible and b) a silent perf degradation, as it requires a copy. Moreover, if we did this I don't see a migration path that doesn't require that users revise all their TClonesArrays-related logic. 2. Adding a `TClonesArrays2RVec` helper function: it would require a copy (or it would have to return a clunky `RVec<T*>`). It is also trivial to implement for users that need it and don't mind the downsides. 3. Adding a RDF toggle to tweak ""read TClonesArrays as RVecs"" or not: requires adding a bunch of extra logic to the column-reading mechanism for a feature that might end up being mostly unused (modern data models don't use TClonesArrays), and it would require the extra copy (i.e. silent performance degradation). The best seems to be 3, but feature toggles complicate internals and are typically not easy to discover (might end up mostly unused). It seems simpler to ask users to explicitly do the conversion in a `Redefine` if they need/want to. Sorry I could not come up with something nice :confused: . I will close this in a few days unless people have something against it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:283,reliability,degrad,degradation,283,"Regarding the titular issue, I have been thinking about it for a bit and I don't think we can do much better than what @pieterdavid already implemented. Solutions considered:. 1. Implicitly converting `TClonesArrays` to `RVecs` would be a) backward-incompatible and b) a silent perf degradation, as it requires a copy. Moreover, if we did this I don't see a migration path that doesn't require that users revise all their TClonesArrays-related logic. 2. Adding a `TClonesArrays2RVec` helper function: it would require a copy (or it would have to return a clunky `RVec<T*>`). It is also trivial to implement for users that need it and don't mind the downsides. 3. Adding a RDF toggle to tweak ""read TClonesArrays as RVecs"" or not: requires adding a bunch of extra logic to the column-reading mechanism for a feature that might end up being mostly unused (modern data models don't use TClonesArrays), and it would require the extra copy (i.e. silent performance degradation). The best seems to be 3, but feature toggles complicate internals and are typically not easy to discover (might end up mostly unused). It seems simpler to ask users to explicitly do the conversion in a `Redefine` if they need/want to. Sorry I could not come up with something nice :confused: . I will close this in a few days unless people have something against it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:378,reliability,doe,doesn,378,"Regarding the titular issue, I have been thinking about it for a bit and I don't think we can do much better than what @pieterdavid already implemented. Solutions considered:. 1. Implicitly converting `TClonesArrays` to `RVecs` would be a) backward-incompatible and b) a silent perf degradation, as it requires a copy. Moreover, if we did this I don't see a migration path that doesn't require that users revise all their TClonesArrays-related logic. 2. Adding a `TClonesArrays2RVec` helper function: it would require a copy (or it would have to return a clunky `RVec<T*>`). It is also trivial to implement for users that need it and don't mind the downsides. 3. Adding a RDF toggle to tweak ""read TClonesArrays as RVecs"" or not: requires adding a bunch of extra logic to the column-reading mechanism for a feature that might end up being mostly unused (modern data models don't use TClonesArrays), and it would require the extra copy (i.e. silent performance degradation). The best seems to be 3, but feature toggles complicate internals and are typically not easy to discover (might end up mostly unused). It seems simpler to ask users to explicitly do the conversion in a `Redefine` if they need/want to. Sorry I could not come up with something nice :confused: . I will close this in a few days unless people have something against it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:960,reliability,degrad,degradation,960,"Regarding the titular issue, I have been thinking about it for a bit and I don't think we can do much better than what @pieterdavid already implemented. Solutions considered:. 1. Implicitly converting `TClonesArrays` to `RVecs` would be a) backward-incompatible and b) a silent perf degradation, as it requires a copy. Moreover, if we did this I don't see a migration path that doesn't require that users revise all their TClonesArrays-related logic. 2. Adding a `TClonesArrays2RVec` helper function: it would require a copy (or it would have to return a clunky `RVec<T*>`). It is also trivial to implement for users that need it and don't mind the downsides. 3. Adding a RDF toggle to tweak ""read TClonesArrays as RVecs"" or not: requires adding a bunch of extra logic to the column-reading mechanism for a feature that might end up being mostly unused (modern data models don't use TClonesArrays), and it would require the extra copy (i.e. silent performance degradation). The best seems to be 3, but feature toggles complicate internals and are typically not easy to discover (might end up mostly unused). It seems simpler to ask users to explicitly do the conversion in a `Redefine` if they need/want to. Sorry I could not come up with something nice :confused: . I will close this in a few days unless people have something against it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:444,safety,log,logic,444,"Regarding the titular issue, I have been thinking about it for a bit and I don't think we can do much better than what @pieterdavid already implemented. Solutions considered:. 1. Implicitly converting `TClonesArrays` to `RVecs` would be a) backward-incompatible and b) a silent perf degradation, as it requires a copy. Moreover, if we did this I don't see a migration path that doesn't require that users revise all their TClonesArrays-related logic. 2. Adding a `TClonesArrays2RVec` helper function: it would require a copy (or it would have to return a clunky `RVec<T*>`). It is also trivial to implement for users that need it and don't mind the downsides. 3. Adding a RDF toggle to tweak ""read TClonesArrays as RVecs"" or not: requires adding a bunch of extra logic to the column-reading mechanism for a feature that might end up being mostly unused (modern data models don't use TClonesArrays), and it would require the extra copy (i.e. silent performance degradation). The best seems to be 3, but feature toggles complicate internals and are typically not easy to discover (might end up mostly unused). It seems simpler to ask users to explicitly do the conversion in a `Redefine` if they need/want to. Sorry I could not come up with something nice :confused: . I will close this in a few days unless people have something against it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:763,safety,log,logic,763,"Regarding the titular issue, I have been thinking about it for a bit and I don't think we can do much better than what @pieterdavid already implemented. Solutions considered:. 1. Implicitly converting `TClonesArrays` to `RVecs` would be a) backward-incompatible and b) a silent perf degradation, as it requires a copy. Moreover, if we did this I don't see a migration path that doesn't require that users revise all their TClonesArrays-related logic. 2. Adding a `TClonesArrays2RVec` helper function: it would require a copy (or it would have to return a clunky `RVec<T*>`). It is also trivial to implement for users that need it and don't mind the downsides. 3. Adding a RDF toggle to tweak ""read TClonesArrays as RVecs"" or not: requires adding a bunch of extra logic to the column-reading mechanism for a feature that might end up being mostly unused (modern data models don't use TClonesArrays), and it would require the extra copy (i.e. silent performance degradation). The best seems to be 3, but feature toggles complicate internals and are typically not easy to discover (might end up mostly unused). It seems simpler to ask users to explicitly do the conversion in a `Redefine` if they need/want to. Sorry I could not come up with something nice :confused: . I will close this in a few days unless people have something against it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:1018,safety,compl,complicate,1018,"Regarding the titular issue, I have been thinking about it for a bit and I don't think we can do much better than what @pieterdavid already implemented. Solutions considered:. 1. Implicitly converting `TClonesArrays` to `RVecs` would be a) backward-incompatible and b) a silent perf degradation, as it requires a copy. Moreover, if we did this I don't see a migration path that doesn't require that users revise all their TClonesArrays-related logic. 2. Adding a `TClonesArrays2RVec` helper function: it would require a copy (or it would have to return a clunky `RVec<T*>`). It is also trivial to implement for users that need it and don't mind the downsides. 3. Adding a RDF toggle to tweak ""read TClonesArrays as RVecs"" or not: requires adding a bunch of extra logic to the column-reading mechanism for a feature that might end up being mostly unused (modern data models don't use TClonesArrays), and it would require the extra copy (i.e. silent performance degradation). The best seems to be 3, but feature toggles complicate internals and are typically not easy to discover (might end up mostly unused). It seems simpler to ask users to explicitly do the conversion in a `Redefine` if they need/want to. Sorry I could not come up with something nice :confused: . I will close this in a few days unless people have something against it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:444,security,log,logic,444,"Regarding the titular issue, I have been thinking about it for a bit and I don't think we can do much better than what @pieterdavid already implemented. Solutions considered:. 1. Implicitly converting `TClonesArrays` to `RVecs` would be a) backward-incompatible and b) a silent perf degradation, as it requires a copy. Moreover, if we did this I don't see a migration path that doesn't require that users revise all their TClonesArrays-related logic. 2. Adding a `TClonesArrays2RVec` helper function: it would require a copy (or it would have to return a clunky `RVec<T*>`). It is also trivial to implement for users that need it and don't mind the downsides. 3. Adding a RDF toggle to tweak ""read TClonesArrays as RVecs"" or not: requires adding a bunch of extra logic to the column-reading mechanism for a feature that might end up being mostly unused (modern data models don't use TClonesArrays), and it would require the extra copy (i.e. silent performance degradation). The best seems to be 3, but feature toggles complicate internals and are typically not easy to discover (might end up mostly unused). It seems simpler to ask users to explicitly do the conversion in a `Redefine` if they need/want to. Sorry I could not come up with something nice :confused: . I will close this in a few days unless people have something against it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:763,security,log,logic,763,"Regarding the titular issue, I have been thinking about it for a bit and I don't think we can do much better than what @pieterdavid already implemented. Solutions considered:. 1. Implicitly converting `TClonesArrays` to `RVecs` would be a) backward-incompatible and b) a silent perf degradation, as it requires a copy. Moreover, if we did this I don't see a migration path that doesn't require that users revise all their TClonesArrays-related logic. 2. Adding a `TClonesArrays2RVec` helper function: it would require a copy (or it would have to return a clunky `RVec<T*>`). It is also trivial to implement for users that need it and don't mind the downsides. 3. Adding a RDF toggle to tweak ""read TClonesArrays as RVecs"" or not: requires adding a bunch of extra logic to the column-reading mechanism for a feature that might end up being mostly unused (modern data models don't use TClonesArrays), and it would require the extra copy (i.e. silent performance degradation). The best seems to be 3, but feature toggles complicate internals and are typically not easy to discover (might end up mostly unused). It seems simpler to ask users to explicitly do the conversion in a `Redefine` if they need/want to. Sorry I could not come up with something nice :confused: . I will close this in a few days unless people have something against it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:866,security,model,models,866,"Regarding the titular issue, I have been thinking about it for a bit and I don't think we can do much better than what @pieterdavid already implemented. Solutions considered:. 1. Implicitly converting `TClonesArrays` to `RVecs` would be a) backward-incompatible and b) a silent perf degradation, as it requires a copy. Moreover, if we did this I don't see a migration path that doesn't require that users revise all their TClonesArrays-related logic. 2. Adding a `TClonesArrays2RVec` helper function: it would require a copy (or it would have to return a clunky `RVec<T*>`). It is also trivial to implement for users that need it and don't mind the downsides. 3. Adding a RDF toggle to tweak ""read TClonesArrays as RVecs"" or not: requires adding a bunch of extra logic to the column-reading mechanism for a feature that might end up being mostly unused (modern data models don't use TClonesArrays), and it would require the extra copy (i.e. silent performance degradation). The best seems to be 3, but feature toggles complicate internals and are typically not easy to discover (might end up mostly unused). It seems simpler to ask users to explicitly do the conversion in a `Redefine` if they need/want to. Sorry I could not come up with something nice :confused: . I will close this in a few days unless people have something against it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:1018,security,compl,complicate,1018,"Regarding the titular issue, I have been thinking about it for a bit and I don't think we can do much better than what @pieterdavid already implemented. Solutions considered:. 1. Implicitly converting `TClonesArrays` to `RVecs` would be a) backward-incompatible and b) a silent perf degradation, as it requires a copy. Moreover, if we did this I don't see a migration path that doesn't require that users revise all their TClonesArrays-related logic. 2. Adding a `TClonesArrays2RVec` helper function: it would require a copy (or it would have to return a clunky `RVec<T*>`). It is also trivial to implement for users that need it and don't mind the downsides. 3. Adding a RDF toggle to tweak ""read TClonesArrays as RVecs"" or not: requires adding a bunch of extra logic to the column-reading mechanism for a feature that might end up being mostly unused (modern data models don't use TClonesArrays), and it would require the extra copy (i.e. silent performance degradation). The best seems to be 3, but feature toggles complicate internals and are typically not easy to discover (might end up mostly unused). It seems simpler to ask users to explicitly do the conversion in a `Redefine` if they need/want to. Sorry I could not come up with something nice :confused: . I will close this in a few days unless people have something against it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:444,testability,log,logic,444,"Regarding the titular issue, I have been thinking about it for a bit and I don't think we can do much better than what @pieterdavid already implemented. Solutions considered:. 1. Implicitly converting `TClonesArrays` to `RVecs` would be a) backward-incompatible and b) a silent perf degradation, as it requires a copy. Moreover, if we did this I don't see a migration path that doesn't require that users revise all their TClonesArrays-related logic. 2. Adding a `TClonesArrays2RVec` helper function: it would require a copy (or it would have to return a clunky `RVec<T*>`). It is also trivial to implement for users that need it and don't mind the downsides. 3. Adding a RDF toggle to tweak ""read TClonesArrays as RVecs"" or not: requires adding a bunch of extra logic to the column-reading mechanism for a feature that might end up being mostly unused (modern data models don't use TClonesArrays), and it would require the extra copy (i.e. silent performance degradation). The best seems to be 3, but feature toggles complicate internals and are typically not easy to discover (might end up mostly unused). It seems simpler to ask users to explicitly do the conversion in a `Redefine` if they need/want to. Sorry I could not come up with something nice :confused: . I will close this in a few days unless people have something against it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:763,testability,log,logic,763,"Regarding the titular issue, I have been thinking about it for a bit and I don't think we can do much better than what @pieterdavid already implemented. Solutions considered:. 1. Implicitly converting `TClonesArrays` to `RVecs` would be a) backward-incompatible and b) a silent perf degradation, as it requires a copy. Moreover, if we did this I don't see a migration path that doesn't require that users revise all their TClonesArrays-related logic. 2. Adding a `TClonesArrays2RVec` helper function: it would require a copy (or it would have to return a clunky `RVec<T*>`). It is also trivial to implement for users that need it and don't mind the downsides. 3. Adding a RDF toggle to tweak ""read TClonesArrays as RVecs"" or not: requires adding a bunch of extra logic to the column-reading mechanism for a feature that might end up being mostly unused (modern data models don't use TClonesArrays), and it would require the extra copy (i.e. silent performance degradation). The best seems to be 3, but feature toggles complicate internals and are typically not easy to discover (might end up mostly unused). It seems simpler to ask users to explicitly do the conversion in a `Redefine` if they need/want to. Sorry I could not come up with something nice :confused: . I will close this in a few days unless people have something against it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:1117,testability,simpl,simpler,1117,"Regarding the titular issue, I have been thinking about it for a bit and I don't think we can do much better than what @pieterdavid already implemented. Solutions considered:. 1. Implicitly converting `TClonesArrays` to `RVecs` would be a) backward-incompatible and b) a silent perf degradation, as it requires a copy. Moreover, if we did this I don't see a migration path that doesn't require that users revise all their TClonesArrays-related logic. 2. Adding a `TClonesArrays2RVec` helper function: it would require a copy (or it would have to return a clunky `RVec<T*>`). It is also trivial to implement for users that need it and don't mind the downsides. 3. Adding a RDF toggle to tweak ""read TClonesArrays as RVecs"" or not: requires adding a bunch of extra logic to the column-reading mechanism for a feature that might end up being mostly unused (modern data models don't use TClonesArrays), and it would require the extra copy (i.e. silent performance degradation). The best seems to be 3, but feature toggles complicate internals and are typically not easy to discover (might end up mostly unused). It seems simpler to ask users to explicitly do the conversion in a `Redefine` if they need/want to. Sorry I could not come up with something nice :confused: . I will close this in a few days unless people have something against it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:399,usability,user,users,399,"Regarding the titular issue, I have been thinking about it for a bit and I don't think we can do much better than what @pieterdavid already implemented. Solutions considered:. 1. Implicitly converting `TClonesArrays` to `RVecs` would be a) backward-incompatible and b) a silent perf degradation, as it requires a copy. Moreover, if we did this I don't see a migration path that doesn't require that users revise all their TClonesArrays-related logic. 2. Adding a `TClonesArrays2RVec` helper function: it would require a copy (or it would have to return a clunky `RVec<T*>`). It is also trivial to implement for users that need it and don't mind the downsides. 3. Adding a RDF toggle to tweak ""read TClonesArrays as RVecs"" or not: requires adding a bunch of extra logic to the column-reading mechanism for a feature that might end up being mostly unused (modern data models don't use TClonesArrays), and it would require the extra copy (i.e. silent performance degradation). The best seems to be 3, but feature toggles complicate internals and are typically not easy to discover (might end up mostly unused). It seems simpler to ask users to explicitly do the conversion in a `Redefine` if they need/want to. Sorry I could not come up with something nice :confused: . I will close this in a few days unless people have something against it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:484,usability,help,helper,484,"Regarding the titular issue, I have been thinking about it for a bit and I don't think we can do much better than what @pieterdavid already implemented. Solutions considered:. 1. Implicitly converting `TClonesArrays` to `RVecs` would be a) backward-incompatible and b) a silent perf degradation, as it requires a copy. Moreover, if we did this I don't see a migration path that doesn't require that users revise all their TClonesArrays-related logic. 2. Adding a `TClonesArrays2RVec` helper function: it would require a copy (or it would have to return a clunky `RVec<T*>`). It is also trivial to implement for users that need it and don't mind the downsides. 3. Adding a RDF toggle to tweak ""read TClonesArrays as RVecs"" or not: requires adding a bunch of extra logic to the column-reading mechanism for a feature that might end up being mostly unused (modern data models don't use TClonesArrays), and it would require the extra copy (i.e. silent performance degradation). The best seems to be 3, but feature toggles complicate internals and are typically not easy to discover (might end up mostly unused). It seems simpler to ask users to explicitly do the conversion in a `Redefine` if they need/want to. Sorry I could not come up with something nice :confused: . I will close this in a few days unless people have something against it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:611,usability,user,users,611,"Regarding the titular issue, I have been thinking about it for a bit and I don't think we can do much better than what @pieterdavid already implemented. Solutions considered:. 1. Implicitly converting `TClonesArrays` to `RVecs` would be a) backward-incompatible and b) a silent perf degradation, as it requires a copy. Moreover, if we did this I don't see a migration path that doesn't require that users revise all their TClonesArrays-related logic. 2. Adding a `TClonesArrays2RVec` helper function: it would require a copy (or it would have to return a clunky `RVec<T*>`). It is also trivial to implement for users that need it and don't mind the downsides. 3. Adding a RDF toggle to tweak ""read TClonesArrays as RVecs"" or not: requires adding a bunch of extra logic to the column-reading mechanism for a feature that might end up being mostly unused (modern data models don't use TClonesArrays), and it would require the extra copy (i.e. silent performance degradation). The best seems to be 3, but feature toggles complicate internals and are typically not easy to discover (might end up mostly unused). It seems simpler to ask users to explicitly do the conversion in a `Redefine` if they need/want to. Sorry I could not come up with something nice :confused: . I will close this in a few days unless people have something against it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:948,usability,perform,performance,948,"Regarding the titular issue, I have been thinking about it for a bit and I don't think we can do much better than what @pieterdavid already implemented. Solutions considered:. 1. Implicitly converting `TClonesArrays` to `RVecs` would be a) backward-incompatible and b) a silent perf degradation, as it requires a copy. Moreover, if we did this I don't see a migration path that doesn't require that users revise all their TClonesArrays-related logic. 2. Adding a `TClonesArrays2RVec` helper function: it would require a copy (or it would have to return a clunky `RVec<T*>`). It is also trivial to implement for users that need it and don't mind the downsides. 3. Adding a RDF toggle to tweak ""read TClonesArrays as RVecs"" or not: requires adding a bunch of extra logic to the column-reading mechanism for a feature that might end up being mostly unused (modern data models don't use TClonesArrays), and it would require the extra copy (i.e. silent performance degradation). The best seems to be 3, but feature toggles complicate internals and are typically not easy to discover (might end up mostly unused). It seems simpler to ask users to explicitly do the conversion in a `Redefine` if they need/want to. Sorry I could not come up with something nice :confused: . I will close this in a few days unless people have something against it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:1069,usability,discov,discover,1069,"Regarding the titular issue, I have been thinking about it for a bit and I don't think we can do much better than what @pieterdavid already implemented. Solutions considered:. 1. Implicitly converting `TClonesArrays` to `RVecs` would be a) backward-incompatible and b) a silent perf degradation, as it requires a copy. Moreover, if we did this I don't see a migration path that doesn't require that users revise all their TClonesArrays-related logic. 2. Adding a `TClonesArrays2RVec` helper function: it would require a copy (or it would have to return a clunky `RVec<T*>`). It is also trivial to implement for users that need it and don't mind the downsides. 3. Adding a RDF toggle to tweak ""read TClonesArrays as RVecs"" or not: requires adding a bunch of extra logic to the column-reading mechanism for a feature that might end up being mostly unused (modern data models don't use TClonesArrays), and it would require the extra copy (i.e. silent performance degradation). The best seems to be 3, but feature toggles complicate internals and are typically not easy to discover (might end up mostly unused). It seems simpler to ask users to explicitly do the conversion in a `Redefine` if they need/want to. Sorry I could not come up with something nice :confused: . I will close this in a few days unless people have something against it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:1117,usability,simpl,simpler,1117,"Regarding the titular issue, I have been thinking about it for a bit and I don't think we can do much better than what @pieterdavid already implemented. Solutions considered:. 1. Implicitly converting `TClonesArrays` to `RVecs` would be a) backward-incompatible and b) a silent perf degradation, as it requires a copy. Moreover, if we did this I don't see a migration path that doesn't require that users revise all their TClonesArrays-related logic. 2. Adding a `TClonesArrays2RVec` helper function: it would require a copy (or it would have to return a clunky `RVec<T*>`). It is also trivial to implement for users that need it and don't mind the downsides. 3. Adding a RDF toggle to tweak ""read TClonesArrays as RVecs"" or not: requires adding a bunch of extra logic to the column-reading mechanism for a feature that might end up being mostly unused (modern data models don't use TClonesArrays), and it would require the extra copy (i.e. silent performance degradation). The best seems to be 3, but feature toggles complicate internals and are typically not easy to discover (might end up mostly unused). It seems simpler to ask users to explicitly do the conversion in a `Redefine` if they need/want to. Sorry I could not come up with something nice :confused: . I will close this in a few days unless people have something against it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:1132,usability,user,users,1132,"Regarding the titular issue, I have been thinking about it for a bit and I don't think we can do much better than what @pieterdavid already implemented. Solutions considered:. 1. Implicitly converting `TClonesArrays` to `RVecs` would be a) backward-incompatible and b) a silent perf degradation, as it requires a copy. Moreover, if we did this I don't see a migration path that doesn't require that users revise all their TClonesArrays-related logic. 2. Adding a `TClonesArrays2RVec` helper function: it would require a copy (or it would have to return a clunky `RVec<T*>`). It is also trivial to implement for users that need it and don't mind the downsides. 3. Adding a RDF toggle to tweak ""read TClonesArrays as RVecs"" or not: requires adding a bunch of extra logic to the column-reading mechanism for a feature that might end up being mostly unused (modern data models don't use TClonesArrays), and it would require the extra copy (i.e. silent performance degradation). The best seems to be 3, but feature toggles complicate internals and are typically not easy to discover (might end up mostly unused). It seems simpler to ask users to explicitly do the conversion in a `Redefine` if they need/want to. Sorry I could not come up with something nice :confused: . I will close this in a few days unless people have something against it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:1274,usability,close,close,1274,"Regarding the titular issue, I have been thinking about it for a bit and I don't think we can do much better than what @pieterdavid already implemented. Solutions considered:. 1. Implicitly converting `TClonesArrays` to `RVecs` would be a) backward-incompatible and b) a silent perf degradation, as it requires a copy. Moreover, if we did this I don't see a migration path that doesn't require that users revise all their TClonesArrays-related logic. 2. Adding a `TClonesArrays2RVec` helper function: it would require a copy (or it would have to return a clunky `RVec<T*>`). It is also trivial to implement for users that need it and don't mind the downsides. 3. Adding a RDF toggle to tweak ""read TClonesArrays as RVecs"" or not: requires adding a bunch of extra logic to the column-reading mechanism for a feature that might end up being mostly unused (modern data models don't use TClonesArrays), and it would require the extra copy (i.e. silent performance degradation). The best seems to be 3, but feature toggles complicate internals and are typically not easy to discover (might end up mostly unused). It seems simpler to ask users to explicitly do the conversion in a `Redefine` if they need/want to. Sorry I could not come up with something nice :confused: . I will close this in a few days unless people have something against it.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:139,usability,user,user,139,"Another ingredient here is that we have seen mass migration away from TClonesArray, with Delphes being the only relevant + known remaining user.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:375,energy efficiency,reduc,reduced,375,"Thanks a lot for looking into this @eguiraud ! Fine with me to close then. In case it's useful for anyone who finds this in the future: I implemented the conversion to `RVec<T*>` for bamboo [here](https://github.com/pieterdavid/bamboo-delphesexample), but will probably leave that as a standalone example since it now seems more likely that my colleagues will end up using a reduced flat format.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:154,interoperability,convers,conversion,154,"Thanks a lot for looking into this @eguiraud ! Fine with me to close then. In case it's useful for anyone who finds this in the future: I implemented the conversion to `RVec<T*>` for bamboo [here](https://github.com/pieterdavid/bamboo-delphesexample), but will probably leave that as a standalone example since it now seems more likely that my colleagues will end up using a reduced flat format.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:388,interoperability,format,format,388,"Thanks a lot for looking into this @eguiraud ! Fine with me to close then. In case it's useful for anyone who finds this in the future: I implemented the conversion to `RVec<T*>` for bamboo [here](https://github.com/pieterdavid/bamboo-delphesexample), but will probably leave that as a standalone example since it now seems more likely that my colleagues will end up using a reduced flat format.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/issues/8644:63,usability,close,close,63,"Thanks a lot for looking into this @eguiraud ! Fine with me to close then. In case it's useful for anyone who finds this in the future: I implemented the conversion to `RVec<T*>` for bamboo [here](https://github.com/pieterdavid/bamboo-delphesexample), but will probably leave that as a standalone example since it now seems more likely that my colleagues will end up using a reduced flat format.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8644
https://github.com/root-project/root/pull/8646:53,deployability,build,build,53,"I just tried this quickly on my laptop, with a debug build, running the dimuon example with 30 partitions. With current master. ``` . vpadulan@fedorathinkpad-T550 [~/Projects/rootcode]: python time_distrdf_dimuon_30partitions.py. took 352 seconds. ```. With this commit. ```. vpadulan@fedorathinkpad-T550 [~/Projects/rootcode]: python time_distrdf_dimuon_30partitions.py. took 128 seconds . ```. It's not a thorough benchmark but it gives a nice idea of the gains we might see",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:112,energy efficiency,current,current,112,"I just tried this quickly on my laptop, with a debug build, running the dimuon example with 30 partitions. With current master. ``` . vpadulan@fedorathinkpad-T550 [~/Projects/rootcode]: python time_distrdf_dimuon_30partitions.py. took 352 seconds. ```. With this commit. ```. vpadulan@fedorathinkpad-T550 [~/Projects/rootcode]: python time_distrdf_dimuon_30partitions.py. took 128 seconds . ```. It's not a thorough benchmark but it gives a nice idea of the gains we might see",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:174,availability,cluster,clusters,174,"Latest bunch of commits:. * Include style comments from Enric. * Re-introduce `Range` for the empty source case. * Adapt tests to changes. * Actually take the first and last clusters in the partition instead of computing `min` and `max`. The PR is now ready for review and CI, I'm only missing the check against the issue with tentrylist, but we wouldn't need it if https://github.com/root-project/root/pull/8660 gets accepted",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:174,deployability,cluster,clusters,174,"Latest bunch of commits:. * Include style comments from Enric. * Re-introduce `Range` for the empty source case. * Adapt tests to changes. * Actually take the first and last clusters in the partition instead of computing `min` and `max`. The PR is now ready for review and CI, I'm only missing the check against the issue with tentrylist, but we wouldn't need it if https://github.com/root-project/root/pull/8660 gets accepted",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:115,energy efficiency,Adapt,Adapt,115,"Latest bunch of commits:. * Include style comments from Enric. * Re-introduce `Range` for the empty source case. * Adapt tests to changes. * Actually take the first and last clusters in the partition instead of computing `min` and `max`. The PR is now ready for review and CI, I'm only missing the check against the issue with tentrylist, but we wouldn't need it if https://github.com/root-project/root/pull/8660 gets accepted",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:115,integrability,Adapt,Adapt,115,"Latest bunch of commits:. * Include style comments from Enric. * Re-introduce `Range` for the empty source case. * Adapt tests to changes. * Actually take the first and last clusters in the partition instead of computing `min` and `max`. The PR is now ready for review and CI, I'm only missing the check against the issue with tentrylist, but we wouldn't need it if https://github.com/root-project/root/pull/8660 gets accepted",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:115,interoperability,Adapt,Adapt,115,"Latest bunch of commits:. * Include style comments from Enric. * Re-introduce `Range` for the empty source case. * Adapt tests to changes. * Actually take the first and last clusters in the partition instead of computing `min` and `max`. The PR is now ready for review and CI, I'm only missing the check against the issue with tentrylist, but we wouldn't need it if https://github.com/root-project/root/pull/8660 gets accepted",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:115,modifiability,Adapt,Adapt,115,"Latest bunch of commits:. * Include style comments from Enric. * Re-introduce `Range` for the empty source case. * Adapt tests to changes. * Actually take the first and last clusters in the partition instead of computing `min` and `max`. The PR is now ready for review and CI, I'm only missing the check against the issue with tentrylist, but we wouldn't need it if https://github.com/root-project/root/pull/8660 gets accepted",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:121,safety,test,tests,121,"Latest bunch of commits:. * Include style comments from Enric. * Re-introduce `Range` for the empty source case. * Adapt tests to changes. * Actually take the first and last clusters in the partition instead of computing `min` and `max`. The PR is now ready for review and CI, I'm only missing the check against the issue with tentrylist, but we wouldn't need it if https://github.com/root-project/root/pull/8660 gets accepted",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:262,safety,review,review,262,"Latest bunch of commits:. * Include style comments from Enric. * Re-introduce `Range` for the empty source case. * Adapt tests to changes. * Actually take the first and last clusters in the partition instead of computing `min` and `max`. The PR is now ready for review and CI, I'm only missing the check against the issue with tentrylist, but we wouldn't need it if https://github.com/root-project/root/pull/8660 gets accepted",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:121,testability,test,tests,121,"Latest bunch of commits:. * Include style comments from Enric. * Re-introduce `Range` for the empty source case. * Adapt tests to changes. * Actually take the first and last clusters in the partition instead of computing `min` and `max`. The PR is now ready for review and CI, I'm only missing the check against the issue with tentrylist, but we wouldn't need it if https://github.com/root-project/root/pull/8660 gets accepted",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:262,testability,review,review,262,"Latest bunch of commits:. * Include style comments from Enric. * Re-introduce `Range` for the empty source case. * Adapt tests to changes. * Actually take the first and last clusters in the partition instead of computing `min` and `max`. The PR is now ready for review and CI, I'm only missing the check against the issue with tentrylist, but we wouldn't need it if https://github.com/root-project/root/pull/8660 gets accepted",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:11,deployability,build,build,11,@phsft-bot build,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:363,interoperability,distribut,distributed,363,Latest commits:. * Use new behaviour of TEntryList to support also the use case of multiple times the same treename and filename in the chain. * Add a test to make sure we're counting the right amount of entries in such use case. * Improve some docs. * Address @eguiraud's comment to also add the number of entries in the file to the call to `TChain::Add` in the distributed task,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:92,performance,time,times,92,Latest commits:. * Use new behaviour of TEntryList to support also the use case of multiple times the same treename and filename in the chain. * Add a test to make sure we're counting the right amount of entries in such use case. * Improve some docs. * Address @eguiraud's comment to also add the number of entries in the file to the call to `TChain::Add` in the distributed task,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:151,safety,test,test,151,Latest commits:. * Use new behaviour of TEntryList to support also the use case of multiple times the same treename and filename in the chain. * Add a test to make sure we're counting the right amount of entries in such use case. * Improve some docs. * Address @eguiraud's comment to also add the number of entries in the file to the call to `TChain::Add` in the distributed task,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:151,testability,test,test,151,Latest commits:. * Use new behaviour of TEntryList to support also the use case of multiple times the same treename and filename in the chain. * Add a test to make sure we're counting the right amount of entries in such use case. * Improve some docs. * Address @eguiraud's comment to also add the number of entries in the file to the call to `TChain::Add` in the distributed task,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:27,usability,behavi,behaviour,27,Latest commits:. * Use new behaviour of TEntryList to support also the use case of multiple times the same treename and filename in the chain. * Add a test to make sure we're counting the right amount of entries in such use case. * Improve some docs. * Address @eguiraud's comment to also add the number of entries in the file to the call to `TChain::Add` in the distributed task,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:54,usability,support,support,54,Latest commits:. * Use new behaviour of TEntryList to support also the use case of multiple times the same treename and filename in the chain. * Add a test to make sure we're counting the right amount of entries in such use case. * Improve some docs. * Address @eguiraud's comment to also add the number of entries in the file to the call to `TChain::Add` in the distributed task,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:11,deployability,build,build,11,@phsft-bot build,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:266,availability,redund,redundant,266,"> I understand this will be simplified when we can pass a configuration object to RDataFrame where we specify the range. When reading trees/chains, I'm not sure the configuration object will allow anything more than what we can already do with TEntryLists? IIUC the redundant info is that `globalend = globalstart + (localends - localstarts).sum()` (i.e. we could avoid passing `globalend`). However: do you still need `globalstart` and `globalend` at all when using TEntryLists?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:58,deployability,configurat,configuration,58,"> I understand this will be simplified when we can pass a configuration object to RDataFrame where we specify the range. When reading trees/chains, I'm not sure the configuration object will allow anything more than what we can already do with TEntryLists? IIUC the redundant info is that `globalend = globalstart + (localends - localstarts).sum()` (i.e. we could avoid passing `globalend`). However: do you still need `globalstart` and `globalend` at all when using TEntryLists?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:165,deployability,configurat,configuration,165,"> I understand this will be simplified when we can pass a configuration object to RDataFrame where we specify the range. When reading trees/chains, I'm not sure the configuration object will allow anything more than what we can already do with TEntryLists? IIUC the redundant info is that `globalend = globalstart + (localends - localstarts).sum()` (i.e. we could avoid passing `globalend`). However: do you still need `globalstart` and `globalend` at all when using TEntryLists?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:266,deployability,redundan,redundant,266,"> I understand this will be simplified when we can pass a configuration object to RDataFrame where we specify the range. When reading trees/chains, I'm not sure the configuration object will allow anything more than what we can already do with TEntryLists? IIUC the redundant info is that `globalend = globalstart + (localends - localstarts).sum()` (i.e. we could avoid passing `globalend`). However: do you still need `globalstart` and `globalend` at all when using TEntryLists?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:58,integrability,configur,configuration,58,"> I understand this will be simplified when we can pass a configuration object to RDataFrame where we specify the range. When reading trees/chains, I'm not sure the configuration object will allow anything more than what we can already do with TEntryLists? IIUC the redundant info is that `globalend = globalstart + (localends - localstarts).sum()` (i.e. we could avoid passing `globalend`). However: do you still need `globalstart` and `globalend` at all when using TEntryLists?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:165,integrability,configur,configuration,165,"> I understand this will be simplified when we can pass a configuration object to RDataFrame where we specify the range. When reading trees/chains, I'm not sure the configuration object will allow anything more than what we can already do with TEntryLists? IIUC the redundant info is that `globalend = globalstart + (localends - localstarts).sum()` (i.e. we could avoid passing `globalend`). However: do you still need `globalstart` and `globalend` at all when using TEntryLists?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:102,interoperability,specif,specify,102,"> I understand this will be simplified when we can pass a configuration object to RDataFrame where we specify the range. When reading trees/chains, I'm not sure the configuration object will allow anything more than what we can already do with TEntryLists? IIUC the redundant info is that `globalend = globalstart + (localends - localstarts).sum()` (i.e. we could avoid passing `globalend`). However: do you still need `globalstart` and `globalend` at all when using TEntryLists?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:58,modifiability,configur,configuration,58,"> I understand this will be simplified when we can pass a configuration object to RDataFrame where we specify the range. When reading trees/chains, I'm not sure the configuration object will allow anything more than what we can already do with TEntryLists? IIUC the redundant info is that `globalend = globalstart + (localends - localstarts).sum()` (i.e. we could avoid passing `globalend`). However: do you still need `globalstart` and `globalend` at all when using TEntryLists?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:165,modifiability,configur,configuration,165,"> I understand this will be simplified when we can pass a configuration object to RDataFrame where we specify the range. When reading trees/chains, I'm not sure the configuration object will allow anything more than what we can already do with TEntryLists? IIUC the redundant info is that `globalend = globalstart + (localends - localstarts).sum()` (i.e. we could avoid passing `globalend`). However: do you still need `globalstart` and `globalend` at all when using TEntryLists?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:266,reliability,redundan,redundant,266,"> I understand this will be simplified when we can pass a configuration object to RDataFrame where we specify the range. When reading trees/chains, I'm not sure the configuration object will allow anything more than what we can already do with TEntryLists? IIUC the redundant info is that `globalend = globalstart + (localends - localstarts).sum()` (i.e. we could avoid passing `globalend`). However: do you still need `globalstart` and `globalend` at all when using TEntryLists?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:266,safety,redund,redundant,266,"> I understand this will be simplified when we can pass a configuration object to RDataFrame where we specify the range. When reading trees/chains, I'm not sure the configuration object will allow anything more than what we can already do with TEntryLists? IIUC the redundant info is that `globalend = globalstart + (localends - localstarts).sum()` (i.e. we could avoid passing `globalend`). However: do you still need `globalstart` and `globalend` at all when using TEntryLists?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:364,safety,avoid,avoid,364,"> I understand this will be simplified when we can pass a configuration object to RDataFrame where we specify the range. When reading trees/chains, I'm not sure the configuration object will allow anything more than what we can already do with TEntryLists? IIUC the redundant info is that `globalend = globalstart + (localends - localstarts).sum()` (i.e. we could avoid passing `globalend`). However: do you still need `globalstart` and `globalend` at all when using TEntryLists?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:58,security,configur,configuration,58,"> I understand this will be simplified when we can pass a configuration object to RDataFrame where we specify the range. When reading trees/chains, I'm not sure the configuration object will allow anything more than what we can already do with TEntryLists? IIUC the redundant info is that `globalend = globalstart + (localends - localstarts).sum()` (i.e. we could avoid passing `globalend`). However: do you still need `globalstart` and `globalend` at all when using TEntryLists?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:165,security,configur,configuration,165,"> I understand this will be simplified when we can pass a configuration object to RDataFrame where we specify the range. When reading trees/chains, I'm not sure the configuration object will allow anything more than what we can already do with TEntryLists? IIUC the redundant info is that `globalend = globalstart + (localends - localstarts).sum()` (i.e. we could avoid passing `globalend`). However: do you still need `globalstart` and `globalend` at all when using TEntryLists?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:4,testability,understand,understand,4,"> I understand this will be simplified when we can pass a configuration object to RDataFrame where we specify the range. When reading trees/chains, I'm not sure the configuration object will allow anything more than what we can already do with TEntryLists? IIUC the redundant info is that `globalend = globalstart + (localends - localstarts).sum()` (i.e. we could avoid passing `globalend`). However: do you still need `globalstart` and `globalend` at all when using TEntryLists?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:28,testability,simpl,simplified,28,"> I understand this will be simplified when we can pass a configuration object to RDataFrame where we specify the range. When reading trees/chains, I'm not sure the configuration object will allow anything more than what we can already do with TEntryLists? IIUC the redundant info is that `globalend = globalstart + (localends - localstarts).sum()` (i.e. we could avoid passing `globalend`). However: do you still need `globalstart` and `globalend` at all when using TEntryLists?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:28,usability,simpl,simplified,28,"> I understand this will be simplified when we can pass a configuration object to RDataFrame where we specify the range. When reading trees/chains, I'm not sure the configuration object will allow anything more than what we can already do with TEntryLists? IIUC the redundant info is that `globalend = globalstart + (localends - localstarts).sum()` (i.e. we could avoid passing `globalend`). However: do you still need `globalstart` and `globalend` at all when using TEntryLists?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:328,availability,redund,redundant,328,"> When reading trees/chains, I'm not sure the configuration object will allow anything more than what we can already do with TEntryLists? But that's the thing, with the configuration object we won't need to construct TEntryLists, at least not at the DistRDF level. This means we won't need localstarts and localends. > IIUC the redundant info is that globalend = globalstart + (localends - localstarts).sum() (i.e. we could avoid passing globalend). You can avoid passing `localstarts` and `localends`, since you can calculate them from `globalstart, globalend, filelist, treesnentries`. The latter is what I expect we would need if we use RDF with a configuration object: globalstart and globalend would be included in the configuration object to represent a range in the dataset, and `filelist` and `treesnentries` would be used to construct the TChain with which we would construct the RDF.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:46,deployability,configurat,configuration,46,"> When reading trees/chains, I'm not sure the configuration object will allow anything more than what we can already do with TEntryLists? But that's the thing, with the configuration object we won't need to construct TEntryLists, at least not at the DistRDF level. This means we won't need localstarts and localends. > IIUC the redundant info is that globalend = globalstart + (localends - localstarts).sum() (i.e. we could avoid passing globalend). You can avoid passing `localstarts` and `localends`, since you can calculate them from `globalstart, globalend, filelist, treesnentries`. The latter is what I expect we would need if we use RDF with a configuration object: globalstart and globalend would be included in the configuration object to represent a range in the dataset, and `filelist` and `treesnentries` would be used to construct the TChain with which we would construct the RDF.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:169,deployability,configurat,configuration,169,"> When reading trees/chains, I'm not sure the configuration object will allow anything more than what we can already do with TEntryLists? But that's the thing, with the configuration object we won't need to construct TEntryLists, at least not at the DistRDF level. This means we won't need localstarts and localends. > IIUC the redundant info is that globalend = globalstart + (localends - localstarts).sum() (i.e. we could avoid passing globalend). You can avoid passing `localstarts` and `localends`, since you can calculate them from `globalstart, globalend, filelist, treesnentries`. The latter is what I expect we would need if we use RDF with a configuration object: globalstart and globalend would be included in the configuration object to represent a range in the dataset, and `filelist` and `treesnentries` would be used to construct the TChain with which we would construct the RDF.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:328,deployability,redundan,redundant,328,"> When reading trees/chains, I'm not sure the configuration object will allow anything more than what we can already do with TEntryLists? But that's the thing, with the configuration object we won't need to construct TEntryLists, at least not at the DistRDF level. This means we won't need localstarts and localends. > IIUC the redundant info is that globalend = globalstart + (localends - localstarts).sum() (i.e. we could avoid passing globalend). You can avoid passing `localstarts` and `localends`, since you can calculate them from `globalstart, globalend, filelist, treesnentries`. The latter is what I expect we would need if we use RDF with a configuration object: globalstart and globalend would be included in the configuration object to represent a range in the dataset, and `filelist` and `treesnentries` would be used to construct the TChain with which we would construct the RDF.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:651,deployability,configurat,configuration,651,"> When reading trees/chains, I'm not sure the configuration object will allow anything more than what we can already do with TEntryLists? But that's the thing, with the configuration object we won't need to construct TEntryLists, at least not at the DistRDF level. This means we won't need localstarts and localends. > IIUC the redundant info is that globalend = globalstart + (localends - localstarts).sum() (i.e. we could avoid passing globalend). You can avoid passing `localstarts` and `localends`, since you can calculate them from `globalstart, globalend, filelist, treesnentries`. The latter is what I expect we would need if we use RDF with a configuration object: globalstart and globalend would be included in the configuration object to represent a range in the dataset, and `filelist` and `treesnentries` would be used to construct the TChain with which we would construct the RDF.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:724,deployability,configurat,configuration,724,"> When reading trees/chains, I'm not sure the configuration object will allow anything more than what we can already do with TEntryLists? But that's the thing, with the configuration object we won't need to construct TEntryLists, at least not at the DistRDF level. This means we won't need localstarts and localends. > IIUC the redundant info is that globalend = globalstart + (localends - localstarts).sum() (i.e. we could avoid passing globalend). You can avoid passing `localstarts` and `localends`, since you can calculate them from `globalstart, globalend, filelist, treesnentries`. The latter is what I expect we would need if we use RDF with a configuration object: globalstart and globalend would be included in the configuration object to represent a range in the dataset, and `filelist` and `treesnentries` would be used to construct the TChain with which we would construct the RDF.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:46,integrability,configur,configuration,46,"> When reading trees/chains, I'm not sure the configuration object will allow anything more than what we can already do with TEntryLists? But that's the thing, with the configuration object we won't need to construct TEntryLists, at least not at the DistRDF level. This means we won't need localstarts and localends. > IIUC the redundant info is that globalend = globalstart + (localends - localstarts).sum() (i.e. we could avoid passing globalend). You can avoid passing `localstarts` and `localends`, since you can calculate them from `globalstart, globalend, filelist, treesnentries`. The latter is what I expect we would need if we use RDF with a configuration object: globalstart and globalend would be included in the configuration object to represent a range in the dataset, and `filelist` and `treesnentries` would be used to construct the TChain with which we would construct the RDF.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:169,integrability,configur,configuration,169,"> When reading trees/chains, I'm not sure the configuration object will allow anything more than what we can already do with TEntryLists? But that's the thing, with the configuration object we won't need to construct TEntryLists, at least not at the DistRDF level. This means we won't need localstarts and localends. > IIUC the redundant info is that globalend = globalstart + (localends - localstarts).sum() (i.e. we could avoid passing globalend). You can avoid passing `localstarts` and `localends`, since you can calculate them from `globalstart, globalend, filelist, treesnentries`. The latter is what I expect we would need if we use RDF with a configuration object: globalstart and globalend would be included in the configuration object to represent a range in the dataset, and `filelist` and `treesnentries` would be used to construct the TChain with which we would construct the RDF.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:651,integrability,configur,configuration,651,"> When reading trees/chains, I'm not sure the configuration object will allow anything more than what we can already do with TEntryLists? But that's the thing, with the configuration object we won't need to construct TEntryLists, at least not at the DistRDF level. This means we won't need localstarts and localends. > IIUC the redundant info is that globalend = globalstart + (localends - localstarts).sum() (i.e. we could avoid passing globalend). You can avoid passing `localstarts` and `localends`, since you can calculate them from `globalstart, globalend, filelist, treesnentries`. The latter is what I expect we would need if we use RDF with a configuration object: globalstart and globalend would be included in the configuration object to represent a range in the dataset, and `filelist` and `treesnentries` would be used to construct the TChain with which we would construct the RDF.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:724,integrability,configur,configuration,724,"> When reading trees/chains, I'm not sure the configuration object will allow anything more than what we can already do with TEntryLists? But that's the thing, with the configuration object we won't need to construct TEntryLists, at least not at the DistRDF level. This means we won't need localstarts and localends. > IIUC the redundant info is that globalend = globalstart + (localends - localstarts).sum() (i.e. we could avoid passing globalend). You can avoid passing `localstarts` and `localends`, since you can calculate them from `globalstart, globalend, filelist, treesnentries`. The latter is what I expect we would need if we use RDF with a configuration object: globalstart and globalend would be included in the configuration object to represent a range in the dataset, and `filelist` and `treesnentries` would be used to construct the TChain with which we would construct the RDF.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:46,modifiability,configur,configuration,46,"> When reading trees/chains, I'm not sure the configuration object will allow anything more than what we can already do with TEntryLists? But that's the thing, with the configuration object we won't need to construct TEntryLists, at least not at the DistRDF level. This means we won't need localstarts and localends. > IIUC the redundant info is that globalend = globalstart + (localends - localstarts).sum() (i.e. we could avoid passing globalend). You can avoid passing `localstarts` and `localends`, since you can calculate them from `globalstart, globalend, filelist, treesnentries`. The latter is what I expect we would need if we use RDF with a configuration object: globalstart and globalend would be included in the configuration object to represent a range in the dataset, and `filelist` and `treesnentries` would be used to construct the TChain with which we would construct the RDF.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:169,modifiability,configur,configuration,169,"> When reading trees/chains, I'm not sure the configuration object will allow anything more than what we can already do with TEntryLists? But that's the thing, with the configuration object we won't need to construct TEntryLists, at least not at the DistRDF level. This means we won't need localstarts and localends. > IIUC the redundant info is that globalend = globalstart + (localends - localstarts).sum() (i.e. we could avoid passing globalend). You can avoid passing `localstarts` and `localends`, since you can calculate them from `globalstart, globalend, filelist, treesnentries`. The latter is what I expect we would need if we use RDF with a configuration object: globalstart and globalend would be included in the configuration object to represent a range in the dataset, and `filelist` and `treesnentries` would be used to construct the TChain with which we would construct the RDF.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:651,modifiability,configur,configuration,651,"> When reading trees/chains, I'm not sure the configuration object will allow anything more than what we can already do with TEntryLists? But that's the thing, with the configuration object we won't need to construct TEntryLists, at least not at the DistRDF level. This means we won't need localstarts and localends. > IIUC the redundant info is that globalend = globalstart + (localends - localstarts).sum() (i.e. we could avoid passing globalend). You can avoid passing `localstarts` and `localends`, since you can calculate them from `globalstart, globalend, filelist, treesnentries`. The latter is what I expect we would need if we use RDF with a configuration object: globalstart and globalend would be included in the configuration object to represent a range in the dataset, and `filelist` and `treesnentries` would be used to construct the TChain with which we would construct the RDF.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:724,modifiability,configur,configuration,724,"> When reading trees/chains, I'm not sure the configuration object will allow anything more than what we can already do with TEntryLists? But that's the thing, with the configuration object we won't need to construct TEntryLists, at least not at the DistRDF level. This means we won't need localstarts and localends. > IIUC the redundant info is that globalend = globalstart + (localends - localstarts).sum() (i.e. we could avoid passing globalend). You can avoid passing `localstarts` and `localends`, since you can calculate them from `globalstart, globalend, filelist, treesnentries`. The latter is what I expect we would need if we use RDF with a configuration object: globalstart and globalend would be included in the configuration object to represent a range in the dataset, and `filelist` and `treesnentries` would be used to construct the TChain with which we would construct the RDF.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:328,reliability,redundan,redundant,328,"> When reading trees/chains, I'm not sure the configuration object will allow anything more than what we can already do with TEntryLists? But that's the thing, with the configuration object we won't need to construct TEntryLists, at least not at the DistRDF level. This means we won't need localstarts and localends. > IIUC the redundant info is that globalend = globalstart + (localends - localstarts).sum() (i.e. we could avoid passing globalend). You can avoid passing `localstarts` and `localends`, since you can calculate them from `globalstart, globalend, filelist, treesnentries`. The latter is what I expect we would need if we use RDF with a configuration object: globalstart and globalend would be included in the configuration object to represent a range in the dataset, and `filelist` and `treesnentries` would be used to construct the TChain with which we would construct the RDF.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:328,safety,redund,redundant,328,"> When reading trees/chains, I'm not sure the configuration object will allow anything more than what we can already do with TEntryLists? But that's the thing, with the configuration object we won't need to construct TEntryLists, at least not at the DistRDF level. This means we won't need localstarts and localends. > IIUC the redundant info is that globalend = globalstart + (localends - localstarts).sum() (i.e. we could avoid passing globalend). You can avoid passing `localstarts` and `localends`, since you can calculate them from `globalstart, globalend, filelist, treesnentries`. The latter is what I expect we would need if we use RDF with a configuration object: globalstart and globalend would be included in the configuration object to represent a range in the dataset, and `filelist` and `treesnentries` would be used to construct the TChain with which we would construct the RDF.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:424,safety,avoid,avoid,424,"> When reading trees/chains, I'm not sure the configuration object will allow anything more than what we can already do with TEntryLists? But that's the thing, with the configuration object we won't need to construct TEntryLists, at least not at the DistRDF level. This means we won't need localstarts and localends. > IIUC the redundant info is that globalend = globalstart + (localends - localstarts).sum() (i.e. we could avoid passing globalend). You can avoid passing `localstarts` and `localends`, since you can calculate them from `globalstart, globalend, filelist, treesnentries`. The latter is what I expect we would need if we use RDF with a configuration object: globalstart and globalend would be included in the configuration object to represent a range in the dataset, and `filelist` and `treesnentries` would be used to construct the TChain with which we would construct the RDF.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:458,safety,avoid,avoid,458,"> When reading trees/chains, I'm not sure the configuration object will allow anything more than what we can already do with TEntryLists? But that's the thing, with the configuration object we won't need to construct TEntryLists, at least not at the DistRDF level. This means we won't need localstarts and localends. > IIUC the redundant info is that globalend = globalstart + (localends - localstarts).sum() (i.e. we could avoid passing globalend). You can avoid passing `localstarts` and `localends`, since you can calculate them from `globalstart, globalend, filelist, treesnentries`. The latter is what I expect we would need if we use RDF with a configuration object: globalstart and globalend would be included in the configuration object to represent a range in the dataset, and `filelist` and `treesnentries` would be used to construct the TChain with which we would construct the RDF.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:46,security,configur,configuration,46,"> When reading trees/chains, I'm not sure the configuration object will allow anything more than what we can already do with TEntryLists? But that's the thing, with the configuration object we won't need to construct TEntryLists, at least not at the DistRDF level. This means we won't need localstarts and localends. > IIUC the redundant info is that globalend = globalstart + (localends - localstarts).sum() (i.e. we could avoid passing globalend). You can avoid passing `localstarts` and `localends`, since you can calculate them from `globalstart, globalend, filelist, treesnentries`. The latter is what I expect we would need if we use RDF with a configuration object: globalstart and globalend would be included in the configuration object to represent a range in the dataset, and `filelist` and `treesnentries` would be used to construct the TChain with which we would construct the RDF.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:169,security,configur,configuration,169,"> When reading trees/chains, I'm not sure the configuration object will allow anything more than what we can already do with TEntryLists? But that's the thing, with the configuration object we won't need to construct TEntryLists, at least not at the DistRDF level. This means we won't need localstarts and localends. > IIUC the redundant info is that globalend = globalstart + (localends - localstarts).sum() (i.e. we could avoid passing globalend). You can avoid passing `localstarts` and `localends`, since you can calculate them from `globalstart, globalend, filelist, treesnentries`. The latter is what I expect we would need if we use RDF with a configuration object: globalstart and globalend would be included in the configuration object to represent a range in the dataset, and `filelist` and `treesnentries` would be used to construct the TChain with which we would construct the RDF.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:651,security,configur,configuration,651,"> When reading trees/chains, I'm not sure the configuration object will allow anything more than what we can already do with TEntryLists? But that's the thing, with the configuration object we won't need to construct TEntryLists, at least not at the DistRDF level. This means we won't need localstarts and localends. > IIUC the redundant info is that globalend = globalstart + (localends - localstarts).sum() (i.e. we could avoid passing globalend). You can avoid passing `localstarts` and `localends`, since you can calculate them from `globalstart, globalend, filelist, treesnentries`. The latter is what I expect we would need if we use RDF with a configuration object: globalstart and globalend would be included in the configuration object to represent a range in the dataset, and `filelist` and `treesnentries` would be used to construct the TChain with which we would construct the RDF.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:724,security,configur,configuration,724,"> When reading trees/chains, I'm not sure the configuration object will allow anything more than what we can already do with TEntryLists? But that's the thing, with the configuration object we won't need to construct TEntryLists, at least not at the DistRDF level. This means we won't need localstarts and localends. > IIUC the redundant info is that globalend = globalstart + (localends - localstarts).sum() (i.e. we could avoid passing globalend). You can avoid passing `localstarts` and `localends`, since you can calculate them from `globalstart, globalend, filelist, treesnentries`. The latter is what I expect we would need if we use RDF with a configuration object: globalstart and globalend would be included in the configuration object to represent a range in the dataset, and `filelist` and `treesnentries` would be used to construct the TChain with which we would construct the RDF.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:127,energy efficiency,current,currently,127,"> you can calculate them from globalstart, globalend, filelist, treesnentries. yes if you pass all files and all tree entries. currently iirc we are only passing the file names and the tree entries for the files and trees that will be actually touched by the task. otoh i'm still thinking that `globalstart` and `globalend` could go away, what do we need them for? EDIT: ah we use them for the call to `SetCacheEntryRange`, and they are not really global but they are relative to the task-specific chain uhm....it feels like some simplification is possible, if they are relative to the task-specific chain can't they be calculated from the rest? EDIT 2: Ideally the call to `SetCacheEntryRange` should not be needed at all, `TTreeProcessorMT` and `TTreeReader` should take care of it...",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:489,interoperability,specif,specific,489,"> you can calculate them from globalstart, globalend, filelist, treesnentries. yes if you pass all files and all tree entries. currently iirc we are only passing the file names and the tree entries for the files and trees that will be actually touched by the task. otoh i'm still thinking that `globalstart` and `globalend` could go away, what do we need them for? EDIT: ah we use them for the call to `SetCacheEntryRange`, and they are not really global but they are relative to the task-specific chain uhm....it feels like some simplification is possible, if they are relative to the task-specific chain can't they be calculated from the rest? EDIT 2: Ideally the call to `SetCacheEntryRange` should not be needed at all, `TTreeProcessorMT` and `TTreeReader` should take care of it...",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:591,interoperability,specif,specific,591,"> you can calculate them from globalstart, globalend, filelist, treesnentries. yes if you pass all files and all tree entries. currently iirc we are only passing the file names and the tree entries for the files and trees that will be actually touched by the task. otoh i'm still thinking that `globalstart` and `globalend` could go away, what do we need them for? EDIT: ah we use them for the call to `SetCacheEntryRange`, and they are not really global but they are relative to the task-specific chain uhm....it feels like some simplification is possible, if they are relative to the task-specific chain can't they be calculated from the rest? EDIT 2: Ideally the call to `SetCacheEntryRange` should not be needed at all, `TTreeProcessorMT` and `TTreeReader` should take care of it...",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:530,testability,simpl,simplification,530,"> you can calculate them from globalstart, globalend, filelist, treesnentries. yes if you pass all files and all tree entries. currently iirc we are only passing the file names and the tree entries for the files and trees that will be actually touched by the task. otoh i'm still thinking that `globalstart` and `globalend` could go away, what do we need them for? EDIT: ah we use them for the call to `SetCacheEntryRange`, and they are not really global but they are relative to the task-specific chain uhm....it feels like some simplification is possible, if they are relative to the task-specific chain can't they be calculated from the rest? EDIT 2: Ideally the call to `SetCacheEntryRange` should not be needed at all, `TTreeProcessorMT` and `TTreeReader` should take care of it...",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:530,usability,simpl,simplification,530,"> you can calculate them from globalstart, globalend, filelist, treesnentries. yes if you pass all files and all tree entries. currently iirc we are only passing the file names and the tree entries for the files and trees that will be actually touched by the task. otoh i'm still thinking that `globalstart` and `globalend` could go away, what do we need them for? EDIT: ah we use them for the call to `SetCacheEntryRange`, and they are not really global but they are relative to the task-specific chain uhm....it feels like some simplification is possible, if they are relative to the task-specific chain can't they be calculated from the rest? EDIT 2: Ideally the call to `SetCacheEntryRange` should not be needed at all, `TTreeProcessorMT` and `TTreeReader` should take care of it...",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:124,interoperability,specif,specific,124,"> EDIT: ah we use them for the call to SetCacheEntryRange, and they are not really global but they are relative to the task-specific chain uhm....it feels like some simplification is possible, if they are relative to the task-specific chain can't they be calculated from the rest? Yes exactly, they are not global global, but global to the local partial chain that the task needs to process. That's why I say you just strictly need `globalstart, globalend, filelist, treesnentries`.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:226,interoperability,specif,specific,226,"> EDIT: ah we use them for the call to SetCacheEntryRange, and they are not really global but they are relative to the task-specific chain uhm....it feels like some simplification is possible, if they are relative to the task-specific chain can't they be calculated from the rest? Yes exactly, they are not global global, but global to the local partial chain that the task needs to process. That's why I say you just strictly need `globalstart, globalend, filelist, treesnentries`.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:165,testability,simpl,simplification,165,"> EDIT: ah we use them for the call to SetCacheEntryRange, and they are not really global but they are relative to the task-specific chain uhm....it feels like some simplification is possible, if they are relative to the task-specific chain can't they be calculated from the rest? Yes exactly, they are not global global, but global to the local partial chain that the task needs to process. That's why I say you just strictly need `globalstart, globalend, filelist, treesnentries`.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:165,usability,simpl,simplification,165,"> EDIT: ah we use them for the call to SetCacheEntryRange, and they are not really global but they are relative to the task-specific chain uhm....it feels like some simplification is possible, if they are relative to the task-specific chain can't they be calculated from the rest? Yes exactly, they are not global global, but global to the local partial chain that the task needs to process. That's why I say you just strictly need `globalstart, globalend, filelist, treesnentries`.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:10,energy efficiency,green,green,10,"Tests are green, I'm merging. Thanks a lot for your help @etejedor @eguiraud :smile: !",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:0,safety,Test,Tests,0,"Tests are green, I'm merging. Thanks a lot for your help @etejedor @eguiraud :smile: !",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:0,testability,Test,Tests,0,"Tests are green, I'm merging. Thanks a lot for your help @etejedor @eguiraud :smile: !",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/pull/8646:52,usability,help,help,52,"Tests are green, I'm merging. Thanks a lot for your help @etejedor @eguiraud :smile: !",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8646
https://github.com/root-project/root/issues/8647:43,reliability,doe,doesn,43,"> I was using myHtml->Write(...). OK, this doesn't work for me. And you should not try. As I said, all the GUI classes are not meant to be saved in ROOT files.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8647
https://github.com/root-project/root/pull/8654:98,usability,user,user-images,98,This should look something like the following:. ![Bildschirmfoto vom 2021-07-09 17-44-31](https://user-images.githubusercontent.com/1613332/125422186-10b29e67-e003-4654-80c6-5491accbade1.png),MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8654
https://github.com/root-project/root/pull/8654:239,performance,time,time,239,This is great! My only comment is that I would run this every day on all closed issues instead of only when the issue is closed (which will also cause a lot of false positives because often issues are auto-closed on PR mergers giving zero time to devs to add the issue to a fixed project),MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8654
https://github.com/root-project/root/pull/8654:73,usability,close,closed,73,This is great! My only comment is that I would run this every day on all closed issues instead of only when the issue is closed (which will also cause a lot of false positives because often issues are auto-closed on PR mergers giving zero time to devs to add the issue to a fixed project),MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8654
https://github.com/root-project/root/pull/8654:121,usability,close,closed,121,This is great! My only comment is that I would run this every day on all closed issues instead of only when the issue is closed (which will also cause a lot of false positives because often issues are auto-closed on PR mergers giving zero time to devs to add the issue to a fixed project),MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8654
https://github.com/root-project/root/pull/8654:206,usability,close,closed,206,This is great! My only comment is that I would run this every day on all closed issues instead of only when the issue is closed (which will also cause a lot of false positives because often issues are auto-closed on PR mergers giving zero time to devs to add the issue to a fixed project),MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8654
https://github.com/root-project/root/pull/8654:21,testability,understand,understanding,21,"Okay, that wasn't my understanding of what we want, but certainly possible",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8654
https://github.com/root-project/root/issues/8655:43,deployability,instal,installed,43,"The headers of a builtin nlohmann/json are installed at `${CMAKE_INSTALL_PREFIX}/include/nlohmann`, which (as far as installing third-party stuff goes) is probably ok? . Or you would rather have ROOT install those headers at `${CMAKE_INSTALL_PREFIX}/include/ROOT_builtins/nlohmann`? If this was the case, projects building against ROOT would not find the builtins anymore, at least not without extra care.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8655
https://github.com/root-project/root/issues/8655:117,deployability,instal,installing,117,"The headers of a builtin nlohmann/json are installed at `${CMAKE_INSTALL_PREFIX}/include/nlohmann`, which (as far as installing third-party stuff goes) is probably ok? . Or you would rather have ROOT install those headers at `${CMAKE_INSTALL_PREFIX}/include/ROOT_builtins/nlohmann`? If this was the case, projects building against ROOT would not find the builtins anymore, at least not without extra care.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8655
https://github.com/root-project/root/issues/8655:200,deployability,instal,install,200,"The headers of a builtin nlohmann/json are installed at `${CMAKE_INSTALL_PREFIX}/include/nlohmann`, which (as far as installing third-party stuff goes) is probably ok? . Or you would rather have ROOT install those headers at `${CMAKE_INSTALL_PREFIX}/include/ROOT_builtins/nlohmann`? If this was the case, projects building against ROOT would not find the builtins anymore, at least not without extra care.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8655
https://github.com/root-project/root/issues/8655:314,deployability,build,building,314,"The headers of a builtin nlohmann/json are installed at `${CMAKE_INSTALL_PREFIX}/include/nlohmann`, which (as far as installing third-party stuff goes) is probably ok? . Or you would rather have ROOT install those headers at `${CMAKE_INSTALL_PREFIX}/include/ROOT_builtins/nlohmann`? If this was the case, projects building against ROOT would not find the builtins anymore, at least not without extra care.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8655
https://github.com/root-project/root/issues/8655:68,deployability,instal,installed,68,"I guess the question is whether ROOT should interfere with what you installed in your system. Let's say you have something installed in `/usr/include/nlohmann`. Now you install ROOT, and for some reason it came with a builtin nlohmann_json (e.g. because of a different version). Now, everything in your system that was relying on the original nlohman_json version is broken. I thought that as far as reasonably possible, ROOT should not interfere with what you install in your system (i.e. it should leave all headers and libraries untouched), unless they are ROOT's own headers/libraries? You are correct that projects building against that installed ROOT would need to know about the include location, but I would think that this is ROOT's job (by exporting CMake targets such as ROOT::nlohmann). Maybe that's what users expect of ROOT, but I was at least surprised to have stuff installed that I didn't ask for. Let me rephrase this, maybe:. You install X from source. Would you expect that this also installs Y, and possibly overwrites / corrupts any existing Ys?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8655
https://github.com/root-project/root/issues/8655:123,deployability,instal,installed,123,"I guess the question is whether ROOT should interfere with what you installed in your system. Let's say you have something installed in `/usr/include/nlohmann`. Now you install ROOT, and for some reason it came with a builtin nlohmann_json (e.g. because of a different version). Now, everything in your system that was relying on the original nlohman_json version is broken. I thought that as far as reasonably possible, ROOT should not interfere with what you install in your system (i.e. it should leave all headers and libraries untouched), unless they are ROOT's own headers/libraries? You are correct that projects building against that installed ROOT would need to know about the include location, but I would think that this is ROOT's job (by exporting CMake targets such as ROOT::nlohmann). Maybe that's what users expect of ROOT, but I was at least surprised to have stuff installed that I didn't ask for. Let me rephrase this, maybe:. You install X from source. Would you expect that this also installs Y, and possibly overwrites / corrupts any existing Ys?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8655
https://github.com/root-project/root/issues/8655:169,deployability,instal,install,169,"I guess the question is whether ROOT should interfere with what you installed in your system. Let's say you have something installed in `/usr/include/nlohmann`. Now you install ROOT, and for some reason it came with a builtin nlohmann_json (e.g. because of a different version). Now, everything in your system that was relying on the original nlohman_json version is broken. I thought that as far as reasonably possible, ROOT should not interfere with what you install in your system (i.e. it should leave all headers and libraries untouched), unless they are ROOT's own headers/libraries? You are correct that projects building against that installed ROOT would need to know about the include location, but I would think that this is ROOT's job (by exporting CMake targets such as ROOT::nlohmann). Maybe that's what users expect of ROOT, but I was at least surprised to have stuff installed that I didn't ask for. Let me rephrase this, maybe:. You install X from source. Would you expect that this also installs Y, and possibly overwrites / corrupts any existing Ys?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8655
https://github.com/root-project/root/issues/8655:269,deployability,version,version,269,"I guess the question is whether ROOT should interfere with what you installed in your system. Let's say you have something installed in `/usr/include/nlohmann`. Now you install ROOT, and for some reason it came with a builtin nlohmann_json (e.g. because of a different version). Now, everything in your system that was relying on the original nlohman_json version is broken. I thought that as far as reasonably possible, ROOT should not interfere with what you install in your system (i.e. it should leave all headers and libraries untouched), unless they are ROOT's own headers/libraries? You are correct that projects building against that installed ROOT would need to know about the include location, but I would think that this is ROOT's job (by exporting CMake targets such as ROOT::nlohmann). Maybe that's what users expect of ROOT, but I was at least surprised to have stuff installed that I didn't ask for. Let me rephrase this, maybe:. You install X from source. Would you expect that this also installs Y, and possibly overwrites / corrupts any existing Ys?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8655
https://github.com/root-project/root/issues/8655:356,deployability,version,version,356,"I guess the question is whether ROOT should interfere with what you installed in your system. Let's say you have something installed in `/usr/include/nlohmann`. Now you install ROOT, and for some reason it came with a builtin nlohmann_json (e.g. because of a different version). Now, everything in your system that was relying on the original nlohman_json version is broken. I thought that as far as reasonably possible, ROOT should not interfere with what you install in your system (i.e. it should leave all headers and libraries untouched), unless they are ROOT's own headers/libraries? You are correct that projects building against that installed ROOT would need to know about the include location, but I would think that this is ROOT's job (by exporting CMake targets such as ROOT::nlohmann). Maybe that's what users expect of ROOT, but I was at least surprised to have stuff installed that I didn't ask for. Let me rephrase this, maybe:. You install X from source. Would you expect that this also installs Y, and possibly overwrites / corrupts any existing Ys?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8655
https://github.com/root-project/root/issues/8655:461,deployability,instal,install,461,"I guess the question is whether ROOT should interfere with what you installed in your system. Let's say you have something installed in `/usr/include/nlohmann`. Now you install ROOT, and for some reason it came with a builtin nlohmann_json (e.g. because of a different version). Now, everything in your system that was relying on the original nlohman_json version is broken. I thought that as far as reasonably possible, ROOT should not interfere with what you install in your system (i.e. it should leave all headers and libraries untouched), unless they are ROOT's own headers/libraries? You are correct that projects building against that installed ROOT would need to know about the include location, but I would think that this is ROOT's job (by exporting CMake targets such as ROOT::nlohmann). Maybe that's what users expect of ROOT, but I was at least surprised to have stuff installed that I didn't ask for. Let me rephrase this, maybe:. You install X from source. Would you expect that this also installs Y, and possibly overwrites / corrupts any existing Ys?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8655
https://github.com/root-project/root/issues/8655:620,deployability,build,building,620,"I guess the question is whether ROOT should interfere with what you installed in your system. Let's say you have something installed in `/usr/include/nlohmann`. Now you install ROOT, and for some reason it came with a builtin nlohmann_json (e.g. because of a different version). Now, everything in your system that was relying on the original nlohman_json version is broken. I thought that as far as reasonably possible, ROOT should not interfere with what you install in your system (i.e. it should leave all headers and libraries untouched), unless they are ROOT's own headers/libraries? You are correct that projects building against that installed ROOT would need to know about the include location, but I would think that this is ROOT's job (by exporting CMake targets such as ROOT::nlohmann). Maybe that's what users expect of ROOT, but I was at least surprised to have stuff installed that I didn't ask for. Let me rephrase this, maybe:. You install X from source. Would you expect that this also installs Y, and possibly overwrites / corrupts any existing Ys?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8655
https://github.com/root-project/root/issues/8655:642,deployability,instal,installed,642,"I guess the question is whether ROOT should interfere with what you installed in your system. Let's say you have something installed in `/usr/include/nlohmann`. Now you install ROOT, and for some reason it came with a builtin nlohmann_json (e.g. because of a different version). Now, everything in your system that was relying on the original nlohman_json version is broken. I thought that as far as reasonably possible, ROOT should not interfere with what you install in your system (i.e. it should leave all headers and libraries untouched), unless they are ROOT's own headers/libraries? You are correct that projects building against that installed ROOT would need to know about the include location, but I would think that this is ROOT's job (by exporting CMake targets such as ROOT::nlohmann). Maybe that's what users expect of ROOT, but I was at least surprised to have stuff installed that I didn't ask for. Let me rephrase this, maybe:. You install X from source. Would you expect that this also installs Y, and possibly overwrites / corrupts any existing Ys?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8655
https://github.com/root-project/root/issues/8655:882,deployability,instal,installed,882,"I guess the question is whether ROOT should interfere with what you installed in your system. Let's say you have something installed in `/usr/include/nlohmann`. Now you install ROOT, and for some reason it came with a builtin nlohmann_json (e.g. because of a different version). Now, everything in your system that was relying on the original nlohman_json version is broken. I thought that as far as reasonably possible, ROOT should not interfere with what you install in your system (i.e. it should leave all headers and libraries untouched), unless they are ROOT's own headers/libraries? You are correct that projects building against that installed ROOT would need to know about the include location, but I would think that this is ROOT's job (by exporting CMake targets such as ROOT::nlohmann). Maybe that's what users expect of ROOT, but I was at least surprised to have stuff installed that I didn't ask for. Let me rephrase this, maybe:. You install X from source. Would you expect that this also installs Y, and possibly overwrites / corrupts any existing Ys?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8655
https://github.com/root-project/root/issues/8655:949,deployability,instal,install,949,"I guess the question is whether ROOT should interfere with what you installed in your system. Let's say you have something installed in `/usr/include/nlohmann`. Now you install ROOT, and for some reason it came with a builtin nlohmann_json (e.g. because of a different version). Now, everything in your system that was relying on the original nlohman_json version is broken. I thought that as far as reasonably possible, ROOT should not interfere with what you install in your system (i.e. it should leave all headers and libraries untouched), unless they are ROOT's own headers/libraries? You are correct that projects building against that installed ROOT would need to know about the include location, but I would think that this is ROOT's job (by exporting CMake targets such as ROOT::nlohmann). Maybe that's what users expect of ROOT, but I was at least surprised to have stuff installed that I didn't ask for. Let me rephrase this, maybe:. You install X from source. Would you expect that this also installs Y, and possibly overwrites / corrupts any existing Ys?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8655
https://github.com/root-project/root/issues/8655:1004,deployability,instal,installs,1004,"I guess the question is whether ROOT should interfere with what you installed in your system. Let's say you have something installed in `/usr/include/nlohmann`. Now you install ROOT, and for some reason it came with a builtin nlohmann_json (e.g. because of a different version). Now, everything in your system that was relying on the original nlohman_json version is broken. I thought that as far as reasonably possible, ROOT should not interfere with what you install in your system (i.e. it should leave all headers and libraries untouched), unless they are ROOT's own headers/libraries? You are correct that projects building against that installed ROOT would need to know about the include location, but I would think that this is ROOT's job (by exporting CMake targets such as ROOT::nlohmann). Maybe that's what users expect of ROOT, but I was at least surprised to have stuff installed that I didn't ask for. Let me rephrase this, maybe:. You install X from source. Would you expect that this also installs Y, and possibly overwrites / corrupts any existing Ys?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8655
https://github.com/root-project/root/issues/8655:269,integrability,version,version,269,"I guess the question is whether ROOT should interfere with what you installed in your system. Let's say you have something installed in `/usr/include/nlohmann`. Now you install ROOT, and for some reason it came with a builtin nlohmann_json (e.g. because of a different version). Now, everything in your system that was relying on the original nlohman_json version is broken. I thought that as far as reasonably possible, ROOT should not interfere with what you install in your system (i.e. it should leave all headers and libraries untouched), unless they are ROOT's own headers/libraries? You are correct that projects building against that installed ROOT would need to know about the include location, but I would think that this is ROOT's job (by exporting CMake targets such as ROOT::nlohmann). Maybe that's what users expect of ROOT, but I was at least surprised to have stuff installed that I didn't ask for. Let me rephrase this, maybe:. You install X from source. Would you expect that this also installs Y, and possibly overwrites / corrupts any existing Ys?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8655
https://github.com/root-project/root/issues/8655:356,integrability,version,version,356,"I guess the question is whether ROOT should interfere with what you installed in your system. Let's say you have something installed in `/usr/include/nlohmann`. Now you install ROOT, and for some reason it came with a builtin nlohmann_json (e.g. because of a different version). Now, everything in your system that was relying on the original nlohman_json version is broken. I thought that as far as reasonably possible, ROOT should not interfere with what you install in your system (i.e. it should leave all headers and libraries untouched), unless they are ROOT's own headers/libraries? You are correct that projects building against that installed ROOT would need to know about the include location, but I would think that this is ROOT's job (by exporting CMake targets such as ROOT::nlohmann). Maybe that's what users expect of ROOT, but I was at least surprised to have stuff installed that I didn't ask for. Let me rephrase this, maybe:. You install X from source. Would you expect that this also installs Y, and possibly overwrites / corrupts any existing Ys?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8655
https://github.com/root-project/root/issues/8655:269,modifiability,version,version,269,"I guess the question is whether ROOT should interfere with what you installed in your system. Let's say you have something installed in `/usr/include/nlohmann`. Now you install ROOT, and for some reason it came with a builtin nlohmann_json (e.g. because of a different version). Now, everything in your system that was relying on the original nlohman_json version is broken. I thought that as far as reasonably possible, ROOT should not interfere with what you install in your system (i.e. it should leave all headers and libraries untouched), unless they are ROOT's own headers/libraries? You are correct that projects building against that installed ROOT would need to know about the include location, but I would think that this is ROOT's job (by exporting CMake targets such as ROOT::nlohmann). Maybe that's what users expect of ROOT, but I was at least surprised to have stuff installed that I didn't ask for. Let me rephrase this, maybe:. You install X from source. Would you expect that this also installs Y, and possibly overwrites / corrupts any existing Ys?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8655
https://github.com/root-project/root/issues/8655:356,modifiability,version,version,356,"I guess the question is whether ROOT should interfere with what you installed in your system. Let's say you have something installed in `/usr/include/nlohmann`. Now you install ROOT, and for some reason it came with a builtin nlohmann_json (e.g. because of a different version). Now, everything in your system that was relying on the original nlohman_json version is broken. I thought that as far as reasonably possible, ROOT should not interfere with what you install in your system (i.e. it should leave all headers and libraries untouched), unless they are ROOT's own headers/libraries? You are correct that projects building against that installed ROOT would need to know about the include location, but I would think that this is ROOT's job (by exporting CMake targets such as ROOT::nlohmann). Maybe that's what users expect of ROOT, but I was at least surprised to have stuff installed that I didn't ask for. Let me rephrase this, maybe:. You install X from source. Would you expect that this also installs Y, and possibly overwrites / corrupts any existing Ys?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8655
https://github.com/root-project/root/issues/8655:817,usability,user,users,817,"I guess the question is whether ROOT should interfere with what you installed in your system. Let's say you have something installed in `/usr/include/nlohmann`. Now you install ROOT, and for some reason it came with a builtin nlohmann_json (e.g. because of a different version). Now, everything in your system that was relying on the original nlohman_json version is broken. I thought that as far as reasonably possible, ROOT should not interfere with what you install in your system (i.e. it should leave all headers and libraries untouched), unless they are ROOT's own headers/libraries? You are correct that projects building against that installed ROOT would need to know about the include location, but I would think that this is ROOT's job (by exporting CMake targets such as ROOT::nlohmann). Maybe that's what users expect of ROOT, but I was at least surprised to have stuff installed that I didn't ask for. Let me rephrase this, maybe:. You install X from source. Would you expect that this also installs Y, and possibly overwrites / corrupts any existing Ys?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8655
https://github.com/root-project/root/pull/8656:11,deployability,build,build,11,@phsft-bot build,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8656
https://github.com/root-project/root/pull/8656:11,deployability,build,build,11,@phsft-bot build,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8656
https://github.com/root-project/root/pull/8656:13,deployability,fail,fails,13,I think test fails because of https://github.com/root-project/root/commit/f68ed04667d7c2a4c99ecd44aea1f37252004f82. Maybe the cmake version can be change to the one from Ubuntu snap?,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8656
https://github.com/root-project/root/pull/8656:132,deployability,version,version,132,I think test fails because of https://github.com/root-project/root/commit/f68ed04667d7c2a4c99ecd44aea1f37252004f82. Maybe the cmake version can be change to the one from Ubuntu snap?,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8656
https://github.com/root-project/root/pull/8656:132,integrability,version,version,132,I think test fails because of https://github.com/root-project/root/commit/f68ed04667d7c2a4c99ecd44aea1f37252004f82. Maybe the cmake version can be change to the one from Ubuntu snap?,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8656
https://github.com/root-project/root/pull/8656:132,modifiability,version,version,132,I think test fails because of https://github.com/root-project/root/commit/f68ed04667d7c2a4c99ecd44aea1f37252004f82. Maybe the cmake version can be change to the one from Ubuntu snap?,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8656
https://github.com/root-project/root/pull/8656:13,reliability,fail,fails,13,I think test fails because of https://github.com/root-project/root/commit/f68ed04667d7c2a4c99ecd44aea1f37252004f82. Maybe the cmake version can be change to the one from Ubuntu snap?,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8656
https://github.com/root-project/root/pull/8656:8,safety,test,test,8,I think test fails because of https://github.com/root-project/root/commit/f68ed04667d7c2a4c99ecd44aea1f37252004f82. Maybe the cmake version can be change to the one from Ubuntu snap?,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8656
https://github.com/root-project/root/pull/8656:8,testability,test,test,8,I think test fails because of https://github.com/root-project/root/commit/f68ed04667d7c2a4c99ecd44aea1f37252004f82. Maybe the cmake version can be change to the one from Ubuntu snap?,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8656
https://github.com/root-project/root/pull/8657:4,availability,failur,failure,4,The failure is unrelated. Merging,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8657
https://github.com/root-project/root/pull/8657:4,deployability,fail,failure,4,The failure is unrelated. Merging,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8657
https://github.com/root-project/root/pull/8657:4,performance,failur,failure,4,The failure is unrelated. Merging,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8657
https://github.com/root-project/root/pull/8657:4,reliability,fail,failure,4,The failure is unrelated. Merging,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8657
https://github.com/root-project/root/pull/8658:45,deployability,pipelin,pipelines,45,> I wish we could replace our Jenkins Groovy pipelines with more code of this style :-). I guess in principle we could: https://docs.github.com/en/actions/hosting-your-own-runners/about-self-hosted-runners,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8658
https://github.com/root-project/root/pull/8658:45,integrability,pipelin,pipelines,45,> I wish we could replace our Jenkins Groovy pipelines with more code of this style :-). I guess in principle we could: https://docs.github.com/en/actions/hosting-your-own-runners/about-self-hosted-runners,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8658
https://github.com/root-project/root/pull/8658:200,deployability,resourc,resources,200,"> I guess in principle we could: https://docs.github.com/en/actions/hosting-your-own-runners/about-self-hosted-runners. Yeah this wasn't a ""I wish the technology / ... existed"" but ""I wish we had the resources to..."" kind of comment :-)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8658
https://github.com/root-project/root/pull/8658:200,energy efficiency,resourc,resources,200,"> I guess in principle we could: https://docs.github.com/en/actions/hosting-your-own-runners/about-self-hosted-runners. Yeah this wasn't a ""I wish the technology / ... existed"" but ""I wish we had the resources to..."" kind of comment :-)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8658
https://github.com/root-project/root/pull/8658:200,performance,resourc,resources,200,"> I guess in principle we could: https://docs.github.com/en/actions/hosting-your-own-runners/about-self-hosted-runners. Yeah this wasn't a ""I wish the technology / ... existed"" but ""I wish we had the resources to..."" kind of comment :-)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8658
https://github.com/root-project/root/pull/8658:200,safety,resourc,resources,200,"> I guess in principle we could: https://docs.github.com/en/actions/hosting-your-own-runners/about-self-hosted-runners. Yeah this wasn't a ""I wish the technology / ... existed"" but ""I wish we had the resources to..."" kind of comment :-)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8658
https://github.com/root-project/root/pull/8658:200,testability,resourc,resources,200,"> I guess in principle we could: https://docs.github.com/en/actions/hosting-your-own-runners/about-self-hosted-runners. Yeah this wasn't a ""I wish the technology / ... existed"" but ""I wish we had the resources to..."" kind of comment :-)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8658
https://github.com/root-project/root/pull/8658:769,availability,servic,service,769,"<img src=""https://www.deepcode.ai/icons/green_check.svg"" width= ""50px"" align= ""left""/> Congratulations :tada:. DeepCode [analyzed](https://www.deepcode.ai/app/gh/root-project/root/9b5892decf3ae940ef4b5ef2bba7aca585b7a8d9/root-project/root/c7a05db3d71f3640428ca3bc8ceefc2890d72784/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) your code in 3.542 seconds and we found no issues. Enjoy a moment of no bugs :sunny:. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/9b5892decf3ae940ef4b5ef2bba7aca585b7a8d9/root-project/root/c7a05db3d71f3640428ca3bc8ceefc2890d72784/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8658
https://github.com/root-project/root/pull/8658:769,deployability,servic,service,769,"<img src=""https://www.deepcode.ai/icons/green_check.svg"" width= ""50px"" align= ""left""/> Congratulations :tada:. DeepCode [analyzed](https://www.deepcode.ai/app/gh/root-project/root/9b5892decf3ae940ef4b5ef2bba7aca585b7a8d9/root-project/root/c7a05db3d71f3640428ca3bc8ceefc2890d72784/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) your code in 3.542 seconds and we found no issues. Enjoy a moment of no bugs :sunny:. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/9b5892decf3ae940ef4b5ef2bba7aca585b7a8d9/root-project/root/c7a05db3d71f3640428ca3bc8ceefc2890d72784/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8658
https://github.com/root-project/root/pull/8658:781,deployability,API,API,781,"<img src=""https://www.deepcode.ai/icons/green_check.svg"" width= ""50px"" align= ""left""/> Congratulations :tada:. DeepCode [analyzed](https://www.deepcode.ai/app/gh/root-project/root/9b5892decf3ae940ef4b5ef2bba7aca585b7a8d9/root-project/root/c7a05db3d71f3640428ca3bc8ceefc2890d72784/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) your code in 3.542 seconds and we found no issues. Enjoy a moment of no bugs :sunny:. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/9b5892decf3ae940ef4b5ef2bba7aca585b7a8d9/root-project/root/c7a05db3d71f3640428ca3bc8ceefc2890d72784/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8658
https://github.com/root-project/root/pull/8658:769,integrability,servic,service,769,"<img src=""https://www.deepcode.ai/icons/green_check.svg"" width= ""50px"" align= ""left""/> Congratulations :tada:. DeepCode [analyzed](https://www.deepcode.ai/app/gh/root-project/root/9b5892decf3ae940ef4b5ef2bba7aca585b7a8d9/root-project/root/c7a05db3d71f3640428ca3bc8ceefc2890d72784/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) your code in 3.542 seconds and we found no issues. Enjoy a moment of no bugs :sunny:. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/9b5892decf3ae940ef4b5ef2bba7aca585b7a8d9/root-project/root/c7a05db3d71f3640428ca3bc8ceefc2890d72784/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8658
https://github.com/root-project/root/pull/8658:781,integrability,API,API,781,"<img src=""https://www.deepcode.ai/icons/green_check.svg"" width= ""50px"" align= ""left""/> Congratulations :tada:. DeepCode [analyzed](https://www.deepcode.ai/app/gh/root-project/root/9b5892decf3ae940ef4b5ef2bba7aca585b7a8d9/root-project/root/c7a05db3d71f3640428ca3bc8ceefc2890d72784/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) your code in 3.542 seconds and we found no issues. Enjoy a moment of no bugs :sunny:. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/9b5892decf3ae940ef4b5ef2bba7aca585b7a8d9/root-project/root/c7a05db3d71f3640428ca3bc8ceefc2890d72784/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8658
https://github.com/root-project/root/pull/8658:781,interoperability,API,API,781,"<img src=""https://www.deepcode.ai/icons/green_check.svg"" width= ""50px"" align= ""left""/> Congratulations :tada:. DeepCode [analyzed](https://www.deepcode.ai/app/gh/root-project/root/9b5892decf3ae940ef4b5ef2bba7aca585b7a8d9/root-project/root/c7a05db3d71f3640428ca3bc8ceefc2890d72784/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) your code in 3.542 seconds and we found no issues. Enjoy a moment of no bugs :sunny:. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/9b5892decf3ae940ef4b5ef2bba7aca585b7a8d9/root-project/root/c7a05db3d71f3640428ca3bc8ceefc2890d72784/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8658
https://github.com/root-project/root/pull/8658:971,interoperability,plug,plugins,971,"<img src=""https://www.deepcode.ai/icons/green_check.svg"" width= ""50px"" align= ""left""/> Congratulations :tada:. DeepCode [analyzed](https://www.deepcode.ai/app/gh/root-project/root/9b5892decf3ae940ef4b5ef2bba7aca585b7a8d9/root-project/root/c7a05db3d71f3640428ca3bc8ceefc2890d72784/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) your code in 3.542 seconds and we found no issues. Enjoy a moment of no bugs :sunny:. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/9b5892decf3ae940ef4b5ef2bba7aca585b7a8d9/root-project/root/c7a05db3d71f3640428ca3bc8ceefc2890d72784/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8658
https://github.com/root-project/root/pull/8658:1043,interoperability,plug,plugin,1043,"<img src=""https://www.deepcode.ai/icons/green_check.svg"" width= ""50px"" align= ""left""/> Congratulations :tada:. DeepCode [analyzed](https://www.deepcode.ai/app/gh/root-project/root/9b5892decf3ae940ef4b5ef2bba7aca585b7a8d9/root-project/root/c7a05db3d71f3640428ca3bc8ceefc2890d72784/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) your code in 3.542 seconds and we found no issues. Enjoy a moment of no bugs :sunny:. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/9b5892decf3ae940ef4b5ef2bba7aca585b7a8d9/root-project/root/c7a05db3d71f3640428ca3bc8ceefc2890d72784/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8658
https://github.com/root-project/root/pull/8658:1094,interoperability,plug,plugin,1094,"<img src=""https://www.deepcode.ai/icons/green_check.svg"" width= ""50px"" align= ""left""/> Congratulations :tada:. DeepCode [analyzed](https://www.deepcode.ai/app/gh/root-project/root/9b5892decf3ae940ef4b5ef2bba7aca585b7a8d9/root-project/root/c7a05db3d71f3640428ca3bc8ceefc2890d72784/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) your code in 3.542 seconds and we found no issues. Enjoy a moment of no bugs :sunny:. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/9b5892decf3ae940ef4b5ef2bba7aca585b7a8d9/root-project/root/c7a05db3d71f3640428ca3bc8ceefc2890d72784/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8658
https://github.com/root-project/root/pull/8658:769,modifiability,servic,service,769,"<img src=""https://www.deepcode.ai/icons/green_check.svg"" width= ""50px"" align= ""left""/> Congratulations :tada:. DeepCode [analyzed](https://www.deepcode.ai/app/gh/root-project/root/9b5892decf3ae940ef4b5ef2bba7aca585b7a8d9/root-project/root/c7a05db3d71f3640428ca3bc8ceefc2890d72784/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) your code in 3.542 seconds and we found no issues. Enjoy a moment of no bugs :sunny:. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/9b5892decf3ae940ef4b5ef2bba7aca585b7a8d9/root-project/root/c7a05db3d71f3640428ca3bc8ceefc2890d72784/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8658
https://github.com/root-project/root/pull/8658:118,modifiability,paramet,parameter,118,"Ha, the bots indeed catched an issue (pun intended) where the code was using `issue.number` even though I renamed the parameter to `issue_number` and this only worked because of weird Javascript rules!",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8658
https://github.com/root-project/root/pull/8660:512,availability,error,error,512,"The latest commit reintroduces the `Option_t*` parameter in `TChain::SetEntryList` and adds another possible value for it `""sync""`. With this new option, we can support the usecase described in this PR while still relying on the preexisting default behaviour. The option assumes that there is a 1:1 mapping between the trees in the chain and the sub lists in the entry list. This assumption is verified in two ways:. 1. The number of sub lists is equal to the number of trees in the chain. If not, the following error is thrown. ```. terminate called after throwing an instance of 'std::runtime_error'. what(): In 'TChain::SetEntryList': the number of sub entry lists in the input TEntryList (1) is not equal to the number of files in the chain (2). Aborted (core dumped). ```. 2. That each sub list corresponds to the correct treename, filename at the same index in the chain. If not, the following error is thrown:. ```. terminate called after throwing an instance of 'std::runtime_error'. what(): In 'TChain::SetEntryList': the sub entry list at index 1 doesn't correspond to treename 'entries' and filename 'file_20entries_1.root': it has treename 'entries' and filename 'file_20entries_2.root'. Aborted (core dumped). ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8660
https://github.com/root-project/root/pull/8660:900,availability,error,error,900,"The latest commit reintroduces the `Option_t*` parameter in `TChain::SetEntryList` and adds another possible value for it `""sync""`. With this new option, we can support the usecase described in this PR while still relying on the preexisting default behaviour. The option assumes that there is a 1:1 mapping between the trees in the chain and the sub lists in the entry list. This assumption is verified in two ways:. 1. The number of sub lists is equal to the number of trees in the chain. If not, the following error is thrown. ```. terminate called after throwing an instance of 'std::runtime_error'. what(): In 'TChain::SetEntryList': the number of sub entry lists in the input TEntryList (1) is not equal to the number of files in the chain (2). Aborted (core dumped). ```. 2. That each sub list corresponds to the correct treename, filename at the same index in the chain. If not, the following error is thrown:. ```. terminate called after throwing an instance of 'std::runtime_error'. what(): In 'TChain::SetEntryList': the sub entry list at index 1 doesn't correspond to treename 'entries' and filename 'file_20entries_1.root': it has treename 'entries' and filename 'file_20entries_2.root'. Aborted (core dumped). ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8660
https://github.com/root-project/root/pull/8660:759,energy efficiency,core,core,759,"The latest commit reintroduces the `Option_t*` parameter in `TChain::SetEntryList` and adds another possible value for it `""sync""`. With this new option, we can support the usecase described in this PR while still relying on the preexisting default behaviour. The option assumes that there is a 1:1 mapping between the trees in the chain and the sub lists in the entry list. This assumption is verified in two ways:. 1. The number of sub lists is equal to the number of trees in the chain. If not, the following error is thrown. ```. terminate called after throwing an instance of 'std::runtime_error'. what(): In 'TChain::SetEntryList': the number of sub entry lists in the input TEntryList (1) is not equal to the number of files in the chain (2). Aborted (core dumped). ```. 2. That each sub list corresponds to the correct treename, filename at the same index in the chain. If not, the following error is thrown:. ```. terminate called after throwing an instance of 'std::runtime_error'. what(): In 'TChain::SetEntryList': the sub entry list at index 1 doesn't correspond to treename 'entries' and filename 'file_20entries_1.root': it has treename 'entries' and filename 'file_20entries_2.root'. Aborted (core dumped). ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8660
https://github.com/root-project/root/pull/8660:1209,energy efficiency,core,core,1209,"The latest commit reintroduces the `Option_t*` parameter in `TChain::SetEntryList` and adds another possible value for it `""sync""`. With this new option, we can support the usecase described in this PR while still relying on the preexisting default behaviour. The option assumes that there is a 1:1 mapping between the trees in the chain and the sub lists in the entry list. This assumption is verified in two ways:. 1. The number of sub lists is equal to the number of trees in the chain. If not, the following error is thrown. ```. terminate called after throwing an instance of 'std::runtime_error'. what(): In 'TChain::SetEntryList': the number of sub entry lists in the input TEntryList (1) is not equal to the number of files in the chain (2). Aborted (core dumped). ```. 2. That each sub list corresponds to the correct treename, filename at the same index in the chain. If not, the following error is thrown:. ```. terminate called after throwing an instance of 'std::runtime_error'. what(): In 'TChain::SetEntryList': the sub entry list at index 1 doesn't correspond to treename 'entries' and filename 'file_20entries_1.root': it has treename 'entries' and filename 'file_20entries_2.root'. Aborted (core dumped). ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8660
https://github.com/root-project/root/pull/8660:346,integrability,sub,sub,346,"The latest commit reintroduces the `Option_t*` parameter in `TChain::SetEntryList` and adds another possible value for it `""sync""`. With this new option, we can support the usecase described in this PR while still relying on the preexisting default behaviour. The option assumes that there is a 1:1 mapping between the trees in the chain and the sub lists in the entry list. This assumption is verified in two ways:. 1. The number of sub lists is equal to the number of trees in the chain. If not, the following error is thrown. ```. terminate called after throwing an instance of 'std::runtime_error'. what(): In 'TChain::SetEntryList': the number of sub entry lists in the input TEntryList (1) is not equal to the number of files in the chain (2). Aborted (core dumped). ```. 2. That each sub list corresponds to the correct treename, filename at the same index in the chain. If not, the following error is thrown:. ```. terminate called after throwing an instance of 'std::runtime_error'. what(): In 'TChain::SetEntryList': the sub entry list at index 1 doesn't correspond to treename 'entries' and filename 'file_20entries_1.root': it has treename 'entries' and filename 'file_20entries_2.root'. Aborted (core dumped). ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8660
https://github.com/root-project/root/pull/8660:434,integrability,sub,sub,434,"The latest commit reintroduces the `Option_t*` parameter in `TChain::SetEntryList` and adds another possible value for it `""sync""`. With this new option, we can support the usecase described in this PR while still relying on the preexisting default behaviour. The option assumes that there is a 1:1 mapping between the trees in the chain and the sub lists in the entry list. This assumption is verified in two ways:. 1. The number of sub lists is equal to the number of trees in the chain. If not, the following error is thrown. ```. terminate called after throwing an instance of 'std::runtime_error'. what(): In 'TChain::SetEntryList': the number of sub entry lists in the input TEntryList (1) is not equal to the number of files in the chain (2). Aborted (core dumped). ```. 2. That each sub list corresponds to the correct treename, filename at the same index in the chain. If not, the following error is thrown:. ```. terminate called after throwing an instance of 'std::runtime_error'. what(): In 'TChain::SetEntryList': the sub entry list at index 1 doesn't correspond to treename 'entries' and filename 'file_20entries_1.root': it has treename 'entries' and filename 'file_20entries_2.root'. Aborted (core dumped). ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8660
https://github.com/root-project/root/pull/8660:652,integrability,sub,sub,652,"The latest commit reintroduces the `Option_t*` parameter in `TChain::SetEntryList` and adds another possible value for it `""sync""`. With this new option, we can support the usecase described in this PR while still relying on the preexisting default behaviour. The option assumes that there is a 1:1 mapping between the trees in the chain and the sub lists in the entry list. This assumption is verified in two ways:. 1. The number of sub lists is equal to the number of trees in the chain. If not, the following error is thrown. ```. terminate called after throwing an instance of 'std::runtime_error'. what(): In 'TChain::SetEntryList': the number of sub entry lists in the input TEntryList (1) is not equal to the number of files in the chain (2). Aborted (core dumped). ```. 2. That each sub list corresponds to the correct treename, filename at the same index in the chain. If not, the following error is thrown:. ```. terminate called after throwing an instance of 'std::runtime_error'. what(): In 'TChain::SetEntryList': the sub entry list at index 1 doesn't correspond to treename 'entries' and filename 'file_20entries_1.root': it has treename 'entries' and filename 'file_20entries_2.root'. Aborted (core dumped). ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8660
https://github.com/root-project/root/pull/8660:791,integrability,sub,sub,791,"The latest commit reintroduces the `Option_t*` parameter in `TChain::SetEntryList` and adds another possible value for it `""sync""`. With this new option, we can support the usecase described in this PR while still relying on the preexisting default behaviour. The option assumes that there is a 1:1 mapping between the trees in the chain and the sub lists in the entry list. This assumption is verified in two ways:. 1. The number of sub lists is equal to the number of trees in the chain. If not, the following error is thrown. ```. terminate called after throwing an instance of 'std::runtime_error'. what(): In 'TChain::SetEntryList': the number of sub entry lists in the input TEntryList (1) is not equal to the number of files in the chain (2). Aborted (core dumped). ```. 2. That each sub list corresponds to the correct treename, filename at the same index in the chain. If not, the following error is thrown:. ```. terminate called after throwing an instance of 'std::runtime_error'. what(): In 'TChain::SetEntryList': the sub entry list at index 1 doesn't correspond to treename 'entries' and filename 'file_20entries_1.root': it has treename 'entries' and filename 'file_20entries_2.root'. Aborted (core dumped). ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8660
https://github.com/root-project/root/pull/8660:1031,integrability,sub,sub,1031,"The latest commit reintroduces the `Option_t*` parameter in `TChain::SetEntryList` and adds another possible value for it `""sync""`. With this new option, we can support the usecase described in this PR while still relying on the preexisting default behaviour. The option assumes that there is a 1:1 mapping between the trees in the chain and the sub lists in the entry list. This assumption is verified in two ways:. 1. The number of sub lists is equal to the number of trees in the chain. If not, the following error is thrown. ```. terminate called after throwing an instance of 'std::runtime_error'. what(): In 'TChain::SetEntryList': the number of sub entry lists in the input TEntryList (1) is not equal to the number of files in the chain (2). Aborted (core dumped). ```. 2. That each sub list corresponds to the correct treename, filename at the same index in the chain. If not, the following error is thrown:. ```. terminate called after throwing an instance of 'std::runtime_error'. what(): In 'TChain::SetEntryList': the sub entry list at index 1 doesn't correspond to treename 'entries' and filename 'file_20entries_1.root': it has treename 'entries' and filename 'file_20entries_2.root'. Aborted (core dumped). ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8660
https://github.com/root-project/root/pull/8660:47,modifiability,paramet,parameter,47,"The latest commit reintroduces the `Option_t*` parameter in `TChain::SetEntryList` and adds another possible value for it `""sync""`. With this new option, we can support the usecase described in this PR while still relying on the preexisting default behaviour. The option assumes that there is a 1:1 mapping between the trees in the chain and the sub lists in the entry list. This assumption is verified in two ways:. 1. The number of sub lists is equal to the number of trees in the chain. If not, the following error is thrown. ```. terminate called after throwing an instance of 'std::runtime_error'. what(): In 'TChain::SetEntryList': the number of sub entry lists in the input TEntryList (1) is not equal to the number of files in the chain (2). Aborted (core dumped). ```. 2. That each sub list corresponds to the correct treename, filename at the same index in the chain. If not, the following error is thrown:. ```. terminate called after throwing an instance of 'std::runtime_error'. what(): In 'TChain::SetEntryList': the sub entry list at index 1 doesn't correspond to treename 'entries' and filename 'file_20entries_1.root': it has treename 'entries' and filename 'file_20entries_2.root'. Aborted (core dumped). ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8660
https://github.com/root-project/root/pull/8660:512,performance,error,error,512,"The latest commit reintroduces the `Option_t*` parameter in `TChain::SetEntryList` and adds another possible value for it `""sync""`. With this new option, we can support the usecase described in this PR while still relying on the preexisting default behaviour. The option assumes that there is a 1:1 mapping between the trees in the chain and the sub lists in the entry list. This assumption is verified in two ways:. 1. The number of sub lists is equal to the number of trees in the chain. If not, the following error is thrown. ```. terminate called after throwing an instance of 'std::runtime_error'. what(): In 'TChain::SetEntryList': the number of sub entry lists in the input TEntryList (1) is not equal to the number of files in the chain (2). Aborted (core dumped). ```. 2. That each sub list corresponds to the correct treename, filename at the same index in the chain. If not, the following error is thrown:. ```. terminate called after throwing an instance of 'std::runtime_error'. what(): In 'TChain::SetEntryList': the sub entry list at index 1 doesn't correspond to treename 'entries' and filename 'file_20entries_1.root': it has treename 'entries' and filename 'file_20entries_2.root'. Aborted (core dumped). ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8660
https://github.com/root-project/root/pull/8660:900,performance,error,error,900,"The latest commit reintroduces the `Option_t*` parameter in `TChain::SetEntryList` and adds another possible value for it `""sync""`. With this new option, we can support the usecase described in this PR while still relying on the preexisting default behaviour. The option assumes that there is a 1:1 mapping between the trees in the chain and the sub lists in the entry list. This assumption is verified in two ways:. 1. The number of sub lists is equal to the number of trees in the chain. If not, the following error is thrown. ```. terminate called after throwing an instance of 'std::runtime_error'. what(): In 'TChain::SetEntryList': the number of sub entry lists in the input TEntryList (1) is not equal to the number of files in the chain (2). Aborted (core dumped). ```. 2. That each sub list corresponds to the correct treename, filename at the same index in the chain. If not, the following error is thrown:. ```. terminate called after throwing an instance of 'std::runtime_error'. what(): In 'TChain::SetEntryList': the sub entry list at index 1 doesn't correspond to treename 'entries' and filename 'file_20entries_1.root': it has treename 'entries' and filename 'file_20entries_2.root'. Aborted (core dumped). ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8660
https://github.com/root-project/root/pull/8660:1057,reliability,doe,doesn,1057,"The latest commit reintroduces the `Option_t*` parameter in `TChain::SetEntryList` and adds another possible value for it `""sync""`. With this new option, we can support the usecase described in this PR while still relying on the preexisting default behaviour. The option assumes that there is a 1:1 mapping between the trees in the chain and the sub lists in the entry list. This assumption is verified in two ways:. 1. The number of sub lists is equal to the number of trees in the chain. If not, the following error is thrown. ```. terminate called after throwing an instance of 'std::runtime_error'. what(): In 'TChain::SetEntryList': the number of sub entry lists in the input TEntryList (1) is not equal to the number of files in the chain (2). Aborted (core dumped). ```. 2. That each sub list corresponds to the correct treename, filename at the same index in the chain. If not, the following error is thrown:. ```. terminate called after throwing an instance of 'std::runtime_error'. what(): In 'TChain::SetEntryList': the sub entry list at index 1 doesn't correspond to treename 'entries' and filename 'file_20entries_1.root': it has treename 'entries' and filename 'file_20entries_2.root'. Aborted (core dumped). ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8660
https://github.com/root-project/root/pull/8660:512,safety,error,error,512,"The latest commit reintroduces the `Option_t*` parameter in `TChain::SetEntryList` and adds another possible value for it `""sync""`. With this new option, we can support the usecase described in this PR while still relying on the preexisting default behaviour. The option assumes that there is a 1:1 mapping between the trees in the chain and the sub lists in the entry list. This assumption is verified in two ways:. 1. The number of sub lists is equal to the number of trees in the chain. If not, the following error is thrown. ```. terminate called after throwing an instance of 'std::runtime_error'. what(): In 'TChain::SetEntryList': the number of sub entry lists in the input TEntryList (1) is not equal to the number of files in the chain (2). Aborted (core dumped). ```. 2. That each sub list corresponds to the correct treename, filename at the same index in the chain. If not, the following error is thrown:. ```. terminate called after throwing an instance of 'std::runtime_error'. what(): In 'TChain::SetEntryList': the sub entry list at index 1 doesn't correspond to treename 'entries' and filename 'file_20entries_1.root': it has treename 'entries' and filename 'file_20entries_2.root'. Aborted (core dumped). ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8660
https://github.com/root-project/root/pull/8660:675,safety,input,input,675,"The latest commit reintroduces the `Option_t*` parameter in `TChain::SetEntryList` and adds another possible value for it `""sync""`. With this new option, we can support the usecase described in this PR while still relying on the preexisting default behaviour. The option assumes that there is a 1:1 mapping between the trees in the chain and the sub lists in the entry list. This assumption is verified in two ways:. 1. The number of sub lists is equal to the number of trees in the chain. If not, the following error is thrown. ```. terminate called after throwing an instance of 'std::runtime_error'. what(): In 'TChain::SetEntryList': the number of sub entry lists in the input TEntryList (1) is not equal to the number of files in the chain (2). Aborted (core dumped). ```. 2. That each sub list corresponds to the correct treename, filename at the same index in the chain. If not, the following error is thrown:. ```. terminate called after throwing an instance of 'std::runtime_error'. what(): In 'TChain::SetEntryList': the sub entry list at index 1 doesn't correspond to treename 'entries' and filename 'file_20entries_1.root': it has treename 'entries' and filename 'file_20entries_2.root'. Aborted (core dumped). ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8660
https://github.com/root-project/root/pull/8660:900,safety,error,error,900,"The latest commit reintroduces the `Option_t*` parameter in `TChain::SetEntryList` and adds another possible value for it `""sync""`. With this new option, we can support the usecase described in this PR while still relying on the preexisting default behaviour. The option assumes that there is a 1:1 mapping between the trees in the chain and the sub lists in the entry list. This assumption is verified in two ways:. 1. The number of sub lists is equal to the number of trees in the chain. If not, the following error is thrown. ```. terminate called after throwing an instance of 'std::runtime_error'. what(): In 'TChain::SetEntryList': the number of sub entry lists in the input TEntryList (1) is not equal to the number of files in the chain (2). Aborted (core dumped). ```. 2. That each sub list corresponds to the correct treename, filename at the same index in the chain. If not, the following error is thrown:. ```. terminate called after throwing an instance of 'std::runtime_error'. what(): In 'TChain::SetEntryList': the sub entry list at index 1 doesn't correspond to treename 'entries' and filename 'file_20entries_1.root': it has treename 'entries' and filename 'file_20entries_2.root'. Aborted (core dumped). ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8660
https://github.com/root-project/root/pull/8660:394,testability,verif,verified,394,"The latest commit reintroduces the `Option_t*` parameter in `TChain::SetEntryList` and adds another possible value for it `""sync""`. With this new option, we can support the usecase described in this PR while still relying on the preexisting default behaviour. The option assumes that there is a 1:1 mapping between the trees in the chain and the sub lists in the entry list. This assumption is verified in two ways:. 1. The number of sub lists is equal to the number of trees in the chain. If not, the following error is thrown. ```. terminate called after throwing an instance of 'std::runtime_error'. what(): In 'TChain::SetEntryList': the number of sub entry lists in the input TEntryList (1) is not equal to the number of files in the chain (2). Aborted (core dumped). ```. 2. That each sub list corresponds to the correct treename, filename at the same index in the chain. If not, the following error is thrown:. ```. terminate called after throwing an instance of 'std::runtime_error'. what(): In 'TChain::SetEntryList': the sub entry list at index 1 doesn't correspond to treename 'entries' and filename 'file_20entries_1.root': it has treename 'entries' and filename 'file_20entries_2.root'. Aborted (core dumped). ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8660
https://github.com/root-project/root/pull/8660:161,usability,support,support,161,"The latest commit reintroduces the `Option_t*` parameter in `TChain::SetEntryList` and adds another possible value for it `""sync""`. With this new option, we can support the usecase described in this PR while still relying on the preexisting default behaviour. The option assumes that there is a 1:1 mapping between the trees in the chain and the sub lists in the entry list. This assumption is verified in two ways:. 1. The number of sub lists is equal to the number of trees in the chain. If not, the following error is thrown. ```. terminate called after throwing an instance of 'std::runtime_error'. what(): In 'TChain::SetEntryList': the number of sub entry lists in the input TEntryList (1) is not equal to the number of files in the chain (2). Aborted (core dumped). ```. 2. That each sub list corresponds to the correct treename, filename at the same index in the chain. If not, the following error is thrown:. ```. terminate called after throwing an instance of 'std::runtime_error'. what(): In 'TChain::SetEntryList': the sub entry list at index 1 doesn't correspond to treename 'entries' and filename 'file_20entries_1.root': it has treename 'entries' and filename 'file_20entries_2.root'. Aborted (core dumped). ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8660
https://github.com/root-project/root/pull/8660:249,usability,behavi,behaviour,249,"The latest commit reintroduces the `Option_t*` parameter in `TChain::SetEntryList` and adds another possible value for it `""sync""`. With this new option, we can support the usecase described in this PR while still relying on the preexisting default behaviour. The option assumes that there is a 1:1 mapping between the trees in the chain and the sub lists in the entry list. This assumption is verified in two ways:. 1. The number of sub lists is equal to the number of trees in the chain. If not, the following error is thrown. ```. terminate called after throwing an instance of 'std::runtime_error'. what(): In 'TChain::SetEntryList': the number of sub entry lists in the input TEntryList (1) is not equal to the number of files in the chain (2). Aborted (core dumped). ```. 2. That each sub list corresponds to the correct treename, filename at the same index in the chain. If not, the following error is thrown:. ```. terminate called after throwing an instance of 'std::runtime_error'. what(): In 'TChain::SetEntryList': the sub entry list at index 1 doesn't correspond to treename 'entries' and filename 'file_20entries_1.root': it has treename 'entries' and filename 'file_20entries_2.root'. Aborted (core dumped). ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8660
https://github.com/root-project/root/pull/8660:512,usability,error,error,512,"The latest commit reintroduces the `Option_t*` parameter in `TChain::SetEntryList` and adds another possible value for it `""sync""`. With this new option, we can support the usecase described in this PR while still relying on the preexisting default behaviour. The option assumes that there is a 1:1 mapping between the trees in the chain and the sub lists in the entry list. This assumption is verified in two ways:. 1. The number of sub lists is equal to the number of trees in the chain. If not, the following error is thrown. ```. terminate called after throwing an instance of 'std::runtime_error'. what(): In 'TChain::SetEntryList': the number of sub entry lists in the input TEntryList (1) is not equal to the number of files in the chain (2). Aborted (core dumped). ```. 2. That each sub list corresponds to the correct treename, filename at the same index in the chain. If not, the following error is thrown:. ```. terminate called after throwing an instance of 'std::runtime_error'. what(): In 'TChain::SetEntryList': the sub entry list at index 1 doesn't correspond to treename 'entries' and filename 'file_20entries_1.root': it has treename 'entries' and filename 'file_20entries_2.root'. Aborted (core dumped). ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8660
https://github.com/root-project/root/pull/8660:675,usability,input,input,675,"The latest commit reintroduces the `Option_t*` parameter in `TChain::SetEntryList` and adds another possible value for it `""sync""`. With this new option, we can support the usecase described in this PR while still relying on the preexisting default behaviour. The option assumes that there is a 1:1 mapping between the trees in the chain and the sub lists in the entry list. This assumption is verified in two ways:. 1. The number of sub lists is equal to the number of trees in the chain. If not, the following error is thrown. ```. terminate called after throwing an instance of 'std::runtime_error'. what(): In 'TChain::SetEntryList': the number of sub entry lists in the input TEntryList (1) is not equal to the number of files in the chain (2). Aborted (core dumped). ```. 2. That each sub list corresponds to the correct treename, filename at the same index in the chain. If not, the following error is thrown:. ```. terminate called after throwing an instance of 'std::runtime_error'. what(): In 'TChain::SetEntryList': the sub entry list at index 1 doesn't correspond to treename 'entries' and filename 'file_20entries_1.root': it has treename 'entries' and filename 'file_20entries_2.root'. Aborted (core dumped). ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8660
https://github.com/root-project/root/pull/8660:900,usability,error,error,900,"The latest commit reintroduces the `Option_t*` parameter in `TChain::SetEntryList` and adds another possible value for it `""sync""`. With this new option, we can support the usecase described in this PR while still relying on the preexisting default behaviour. The option assumes that there is a 1:1 mapping between the trees in the chain and the sub lists in the entry list. This assumption is verified in two ways:. 1. The number of sub lists is equal to the number of trees in the chain. If not, the following error is thrown. ```. terminate called after throwing an instance of 'std::runtime_error'. what(): In 'TChain::SetEntryList': the number of sub entry lists in the input TEntryList (1) is not equal to the number of files in the chain (2). Aborted (core dumped). ```. 2. That each sub list corresponds to the correct treename, filename at the same index in the chain. If not, the following error is thrown:. ```. terminate called after throwing an instance of 'std::runtime_error'. what(): In 'TChain::SetEntryList': the sub entry list at index 1 doesn't correspond to treename 'entries' and filename 'file_20entries_1.root': it has treename 'entries' and filename 'file_20entries_2.root'. Aborted (core dumped). ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8660
https://github.com/root-project/root/pull/8660:31,usability,support,support,31,"> With this new option, we can support the usecase described in this PR while still relying on the preexisting default behaviour. The new scheme indeed seems like a reasonable solution.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8660
https://github.com/root-project/root/pull/8660:119,usability,behavi,behaviour,119,"> With this new option, we can support the usecase described in this PR while still relying on the preexisting default behaviour. The new scheme indeed seems like a reasonable solution.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8660
https://github.com/root-project/root/pull/8660:95,safety,test,tests,95,> The new scheme indeed seems like a reasonable solution. Thanks Philippe! I've uploaded a few tests. I'd like to have also @eguiraud 's opinion,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8660
https://github.com/root-project/root/pull/8660:95,testability,test,tests,95,> The new scheme indeed seems like a reasonable solution. Thanks Philippe! I've uploaded a few tests. I'd like to have also @eguiraud 's opinion,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8660
https://github.com/root-project/root/pull/8660:153,integrability,sub,sublists,153,As per our discussion:. - [x] add an `\attention` or similar to `TEntryList::Add`'s docs to tell users to use `AddSublist` instead if they want multiple sublists for the same treename and filename,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8660
https://github.com/root-project/root/pull/8660:97,usability,user,users,97,As per our discussion:. - [x] add an `\attention` or similar to `TEntryList::Add`'s docs to tell users to use `AddSublist` instead if they want multiple sublists for the same treename and filename,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8660
https://github.com/root-project/root/pull/8660:60,energy efficiency,green,green,60,I squashed and rebased the commits. Will merge if tests are green :+1:,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8660
https://github.com/root-project/root/pull/8660:50,safety,test,tests,50,I squashed and rebased the commits. Will merge if tests are green :+1:,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8660
https://github.com/root-project/root/pull/8660:50,testability,test,tests,50,I squashed and rebased the commits. Will merge if tests are green :+1:,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8660
https://github.com/root-project/root/pull/8661:38,performance,perform,performing,38,Restarted all tests to see how we are performing...,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8661
https://github.com/root-project/root/pull/8661:14,safety,test,tests,14,Restarted all tests to see how we are performing...,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8661
https://github.com/root-project/root/pull/8661:14,testability,test,tests,14,Restarted all tests to see how we are performing...,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8661
https://github.com/root-project/root/pull/8661:38,usability,perform,performing,38,Restarted all tests to see how we are performing...,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8661
https://github.com/root-project/root/pull/8663:51,integrability,messag,message,51,"@ferdymercury, could you please improve the commit message before I can merge this?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8663
https://github.com/root-project/root/pull/8663:51,interoperability,messag,message,51,"@ferdymercury, could you please improve the commit message before I can merge this?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8663
https://github.com/root-project/root/issues/8665:243,integrability,pub,public,243,"The code below works fine for me on Windows:. ```. //______________________________________________________________________________. //. //. //______________________________________________________________________________. class MyMainFrame : public TGMainFrame {. public:. MyMainFrame(const TGWindow *p, UInt_t w, UInt_t h);. virtual ~MyMainFrame() { Cleanup(); }. void CloseWindow() { delete this; }. Bool_t HandleKey(Event_t *event);. . ClassDef(MyMainFrame, 0). };. //______________________________________________________________________________. MyMainFrame::MyMainFrame(const TGWindow *p, UInt_t w, UInt_t h) : TGMainFrame(p, w, h). {. gVirtualX->GrabKey(GetId(), gVirtualX->KeysymToKeycode(kKey_c), kKeyControlMask, kTRUE);. MapSubwindows();. Layout();. MapWindow();. Resize(150,100);. }. //______________________________________________________________________________. Bool_t MyMainFrame::HandleKey(Event_t *event). {. char tmp[2];. UInt_t keysym;. gVirtualX->LookupString(event, tmp, sizeof(tmp), keysym);. if (event->fType == kGKeyPress) {. if (event->fState & kKeyControlMask) { // Cntrl key modifier pressed. switch ((EKeySym)keysym & ~0x20) { // treat upper and lower the same. case kKey_C:. std::cout << ""CTRL+C key pressed!"" << std::endl;. break;. default:. break;. }. }. }. return kTRUE;. }. //______________________________________________________________________________. void test_grab_key(). {. MyMainFrame *main = new MyMainFrame(gClient->GetRoot(), 150, 100);. }. ```. I'll try on Linux and let you know",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:265,integrability,pub,public,265,"The code below works fine for me on Windows:. ```. //______________________________________________________________________________. //. //. //______________________________________________________________________________. class MyMainFrame : public TGMainFrame {. public:. MyMainFrame(const TGWindow *p, UInt_t w, UInt_t h);. virtual ~MyMainFrame() { Cleanup(); }. void CloseWindow() { delete this; }. Bool_t HandleKey(Event_t *event);. . ClassDef(MyMainFrame, 0). };. //______________________________________________________________________________. MyMainFrame::MyMainFrame(const TGWindow *p, UInt_t w, UInt_t h) : TGMainFrame(p, w, h). {. gVirtualX->GrabKey(GetId(), gVirtualX->KeysymToKeycode(kKey_c), kKeyControlMask, kTRUE);. MapSubwindows();. Layout();. MapWindow();. Resize(150,100);. }. //______________________________________________________________________________. Bool_t MyMainFrame::HandleKey(Event_t *event). {. char tmp[2];. UInt_t keysym;. gVirtualX->LookupString(event, tmp, sizeof(tmp), keysym);. if (event->fType == kGKeyPress) {. if (event->fState & kKeyControlMask) { // Cntrl key modifier pressed. switch ((EKeySym)keysym & ~0x20) { // treat upper and lower the same. case kKey_C:. std::cout << ""CTRL+C key pressed!"" << std::endl;. break;. default:. break;. }. }. }. return kTRUE;. }. //______________________________________________________________________________. void test_grab_key(). {. MyMainFrame *main = new MyMainFrame(gClient->GetRoot(), 150, 100);. }. ```. I'll try on Linux and let you know",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:429,integrability,event,event,429,"The code below works fine for me on Windows:. ```. //______________________________________________________________________________. //. //. //______________________________________________________________________________. class MyMainFrame : public TGMainFrame {. public:. MyMainFrame(const TGWindow *p, UInt_t w, UInt_t h);. virtual ~MyMainFrame() { Cleanup(); }. void CloseWindow() { delete this; }. Bool_t HandleKey(Event_t *event);. . ClassDef(MyMainFrame, 0). };. //______________________________________________________________________________. MyMainFrame::MyMainFrame(const TGWindow *p, UInt_t w, UInt_t h) : TGMainFrame(p, w, h). {. gVirtualX->GrabKey(GetId(), gVirtualX->KeysymToKeycode(kKey_c), kKeyControlMask, kTRUE);. MapSubwindows();. Layout();. MapWindow();. Resize(150,100);. }. //______________________________________________________________________________. Bool_t MyMainFrame::HandleKey(Event_t *event). {. char tmp[2];. UInt_t keysym;. gVirtualX->LookupString(event, tmp, sizeof(tmp), keysym);. if (event->fType == kGKeyPress) {. if (event->fState & kKeyControlMask) { // Cntrl key modifier pressed. switch ((EKeySym)keysym & ~0x20) { // treat upper and lower the same. case kKey_C:. std::cout << ""CTRL+C key pressed!"" << std::endl;. break;. default:. break;. }. }. }. return kTRUE;. }. //______________________________________________________________________________. void test_grab_key(). {. MyMainFrame *main = new MyMainFrame(gClient->GetRoot(), 150, 100);. }. ```. I'll try on Linux and let you know",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:918,integrability,event,event,918,"The code below works fine for me on Windows:. ```. //______________________________________________________________________________. //. //. //______________________________________________________________________________. class MyMainFrame : public TGMainFrame {. public:. MyMainFrame(const TGWindow *p, UInt_t w, UInt_t h);. virtual ~MyMainFrame() { Cleanup(); }. void CloseWindow() { delete this; }. Bool_t HandleKey(Event_t *event);. . ClassDef(MyMainFrame, 0). };. //______________________________________________________________________________. MyMainFrame::MyMainFrame(const TGWindow *p, UInt_t w, UInt_t h) : TGMainFrame(p, w, h). {. gVirtualX->GrabKey(GetId(), gVirtualX->KeysymToKeycode(kKey_c), kKeyControlMask, kTRUE);. MapSubwindows();. Layout();. MapWindow();. Resize(150,100);. }. //______________________________________________________________________________. Bool_t MyMainFrame::HandleKey(Event_t *event). {. char tmp[2];. UInt_t keysym;. gVirtualX->LookupString(event, tmp, sizeof(tmp), keysym);. if (event->fType == kGKeyPress) {. if (event->fState & kKeyControlMask) { // Cntrl key modifier pressed. switch ((EKeySym)keysym & ~0x20) { // treat upper and lower the same. case kKey_C:. std::cout << ""CTRL+C key pressed!"" << std::endl;. break;. default:. break;. }. }. }. return kTRUE;. }. //______________________________________________________________________________. void test_grab_key(). {. MyMainFrame *main = new MyMainFrame(gClient->GetRoot(), 150, 100);. }. ```. I'll try on Linux and let you know",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:983,integrability,event,event,983,"The code below works fine for me on Windows:. ```. //______________________________________________________________________________. //. //. //______________________________________________________________________________. class MyMainFrame : public TGMainFrame {. public:. MyMainFrame(const TGWindow *p, UInt_t w, UInt_t h);. virtual ~MyMainFrame() { Cleanup(); }. void CloseWindow() { delete this; }. Bool_t HandleKey(Event_t *event);. . ClassDef(MyMainFrame, 0). };. //______________________________________________________________________________. MyMainFrame::MyMainFrame(const TGWindow *p, UInt_t w, UInt_t h) : TGMainFrame(p, w, h). {. gVirtualX->GrabKey(GetId(), gVirtualX->KeysymToKeycode(kKey_c), kKeyControlMask, kTRUE);. MapSubwindows();. Layout();. MapWindow();. Resize(150,100);. }. //______________________________________________________________________________. Bool_t MyMainFrame::HandleKey(Event_t *event). {. char tmp[2];. UInt_t keysym;. gVirtualX->LookupString(event, tmp, sizeof(tmp), keysym);. if (event->fType == kGKeyPress) {. if (event->fState & kKeyControlMask) { // Cntrl key modifier pressed. switch ((EKeySym)keysym & ~0x20) { // treat upper and lower the same. case kKey_C:. std::cout << ""CTRL+C key pressed!"" << std::endl;. break;. default:. break;. }. }. }. return kTRUE;. }. //______________________________________________________________________________. void test_grab_key(). {. MyMainFrame *main = new MyMainFrame(gClient->GetRoot(), 150, 100);. }. ```. I'll try on Linux and let you know",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:1022,integrability,event,event,1022,"The code below works fine for me on Windows:. ```. //______________________________________________________________________________. //. //. //______________________________________________________________________________. class MyMainFrame : public TGMainFrame {. public:. MyMainFrame(const TGWindow *p, UInt_t w, UInt_t h);. virtual ~MyMainFrame() { Cleanup(); }. void CloseWindow() { delete this; }. Bool_t HandleKey(Event_t *event);. . ClassDef(MyMainFrame, 0). };. //______________________________________________________________________________. MyMainFrame::MyMainFrame(const TGWindow *p, UInt_t w, UInt_t h) : TGMainFrame(p, w, h). {. gVirtualX->GrabKey(GetId(), gVirtualX->KeysymToKeycode(kKey_c), kKeyControlMask, kTRUE);. MapSubwindows();. Layout();. MapWindow();. Resize(150,100);. }. //______________________________________________________________________________. Bool_t MyMainFrame::HandleKey(Event_t *event). {. char tmp[2];. UInt_t keysym;. gVirtualX->LookupString(event, tmp, sizeof(tmp), keysym);. if (event->fType == kGKeyPress) {. if (event->fState & kKeyControlMask) { // Cntrl key modifier pressed. switch ((EKeySym)keysym & ~0x20) { // treat upper and lower the same. case kKey_C:. std::cout << ""CTRL+C key pressed!"" << std::endl;. break;. default:. break;. }. }. }. return kTRUE;. }. //______________________________________________________________________________. void test_grab_key(). {. MyMainFrame *main = new MyMainFrame(gClient->GetRoot(), 150, 100);. }. ```. I'll try on Linux and let you know",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:1057,integrability,event,event,1057,"The code below works fine for me on Windows:. ```. //______________________________________________________________________________. //. //. //______________________________________________________________________________. class MyMainFrame : public TGMainFrame {. public:. MyMainFrame(const TGWindow *p, UInt_t w, UInt_t h);. virtual ~MyMainFrame() { Cleanup(); }. void CloseWindow() { delete this; }. Bool_t HandleKey(Event_t *event);. . ClassDef(MyMainFrame, 0). };. //______________________________________________________________________________. MyMainFrame::MyMainFrame(const TGWindow *p, UInt_t w, UInt_t h) : TGMainFrame(p, w, h). {. gVirtualX->GrabKey(GetId(), gVirtualX->KeysymToKeycode(kKey_c), kKeyControlMask, kTRUE);. MapSubwindows();. Layout();. MapWindow();. Resize(150,100);. }. //______________________________________________________________________________. Bool_t MyMainFrame::HandleKey(Event_t *event). {. char tmp[2];. UInt_t keysym;. gVirtualX->LookupString(event, tmp, sizeof(tmp), keysym);. if (event->fType == kGKeyPress) {. if (event->fState & kKeyControlMask) { // Cntrl key modifier pressed. switch ((EKeySym)keysym & ~0x20) { // treat upper and lower the same. case kKey_C:. std::cout << ""CTRL+C key pressed!"" << std::endl;. break;. default:. break;. }. }. }. return kTRUE;. }. //______________________________________________________________________________. void test_grab_key(). {. MyMainFrame *main = new MyMainFrame(gClient->GetRoot(), 150, 100);. }. ```. I'll try on Linux and let you know",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:1105,security,modif,modifier,1105,"The code below works fine for me on Windows:. ```. //______________________________________________________________________________. //. //. //______________________________________________________________________________. class MyMainFrame : public TGMainFrame {. public:. MyMainFrame(const TGWindow *p, UInt_t w, UInt_t h);. virtual ~MyMainFrame() { Cleanup(); }. void CloseWindow() { delete this; }. Bool_t HandleKey(Event_t *event);. . ClassDef(MyMainFrame, 0). };. //______________________________________________________________________________. MyMainFrame::MyMainFrame(const TGWindow *p, UInt_t w, UInt_t h) : TGMainFrame(p, w, h). {. gVirtualX->GrabKey(GetId(), gVirtualX->KeysymToKeycode(kKey_c), kKeyControlMask, kTRUE);. MapSubwindows();. Layout();. MapWindow();. Resize(150,100);. }. //______________________________________________________________________________. Bool_t MyMainFrame::HandleKey(Event_t *event). {. char tmp[2];. UInt_t keysym;. gVirtualX->LookupString(event, tmp, sizeof(tmp), keysym);. if (event->fType == kGKeyPress) {. if (event->fState & kKeyControlMask) { // Cntrl key modifier pressed. switch ((EKeySym)keysym & ~0x20) { // treat upper and lower the same. case kKey_C:. std::cout << ""CTRL+C key pressed!"" << std::endl;. break;. default:. break;. }. }. }. return kTRUE;. }. //______________________________________________________________________________. void test_grab_key(). {. MyMainFrame *main = new MyMainFrame(gClient->GetRoot(), 150, 100);. }. ```. I'll try on Linux and let you know",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:371,usability,Close,CloseWindow,371,"The code below works fine for me on Windows:. ```. //______________________________________________________________________________. //. //. //______________________________________________________________________________. class MyMainFrame : public TGMainFrame {. public:. MyMainFrame(const TGWindow *p, UInt_t w, UInt_t h);. virtual ~MyMainFrame() { Cleanup(); }. void CloseWindow() { delete this; }. Bool_t HandleKey(Event_t *event);. . ClassDef(MyMainFrame, 0). };. //______________________________________________________________________________. MyMainFrame::MyMainFrame(const TGWindow *p, UInt_t w, UInt_t h) : TGMainFrame(p, w, h). {. gVirtualX->GrabKey(GetId(), gVirtualX->KeysymToKeycode(kKey_c), kKeyControlMask, kTRUE);. MapSubwindows();. Layout();. MapWindow();. Resize(150,100);. }. //______________________________________________________________________________. Bool_t MyMainFrame::HandleKey(Event_t *event). {. char tmp[2];. UInt_t keysym;. gVirtualX->LookupString(event, tmp, sizeof(tmp), keysym);. if (event->fType == kGKeyPress) {. if (event->fState & kKeyControlMask) { // Cntrl key modifier pressed. switch ((EKeySym)keysym & ~0x20) { // treat upper and lower the same. case kKey_C:. std::cout << ""CTRL+C key pressed!"" << std::endl;. break;. default:. break;. }. }. }. return kTRUE;. }. //______________________________________________________________________________. void test_grab_key(). {. MyMainFrame *main = new MyMainFrame(gClient->GetRoot(), 150, 100);. }. ```. I'll try on Linux and let you know",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:39,integrability,event,events,39,Maybe your system catches the CTRL+Key events somehow,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:27,reliability,doe,doesn,27,"And `gVirtualX->GrabKey()` doesn't do much:. ```. void TGX11::GrabKey(Window_t id, Int_t keycode, UInt_t modifier, Bool_t grab). {. UInt_t xmod;. MapModifierState(modifier, xmod);. if (grab). XGrabKey((Display*)fDisplay, keycode, xmod, (Window) id, True,. GrabModeAsync, GrabModeAsync);. else. XUngrabKey((Display*)fDisplay, keycode, xmod, (Window) id);. }. ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:105,security,modif,modifier,105,"And `gVirtualX->GrabKey()` doesn't do much:. ```. void TGX11::GrabKey(Window_t id, Int_t keycode, UInt_t modifier, Bool_t grab). {. UInt_t xmod;. MapModifierState(modifier, xmod);. if (grab). XGrabKey((Display*)fDisplay, keycode, xmod, (Window) id, True,. GrabModeAsync, GrabModeAsync);. else. XUngrabKey((Display*)fDisplay, keycode, xmod, (Window) id);. }. ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:163,security,modif,modifier,163,"And `gVirtualX->GrabKey()` doesn't do much:. ```. void TGX11::GrabKey(Window_t id, Int_t keycode, UInt_t modifier, Bool_t grab). {. UInt_t xmod;. MapModifierState(modifier, xmod);. if (grab). XGrabKey((Display*)fDisplay, keycode, xmod, (Window) id, True,. GrabModeAsync, GrabModeAsync);. else. XUngrabKey((Display*)fDisplay, keycode, xmod, (Window) id);. }. ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:38,integrability,messag,message,38,"Nope, I tried, only CTRL+C prints the message ""CTRL+C pressed"". Shift+C does not. `if (event->fState & kKeyControlMask)`. is evaluating to true only if Ctrl+C is pressed. But then it's a mistery why it does not work with GrabKey. Could you point at which function in `TUnixSystem` should I run the debugger to see what happens?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:87,integrability,event,event,87,"Nope, I tried, only CTRL+C prints the message ""CTRL+C pressed"". Shift+C does not. `if (event->fState & kKeyControlMask)`. is evaluating to true only if Ctrl+C is pressed. But then it's a mistery why it does not work with GrabKey. Could you point at which function in `TUnixSystem` should I run the debugger to see what happens?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:38,interoperability,messag,message,38,"Nope, I tried, only CTRL+C prints the message ""CTRL+C pressed"". Shift+C does not. `if (event->fState & kKeyControlMask)`. is evaluating to true only if Ctrl+C is pressed. But then it's a mistery why it does not work with GrabKey. Could you point at which function in `TUnixSystem` should I run the debugger to see what happens?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:72,reliability,doe,does,72,"Nope, I tried, only CTRL+C prints the message ""CTRL+C pressed"". Shift+C does not. `if (event->fState & kKeyControlMask)`. is evaluating to true only if Ctrl+C is pressed. But then it's a mistery why it does not work with GrabKey. Could you point at which function in `TUnixSystem` should I run the debugger to see what happens?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:202,reliability,doe,does,202,"Nope, I tried, only CTRL+C prints the message ""CTRL+C pressed"". Shift+C does not. `if (event->fState & kKeyControlMask)`. is evaluating to true only if Ctrl+C is pressed. But then it's a mistery why it does not work with GrabKey. Could you point at which function in `TUnixSystem` should I run the debugger to see what happens?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:49,integrability,event,event,49,"Well, obviously you have to comment out the `if (event->fState & kKeyControlMask)` line. And I just did, it's in `graf2d/x11/src/GX11Gui.cxx`",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:24,usability,help,help,24,"Thanks Bertrand for the help. Here is what I see from the debugger:. with kKeyControlMask. ![image](https://user-images.githubusercontent.com/10653970/125524664-42273c47-5c81-4412-b437-04c5288bb2e8.png). with kAnyModifierMask. ![image](https://user-images.githubusercontent.com/10653970/125524815-fb7dd292-cd64-4256-ad8f-410588aa9b91.png). (Side note: There is one additional GrabKey called by TGMainFrame itself, but I skipped it.):. ![image](https://user-images.githubusercontent.com/10653970/125524240-89770473-95ab-4916-9501-3b2162819a95.png). Side note 2: Playing around and halting the debugger, sometimes my keyboard key-repetition stops. gVirtualX calls in several places SetAutoRepeat(false)... Not sure if that has anything to do. I have then to reset it using `xset r on`.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:108,usability,user,user-images,108,"Thanks Bertrand for the help. Here is what I see from the debugger:. with kKeyControlMask. ![image](https://user-images.githubusercontent.com/10653970/125524664-42273c47-5c81-4412-b437-04c5288bb2e8.png). with kAnyModifierMask. ![image](https://user-images.githubusercontent.com/10653970/125524815-fb7dd292-cd64-4256-ad8f-410588aa9b91.png). (Side note: There is one additional GrabKey called by TGMainFrame itself, but I skipped it.):. ![image](https://user-images.githubusercontent.com/10653970/125524240-89770473-95ab-4916-9501-3b2162819a95.png). Side note 2: Playing around and halting the debugger, sometimes my keyboard key-repetition stops. gVirtualX calls in several places SetAutoRepeat(false)... Not sure if that has anything to do. I have then to reset it using `xset r on`.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:244,usability,user,user-images,244,"Thanks Bertrand for the help. Here is what I see from the debugger:. with kKeyControlMask. ![image](https://user-images.githubusercontent.com/10653970/125524664-42273c47-5c81-4412-b437-04c5288bb2e8.png). with kAnyModifierMask. ![image](https://user-images.githubusercontent.com/10653970/125524815-fb7dd292-cd64-4256-ad8f-410588aa9b91.png). (Side note: There is one additional GrabKey called by TGMainFrame itself, but I skipped it.):. ![image](https://user-images.githubusercontent.com/10653970/125524240-89770473-95ab-4916-9501-3b2162819a95.png). Side note 2: Playing around and halting the debugger, sometimes my keyboard key-repetition stops. gVirtualX calls in several places SetAutoRepeat(false)... Not sure if that has anything to do. I have then to reset it using `xset r on`.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:452,usability,user,user-images,452,"Thanks Bertrand for the help. Here is what I see from the debugger:. with kKeyControlMask. ![image](https://user-images.githubusercontent.com/10653970/125524664-42273c47-5c81-4412-b437-04c5288bb2e8.png). with kAnyModifierMask. ![image](https://user-images.githubusercontent.com/10653970/125524815-fb7dd292-cd64-4256-ad8f-410588aa9b91.png). (Side note: There is one additional GrabKey called by TGMainFrame itself, but I skipped it.):. ![image](https://user-images.githubusercontent.com/10653970/125524240-89770473-95ab-4916-9501-3b2162819a95.png). Side note 2: Playing around and halting the debugger, sometimes my keyboard key-repetition stops. gVirtualX calls in several places SetAutoRepeat(false)... Not sure if that has anything to do. I have then to reset it using `xset r on`.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:639,usability,stop,stops,639,"Thanks Bertrand for the help. Here is what I see from the debugger:. with kKeyControlMask. ![image](https://user-images.githubusercontent.com/10653970/125524664-42273c47-5c81-4412-b437-04c5288bb2e8.png). with kAnyModifierMask. ![image](https://user-images.githubusercontent.com/10653970/125524815-fb7dd292-cd64-4256-ad8f-410588aa9b91.png). (Side note: There is one additional GrabKey called by TGMainFrame itself, but I skipped it.):. ![image](https://user-images.githubusercontent.com/10653970/125524240-89770473-95ab-4916-9501-3b2162819a95.png). Side note 2: Playing around and halting the debugger, sometimes my keyboard key-repetition stops. gVirtualX calls in several places SetAutoRepeat(false)... Not sure if that has anything to do. I have then to reset it using `xset r on`.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:0,reliability,Doe,Does,0,"Does it work for you if you press CTRL+S in your test window? ![image](https://user-images.githubusercontent.com/10653970/125535842-ef2928b0-e337-4fa4-a523-b41adbab04da.png). (It's not working for me, either)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:49,safety,test,test,49,"Does it work for you if you press CTRL+S in your test window? ![image](https://user-images.githubusercontent.com/10653970/125535842-ef2928b0-e337-4fa4-a523-b41adbab04da.png). (It's not working for me, either)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:49,testability,test,test,49,"Does it work for you if you press CTRL+S in your test window? ![image](https://user-images.githubusercontent.com/10653970/125535842-ef2928b0-e337-4fa4-a523-b41adbab04da.png). (It's not working for me, either)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:79,usability,user,user-images,79,"Does it work for you if you press CTRL+S in your test window? ![image](https://user-images.githubusercontent.com/10653970/125535842-ef2928b0-e337-4fa4-a523-b41adbab04da.png). (It's not working for me, either)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:119,integrability,event,events,119,"So the debug sessions look fine, the values are correct. And in order to properly handle the default `TGMainFrame` key events, you must replace `return kTRUE;` by `return TGMainFrame::HandleKey(event);` in `Bool_t MyMainFrame::HandleKey(Event_t *event)`",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:194,integrability,event,event,194,"So the debug sessions look fine, the values are correct. And in order to properly handle the default `TGMainFrame` key events, you must replace `return kTRUE;` by `return TGMainFrame::HandleKey(event);` in `Bool_t MyMainFrame::HandleKey(Event_t *event)`",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:246,integrability,event,event,246,"So the debug sessions look fine, the values are correct. And in order to properly handle the default `TGMainFrame` key events, you must replace `return kTRUE;` by `return TGMainFrame::HandleKey(event);` in `Bool_t MyMainFrame::HandleKey(Event_t *event)`",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:13,security,session,sessions,13,"So the debug sessions look fine, the values are correct. And in order to properly handle the default `TGMainFrame` key events, you must replace `return kTRUE;` by `return TGMainFrame::HandleKey(event);` in `Bool_t MyMainFrame::HandleKey(Event_t *event)`",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:80,integrability,event,event,80,"Thanks for the suggestion. I have changed return kTRUE with:. ```. std::cout << event << "" "" << event->fType << "" "" << event->fState << std::endl;. return TGMainFrame::HandleKey(event);. ```. When I run it, pressing CTRL+C or pressing CTRL+S has no effect, nothing is printed. If I change kKeyControlMask to kAnyModifier, CTRL+C works, but CTRL+S does not work. ```. CTRL+C key pressed! 0x7ffdaf78cda0 0 20. 0x7ffdaf78cda0 1 20. ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:96,integrability,event,event,96,"Thanks for the suggestion. I have changed return kTRUE with:. ```. std::cout << event << "" "" << event->fType << "" "" << event->fState << std::endl;. return TGMainFrame::HandleKey(event);. ```. When I run it, pressing CTRL+C or pressing CTRL+S has no effect, nothing is printed. If I change kKeyControlMask to kAnyModifier, CTRL+C works, but CTRL+S does not work. ```. CTRL+C key pressed! 0x7ffdaf78cda0 0 20. 0x7ffdaf78cda0 1 20. ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:119,integrability,event,event,119,"Thanks for the suggestion. I have changed return kTRUE with:. ```. std::cout << event << "" "" << event->fType << "" "" << event->fState << std::endl;. return TGMainFrame::HandleKey(event);. ```. When I run it, pressing CTRL+C or pressing CTRL+S has no effect, nothing is printed. If I change kKeyControlMask to kAnyModifier, CTRL+C works, but CTRL+S does not work. ```. CTRL+C key pressed! 0x7ffdaf78cda0 0 20. 0x7ffdaf78cda0 1 20. ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:178,integrability,event,event,178,"Thanks for the suggestion. I have changed return kTRUE with:. ```. std::cout << event << "" "" << event->fType << "" "" << event->fState << std::endl;. return TGMainFrame::HandleKey(event);. ```. When I run it, pressing CTRL+C or pressing CTRL+S has no effect, nothing is printed. If I change kKeyControlMask to kAnyModifier, CTRL+C works, but CTRL+S does not work. ```. CTRL+C key pressed! 0x7ffdaf78cda0 0 20. 0x7ffdaf78cda0 1 20. ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:347,reliability,doe,does,347,"Thanks for the suggestion. I have changed return kTRUE with:. ```. std::cout << event << "" "" << event->fType << "" "" << event->fState << std::endl;. return TGMainFrame::HandleKey(event);. ```. When I run it, pressing CTRL+C or pressing CTRL+S has no effect, nothing is printed. If I change kKeyControlMask to kAnyModifier, CTRL+C works, but CTRL+S does not work. ```. CTRL+C key pressed! 0x7ffdaf78cda0 0 20. 0x7ffdaf78cda0 1 20. ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:62,deployability,releas,releases,62,"Thanks for the help. Maybe I can use myself https://www.x.org/releases/X11R7.5/doc/man/man1/xscope.1.html to try to find out ? I tried to printout stuff in TGX11 but when I press CTRL+C or CTRL+S, nothing literally happens, unix is waiting in the `select()` function.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:15,usability,help,help,15,"Thanks for the help. Maybe I can use myself https://www.x.org/releases/X11R7.5/doc/man/man1/xscope.1.html to try to find out ? I tried to printout stuff in TGX11 but when I press CTRL+C or CTRL+S, nothing literally happens, unix is waiting in the `select()` function.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:146,reliability,doe,doesnt-work,146,"It might be unrelated, but while trying to find a similar WM related issue, I found https://askubuntu.com/questions/1024597/ubuntu-16-04-ctrlalts-doesnt-work",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:64,deployability,releas,releases,64,"> Thanks for the help. Maybe I can use myself https://www.x.org/releases/X11R7.5/doc/man/man1/xscope.1.html to try to find out ? I tried to printout stuff in TGX11 but when I press CTRL+C or CTRL+S, nothing literally happens, unix is waiting in the `select()` function. I can't help, I'm working mostly on Windows",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:17,usability,help,help,17,"> Thanks for the help. Maybe I can use myself https://www.x.org/releases/X11R7.5/doc/man/man1/xscope.1.html to try to find out ? I tried to printout stuff in TGX11 but when I press CTRL+C or CTRL+S, nothing literally happens, unix is waiting in the `select()` function. I can't help, I'm working mostly on Windows",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:278,usability,help,help,278,"> Thanks for the help. Maybe I can use myself https://www.x.org/releases/X11R7.5/doc/man/man1/xscope.1.html to try to find out ? I tried to printout stuff in TGX11 but when I press CTRL+C or CTRL+S, nothing literally happens, unix is waiting in the `select()` function. I can't help, I'm working mostly on Windows",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:352,availability,state,state,352,"I used ""xtrace"", and I got messages printed when I use kAnyModifier, but none without it. ```. _root [1] 000:<:0549: 12: Request(102): ChangeKeyboardControl values={auto-repeat-mode=On(0x01)}. 000:>:0549: Event KeyPress(2) keycode=0x36 time=0x174cf013 root=0x00000185 event=0x07e000e7 child=None(0x00000000) root-x=939 root-y=542 event-x=54 event-y=38 state=Control,Mod2 same-screen=true(0x01). CTRL+C key pressed! 0x7ffdd1322ea0 0 20. 000:>:0549: Event KeyRelease(3) keycode=0x36 time=0x174cf0b3 root=0x00000185 event=0x07e000e7 child=None(0x00000000) root-x=939 root-y=542 event-x=54 event-y=38 state=Control,Mod2 same-screen=true(0x01). 0x7ffdd1322ea0 1 20_. ```. Now to the funny part. I added the following to the mask:. `kKeyShiftMask | kKeyLockMask | kKeyControlMask | kKeyMod1Mask | kKeyMod2Mask | kKeyMod3Mask | kKeyMod4Mask | kKeyMod5Mask | kButton1Mask | kButton2Mask | kButton3Mask | kButton4Mask | kButton5Mask | kButton6Mask | kButton7Mask`. and it still didn't work. Then I added `| kAnyModifier`, and it worked. Then I removed all of them and replace with just `kKeyControlMask`. And then it worked. So now it works perfectly as it should and lets me even more confused. It seems there is something fishy going on with X11 being let in an undefined state that somehow playing around was enough to 'reset it'.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:597,availability,state,state,597,"I used ""xtrace"", and I got messages printed when I use kAnyModifier, but none without it. ```. _root [1] 000:<:0549: 12: Request(102): ChangeKeyboardControl values={auto-repeat-mode=On(0x01)}. 000:>:0549: Event KeyPress(2) keycode=0x36 time=0x174cf013 root=0x00000185 event=0x07e000e7 child=None(0x00000000) root-x=939 root-y=542 event-x=54 event-y=38 state=Control,Mod2 same-screen=true(0x01). CTRL+C key pressed! 0x7ffdd1322ea0 0 20. 000:>:0549: Event KeyRelease(3) keycode=0x36 time=0x174cf0b3 root=0x00000185 event=0x07e000e7 child=None(0x00000000) root-x=939 root-y=542 event-x=54 event-y=38 state=Control,Mod2 same-screen=true(0x01). 0x7ffdd1322ea0 1 20_. ```. Now to the funny part. I added the following to the mask:. `kKeyShiftMask | kKeyLockMask | kKeyControlMask | kKeyMod1Mask | kKeyMod2Mask | kKeyMod3Mask | kKeyMod4Mask | kKeyMod5Mask | kButton1Mask | kButton2Mask | kButton3Mask | kButton4Mask | kButton5Mask | kButton6Mask | kButton7Mask`. and it still didn't work. Then I added `| kAnyModifier`, and it worked. Then I removed all of them and replace with just `kKeyControlMask`. And then it worked. So now it works perfectly as it should and lets me even more confused. It seems there is something fishy going on with X11 being let in an undefined state that somehow playing around was enough to 'reset it'.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:719,availability,mask,mask,719,"I used ""xtrace"", and I got messages printed when I use kAnyModifier, but none without it. ```. _root [1] 000:<:0549: 12: Request(102): ChangeKeyboardControl values={auto-repeat-mode=On(0x01)}. 000:>:0549: Event KeyPress(2) keycode=0x36 time=0x174cf013 root=0x00000185 event=0x07e000e7 child=None(0x00000000) root-x=939 root-y=542 event-x=54 event-y=38 state=Control,Mod2 same-screen=true(0x01). CTRL+C key pressed! 0x7ffdd1322ea0 0 20. 000:>:0549: Event KeyRelease(3) keycode=0x36 time=0x174cf0b3 root=0x00000185 event=0x07e000e7 child=None(0x00000000) root-x=939 root-y=542 event-x=54 event-y=38 state=Control,Mod2 same-screen=true(0x01). 0x7ffdd1322ea0 1 20_. ```. Now to the funny part. I added the following to the mask:. `kKeyShiftMask | kKeyLockMask | kKeyControlMask | kKeyMod1Mask | kKeyMod2Mask | kKeyMod3Mask | kKeyMod4Mask | kKeyMod5Mask | kButton1Mask | kButton2Mask | kButton3Mask | kButton4Mask | kButton5Mask | kButton6Mask | kButton7Mask`. and it still didn't work. Then I added `| kAnyModifier`, and it worked. Then I removed all of them and replace with just `kKeyControlMask`. And then it worked. So now it works perfectly as it should and lets me even more confused. It seems there is something fishy going on with X11 being let in an undefined state that somehow playing around was enough to 'reset it'.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:1265,availability,state,state,1265,"I used ""xtrace"", and I got messages printed when I use kAnyModifier, but none without it. ```. _root [1] 000:<:0549: 12: Request(102): ChangeKeyboardControl values={auto-repeat-mode=On(0x01)}. 000:>:0549: Event KeyPress(2) keycode=0x36 time=0x174cf013 root=0x00000185 event=0x07e000e7 child=None(0x00000000) root-x=939 root-y=542 event-x=54 event-y=38 state=Control,Mod2 same-screen=true(0x01). CTRL+C key pressed! 0x7ffdd1322ea0 0 20. 000:>:0549: Event KeyRelease(3) keycode=0x36 time=0x174cf0b3 root=0x00000185 event=0x07e000e7 child=None(0x00000000) root-x=939 root-y=542 event-x=54 event-y=38 state=Control,Mod2 same-screen=true(0x01). 0x7ffdd1322ea0 1 20_. ```. Now to the funny part. I added the following to the mask:. `kKeyShiftMask | kKeyLockMask | kKeyControlMask | kKeyMod1Mask | kKeyMod2Mask | kKeyMod3Mask | kKeyMod4Mask | kKeyMod5Mask | kButton1Mask | kButton2Mask | kButton3Mask | kButton4Mask | kButton5Mask | kButton6Mask | kButton7Mask`. and it still didn't work. Then I added `| kAnyModifier`, and it worked. Then I removed all of them and replace with just `kKeyControlMask`. And then it worked. So now it works perfectly as it should and lets me even more confused. It seems there is something fishy going on with X11 being let in an undefined state that somehow playing around was enough to 'reset it'.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:27,integrability,messag,messages,27,"I used ""xtrace"", and I got messages printed when I use kAnyModifier, but none without it. ```. _root [1] 000:<:0549: 12: Request(102): ChangeKeyboardControl values={auto-repeat-mode=On(0x01)}. 000:>:0549: Event KeyPress(2) keycode=0x36 time=0x174cf013 root=0x00000185 event=0x07e000e7 child=None(0x00000000) root-x=939 root-y=542 event-x=54 event-y=38 state=Control,Mod2 same-screen=true(0x01). CTRL+C key pressed! 0x7ffdd1322ea0 0 20. 000:>:0549: Event KeyRelease(3) keycode=0x36 time=0x174cf0b3 root=0x00000185 event=0x07e000e7 child=None(0x00000000) root-x=939 root-y=542 event-x=54 event-y=38 state=Control,Mod2 same-screen=true(0x01). 0x7ffdd1322ea0 1 20_. ```. Now to the funny part. I added the following to the mask:. `kKeyShiftMask | kKeyLockMask | kKeyControlMask | kKeyMod1Mask | kKeyMod2Mask | kKeyMod3Mask | kKeyMod4Mask | kKeyMod5Mask | kButton1Mask | kButton2Mask | kButton3Mask | kButton4Mask | kButton5Mask | kButton6Mask | kButton7Mask`. and it still didn't work. Then I added `| kAnyModifier`, and it worked. Then I removed all of them and replace with just `kKeyControlMask`. And then it worked. So now it works perfectly as it should and lets me even more confused. It seems there is something fishy going on with X11 being let in an undefined state that somehow playing around was enough to 'reset it'.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:205,integrability,Event,Event,205,"I used ""xtrace"", and I got messages printed when I use kAnyModifier, but none without it. ```. _root [1] 000:<:0549: 12: Request(102): ChangeKeyboardControl values={auto-repeat-mode=On(0x01)}. 000:>:0549: Event KeyPress(2) keycode=0x36 time=0x174cf013 root=0x00000185 event=0x07e000e7 child=None(0x00000000) root-x=939 root-y=542 event-x=54 event-y=38 state=Control,Mod2 same-screen=true(0x01). CTRL+C key pressed! 0x7ffdd1322ea0 0 20. 000:>:0549: Event KeyRelease(3) keycode=0x36 time=0x174cf0b3 root=0x00000185 event=0x07e000e7 child=None(0x00000000) root-x=939 root-y=542 event-x=54 event-y=38 state=Control,Mod2 same-screen=true(0x01). 0x7ffdd1322ea0 1 20_. ```. Now to the funny part. I added the following to the mask:. `kKeyShiftMask | kKeyLockMask | kKeyControlMask | kKeyMod1Mask | kKeyMod2Mask | kKeyMod3Mask | kKeyMod4Mask | kKeyMod5Mask | kButton1Mask | kButton2Mask | kButton3Mask | kButton4Mask | kButton5Mask | kButton6Mask | kButton7Mask`. and it still didn't work. Then I added `| kAnyModifier`, and it worked. Then I removed all of them and replace with just `kKeyControlMask`. And then it worked. So now it works perfectly as it should and lets me even more confused. It seems there is something fishy going on with X11 being let in an undefined state that somehow playing around was enough to 'reset it'.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:268,integrability,event,event,268,"I used ""xtrace"", and I got messages printed when I use kAnyModifier, but none without it. ```. _root [1] 000:<:0549: 12: Request(102): ChangeKeyboardControl values={auto-repeat-mode=On(0x01)}. 000:>:0549: Event KeyPress(2) keycode=0x36 time=0x174cf013 root=0x00000185 event=0x07e000e7 child=None(0x00000000) root-x=939 root-y=542 event-x=54 event-y=38 state=Control,Mod2 same-screen=true(0x01). CTRL+C key pressed! 0x7ffdd1322ea0 0 20. 000:>:0549: Event KeyRelease(3) keycode=0x36 time=0x174cf0b3 root=0x00000185 event=0x07e000e7 child=None(0x00000000) root-x=939 root-y=542 event-x=54 event-y=38 state=Control,Mod2 same-screen=true(0x01). 0x7ffdd1322ea0 1 20_. ```. Now to the funny part. I added the following to the mask:. `kKeyShiftMask | kKeyLockMask | kKeyControlMask | kKeyMod1Mask | kKeyMod2Mask | kKeyMod3Mask | kKeyMod4Mask | kKeyMod5Mask | kButton1Mask | kButton2Mask | kButton3Mask | kButton4Mask | kButton5Mask | kButton6Mask | kButton7Mask`. and it still didn't work. Then I added `| kAnyModifier`, and it worked. Then I removed all of them and replace with just `kKeyControlMask`. And then it worked. So now it works perfectly as it should and lets me even more confused. It seems there is something fishy going on with X11 being let in an undefined state that somehow playing around was enough to 'reset it'.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:330,integrability,event,event-x,330,"I used ""xtrace"", and I got messages printed when I use kAnyModifier, but none without it. ```. _root [1] 000:<:0549: 12: Request(102): ChangeKeyboardControl values={auto-repeat-mode=On(0x01)}. 000:>:0549: Event KeyPress(2) keycode=0x36 time=0x174cf013 root=0x00000185 event=0x07e000e7 child=None(0x00000000) root-x=939 root-y=542 event-x=54 event-y=38 state=Control,Mod2 same-screen=true(0x01). CTRL+C key pressed! 0x7ffdd1322ea0 0 20. 000:>:0549: Event KeyRelease(3) keycode=0x36 time=0x174cf0b3 root=0x00000185 event=0x07e000e7 child=None(0x00000000) root-x=939 root-y=542 event-x=54 event-y=38 state=Control,Mod2 same-screen=true(0x01). 0x7ffdd1322ea0 1 20_. ```. Now to the funny part. I added the following to the mask:. `kKeyShiftMask | kKeyLockMask | kKeyControlMask | kKeyMod1Mask | kKeyMod2Mask | kKeyMod3Mask | kKeyMod4Mask | kKeyMod5Mask | kButton1Mask | kButton2Mask | kButton3Mask | kButton4Mask | kButton5Mask | kButton6Mask | kButton7Mask`. and it still didn't work. Then I added `| kAnyModifier`, and it worked. Then I removed all of them and replace with just `kKeyControlMask`. And then it worked. So now it works perfectly as it should and lets me even more confused. It seems there is something fishy going on with X11 being let in an undefined state that somehow playing around was enough to 'reset it'.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:341,integrability,event,event-y,341,"I used ""xtrace"", and I got messages printed when I use kAnyModifier, but none without it. ```. _root [1] 000:<:0549: 12: Request(102): ChangeKeyboardControl values={auto-repeat-mode=On(0x01)}. 000:>:0549: Event KeyPress(2) keycode=0x36 time=0x174cf013 root=0x00000185 event=0x07e000e7 child=None(0x00000000) root-x=939 root-y=542 event-x=54 event-y=38 state=Control,Mod2 same-screen=true(0x01). CTRL+C key pressed! 0x7ffdd1322ea0 0 20. 000:>:0549: Event KeyRelease(3) keycode=0x36 time=0x174cf0b3 root=0x00000185 event=0x07e000e7 child=None(0x00000000) root-x=939 root-y=542 event-x=54 event-y=38 state=Control,Mod2 same-screen=true(0x01). 0x7ffdd1322ea0 1 20_. ```. Now to the funny part. I added the following to the mask:. `kKeyShiftMask | kKeyLockMask | kKeyControlMask | kKeyMod1Mask | kKeyMod2Mask | kKeyMod3Mask | kKeyMod4Mask | kKeyMod5Mask | kButton1Mask | kButton2Mask | kButton3Mask | kButton4Mask | kButton5Mask | kButton6Mask | kButton7Mask`. and it still didn't work. Then I added `| kAnyModifier`, and it worked. Then I removed all of them and replace with just `kKeyControlMask`. And then it worked. So now it works perfectly as it should and lets me even more confused. It seems there is something fishy going on with X11 being let in an undefined state that somehow playing around was enough to 'reset it'.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:352,integrability,state,state,352,"I used ""xtrace"", and I got messages printed when I use kAnyModifier, but none without it. ```. _root [1] 000:<:0549: 12: Request(102): ChangeKeyboardControl values={auto-repeat-mode=On(0x01)}. 000:>:0549: Event KeyPress(2) keycode=0x36 time=0x174cf013 root=0x00000185 event=0x07e000e7 child=None(0x00000000) root-x=939 root-y=542 event-x=54 event-y=38 state=Control,Mod2 same-screen=true(0x01). CTRL+C key pressed! 0x7ffdd1322ea0 0 20. 000:>:0549: Event KeyRelease(3) keycode=0x36 time=0x174cf0b3 root=0x00000185 event=0x07e000e7 child=None(0x00000000) root-x=939 root-y=542 event-x=54 event-y=38 state=Control,Mod2 same-screen=true(0x01). 0x7ffdd1322ea0 1 20_. ```. Now to the funny part. I added the following to the mask:. `kKeyShiftMask | kKeyLockMask | kKeyControlMask | kKeyMod1Mask | kKeyMod2Mask | kKeyMod3Mask | kKeyMod4Mask | kKeyMod5Mask | kButton1Mask | kButton2Mask | kButton3Mask | kButton4Mask | kButton5Mask | kButton6Mask | kButton7Mask`. and it still didn't work. Then I added `| kAnyModifier`, and it worked. Then I removed all of them and replace with just `kKeyControlMask`. And then it worked. So now it works perfectly as it should and lets me even more confused. It seems there is something fishy going on with X11 being let in an undefined state that somehow playing around was enough to 'reset it'.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:448,integrability,Event,Event,448,"I used ""xtrace"", and I got messages printed when I use kAnyModifier, but none without it. ```. _root [1] 000:<:0549: 12: Request(102): ChangeKeyboardControl values={auto-repeat-mode=On(0x01)}. 000:>:0549: Event KeyPress(2) keycode=0x36 time=0x174cf013 root=0x00000185 event=0x07e000e7 child=None(0x00000000) root-x=939 root-y=542 event-x=54 event-y=38 state=Control,Mod2 same-screen=true(0x01). CTRL+C key pressed! 0x7ffdd1322ea0 0 20. 000:>:0549: Event KeyRelease(3) keycode=0x36 time=0x174cf0b3 root=0x00000185 event=0x07e000e7 child=None(0x00000000) root-x=939 root-y=542 event-x=54 event-y=38 state=Control,Mod2 same-screen=true(0x01). 0x7ffdd1322ea0 1 20_. ```. Now to the funny part. I added the following to the mask:. `kKeyShiftMask | kKeyLockMask | kKeyControlMask | kKeyMod1Mask | kKeyMod2Mask | kKeyMod3Mask | kKeyMod4Mask | kKeyMod5Mask | kButton1Mask | kButton2Mask | kButton3Mask | kButton4Mask | kButton5Mask | kButton6Mask | kButton7Mask`. and it still didn't work. Then I added `| kAnyModifier`, and it worked. Then I removed all of them and replace with just `kKeyControlMask`. And then it worked. So now it works perfectly as it should and lets me even more confused. It seems there is something fishy going on with X11 being let in an undefined state that somehow playing around was enough to 'reset it'.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:513,integrability,event,event,513,"I used ""xtrace"", and I got messages printed when I use kAnyModifier, but none without it. ```. _root [1] 000:<:0549: 12: Request(102): ChangeKeyboardControl values={auto-repeat-mode=On(0x01)}. 000:>:0549: Event KeyPress(2) keycode=0x36 time=0x174cf013 root=0x00000185 event=0x07e000e7 child=None(0x00000000) root-x=939 root-y=542 event-x=54 event-y=38 state=Control,Mod2 same-screen=true(0x01). CTRL+C key pressed! 0x7ffdd1322ea0 0 20. 000:>:0549: Event KeyRelease(3) keycode=0x36 time=0x174cf0b3 root=0x00000185 event=0x07e000e7 child=None(0x00000000) root-x=939 root-y=542 event-x=54 event-y=38 state=Control,Mod2 same-screen=true(0x01). 0x7ffdd1322ea0 1 20_. ```. Now to the funny part. I added the following to the mask:. `kKeyShiftMask | kKeyLockMask | kKeyControlMask | kKeyMod1Mask | kKeyMod2Mask | kKeyMod3Mask | kKeyMod4Mask | kKeyMod5Mask | kButton1Mask | kButton2Mask | kButton3Mask | kButton4Mask | kButton5Mask | kButton6Mask | kButton7Mask`. and it still didn't work. Then I added `| kAnyModifier`, and it worked. Then I removed all of them and replace with just `kKeyControlMask`. And then it worked. So now it works perfectly as it should and lets me even more confused. It seems there is something fishy going on with X11 being let in an undefined state that somehow playing around was enough to 'reset it'.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:575,integrability,event,event-x,575,"I used ""xtrace"", and I got messages printed when I use kAnyModifier, but none without it. ```. _root [1] 000:<:0549: 12: Request(102): ChangeKeyboardControl values={auto-repeat-mode=On(0x01)}. 000:>:0549: Event KeyPress(2) keycode=0x36 time=0x174cf013 root=0x00000185 event=0x07e000e7 child=None(0x00000000) root-x=939 root-y=542 event-x=54 event-y=38 state=Control,Mod2 same-screen=true(0x01). CTRL+C key pressed! 0x7ffdd1322ea0 0 20. 000:>:0549: Event KeyRelease(3) keycode=0x36 time=0x174cf0b3 root=0x00000185 event=0x07e000e7 child=None(0x00000000) root-x=939 root-y=542 event-x=54 event-y=38 state=Control,Mod2 same-screen=true(0x01). 0x7ffdd1322ea0 1 20_. ```. Now to the funny part. I added the following to the mask:. `kKeyShiftMask | kKeyLockMask | kKeyControlMask | kKeyMod1Mask | kKeyMod2Mask | kKeyMod3Mask | kKeyMod4Mask | kKeyMod5Mask | kButton1Mask | kButton2Mask | kButton3Mask | kButton4Mask | kButton5Mask | kButton6Mask | kButton7Mask`. and it still didn't work. Then I added `| kAnyModifier`, and it worked. Then I removed all of them and replace with just `kKeyControlMask`. And then it worked. So now it works perfectly as it should and lets me even more confused. It seems there is something fishy going on with X11 being let in an undefined state that somehow playing around was enough to 'reset it'.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:586,integrability,event,event-y,586,"I used ""xtrace"", and I got messages printed when I use kAnyModifier, but none without it. ```. _root [1] 000:<:0549: 12: Request(102): ChangeKeyboardControl values={auto-repeat-mode=On(0x01)}. 000:>:0549: Event KeyPress(2) keycode=0x36 time=0x174cf013 root=0x00000185 event=0x07e000e7 child=None(0x00000000) root-x=939 root-y=542 event-x=54 event-y=38 state=Control,Mod2 same-screen=true(0x01). CTRL+C key pressed! 0x7ffdd1322ea0 0 20. 000:>:0549: Event KeyRelease(3) keycode=0x36 time=0x174cf0b3 root=0x00000185 event=0x07e000e7 child=None(0x00000000) root-x=939 root-y=542 event-x=54 event-y=38 state=Control,Mod2 same-screen=true(0x01). 0x7ffdd1322ea0 1 20_. ```. Now to the funny part. I added the following to the mask:. `kKeyShiftMask | kKeyLockMask | kKeyControlMask | kKeyMod1Mask | kKeyMod2Mask | kKeyMod3Mask | kKeyMod4Mask | kKeyMod5Mask | kButton1Mask | kButton2Mask | kButton3Mask | kButton4Mask | kButton5Mask | kButton6Mask | kButton7Mask`. and it still didn't work. Then I added `| kAnyModifier`, and it worked. Then I removed all of them and replace with just `kKeyControlMask`. And then it worked. So now it works perfectly as it should and lets me even more confused. It seems there is something fishy going on with X11 being let in an undefined state that somehow playing around was enough to 'reset it'.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:597,integrability,state,state,597,"I used ""xtrace"", and I got messages printed when I use kAnyModifier, but none without it. ```. _root [1] 000:<:0549: 12: Request(102): ChangeKeyboardControl values={auto-repeat-mode=On(0x01)}. 000:>:0549: Event KeyPress(2) keycode=0x36 time=0x174cf013 root=0x00000185 event=0x07e000e7 child=None(0x00000000) root-x=939 root-y=542 event-x=54 event-y=38 state=Control,Mod2 same-screen=true(0x01). CTRL+C key pressed! 0x7ffdd1322ea0 0 20. 000:>:0549: Event KeyRelease(3) keycode=0x36 time=0x174cf0b3 root=0x00000185 event=0x07e000e7 child=None(0x00000000) root-x=939 root-y=542 event-x=54 event-y=38 state=Control,Mod2 same-screen=true(0x01). 0x7ffdd1322ea0 1 20_. ```. Now to the funny part. I added the following to the mask:. `kKeyShiftMask | kKeyLockMask | kKeyControlMask | kKeyMod1Mask | kKeyMod2Mask | kKeyMod3Mask | kKeyMod4Mask | kKeyMod5Mask | kButton1Mask | kButton2Mask | kButton3Mask | kButton4Mask | kButton5Mask | kButton6Mask | kButton7Mask`. and it still didn't work. Then I added `| kAnyModifier`, and it worked. Then I removed all of them and replace with just `kKeyControlMask`. And then it worked. So now it works perfectly as it should and lets me even more confused. It seems there is something fishy going on with X11 being let in an undefined state that somehow playing around was enough to 'reset it'.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:1265,integrability,state,state,1265,"I used ""xtrace"", and I got messages printed when I use kAnyModifier, but none without it. ```. _root [1] 000:<:0549: 12: Request(102): ChangeKeyboardControl values={auto-repeat-mode=On(0x01)}. 000:>:0549: Event KeyPress(2) keycode=0x36 time=0x174cf013 root=0x00000185 event=0x07e000e7 child=None(0x00000000) root-x=939 root-y=542 event-x=54 event-y=38 state=Control,Mod2 same-screen=true(0x01). CTRL+C key pressed! 0x7ffdd1322ea0 0 20. 000:>:0549: Event KeyRelease(3) keycode=0x36 time=0x174cf0b3 root=0x00000185 event=0x07e000e7 child=None(0x00000000) root-x=939 root-y=542 event-x=54 event-y=38 state=Control,Mod2 same-screen=true(0x01). 0x7ffdd1322ea0 1 20_. ```. Now to the funny part. I added the following to the mask:. `kKeyShiftMask | kKeyLockMask | kKeyControlMask | kKeyMod1Mask | kKeyMod2Mask | kKeyMod3Mask | kKeyMod4Mask | kKeyMod5Mask | kButton1Mask | kButton2Mask | kButton3Mask | kButton4Mask | kButton5Mask | kButton6Mask | kButton7Mask`. and it still didn't work. Then I added `| kAnyModifier`, and it worked. Then I removed all of them and replace with just `kKeyControlMask`. And then it worked. So now it works perfectly as it should and lets me even more confused. It seems there is something fishy going on with X11 being let in an undefined state that somehow playing around was enough to 'reset it'.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:27,interoperability,messag,messages,27,"I used ""xtrace"", and I got messages printed when I use kAnyModifier, but none without it. ```. _root [1] 000:<:0549: 12: Request(102): ChangeKeyboardControl values={auto-repeat-mode=On(0x01)}. 000:>:0549: Event KeyPress(2) keycode=0x36 time=0x174cf013 root=0x00000185 event=0x07e000e7 child=None(0x00000000) root-x=939 root-y=542 event-x=54 event-y=38 state=Control,Mod2 same-screen=true(0x01). CTRL+C key pressed! 0x7ffdd1322ea0 0 20. 000:>:0549: Event KeyRelease(3) keycode=0x36 time=0x174cf0b3 root=0x00000185 event=0x07e000e7 child=None(0x00000000) root-x=939 root-y=542 event-x=54 event-y=38 state=Control,Mod2 same-screen=true(0x01). 0x7ffdd1322ea0 1 20_. ```. Now to the funny part. I added the following to the mask:. `kKeyShiftMask | kKeyLockMask | kKeyControlMask | kKeyMod1Mask | kKeyMod2Mask | kKeyMod3Mask | kKeyMod4Mask | kKeyMod5Mask | kButton1Mask | kButton2Mask | kButton3Mask | kButton4Mask | kButton5Mask | kButton6Mask | kButton7Mask`. and it still didn't work. Then I added `| kAnyModifier`, and it worked. Then I removed all of them and replace with just `kKeyControlMask`. And then it worked. So now it works perfectly as it should and lets me even more confused. It seems there is something fishy going on with X11 being let in an undefined state that somehow playing around was enough to 'reset it'.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:236,performance,time,time,236,"I used ""xtrace"", and I got messages printed when I use kAnyModifier, but none without it. ```. _root [1] 000:<:0549: 12: Request(102): ChangeKeyboardControl values={auto-repeat-mode=On(0x01)}. 000:>:0549: Event KeyPress(2) keycode=0x36 time=0x174cf013 root=0x00000185 event=0x07e000e7 child=None(0x00000000) root-x=939 root-y=542 event-x=54 event-y=38 state=Control,Mod2 same-screen=true(0x01). CTRL+C key pressed! 0x7ffdd1322ea0 0 20. 000:>:0549: Event KeyRelease(3) keycode=0x36 time=0x174cf0b3 root=0x00000185 event=0x07e000e7 child=None(0x00000000) root-x=939 root-y=542 event-x=54 event-y=38 state=Control,Mod2 same-screen=true(0x01). 0x7ffdd1322ea0 1 20_. ```. Now to the funny part. I added the following to the mask:. `kKeyShiftMask | kKeyLockMask | kKeyControlMask | kKeyMod1Mask | kKeyMod2Mask | kKeyMod3Mask | kKeyMod4Mask | kKeyMod5Mask | kButton1Mask | kButton2Mask | kButton3Mask | kButton4Mask | kButton5Mask | kButton6Mask | kButton7Mask`. and it still didn't work. Then I added `| kAnyModifier`, and it worked. Then I removed all of them and replace with just `kKeyControlMask`. And then it worked. So now it works perfectly as it should and lets me even more confused. It seems there is something fishy going on with X11 being let in an undefined state that somehow playing around was enough to 'reset it'.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:481,performance,time,time,481,"I used ""xtrace"", and I got messages printed when I use kAnyModifier, but none without it. ```. _root [1] 000:<:0549: 12: Request(102): ChangeKeyboardControl values={auto-repeat-mode=On(0x01)}. 000:>:0549: Event KeyPress(2) keycode=0x36 time=0x174cf013 root=0x00000185 event=0x07e000e7 child=None(0x00000000) root-x=939 root-y=542 event-x=54 event-y=38 state=Control,Mod2 same-screen=true(0x01). CTRL+C key pressed! 0x7ffdd1322ea0 0 20. 000:>:0549: Event KeyRelease(3) keycode=0x36 time=0x174cf0b3 root=0x00000185 event=0x07e000e7 child=None(0x00000000) root-x=939 root-y=542 event-x=54 event-y=38 state=Control,Mod2 same-screen=true(0x01). 0x7ffdd1322ea0 1 20_. ```. Now to the funny part. I added the following to the mask:. `kKeyShiftMask | kKeyLockMask | kKeyControlMask | kKeyMod1Mask | kKeyMod2Mask | kKeyMod3Mask | kKeyMod4Mask | kKeyMod5Mask | kButton1Mask | kButton2Mask | kButton3Mask | kButton4Mask | kButton5Mask | kButton6Mask | kButton7Mask`. and it still didn't work. Then I added `| kAnyModifier`, and it worked. Then I removed all of them and replace with just `kKeyControlMask`. And then it worked. So now it works perfectly as it should and lets me even more confused. It seems there is something fishy going on with X11 being let in an undefined state that somehow playing around was enough to 'reset it'.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:358,security,Control,Control,358,"I used ""xtrace"", and I got messages printed when I use kAnyModifier, but none without it. ```. _root [1] 000:<:0549: 12: Request(102): ChangeKeyboardControl values={auto-repeat-mode=On(0x01)}. 000:>:0549: Event KeyPress(2) keycode=0x36 time=0x174cf013 root=0x00000185 event=0x07e000e7 child=None(0x00000000) root-x=939 root-y=542 event-x=54 event-y=38 state=Control,Mod2 same-screen=true(0x01). CTRL+C key pressed! 0x7ffdd1322ea0 0 20. 000:>:0549: Event KeyRelease(3) keycode=0x36 time=0x174cf0b3 root=0x00000185 event=0x07e000e7 child=None(0x00000000) root-x=939 root-y=542 event-x=54 event-y=38 state=Control,Mod2 same-screen=true(0x01). 0x7ffdd1322ea0 1 20_. ```. Now to the funny part. I added the following to the mask:. `kKeyShiftMask | kKeyLockMask | kKeyControlMask | kKeyMod1Mask | kKeyMod2Mask | kKeyMod3Mask | kKeyMod4Mask | kKeyMod5Mask | kButton1Mask | kButton2Mask | kButton3Mask | kButton4Mask | kButton5Mask | kButton6Mask | kButton7Mask`. and it still didn't work. Then I added `| kAnyModifier`, and it worked. Then I removed all of them and replace with just `kKeyControlMask`. And then it worked. So now it works perfectly as it should and lets me even more confused. It seems there is something fishy going on with X11 being let in an undefined state that somehow playing around was enough to 'reset it'.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:603,security,Control,Control,603,"I used ""xtrace"", and I got messages printed when I use kAnyModifier, but none without it. ```. _root [1] 000:<:0549: 12: Request(102): ChangeKeyboardControl values={auto-repeat-mode=On(0x01)}. 000:>:0549: Event KeyPress(2) keycode=0x36 time=0x174cf013 root=0x00000185 event=0x07e000e7 child=None(0x00000000) root-x=939 root-y=542 event-x=54 event-y=38 state=Control,Mod2 same-screen=true(0x01). CTRL+C key pressed! 0x7ffdd1322ea0 0 20. 000:>:0549: Event KeyRelease(3) keycode=0x36 time=0x174cf0b3 root=0x00000185 event=0x07e000e7 child=None(0x00000000) root-x=939 root-y=542 event-x=54 event-y=38 state=Control,Mod2 same-screen=true(0x01). 0x7ffdd1322ea0 1 20_. ```. Now to the funny part. I added the following to the mask:. `kKeyShiftMask | kKeyLockMask | kKeyControlMask | kKeyMod1Mask | kKeyMod2Mask | kKeyMod3Mask | kKeyMod4Mask | kKeyMod5Mask | kButton1Mask | kButton2Mask | kButton3Mask | kButton4Mask | kButton5Mask | kButton6Mask | kButton7Mask`. and it still didn't work. Then I added `| kAnyModifier`, and it worked. Then I removed all of them and replace with just `kKeyControlMask`. And then it worked. So now it works perfectly as it should and lets me even more confused. It seems there is something fishy going on with X11 being let in an undefined state that somehow playing around was enough to 'reset it'.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:358,testability,Control,Control,358,"I used ""xtrace"", and I got messages printed when I use kAnyModifier, but none without it. ```. _root [1] 000:<:0549: 12: Request(102): ChangeKeyboardControl values={auto-repeat-mode=On(0x01)}. 000:>:0549: Event KeyPress(2) keycode=0x36 time=0x174cf013 root=0x00000185 event=0x07e000e7 child=None(0x00000000) root-x=939 root-y=542 event-x=54 event-y=38 state=Control,Mod2 same-screen=true(0x01). CTRL+C key pressed! 0x7ffdd1322ea0 0 20. 000:>:0549: Event KeyRelease(3) keycode=0x36 time=0x174cf0b3 root=0x00000185 event=0x07e000e7 child=None(0x00000000) root-x=939 root-y=542 event-x=54 event-y=38 state=Control,Mod2 same-screen=true(0x01). 0x7ffdd1322ea0 1 20_. ```. Now to the funny part. I added the following to the mask:. `kKeyShiftMask | kKeyLockMask | kKeyControlMask | kKeyMod1Mask | kKeyMod2Mask | kKeyMod3Mask | kKeyMod4Mask | kKeyMod5Mask | kButton1Mask | kButton2Mask | kButton3Mask | kButton4Mask | kButton5Mask | kButton6Mask | kButton7Mask`. and it still didn't work. Then I added `| kAnyModifier`, and it worked. Then I removed all of them and replace with just `kKeyControlMask`. And then it worked. So now it works perfectly as it should and lets me even more confused. It seems there is something fishy going on with X11 being let in an undefined state that somehow playing around was enough to 'reset it'.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:603,testability,Control,Control,603,"I used ""xtrace"", and I got messages printed when I use kAnyModifier, but none without it. ```. _root [1] 000:<:0549: 12: Request(102): ChangeKeyboardControl values={auto-repeat-mode=On(0x01)}. 000:>:0549: Event KeyPress(2) keycode=0x36 time=0x174cf013 root=0x00000185 event=0x07e000e7 child=None(0x00000000) root-x=939 root-y=542 event-x=54 event-y=38 state=Control,Mod2 same-screen=true(0x01). CTRL+C key pressed! 0x7ffdd1322ea0 0 20. 000:>:0549: Event KeyRelease(3) keycode=0x36 time=0x174cf0b3 root=0x00000185 event=0x07e000e7 child=None(0x00000000) root-x=939 root-y=542 event-x=54 event-y=38 state=Control,Mod2 same-screen=true(0x01). 0x7ffdd1322ea0 1 20_. ```. Now to the funny part. I added the following to the mask:. `kKeyShiftMask | kKeyLockMask | kKeyControlMask | kKeyMod1Mask | kKeyMod2Mask | kKeyMod3Mask | kKeyMod4Mask | kKeyMod5Mask | kButton1Mask | kButton2Mask | kButton3Mask | kButton4Mask | kButton5Mask | kButton6Mask | kButton7Mask`. and it still didn't work. Then I added `| kAnyModifier`, and it worked. Then I removed all of them and replace with just `kKeyControlMask`. And then it worked. So now it works perfectly as it should and lets me even more confused. It seems there is something fishy going on with X11 being let in an undefined state that somehow playing around was enough to 'reset it'.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:20,usability,help,help,20,"Yes, thanks for the help :)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:988,availability,mask,mask,988,"I'm having the problem again now, and I am not sure how to 'reset' my keyboard so that it starts working again. Even restarting my computer didn't make it work again. Searching a bit more, I think this might be related to this ""feature"" of X11:. https://bugs.freedesktop.org/show_bug.cgi?id=99280. https://stackoverflow.com/questions/18160792/python-xlib-xgrabkey-keyrelease-events-not-firing. https://github.com/zhanghai/xkeymacs/issues/1. https://stackoverflow.com/questions/39087079/detect-modifier-key-release-in-x11-root-window. Maybe it also explains why, randomly, my keyboard key repetition is 'deactivated'. And also that sometimes, pressing ALT+Key (in a TGTextbutton) lefts the button engaged / stuck insted of press and release. And it would also explain why changing to ""kAnyModifier"" makes it work. Even if CTRL is still trapped, pressing CTRL+S is detected as pressing just 'S' by X11, and then in HandleKey, doing event->fState & kKeyControlMask does detect correctly the mask. So fState seems to work, and the problem is just with XGrabKey and XUngrabKey sometimes not working in the release sequence I guess.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:306,deployability,stack,stackoverflow,306,"I'm having the problem again now, and I am not sure how to 'reset' my keyboard so that it starts working again. Even restarting my computer didn't make it work again. Searching a bit more, I think this might be related to this ""feature"" of X11:. https://bugs.freedesktop.org/show_bug.cgi?id=99280. https://stackoverflow.com/questions/18160792/python-xlib-xgrabkey-keyrelease-events-not-firing. https://github.com/zhanghai/xkeymacs/issues/1. https://stackoverflow.com/questions/39087079/detect-modifier-key-release-in-x11-root-window. Maybe it also explains why, randomly, my keyboard key repetition is 'deactivated'. And also that sometimes, pressing ALT+Key (in a TGTextbutton) lefts the button engaged / stuck insted of press and release. And it would also explain why changing to ""kAnyModifier"" makes it work. Even if CTRL is still trapped, pressing CTRL+S is detected as pressing just 'S' by X11, and then in HandleKey, doing event->fState & kKeyControlMask does detect correctly the mask. So fState seems to work, and the problem is just with XGrabKey and XUngrabKey sometimes not working in the release sequence I guess.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:449,deployability,stack,stackoverflow,449,"I'm having the problem again now, and I am not sure how to 'reset' my keyboard so that it starts working again. Even restarting my computer didn't make it work again. Searching a bit more, I think this might be related to this ""feature"" of X11:. https://bugs.freedesktop.org/show_bug.cgi?id=99280. https://stackoverflow.com/questions/18160792/python-xlib-xgrabkey-keyrelease-events-not-firing. https://github.com/zhanghai/xkeymacs/issues/1. https://stackoverflow.com/questions/39087079/detect-modifier-key-release-in-x11-root-window. Maybe it also explains why, randomly, my keyboard key repetition is 'deactivated'. And also that sometimes, pressing ALT+Key (in a TGTextbutton) lefts the button engaged / stuck insted of press and release. And it would also explain why changing to ""kAnyModifier"" makes it work. Even if CTRL is still trapped, pressing CTRL+S is detected as pressing just 'S' by X11, and then in HandleKey, doing event->fState & kKeyControlMask does detect correctly the mask. So fState seems to work, and the problem is just with XGrabKey and XUngrabKey sometimes not working in the release sequence I guess.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:506,deployability,releas,release-in-,506,"I'm having the problem again now, and I am not sure how to 'reset' my keyboard so that it starts working again. Even restarting my computer didn't make it work again. Searching a bit more, I think this might be related to this ""feature"" of X11:. https://bugs.freedesktop.org/show_bug.cgi?id=99280. https://stackoverflow.com/questions/18160792/python-xlib-xgrabkey-keyrelease-events-not-firing. https://github.com/zhanghai/xkeymacs/issues/1. https://stackoverflow.com/questions/39087079/detect-modifier-key-release-in-x11-root-window. Maybe it also explains why, randomly, my keyboard key repetition is 'deactivated'. And also that sometimes, pressing ALT+Key (in a TGTextbutton) lefts the button engaged / stuck insted of press and release. And it would also explain why changing to ""kAnyModifier"" makes it work. Even if CTRL is still trapped, pressing CTRL+S is detected as pressing just 'S' by X11, and then in HandleKey, doing event->fState & kKeyControlMask does detect correctly the mask. So fState seems to work, and the problem is just with XGrabKey and XUngrabKey sometimes not working in the release sequence I guess.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:732,deployability,releas,release,732,"I'm having the problem again now, and I am not sure how to 'reset' my keyboard so that it starts working again. Even restarting my computer didn't make it work again. Searching a bit more, I think this might be related to this ""feature"" of X11:. https://bugs.freedesktop.org/show_bug.cgi?id=99280. https://stackoverflow.com/questions/18160792/python-xlib-xgrabkey-keyrelease-events-not-firing. https://github.com/zhanghai/xkeymacs/issues/1. https://stackoverflow.com/questions/39087079/detect-modifier-key-release-in-x11-root-window. Maybe it also explains why, randomly, my keyboard key repetition is 'deactivated'. And also that sometimes, pressing ALT+Key (in a TGTextbutton) lefts the button engaged / stuck insted of press and release. And it would also explain why changing to ""kAnyModifier"" makes it work. Even if CTRL is still trapped, pressing CTRL+S is detected as pressing just 'S' by X11, and then in HandleKey, doing event->fState & kKeyControlMask does detect correctly the mask. So fState seems to work, and the problem is just with XGrabKey and XUngrabKey sometimes not working in the release sequence I guess.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:1101,deployability,releas,release,1101,"I'm having the problem again now, and I am not sure how to 'reset' my keyboard so that it starts working again. Even restarting my computer didn't make it work again. Searching a bit more, I think this might be related to this ""feature"" of X11:. https://bugs.freedesktop.org/show_bug.cgi?id=99280. https://stackoverflow.com/questions/18160792/python-xlib-xgrabkey-keyrelease-events-not-firing. https://github.com/zhanghai/xkeymacs/issues/1. https://stackoverflow.com/questions/39087079/detect-modifier-key-release-in-x11-root-window. Maybe it also explains why, randomly, my keyboard key repetition is 'deactivated'. And also that sometimes, pressing ALT+Key (in a TGTextbutton) lefts the button engaged / stuck insted of press and release. And it would also explain why changing to ""kAnyModifier"" makes it work. Even if CTRL is still trapped, pressing CTRL+S is detected as pressing just 'S' by X11, and then in HandleKey, doing event->fState & kKeyControlMask does detect correctly the mask. So fState seems to work, and the problem is just with XGrabKey and XUngrabKey sometimes not working in the release sequence I guess.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:375,integrability,event,events-not-firing,375,"I'm having the problem again now, and I am not sure how to 'reset' my keyboard so that it starts working again. Even restarting my computer didn't make it work again. Searching a bit more, I think this might be related to this ""feature"" of X11:. https://bugs.freedesktop.org/show_bug.cgi?id=99280. https://stackoverflow.com/questions/18160792/python-xlib-xgrabkey-keyrelease-events-not-firing. https://github.com/zhanghai/xkeymacs/issues/1. https://stackoverflow.com/questions/39087079/detect-modifier-key-release-in-x11-root-window. Maybe it also explains why, randomly, my keyboard key repetition is 'deactivated'. And also that sometimes, pressing ALT+Key (in a TGTextbutton) lefts the button engaged / stuck insted of press and release. And it would also explain why changing to ""kAnyModifier"" makes it work. Even if CTRL is still trapped, pressing CTRL+S is detected as pressing just 'S' by X11, and then in HandleKey, doing event->fState & kKeyControlMask does detect correctly the mask. So fState seems to work, and the problem is just with XGrabKey and XUngrabKey sometimes not working in the release sequence I guess.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:930,integrability,event,event,930,"I'm having the problem again now, and I am not sure how to 'reset' my keyboard so that it starts working again. Even restarting my computer didn't make it work again. Searching a bit more, I think this might be related to this ""feature"" of X11:. https://bugs.freedesktop.org/show_bug.cgi?id=99280. https://stackoverflow.com/questions/18160792/python-xlib-xgrabkey-keyrelease-events-not-firing. https://github.com/zhanghai/xkeymacs/issues/1. https://stackoverflow.com/questions/39087079/detect-modifier-key-release-in-x11-root-window. Maybe it also explains why, randomly, my keyboard key repetition is 'deactivated'. And also that sometimes, pressing ALT+Key (in a TGTextbutton) lefts the button engaged / stuck insted of press and release. And it would also explain why changing to ""kAnyModifier"" makes it work. Even if CTRL is still trapped, pressing CTRL+S is detected as pressing just 'S' by X11, and then in HandleKey, doing event->fState & kKeyControlMask does detect correctly the mask. So fState seems to work, and the problem is just with XGrabKey and XUngrabKey sometimes not working in the release sequence I guess.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:962,reliability,doe,does,962,"I'm having the problem again now, and I am not sure how to 'reset' my keyboard so that it starts working again. Even restarting my computer didn't make it work again. Searching a bit more, I think this might be related to this ""feature"" of X11:. https://bugs.freedesktop.org/show_bug.cgi?id=99280. https://stackoverflow.com/questions/18160792/python-xlib-xgrabkey-keyrelease-events-not-firing. https://github.com/zhanghai/xkeymacs/issues/1. https://stackoverflow.com/questions/39087079/detect-modifier-key-release-in-x11-root-window. Maybe it also explains why, randomly, my keyboard key repetition is 'deactivated'. And also that sometimes, pressing ALT+Key (in a TGTextbutton) lefts the button engaged / stuck insted of press and release. And it would also explain why changing to ""kAnyModifier"" makes it work. Even if CTRL is still trapped, pressing CTRL+S is detected as pressing just 'S' by X11, and then in HandleKey, doing event->fState & kKeyControlMask does detect correctly the mask. So fState seems to work, and the problem is just with XGrabKey and XUngrabKey sometimes not working in the release sequence I guess.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:486,safety,detect,detect-modifier-key-release-in-,486,"I'm having the problem again now, and I am not sure how to 'reset' my keyboard so that it starts working again. Even restarting my computer didn't make it work again. Searching a bit more, I think this might be related to this ""feature"" of X11:. https://bugs.freedesktop.org/show_bug.cgi?id=99280. https://stackoverflow.com/questions/18160792/python-xlib-xgrabkey-keyrelease-events-not-firing. https://github.com/zhanghai/xkeymacs/issues/1. https://stackoverflow.com/questions/39087079/detect-modifier-key-release-in-x11-root-window. Maybe it also explains why, randomly, my keyboard key repetition is 'deactivated'. And also that sometimes, pressing ALT+Key (in a TGTextbutton) lefts the button engaged / stuck insted of press and release. And it would also explain why changing to ""kAnyModifier"" makes it work. Even if CTRL is still trapped, pressing CTRL+S is detected as pressing just 'S' by X11, and then in HandleKey, doing event->fState & kKeyControlMask does detect correctly the mask. So fState seems to work, and the problem is just with XGrabKey and XUngrabKey sometimes not working in the release sequence I guess.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:863,safety,detect,detected,863,"I'm having the problem again now, and I am not sure how to 'reset' my keyboard so that it starts working again. Even restarting my computer didn't make it work again. Searching a bit more, I think this might be related to this ""feature"" of X11:. https://bugs.freedesktop.org/show_bug.cgi?id=99280. https://stackoverflow.com/questions/18160792/python-xlib-xgrabkey-keyrelease-events-not-firing. https://github.com/zhanghai/xkeymacs/issues/1. https://stackoverflow.com/questions/39087079/detect-modifier-key-release-in-x11-root-window. Maybe it also explains why, randomly, my keyboard key repetition is 'deactivated'. And also that sometimes, pressing ALT+Key (in a TGTextbutton) lefts the button engaged / stuck insted of press and release. And it would also explain why changing to ""kAnyModifier"" makes it work. Even if CTRL is still trapped, pressing CTRL+S is detected as pressing just 'S' by X11, and then in HandleKey, doing event->fState & kKeyControlMask does detect correctly the mask. So fState seems to work, and the problem is just with XGrabKey and XUngrabKey sometimes not working in the release sequence I guess.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:967,safety,detect,detect,967,"I'm having the problem again now, and I am not sure how to 'reset' my keyboard so that it starts working again. Even restarting my computer didn't make it work again. Searching a bit more, I think this might be related to this ""feature"" of X11:. https://bugs.freedesktop.org/show_bug.cgi?id=99280. https://stackoverflow.com/questions/18160792/python-xlib-xgrabkey-keyrelease-events-not-firing. https://github.com/zhanghai/xkeymacs/issues/1. https://stackoverflow.com/questions/39087079/detect-modifier-key-release-in-x11-root-window. Maybe it also explains why, randomly, my keyboard key repetition is 'deactivated'. And also that sometimes, pressing ALT+Key (in a TGTextbutton) lefts the button engaged / stuck insted of press and release. And it would also explain why changing to ""kAnyModifier"" makes it work. Even if CTRL is still trapped, pressing CTRL+S is detected as pressing just 'S' by X11, and then in HandleKey, doing event->fState & kKeyControlMask does detect correctly the mask. So fState seems to work, and the problem is just with XGrabKey and XUngrabKey sometimes not working in the release sequence I guess.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:486,security,detect,detect-modifier-key-release-in-,486,"I'm having the problem again now, and I am not sure how to 'reset' my keyboard so that it starts working again. Even restarting my computer didn't make it work again. Searching a bit more, I think this might be related to this ""feature"" of X11:. https://bugs.freedesktop.org/show_bug.cgi?id=99280. https://stackoverflow.com/questions/18160792/python-xlib-xgrabkey-keyrelease-events-not-firing. https://github.com/zhanghai/xkeymacs/issues/1. https://stackoverflow.com/questions/39087079/detect-modifier-key-release-in-x11-root-window. Maybe it also explains why, randomly, my keyboard key repetition is 'deactivated'. And also that sometimes, pressing ALT+Key (in a TGTextbutton) lefts the button engaged / stuck insted of press and release. And it would also explain why changing to ""kAnyModifier"" makes it work. Even if CTRL is still trapped, pressing CTRL+S is detected as pressing just 'S' by X11, and then in HandleKey, doing event->fState & kKeyControlMask does detect correctly the mask. So fState seems to work, and the problem is just with XGrabKey and XUngrabKey sometimes not working in the release sequence I guess.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:863,security,detect,detected,863,"I'm having the problem again now, and I am not sure how to 'reset' my keyboard so that it starts working again. Even restarting my computer didn't make it work again. Searching a bit more, I think this might be related to this ""feature"" of X11:. https://bugs.freedesktop.org/show_bug.cgi?id=99280. https://stackoverflow.com/questions/18160792/python-xlib-xgrabkey-keyrelease-events-not-firing. https://github.com/zhanghai/xkeymacs/issues/1. https://stackoverflow.com/questions/39087079/detect-modifier-key-release-in-x11-root-window. Maybe it also explains why, randomly, my keyboard key repetition is 'deactivated'. And also that sometimes, pressing ALT+Key (in a TGTextbutton) lefts the button engaged / stuck insted of press and release. And it would also explain why changing to ""kAnyModifier"" makes it work. Even if CTRL is still trapped, pressing CTRL+S is detected as pressing just 'S' by X11, and then in HandleKey, doing event->fState & kKeyControlMask does detect correctly the mask. So fState seems to work, and the problem is just with XGrabKey and XUngrabKey sometimes not working in the release sequence I guess.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:967,security,detect,detect,967,"I'm having the problem again now, and I am not sure how to 'reset' my keyboard so that it starts working again. Even restarting my computer didn't make it work again. Searching a bit more, I think this might be related to this ""feature"" of X11:. https://bugs.freedesktop.org/show_bug.cgi?id=99280. https://stackoverflow.com/questions/18160792/python-xlib-xgrabkey-keyrelease-events-not-firing. https://github.com/zhanghai/xkeymacs/issues/1. https://stackoverflow.com/questions/39087079/detect-modifier-key-release-in-x11-root-window. Maybe it also explains why, randomly, my keyboard key repetition is 'deactivated'. And also that sometimes, pressing ALT+Key (in a TGTextbutton) lefts the button engaged / stuck insted of press and release. And it would also explain why changing to ""kAnyModifier"" makes it work. Even if CTRL is still trapped, pressing CTRL+S is detected as pressing just 'S' by X11, and then in HandleKey, doing event->fState & kKeyControlMask does detect correctly the mask. So fState seems to work, and the problem is just with XGrabKey and XUngrabKey sometimes not working in the release sequence I guess.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:98,deployability,fail,fails,98,"Ok, never mind, I finally found out what was happening. The problem was that the kKeyControlMask 'fails' if NumLock key was activated. By changing it to 'any modifier', it was capturing it. Side note: I also called gVirtualX->GrabKey(..., kFALSE) in my destructor classes, just to make sure everything goes back to normal after exiting.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:98,reliability,fail,fails,98,"Ok, never mind, I finally found out what was happening. The problem was that the kKeyControlMask 'fails' if NumLock key was activated. By changing it to 'any modifier', it was capturing it. Side note: I also called gVirtualX->GrabKey(..., kFALSE) in my destructor classes, just to make sure everything goes back to normal after exiting.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/issues/8665:158,security,modif,modifier,158,"Ok, never mind, I finally found out what was happening. The problem was that the kKeyControlMask 'fails' if NumLock key was activated. By changing it to 'any modifier', it was capturing it. Side note: I also called gVirtualX->GrabKey(..., kFALSE) in my destructor classes, just to make sure everything goes back to normal after exiting.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8665
https://github.com/root-project/root/pull/8666:11,deployability,build,build,11,@phsft-bot build,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8666
https://github.com/root-project/root/pull/8666:93,performance,time,timeouts,93,@sanjibansg please fork also [roottest](https://github.com/root-project/roottest) to prevent timeouts on the Windows nodes. Thanks,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8666
https://github.com/root-project/root/pull/8666:85,safety,prevent,prevent,85,@sanjibansg please fork also [roottest](https://github.com/root-project/roottest) to prevent timeouts on the Windows nodes. Thanks,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8666
https://github.com/root-project/root/pull/8666:93,safety,timeout,timeouts,93,@sanjibansg please fork also [roottest](https://github.com/root-project/roottest) to prevent timeouts on the Windows nodes. Thanks,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8666
https://github.com/root-project/root/pull/8666:85,security,preven,prevent,85,@sanjibansg please fork also [roottest](https://github.com/root-project/roottest) to prevent timeouts on the Windows nodes. Thanks,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8666
https://github.com/root-project/root/pull/8666:50,interoperability,conflict,conflict,50,"@sanjibansg : can you please rebase it to fix the conflict file , happening after merging your other PR on restructuring Sofie. Thank you",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8666
https://github.com/root-project/root/pull/8666:52,interoperability,conflict,conflict,52,"> @sanjibansg : can you please rebase it to fix the conflict file , happening after merging your other PR on restructuring Sofie. Thank you. @lmoneta Resolved the merge conflict.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8666
https://github.com/root-project/root/pull/8666:169,interoperability,conflict,conflict,169,"> @sanjibansg : can you please rebase it to fix the conflict file , happening after merging your other PR on restructuring Sofie. Thank you. @lmoneta Resolved the merge conflict.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8666
https://github.com/root-project/root/pull/8666:11,deployability,build,build,11,@phsft-bot build just on ROOT-ubuntu16/nortcxxmod with flags -Dtmva-sofie=On,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8666
https://github.com/root-project/root/pull/8666:11,deployability,build,build,11,@phsft-bot build just on ROOT-ubuntu2004/default with flags -Dtmva-sofie=On.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8666
https://github.com/root-project/root/pull/8666:11,deployability,build,build,11,@phsft-bot build just on ROOT-ubuntu18.04/default with flags -Dtmva-sofie=On.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8666
https://github.com/root-project/root/pull/8666:11,deployability,build,build,11,@phsft-bot build just on ROOT-ubuntu2004/default with flags -Dtmva-sofie=On,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8666
https://github.com/root-project/root/pull/8666:11,deployability,build,build,11,@phsft-bot build just on ROOT-ubuntu18.04/default with flags -Dtmva-sofie=On,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8666
https://github.com/root-project/root/pull/8666:11,deployability,build,build,11,"@phsft-bot build just on ROOT-ubuntu18.04/default, ROOT-ubuntu2004/default with flags -Dtmva-sofie=On",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8666
https://github.com/root-project/root/pull/8667:11,deployability,build,build,11,@phsft-bot build,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8667
https://github.com/root-project/root/pull/8667:0,deployability,Fail,Failing,0,Failing test is unrelated (it's happening in other PRs too),MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8667
https://github.com/root-project/root/pull/8667:0,reliability,Fail,Failing,0,Failing test is unrelated (it's happening in other PRs too),MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8667
https://github.com/root-project/root/pull/8667:8,safety,test,test,8,Failing test is unrelated (it's happening in other PRs too),MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8667
https://github.com/root-project/root/pull/8667:8,testability,test,test,8,Failing test is unrelated (it's happening in other PRs too),MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8667
https://github.com/root-project/root/pull/8669:11,deployability,build,build,11,@phsft-bot build,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8669
https://github.com/root-project/root/pull/8669:11,deployability,build,build,11,@phsft-bot build,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8669
https://github.com/root-project/root/pull/8673:11,deployability,build,build,11,@phsft-bot build,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8673
https://github.com/root-project/root/pull/8673:0,deployability,Fail,Failing,0,Failing test is unrelated (it's happening in other PRs too),MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8673
https://github.com/root-project/root/pull/8673:0,reliability,Fail,Failing,0,Failing test is unrelated (it's happening in other PRs too),MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8673
https://github.com/root-project/root/pull/8673:8,safety,test,test,8,Failing test is unrelated (it's happening in other PRs too),MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8673
https://github.com/root-project/root/pull/8673:8,testability,test,test,8,Failing test is unrelated (it's happening in other PRs too),MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8673
https://github.com/root-project/root/pull/8673:263,integrability,configur,configured,263,"> Sorry, coding conventions (I know it was already in the original code)... Sure, no problem. I suggested a new feature recently which would auto-format these https://github.com/root-project/root/issues/8797, I tend to forget about these because I have QtCreator configured with the default sytle.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8673
https://github.com/root-project/root/pull/8673:146,interoperability,format,format,146,"> Sorry, coding conventions (I know it was already in the original code)... Sure, no problem. I suggested a new feature recently which would auto-format these https://github.com/root-project/root/issues/8797, I tend to forget about these because I have QtCreator configured with the default sytle.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8673
https://github.com/root-project/root/pull/8673:263,modifiability,configur,configured,263,"> Sorry, coding conventions (I know it was already in the original code)... Sure, no problem. I suggested a new feature recently which would auto-format these https://github.com/root-project/root/issues/8797, I tend to forget about these because I have QtCreator configured with the default sytle.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8673
https://github.com/root-project/root/pull/8673:263,security,configur,configured,263,"> Sorry, coding conventions (I know it was already in the original code)... Sure, no problem. I suggested a new feature recently which would auto-format these https://github.com/root-project/root/issues/8797, I tend to forget about these because I have QtCreator configured with the default sytle.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8673
https://github.com/root-project/root/pull/8675:375,availability,slo,slot,375,"Thanks a lot Stephan! On the line of your first to-do item, I think the programming model should be (EDITED after the discussion below):. ```cpp. ROOT::RDF::AddProgressIndicator(df, everyNEvents, nEvents); // last 2 args are optional. ```. rather than. ```cpp. ROOT::RDF::ProgressHelper progress(everyNEvents, nEvents);. h.OnPartialResultSlot(10000, [&progress](unsigned int slot, TH1D& histo){ progress(slot, histo); });. ```. I would also rename `CountEvents` to the slightly scarier `RetrieveNEvents` (which might take string_views rather than `const char *`) to hint at the fact that it can be an expensive call.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:404,availability,slo,slot,404,"Thanks a lot Stephan! On the line of your first to-do item, I think the programming model should be (EDITED after the discussion below):. ```cpp. ROOT::RDF::AddProgressIndicator(df, everyNEvents, nEvents); // last 2 args are optional. ```. rather than. ```cpp. ROOT::RDF::ProgressHelper progress(everyNEvents, nEvents);. h.OnPartialResultSlot(10000, [&progress](unsigned int slot, TH1D& histo){ progress(slot, histo); });. ```. I would also rename `CountEvents` to the slightly scarier `RetrieveNEvents` (which might take string_views rather than `const char *`) to hint at the fact that it can be an expensive call.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:469,availability,sli,slightly,469,"Thanks a lot Stephan! On the line of your first to-do item, I think the programming model should be (EDITED after the discussion below):. ```cpp. ROOT::RDF::AddProgressIndicator(df, everyNEvents, nEvents); // last 2 args are optional. ```. rather than. ```cpp. ROOT::RDF::ProgressHelper progress(everyNEvents, nEvents);. h.OnPartialResultSlot(10000, [&progress](unsigned int slot, TH1D& histo){ progress(slot, histo); });. ```. I would also rename `CountEvents` to the slightly scarier `RetrieveNEvents` (which might take string_views rather than `const char *`) to hint at the fact that it can be an expensive call.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:84,energy efficiency,model,model,84,"Thanks a lot Stephan! On the line of your first to-do item, I think the programming model should be (EDITED after the discussion below):. ```cpp. ROOT::RDF::AddProgressIndicator(df, everyNEvents, nEvents); // last 2 args are optional. ```. rather than. ```cpp. ROOT::RDF::ProgressHelper progress(everyNEvents, nEvents);. h.OnPartialResultSlot(10000, [&progress](unsigned int slot, TH1D& histo){ progress(slot, histo); });. ```. I would also rename `CountEvents` to the slightly scarier `RetrieveNEvents` (which might take string_views rather than `const char *`) to hint at the fact that it can be an expensive call.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:375,reliability,slo,slot,375,"Thanks a lot Stephan! On the line of your first to-do item, I think the programming model should be (EDITED after the discussion below):. ```cpp. ROOT::RDF::AddProgressIndicator(df, everyNEvents, nEvents); // last 2 args are optional. ```. rather than. ```cpp. ROOT::RDF::ProgressHelper progress(everyNEvents, nEvents);. h.OnPartialResultSlot(10000, [&progress](unsigned int slot, TH1D& histo){ progress(slot, histo); });. ```. I would also rename `CountEvents` to the slightly scarier `RetrieveNEvents` (which might take string_views rather than `const char *`) to hint at the fact that it can be an expensive call.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:404,reliability,slo,slot,404,"Thanks a lot Stephan! On the line of your first to-do item, I think the programming model should be (EDITED after the discussion below):. ```cpp. ROOT::RDF::AddProgressIndicator(df, everyNEvents, nEvents); // last 2 args are optional. ```. rather than. ```cpp. ROOT::RDF::ProgressHelper progress(everyNEvents, nEvents);. h.OnPartialResultSlot(10000, [&progress](unsigned int slot, TH1D& histo){ progress(slot, histo); });. ```. I would also rename `CountEvents` to the slightly scarier `RetrieveNEvents` (which might take string_views rather than `const char *`) to hint at the fact that it can be an expensive call.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:469,reliability,sli,slightly,469,"Thanks a lot Stephan! On the line of your first to-do item, I think the programming model should be (EDITED after the discussion below):. ```cpp. ROOT::RDF::AddProgressIndicator(df, everyNEvents, nEvents); // last 2 args are optional. ```. rather than. ```cpp. ROOT::RDF::ProgressHelper progress(everyNEvents, nEvents);. h.OnPartialResultSlot(10000, [&progress](unsigned int slot, TH1D& histo){ progress(slot, histo); });. ```. I would also rename `CountEvents` to the slightly scarier `RetrieveNEvents` (which might take string_views rather than `const char *`) to hint at the fact that it can be an expensive call.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:84,security,model,model,84,"Thanks a lot Stephan! On the line of your first to-do item, I think the programming model should be (EDITED after the discussion below):. ```cpp. ROOT::RDF::AddProgressIndicator(df, everyNEvents, nEvents); // last 2 args are optional. ```. rather than. ```cpp. ROOT::RDF::ProgressHelper progress(everyNEvents, nEvents);. h.OnPartialResultSlot(10000, [&progress](unsigned int slot, TH1D& histo){ progress(slot, histo); });. ```. I would also rename `CountEvents` to the slightly scarier `RetrieveNEvents` (which might take string_views rather than `const char *`) to hint at the fact that it can be an expensive call.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:272,usability,Progress,ProgressHelper,272,"Thanks a lot Stephan! On the line of your first to-do item, I think the programming model should be (EDITED after the discussion below):. ```cpp. ROOT::RDF::AddProgressIndicator(df, everyNEvents, nEvents); // last 2 args are optional. ```. rather than. ```cpp. ROOT::RDF::ProgressHelper progress(everyNEvents, nEvents);. h.OnPartialResultSlot(10000, [&progress](unsigned int slot, TH1D& histo){ progress(slot, histo); });. ```. I would also rename `CountEvents` to the slightly scarier `RetrieveNEvents` (which might take string_views rather than `const char *`) to hint at the fact that it can be an expensive call.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:287,usability,progress,progress,287,"Thanks a lot Stephan! On the line of your first to-do item, I think the programming model should be (EDITED after the discussion below):. ```cpp. ROOT::RDF::AddProgressIndicator(df, everyNEvents, nEvents); // last 2 args are optional. ```. rather than. ```cpp. ROOT::RDF::ProgressHelper progress(everyNEvents, nEvents);. h.OnPartialResultSlot(10000, [&progress](unsigned int slot, TH1D& histo){ progress(slot, histo); });. ```. I would also rename `CountEvents` to the slightly scarier `RetrieveNEvents` (which might take string_views rather than `const char *`) to hint at the fact that it can be an expensive call.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:352,usability,progress,progress,352,"Thanks a lot Stephan! On the line of your first to-do item, I think the programming model should be (EDITED after the discussion below):. ```cpp. ROOT::RDF::AddProgressIndicator(df, everyNEvents, nEvents); // last 2 args are optional. ```. rather than. ```cpp. ROOT::RDF::ProgressHelper progress(everyNEvents, nEvents);. h.OnPartialResultSlot(10000, [&progress](unsigned int slot, TH1D& histo){ progress(slot, histo); });. ```. I would also rename `CountEvents` to the slightly scarier `RetrieveNEvents` (which might take string_views rather than `const char *`) to hint at the fact that it can be an expensive call.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:395,usability,progress,progress,395,"Thanks a lot Stephan! On the line of your first to-do item, I think the programming model should be (EDITED after the discussion below):. ```cpp. ROOT::RDF::AddProgressIndicator(df, everyNEvents, nEvents); // last 2 args are optional. ```. rather than. ```cpp. ROOT::RDF::ProgressHelper progress(everyNEvents, nEvents);. h.OnPartialResultSlot(10000, [&progress](unsigned int slot, TH1D& histo){ progress(slot, histo); });. ```. I would also rename `CountEvents` to the slightly scarier `RetrieveNEvents` (which might take string_views rather than `const char *`) to hint at the fact that it can be an expensive call.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:566,usability,hint,hint,566,"Thanks a lot Stephan! On the line of your first to-do item, I think the programming model should be (EDITED after the discussion below):. ```cpp. ROOT::RDF::AddProgressIndicator(df, everyNEvents, nEvents); // last 2 args are optional. ```. rather than. ```cpp. ROOT::RDF::ProgressHelper progress(everyNEvents, nEvents);. h.OnPartialResultSlot(10000, [&progress](unsigned int slot, TH1D& histo){ progress(slot, histo); });. ```. I would also rename `CountEvents` to the slightly scarier `RetrieveNEvents` (which might take string_views rather than `const char *`) to hint at the fact that it can be an expensive call.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:379,availability,slo,slot,379,"> Thanks a lot Stephan! > On the line of your first to-do item, I think the programming model should be:. > . > ```c++. > auto progbar = ROOT::RDF::MakeProgressBar(df, everyNEvents, nEvents); // last 2 args are optional. > ```. > . > rather than. > . > ```c++. > ROOT::RDF::ProgressHelper progress(everyNEvents, nEvents);. > h.OnPartialResultSlot(10000, [&progress](unsigned int slot, TH1D& histo){ progress(slot, histo); });. > ```. > . > I would also rename `CountEvents` to the slightly scarier `RetrieveNEvents` (which might take string_views rather than `const char *`) to hint at the fact that it can be an expensive call. Excellent! Tell me though: An RDF (= RNode I think?) doesn't take callbacks. I need a `RResultPtr`, correct? Do I have to attach a dummy result or did I miss something?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:408,availability,slo,slot,408,"> Thanks a lot Stephan! > On the line of your first to-do item, I think the programming model should be:. > . > ```c++. > auto progbar = ROOT::RDF::MakeProgressBar(df, everyNEvents, nEvents); // last 2 args are optional. > ```. > . > rather than. > . > ```c++. > ROOT::RDF::ProgressHelper progress(everyNEvents, nEvents);. > h.OnPartialResultSlot(10000, [&progress](unsigned int slot, TH1D& histo){ progress(slot, histo); });. > ```. > . > I would also rename `CountEvents` to the slightly scarier `RetrieveNEvents` (which might take string_views rather than `const char *`) to hint at the fact that it can be an expensive call. Excellent! Tell me though: An RDF (= RNode I think?) doesn't take callbacks. I need a `RResultPtr`, correct? Do I have to attach a dummy result or did I miss something?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:481,availability,sli,slightly,481,"> Thanks a lot Stephan! > On the line of your first to-do item, I think the programming model should be:. > . > ```c++. > auto progbar = ROOT::RDF::MakeProgressBar(df, everyNEvents, nEvents); // last 2 args are optional. > ```. > . > rather than. > . > ```c++. > ROOT::RDF::ProgressHelper progress(everyNEvents, nEvents);. > h.OnPartialResultSlot(10000, [&progress](unsigned int slot, TH1D& histo){ progress(slot, histo); });. > ```. > . > I would also rename `CountEvents` to the slightly scarier `RetrieveNEvents` (which might take string_views rather than `const char *`) to hint at the fact that it can be an expensive call. Excellent! Tell me though: An RDF (= RNode I think?) doesn't take callbacks. I need a `RResultPtr`, correct? Do I have to attach a dummy result or did I miss something?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:88,energy efficiency,model,model,88,"> Thanks a lot Stephan! > On the line of your first to-do item, I think the programming model should be:. > . > ```c++. > auto progbar = ROOT::RDF::MakeProgressBar(df, everyNEvents, nEvents); // last 2 args are optional. > ```. > . > rather than. > . > ```c++. > ROOT::RDF::ProgressHelper progress(everyNEvents, nEvents);. > h.OnPartialResultSlot(10000, [&progress](unsigned int slot, TH1D& histo){ progress(slot, histo); });. > ```. > . > I would also rename `CountEvents` to the slightly scarier `RetrieveNEvents` (which might take string_views rather than `const char *`) to hint at the fact that it can be an expensive call. Excellent! Tell me though: An RDF (= RNode I think?) doesn't take callbacks. I need a `RResultPtr`, correct? Do I have to attach a dummy result or did I miss something?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:379,reliability,slo,slot,379,"> Thanks a lot Stephan! > On the line of your first to-do item, I think the programming model should be:. > . > ```c++. > auto progbar = ROOT::RDF::MakeProgressBar(df, everyNEvents, nEvents); // last 2 args are optional. > ```. > . > rather than. > . > ```c++. > ROOT::RDF::ProgressHelper progress(everyNEvents, nEvents);. > h.OnPartialResultSlot(10000, [&progress](unsigned int slot, TH1D& histo){ progress(slot, histo); });. > ```. > . > I would also rename `CountEvents` to the slightly scarier `RetrieveNEvents` (which might take string_views rather than `const char *`) to hint at the fact that it can be an expensive call. Excellent! Tell me though: An RDF (= RNode I think?) doesn't take callbacks. I need a `RResultPtr`, correct? Do I have to attach a dummy result or did I miss something?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:408,reliability,slo,slot,408,"> Thanks a lot Stephan! > On the line of your first to-do item, I think the programming model should be:. > . > ```c++. > auto progbar = ROOT::RDF::MakeProgressBar(df, everyNEvents, nEvents); // last 2 args are optional. > ```. > . > rather than. > . > ```c++. > ROOT::RDF::ProgressHelper progress(everyNEvents, nEvents);. > h.OnPartialResultSlot(10000, [&progress](unsigned int slot, TH1D& histo){ progress(slot, histo); });. > ```. > . > I would also rename `CountEvents` to the slightly scarier `RetrieveNEvents` (which might take string_views rather than `const char *`) to hint at the fact that it can be an expensive call. Excellent! Tell me though: An RDF (= RNode I think?) doesn't take callbacks. I need a `RResultPtr`, correct? Do I have to attach a dummy result or did I miss something?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:481,reliability,sli,slightly,481,"> Thanks a lot Stephan! > On the line of your first to-do item, I think the programming model should be:. > . > ```c++. > auto progbar = ROOT::RDF::MakeProgressBar(df, everyNEvents, nEvents); // last 2 args are optional. > ```. > . > rather than. > . > ```c++. > ROOT::RDF::ProgressHelper progress(everyNEvents, nEvents);. > h.OnPartialResultSlot(10000, [&progress](unsigned int slot, TH1D& histo){ progress(slot, histo); });. > ```. > . > I would also rename `CountEvents` to the slightly scarier `RetrieveNEvents` (which might take string_views rather than `const char *`) to hint at the fact that it can be an expensive call. Excellent! Tell me though: An RDF (= RNode I think?) doesn't take callbacks. I need a `RResultPtr`, correct? Do I have to attach a dummy result or did I miss something?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:682,reliability,doe,doesn,682,"> Thanks a lot Stephan! > On the line of your first to-do item, I think the programming model should be:. > . > ```c++. > auto progbar = ROOT::RDF::MakeProgressBar(df, everyNEvents, nEvents); // last 2 args are optional. > ```. > . > rather than. > . > ```c++. > ROOT::RDF::ProgressHelper progress(everyNEvents, nEvents);. > h.OnPartialResultSlot(10000, [&progress](unsigned int slot, TH1D& histo){ progress(slot, histo); });. > ```. > . > I would also rename `CountEvents` to the slightly scarier `RetrieveNEvents` (which might take string_views rather than `const char *`) to hint at the fact that it can be an expensive call. Excellent! Tell me though: An RDF (= RNode I think?) doesn't take callbacks. I need a `RResultPtr`, correct? Do I have to attach a dummy result or did I miss something?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:88,security,model,model,88,"> Thanks a lot Stephan! > On the line of your first to-do item, I think the programming model should be:. > . > ```c++. > auto progbar = ROOT::RDF::MakeProgressBar(df, everyNEvents, nEvents); // last 2 args are optional. > ```. > . > rather than. > . > ```c++. > ROOT::RDF::ProgressHelper progress(everyNEvents, nEvents);. > h.OnPartialResultSlot(10000, [&progress](unsigned int slot, TH1D& histo){ progress(slot, histo); });. > ```. > . > I would also rename `CountEvents` to the slightly scarier `RetrieveNEvents` (which might take string_views rather than `const char *`) to hint at the fact that it can be an expensive call. Excellent! Tell me though: An RDF (= RNode I think?) doesn't take callbacks. I need a `RResultPtr`, correct? Do I have to attach a dummy result or did I miss something?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:274,usability,Progress,ProgressHelper,274,"> Thanks a lot Stephan! > On the line of your first to-do item, I think the programming model should be:. > . > ```c++. > auto progbar = ROOT::RDF::MakeProgressBar(df, everyNEvents, nEvents); // last 2 args are optional. > ```. > . > rather than. > . > ```c++. > ROOT::RDF::ProgressHelper progress(everyNEvents, nEvents);. > h.OnPartialResultSlot(10000, [&progress](unsigned int slot, TH1D& histo){ progress(slot, histo); });. > ```. > . > I would also rename `CountEvents` to the slightly scarier `RetrieveNEvents` (which might take string_views rather than `const char *`) to hint at the fact that it can be an expensive call. Excellent! Tell me though: An RDF (= RNode I think?) doesn't take callbacks. I need a `RResultPtr`, correct? Do I have to attach a dummy result or did I miss something?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:289,usability,progress,progress,289,"> Thanks a lot Stephan! > On the line of your first to-do item, I think the programming model should be:. > . > ```c++. > auto progbar = ROOT::RDF::MakeProgressBar(df, everyNEvents, nEvents); // last 2 args are optional. > ```. > . > rather than. > . > ```c++. > ROOT::RDF::ProgressHelper progress(everyNEvents, nEvents);. > h.OnPartialResultSlot(10000, [&progress](unsigned int slot, TH1D& histo){ progress(slot, histo); });. > ```. > . > I would also rename `CountEvents` to the slightly scarier `RetrieveNEvents` (which might take string_views rather than `const char *`) to hint at the fact that it can be an expensive call. Excellent! Tell me though: An RDF (= RNode I think?) doesn't take callbacks. I need a `RResultPtr`, correct? Do I have to attach a dummy result or did I miss something?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:356,usability,progress,progress,356,"> Thanks a lot Stephan! > On the line of your first to-do item, I think the programming model should be:. > . > ```c++. > auto progbar = ROOT::RDF::MakeProgressBar(df, everyNEvents, nEvents); // last 2 args are optional. > ```. > . > rather than. > . > ```c++. > ROOT::RDF::ProgressHelper progress(everyNEvents, nEvents);. > h.OnPartialResultSlot(10000, [&progress](unsigned int slot, TH1D& histo){ progress(slot, histo); });. > ```. > . > I would also rename `CountEvents` to the slightly scarier `RetrieveNEvents` (which might take string_views rather than `const char *`) to hint at the fact that it can be an expensive call. Excellent! Tell me though: An RDF (= RNode I think?) doesn't take callbacks. I need a `RResultPtr`, correct? Do I have to attach a dummy result or did I miss something?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:399,usability,progress,progress,399,"> Thanks a lot Stephan! > On the line of your first to-do item, I think the programming model should be:. > . > ```c++. > auto progbar = ROOT::RDF::MakeProgressBar(df, everyNEvents, nEvents); // last 2 args are optional. > ```. > . > rather than. > . > ```c++. > ROOT::RDF::ProgressHelper progress(everyNEvents, nEvents);. > h.OnPartialResultSlot(10000, [&progress](unsigned int slot, TH1D& histo){ progress(slot, histo); });. > ```. > . > I would also rename `CountEvents` to the slightly scarier `RetrieveNEvents` (which might take string_views rather than `const char *`) to hint at the fact that it can be an expensive call. Excellent! Tell me though: An RDF (= RNode I think?) doesn't take callbacks. I need a `RResultPtr`, correct? Do I have to attach a dummy result or did I miss something?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:578,usability,hint,hint,578,"> Thanks a lot Stephan! > On the line of your first to-do item, I think the programming model should be:. > . > ```c++. > auto progbar = ROOT::RDF::MakeProgressBar(df, everyNEvents, nEvents); // last 2 args are optional. > ```. > . > rather than. > . > ```c++. > ROOT::RDF::ProgressHelper progress(everyNEvents, nEvents);. > h.OnPartialResultSlot(10000, [&progress](unsigned int slot, TH1D& histo){ progress(slot, histo); });. > ```. > . > I would also rename `CountEvents` to the slightly scarier `RetrieveNEvents` (which might take string_views rather than `const char *`) to hint at the fact that it can be an expensive call. Excellent! Tell me though: An RDF (= RNode I think?) doesn't take callbacks. I need a `RResultPtr`, correct? Do I have to attach a dummy result or did I miss something?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:200,integrability,event,events,200,"> ```c++. > auto progbar = ROOT::RDF::MakeProgressBar(df, everyNEvents, nEvents); // last 2 args are optional. > ```. Although the ProgressBar won't be a progress bar when we don't know the number of events ...",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:131,usability,Progress,ProgressBar,131,"> ```c++. > auto progbar = ROOT::RDF::MakeProgressBar(df, everyNEvents, nEvents); // last 2 args are optional. > ```. Although the ProgressBar won't be a progress bar when we don't know the number of events ...",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:154,usability,progress,progress,154,"> ```c++. > auto progbar = ROOT::RDF::MakeProgressBar(df, everyNEvents, nEvents); // last 2 args are optional. > ```. Although the ProgressBar won't be a progress bar when we don't know the number of events ...",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:597,energy efficiency,cool,cool,597,"> Tell me though: An RDF (= RNode I think?) doesn't take callbacks. I need a `RResultPtr`, correct? Do I have to attach a dummy result or did I miss something? OK, I had a look, and it would actually be amazing if I could get access to the LoopManager, because then I could just register the callback using `RLoopManager::RegisterCallback` (cannot link to docs because undocumented functions don't get an anchor in doxygen, any more ü§∑‚Äç‚ôÇÔ∏è ). I don't see a way to get to the [LoopManager](https://root.cern.ch/doc/master/classROOT_1_1Detail_1_1RDF_1_1RLoopManager.html), though. ... and it would be cool because I wouldn't need to throw away the payload of the resultptr, because I'm not using it anyway.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:44,reliability,doe,doesn,44,"> Tell me though: An RDF (= RNode I think?) doesn't take callbacks. I need a `RResultPtr`, correct? Do I have to attach a dummy result or did I miss something? OK, I had a look, and it would actually be amazing if I could get access to the LoopManager, because then I could just register the callback using `RLoopManager::RegisterCallback` (cannot link to docs because undocumented functions don't get an anchor in doxygen, any more ü§∑‚Äç‚ôÇÔ∏è ). I don't see a way to get to the [LoopManager](https://root.cern.ch/doc/master/classROOT_1_1Detail_1_1RDF_1_1RLoopManager.html), though. ... and it would be cool because I wouldn't need to throw away the payload of the resultptr, because I'm not using it anyway.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:226,security,access,access,226,"> Tell me though: An RDF (= RNode I think?) doesn't take callbacks. I need a `RResultPtr`, correct? Do I have to attach a dummy result or did I miss something? OK, I had a look, and it would actually be amazing if I could get access to the LoopManager, because then I could just register the callback using `RLoopManager::RegisterCallback` (cannot link to docs because undocumented functions don't get an anchor in doxygen, any more ü§∑‚Äç‚ôÇÔ∏è ). I don't see a way to get to the [LoopManager](https://root.cern.ch/doc/master/classROOT_1_1Detail_1_1RDF_1_1RLoopManager.html), though. ... and it would be cool because I wouldn't need to throw away the payload of the resultptr, because I'm not using it anyway.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:369,usability,undo,undocumented,369,"> Tell me though: An RDF (= RNode I think?) doesn't take callbacks. I need a `RResultPtr`, correct? Do I have to attach a dummy result or did I miss something? OK, I had a look, and it would actually be amazing if I could get access to the LoopManager, because then I could just register the callback using `RLoopManager::RegisterCallback` (cannot link to docs because undocumented functions don't get an anchor in doxygen, any more ü§∑‚Äç‚ôÇÔ∏è ). I don't see a way to get to the [LoopManager](https://root.cern.ch/doc/master/classROOT_1_1Detail_1_1RDF_1_1RLoopManager.html), though. ... and it would be cool because I wouldn't need to throw away the payload of the resultptr, because I'm not using it anyway.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:789,deployability,updat,updating,789,"> Although the ProgressBar won't be a progress bar when we don't know the number of events ... You're right, better name pending (`AddProgressIndicator`?). > It would actually be amazing if I could get access to the LoopManager, because then I could just register the callback using RLoopManager::RegisterCallback. Yes that's a great idea, there are actually several advantages:. - as you mention, in the callback you wouldn't have to discard a partially evaluated result anymore. - `AddProgressIndicator` wouldn't need to return anything, it can just do what it says without users having to deal with the returned object. - probably the biggest: we wouldn't have to do any magic to make sure that the method is called _on the head node_ (because if you call it on a `Filter`, now you are updating the progress bar every N _filtered_ entries) . Now -- of course we don't just expose the `RLoopManager` to _anyone_, but we trust you, so you can add a `ROOT::Internal::ExtractLoopManager` function as a friend of `RInterface` that just returns `interface.fLoopManager`?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:84,integrability,event,events,84,"> Although the ProgressBar won't be a progress bar when we don't know the number of events ... You're right, better name pending (`AddProgressIndicator`?). > It would actually be amazing if I could get access to the LoopManager, because then I could just register the callback using RLoopManager::RegisterCallback. Yes that's a great idea, there are actually several advantages:. - as you mention, in the callback you wouldn't have to discard a partially evaluated result anymore. - `AddProgressIndicator` wouldn't need to return anything, it can just do what it says without users having to deal with the returned object. - probably the biggest: we wouldn't have to do any magic to make sure that the method is called _on the head node_ (because if you call it on a `Filter`, now you are updating the progress bar every N _filtered_ entries) . Now -- of course we don't just expose the `RLoopManager` to _anyone_, but we trust you, so you can add a `ROOT::Internal::ExtractLoopManager` function as a friend of `RInterface` that just returns `interface.fLoopManager`?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:768,integrability,Filter,Filter,768,"> Although the ProgressBar won't be a progress bar when we don't know the number of events ... You're right, better name pending (`AddProgressIndicator`?). > It would actually be amazing if I could get access to the LoopManager, because then I could just register the callback using RLoopManager::RegisterCallback. Yes that's a great idea, there are actually several advantages:. - as you mention, in the callback you wouldn't have to discard a partially evaluated result anymore. - `AddProgressIndicator` wouldn't need to return anything, it can just do what it says without users having to deal with the returned object. - probably the biggest: we wouldn't have to do any magic to make sure that the method is called _on the head node_ (because if you call it on a `Filter`, now you are updating the progress bar every N _filtered_ entries) . Now -- of course we don't just expose the `RLoopManager` to _anyone_, but we trust you, so you can add a `ROOT::Internal::ExtractLoopManager` function as a friend of `RInterface` that just returns `interface.fLoopManager`?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:1043,integrability,interfac,interface,1043,"> Although the ProgressBar won't be a progress bar when we don't know the number of events ... You're right, better name pending (`AddProgressIndicator`?). > It would actually be amazing if I could get access to the LoopManager, because then I could just register the callback using RLoopManager::RegisterCallback. Yes that's a great idea, there are actually several advantages:. - as you mention, in the callback you wouldn't have to discard a partially evaluated result anymore. - `AddProgressIndicator` wouldn't need to return anything, it can just do what it says without users having to deal with the returned object. - probably the biggest: we wouldn't have to do any magic to make sure that the method is called _on the head node_ (because if you call it on a `Filter`, now you are updating the progress bar every N _filtered_ entries) . Now -- of course we don't just expose the `RLoopManager` to _anyone_, but we trust you, so you can add a `ROOT::Internal::ExtractLoopManager` function as a friend of `RInterface` that just returns `interface.fLoopManager`?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:1043,interoperability,interfac,interface,1043,"> Although the ProgressBar won't be a progress bar when we don't know the number of events ... You're right, better name pending (`AddProgressIndicator`?). > It would actually be amazing if I could get access to the LoopManager, because then I could just register the callback using RLoopManager::RegisterCallback. Yes that's a great idea, there are actually several advantages:. - as you mention, in the callback you wouldn't have to discard a partially evaluated result anymore. - `AddProgressIndicator` wouldn't need to return anything, it can just do what it says without users having to deal with the returned object. - probably the biggest: we wouldn't have to do any magic to make sure that the method is called _on the head node_ (because if you call it on a `Filter`, now you are updating the progress bar every N _filtered_ entries) . Now -- of course we don't just expose the `RLoopManager` to _anyone_, but we trust you, so you can add a `ROOT::Internal::ExtractLoopManager` function as a friend of `RInterface` that just returns `interface.fLoopManager`?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:1043,modifiability,interfac,interface,1043,"> Although the ProgressBar won't be a progress bar when we don't know the number of events ... You're right, better name pending (`AddProgressIndicator`?). > It would actually be amazing if I could get access to the LoopManager, because then I could just register the callback using RLoopManager::RegisterCallback. Yes that's a great idea, there are actually several advantages:. - as you mention, in the callback you wouldn't have to discard a partially evaluated result anymore. - `AddProgressIndicator` wouldn't need to return anything, it can just do what it says without users having to deal with the returned object. - probably the biggest: we wouldn't have to do any magic to make sure that the method is called _on the head node_ (because if you call it on a `Filter`, now you are updating the progress bar every N _filtered_ entries) . Now -- of course we don't just expose the `RLoopManager` to _anyone_, but we trust you, so you can add a `ROOT::Internal::ExtractLoopManager` function as a friend of `RInterface` that just returns `interface.fLoopManager`?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:789,safety,updat,updating,789,"> Although the ProgressBar won't be a progress bar when we don't know the number of events ... You're right, better name pending (`AddProgressIndicator`?). > It would actually be amazing if I could get access to the LoopManager, because then I could just register the callback using RLoopManager::RegisterCallback. Yes that's a great idea, there are actually several advantages:. - as you mention, in the callback you wouldn't have to discard a partially evaluated result anymore. - `AddProgressIndicator` wouldn't need to return anything, it can just do what it says without users having to deal with the returned object. - probably the biggest: we wouldn't have to do any magic to make sure that the method is called _on the head node_ (because if you call it on a `Filter`, now you are updating the progress bar every N _filtered_ entries) . Now -- of course we don't just expose the `RLoopManager` to _anyone_, but we trust you, so you can add a `ROOT::Internal::ExtractLoopManager` function as a friend of `RInterface` that just returns `interface.fLoopManager`?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:202,security,access,access,202,"> Although the ProgressBar won't be a progress bar when we don't know the number of events ... You're right, better name pending (`AddProgressIndicator`?). > It would actually be amazing if I could get access to the LoopManager, because then I could just register the callback using RLoopManager::RegisterCallback. Yes that's a great idea, there are actually several advantages:. - as you mention, in the callback you wouldn't have to discard a partially evaluated result anymore. - `AddProgressIndicator` wouldn't need to return anything, it can just do what it says without users having to deal with the returned object. - probably the biggest: we wouldn't have to do any magic to make sure that the method is called _on the head node_ (because if you call it on a `Filter`, now you are updating the progress bar every N _filtered_ entries) . Now -- of course we don't just expose the `RLoopManager` to _anyone_, but we trust you, so you can add a `ROOT::Internal::ExtractLoopManager` function as a friend of `RInterface` that just returns `interface.fLoopManager`?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:789,security,updat,updating,789,"> Although the ProgressBar won't be a progress bar when we don't know the number of events ... You're right, better name pending (`AddProgressIndicator`?). > It would actually be amazing if I could get access to the LoopManager, because then I could just register the callback using RLoopManager::RegisterCallback. Yes that's a great idea, there are actually several advantages:. - as you mention, in the callback you wouldn't have to discard a partially evaluated result anymore. - `AddProgressIndicator` wouldn't need to return anything, it can just do what it says without users having to deal with the returned object. - probably the biggest: we wouldn't have to do any magic to make sure that the method is called _on the head node_ (because if you call it on a `Filter`, now you are updating the progress bar every N _filtered_ entries) . Now -- of course we don't just expose the `RLoopManager` to _anyone_, but we trust you, so you can add a `ROOT::Internal::ExtractLoopManager` function as a friend of `RInterface` that just returns `interface.fLoopManager`?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:876,security,expos,expose,876,"> Although the ProgressBar won't be a progress bar when we don't know the number of events ... You're right, better name pending (`AddProgressIndicator`?). > It would actually be amazing if I could get access to the LoopManager, because then I could just register the callback using RLoopManager::RegisterCallback. Yes that's a great idea, there are actually several advantages:. - as you mention, in the callback you wouldn't have to discard a partially evaluated result anymore. - `AddProgressIndicator` wouldn't need to return anything, it can just do what it says without users having to deal with the returned object. - probably the biggest: we wouldn't have to do any magic to make sure that the method is called _on the head node_ (because if you call it on a `Filter`, now you are updating the progress bar every N _filtered_ entries) . Now -- of course we don't just expose the `RLoopManager` to _anyone_, but we trust you, so you can add a `ROOT::Internal::ExtractLoopManager` function as a friend of `RInterface` that just returns `interface.fLoopManager`?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:922,security,trust,trust,922,"> Although the ProgressBar won't be a progress bar when we don't know the number of events ... You're right, better name pending (`AddProgressIndicator`?). > It would actually be amazing if I could get access to the LoopManager, because then I could just register the callback using RLoopManager::RegisterCallback. Yes that's a great idea, there are actually several advantages:. - as you mention, in the callback you wouldn't have to discard a partially evaluated result anymore. - `AddProgressIndicator` wouldn't need to return anything, it can just do what it says without users having to deal with the returned object. - probably the biggest: we wouldn't have to do any magic to make sure that the method is called _on the head node_ (because if you call it on a `Filter`, now you are updating the progress bar every N _filtered_ entries) . Now -- of course we don't just expose the `RLoopManager` to _anyone_, but we trust you, so you can add a `ROOT::Internal::ExtractLoopManager` function as a friend of `RInterface` that just returns `interface.fLoopManager`?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:15,usability,Progress,ProgressBar,15,"> Although the ProgressBar won't be a progress bar when we don't know the number of events ... You're right, better name pending (`AddProgressIndicator`?). > It would actually be amazing if I could get access to the LoopManager, because then I could just register the callback using RLoopManager::RegisterCallback. Yes that's a great idea, there are actually several advantages:. - as you mention, in the callback you wouldn't have to discard a partially evaluated result anymore. - `AddProgressIndicator` wouldn't need to return anything, it can just do what it says without users having to deal with the returned object. - probably the biggest: we wouldn't have to do any magic to make sure that the method is called _on the head node_ (because if you call it on a `Filter`, now you are updating the progress bar every N _filtered_ entries) . Now -- of course we don't just expose the `RLoopManager` to _anyone_, but we trust you, so you can add a `ROOT::Internal::ExtractLoopManager` function as a friend of `RInterface` that just returns `interface.fLoopManager`?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:38,usability,progress,progress,38,"> Although the ProgressBar won't be a progress bar when we don't know the number of events ... You're right, better name pending (`AddProgressIndicator`?). > It would actually be amazing if I could get access to the LoopManager, because then I could just register the callback using RLoopManager::RegisterCallback. Yes that's a great idea, there are actually several advantages:. - as you mention, in the callback you wouldn't have to discard a partially evaluated result anymore. - `AddProgressIndicator` wouldn't need to return anything, it can just do what it says without users having to deal with the returned object. - probably the biggest: we wouldn't have to do any magic to make sure that the method is called _on the head node_ (because if you call it on a `Filter`, now you are updating the progress bar every N _filtered_ entries) . Now -- of course we don't just expose the `RLoopManager` to _anyone_, but we trust you, so you can add a `ROOT::Internal::ExtractLoopManager` function as a friend of `RInterface` that just returns `interface.fLoopManager`?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:576,usability,user,users,576,"> Although the ProgressBar won't be a progress bar when we don't know the number of events ... You're right, better name pending (`AddProgressIndicator`?). > It would actually be amazing if I could get access to the LoopManager, because then I could just register the callback using RLoopManager::RegisterCallback. Yes that's a great idea, there are actually several advantages:. - as you mention, in the callback you wouldn't have to discard a partially evaluated result anymore. - `AddProgressIndicator` wouldn't need to return anything, it can just do what it says without users having to deal with the returned object. - probably the biggest: we wouldn't have to do any magic to make sure that the method is called _on the head node_ (because if you call it on a `Filter`, now you are updating the progress bar every N _filtered_ entries) . Now -- of course we don't just expose the `RLoopManager` to _anyone_, but we trust you, so you can add a `ROOT::Internal::ExtractLoopManager` function as a friend of `RInterface` that just returns `interface.fLoopManager`?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:802,usability,progress,progress,802,"> Although the ProgressBar won't be a progress bar when we don't know the number of events ... You're right, better name pending (`AddProgressIndicator`?). > It would actually be amazing if I could get access to the LoopManager, because then I could just register the callback using RLoopManager::RegisterCallback. Yes that's a great idea, there are actually several advantages:. - as you mention, in the callback you wouldn't have to discard a partially evaluated result anymore. - `AddProgressIndicator` wouldn't need to return anything, it can just do what it says without users having to deal with the returned object. - probably the biggest: we wouldn't have to do any magic to make sure that the method is called _on the head node_ (because if you call it on a `Filter`, now you are updating the progress bar every N _filtered_ entries) . Now -- of course we don't just expose the `RLoopManager` to _anyone_, but we trust you, so you can add a `ROOT::Internal::ExtractLoopManager` function as a friend of `RInterface` that just returns `interface.fLoopManager`?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:219,deployability,log,logic,219,"(as per the discussion above, this will not get merged as is because we rather prefer an implementation that can estimate the remaining number of entries/time without having to check all input files beforehand, but the logic to create and update the progress bar, including colors, windows support etc. is still directly re-usable and super useful)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:239,deployability,updat,update,239,"(as per the discussion above, this will not get merged as is because we rather prefer an implementation that can estimate the remaining number of entries/time without having to check all input files beforehand, but the logic to create and update the progress bar, including colors, windows support etc. is still directly re-usable and super useful)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:113,energy efficiency,estimat,estimate,113,"(as per the discussion above, this will not get merged as is because we rather prefer an implementation that can estimate the remaining number of entries/time without having to check all input files beforehand, but the logic to create and update the progress bar, including colors, windows support etc. is still directly re-usable and super useful)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:154,performance,time,time,154,"(as per the discussion above, this will not get merged as is because we rather prefer an implementation that can estimate the remaining number of entries/time without having to check all input files beforehand, but the logic to create and update the progress bar, including colors, windows support etc. is still directly re-usable and super useful)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:187,safety,input,input,187,"(as per the discussion above, this will not get merged as is because we rather prefer an implementation that can estimate the remaining number of entries/time without having to check all input files beforehand, but the logic to create and update the progress bar, including colors, windows support etc. is still directly re-usable and super useful)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:219,safety,log,logic,219,"(as per the discussion above, this will not get merged as is because we rather prefer an implementation that can estimate the remaining number of entries/time without having to check all input files beforehand, but the logic to create and update the progress bar, including colors, windows support etc. is still directly re-usable and super useful)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:239,safety,updat,update,239,"(as per the discussion above, this will not get merged as is because we rather prefer an implementation that can estimate the remaining number of entries/time without having to check all input files beforehand, but the logic to create and update the progress bar, including colors, windows support etc. is still directly re-usable and super useful)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:219,security,log,logic,219,"(as per the discussion above, this will not get merged as is because we rather prefer an implementation that can estimate the remaining number of entries/time without having to check all input files beforehand, but the logic to create and update the progress bar, including colors, windows support etc. is still directly re-usable and super useful)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:239,security,updat,update,239,"(as per the discussion above, this will not get merged as is because we rather prefer an implementation that can estimate the remaining number of entries/time without having to check all input files beforehand, but the logic to create and update the progress bar, including colors, windows support etc. is still directly re-usable and super useful)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:219,testability,log,logic,219,"(as per the discussion above, this will not get merged as is because we rather prefer an implementation that can estimate the remaining number of entries/time without having to check all input files beforehand, but the logic to create and update the progress bar, including colors, windows support etc. is still directly re-usable and super useful)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:79,usability,prefer,prefer,79,"(as per the discussion above, this will not get merged as is because we rather prefer an implementation that can estimate the remaining number of entries/time without having to check all input files beforehand, but the logic to create and update the progress bar, including colors, windows support etc. is still directly re-usable and super useful)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:187,usability,input,input,187,"(as per the discussion above, this will not get merged as is because we rather prefer an implementation that can estimate the remaining number of entries/time without having to check all input files beforehand, but the logic to create and update the progress bar, including colors, windows support etc. is still directly re-usable and super useful)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:250,usability,progress,progress,250,"(as per the discussion above, this will not get merged as is because we rather prefer an implementation that can estimate the remaining number of entries/time without having to check all input files beforehand, but the logic to create and update the progress bar, including colors, windows support etc. is still directly re-usable and super useful)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:290,usability,support,support,290,"(as per the discussion above, this will not get merged as is because we rather prefer an implementation that can estimate the remaining number of entries/time without having to check all input files beforehand, but the logic to create and update the progress bar, including colors, windows support etc. is still directly re-usable and super useful)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8675:324,usability,usab,usable,324,"(as per the discussion above, this will not get merged as is because we rather prefer an implementation that can estimate the remaining number of entries/time without having to check all input files beforehand, but the logic to create and update the progress bar, including colors, windows support etc. is still directly re-usable and super useful)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8675
https://github.com/root-project/root/pull/8676:1768,availability,servic,service,1768,"7/root-project/root/74ab7b0df345dbf4c3aec710a4d01e906862811b/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) found:. - :information_source: **1** minor issue. :point_down:. ## Top issues. <table>. <thead>. <tr>. <th align=""left"">Description</th>. <th align=""left"">Example fixes</th>. </tr>. </thead>. <tbody>. <tr>. <td width=""77%"">No catch method for promise. This may result in an unhandled promise rejection. Occurrences: <ul><li><a href=""https://github.com/root-project/root/blob/74ab7b0df345dbf4c3aec710a4d01e906862811b/ui5/eve7/lib/GlViewerRCore.js#L54"">GlViewerRCore.js:54</a></li> <li><a href=""https://github.com/root-project/root/blob/74ab7b0df345dbf4c3aec710a4d01e906862811b/ui5/eve7/lib/GlViewerRCore.js#L60"">GlViewerRCore.js:60</a></li> <li><a href=""https://github.com/root-project/root/blob/74ab7b0df345dbf4c3aec710a4d01e906862811b/ui5/eve7/lib/GlViewerRCore.js#L277"">GlViewerRCore.js:277</a></li></ul></td>. <td width=""23%""><a href=""https://www.deepcode.ai/app/gh/root-project/root/d356dc18b256eeed3728fc8fc82f4ab840594d37/root-project/root/74ab7b0df345dbf4c3aec710a4d01e906862811b/pr/_/%2Fui5%2Feve7%2Flib%2FGlViewerRCore.js/javascript%2Fdc_interfile_project%2FPromiseNotCaughtGeneral/code/?utm_source=gh_review&"">:wrench: Example fixes</a></td>. </tr>. </tbody>. </table>. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/d356dc18b256eeed3728fc8fc82f4ab840594d37/root-project/root/74ab7b0df345dbf4c3aec710a4d01e906862811b/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8676
https://github.com/root-project/root/pull/8676:1768,deployability,servic,service,1768,"7/root-project/root/74ab7b0df345dbf4c3aec710a4d01e906862811b/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) found:. - :information_source: **1** minor issue. :point_down:. ## Top issues. <table>. <thead>. <tr>. <th align=""left"">Description</th>. <th align=""left"">Example fixes</th>. </tr>. </thead>. <tbody>. <tr>. <td width=""77%"">No catch method for promise. This may result in an unhandled promise rejection. Occurrences: <ul><li><a href=""https://github.com/root-project/root/blob/74ab7b0df345dbf4c3aec710a4d01e906862811b/ui5/eve7/lib/GlViewerRCore.js#L54"">GlViewerRCore.js:54</a></li> <li><a href=""https://github.com/root-project/root/blob/74ab7b0df345dbf4c3aec710a4d01e906862811b/ui5/eve7/lib/GlViewerRCore.js#L60"">GlViewerRCore.js:60</a></li> <li><a href=""https://github.com/root-project/root/blob/74ab7b0df345dbf4c3aec710a4d01e906862811b/ui5/eve7/lib/GlViewerRCore.js#L277"">GlViewerRCore.js:277</a></li></ul></td>. <td width=""23%""><a href=""https://www.deepcode.ai/app/gh/root-project/root/d356dc18b256eeed3728fc8fc82f4ab840594d37/root-project/root/74ab7b0df345dbf4c3aec710a4d01e906862811b/pr/_/%2Fui5%2Feve7%2Flib%2FGlViewerRCore.js/javascript%2Fdc_interfile_project%2FPromiseNotCaughtGeneral/code/?utm_source=gh_review&"">:wrench: Example fixes</a></td>. </tr>. </tbody>. </table>. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/d356dc18b256eeed3728fc8fc82f4ab840594d37/root-project/root/74ab7b0df345dbf4c3aec710a4d01e906862811b/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8676
https://github.com/root-project/root/pull/8676:1780,deployability,API,API,1780,"7/root-project/root/74ab7b0df345dbf4c3aec710a4d01e906862811b/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) found:. - :information_source: **1** minor issue. :point_down:. ## Top issues. <table>. <thead>. <tr>. <th align=""left"">Description</th>. <th align=""left"">Example fixes</th>. </tr>. </thead>. <tbody>. <tr>. <td width=""77%"">No catch method for promise. This may result in an unhandled promise rejection. Occurrences: <ul><li><a href=""https://github.com/root-project/root/blob/74ab7b0df345dbf4c3aec710a4d01e906862811b/ui5/eve7/lib/GlViewerRCore.js#L54"">GlViewerRCore.js:54</a></li> <li><a href=""https://github.com/root-project/root/blob/74ab7b0df345dbf4c3aec710a4d01e906862811b/ui5/eve7/lib/GlViewerRCore.js#L60"">GlViewerRCore.js:60</a></li> <li><a href=""https://github.com/root-project/root/blob/74ab7b0df345dbf4c3aec710a4d01e906862811b/ui5/eve7/lib/GlViewerRCore.js#L277"">GlViewerRCore.js:277</a></li></ul></td>. <td width=""23%""><a href=""https://www.deepcode.ai/app/gh/root-project/root/d356dc18b256eeed3728fc8fc82f4ab840594d37/root-project/root/74ab7b0df345dbf4c3aec710a4d01e906862811b/pr/_/%2Fui5%2Feve7%2Flib%2FGlViewerRCore.js/javascript%2Fdc_interfile_project%2FPromiseNotCaughtGeneral/code/?utm_source=gh_review&"">:wrench: Example fixes</a></td>. </tr>. </tbody>. </table>. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/d356dc18b256eeed3728fc8fc82f4ab840594d37/root-project/root/74ab7b0df345dbf4c3aec710a4d01e906862811b/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8676
https://github.com/root-project/root/pull/8676:1768,integrability,servic,service,1768,"7/root-project/root/74ab7b0df345dbf4c3aec710a4d01e906862811b/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) found:. - :information_source: **1** minor issue. :point_down:. ## Top issues. <table>. <thead>. <tr>. <th align=""left"">Description</th>. <th align=""left"">Example fixes</th>. </tr>. </thead>. <tbody>. <tr>. <td width=""77%"">No catch method for promise. This may result in an unhandled promise rejection. Occurrences: <ul><li><a href=""https://github.com/root-project/root/blob/74ab7b0df345dbf4c3aec710a4d01e906862811b/ui5/eve7/lib/GlViewerRCore.js#L54"">GlViewerRCore.js:54</a></li> <li><a href=""https://github.com/root-project/root/blob/74ab7b0df345dbf4c3aec710a4d01e906862811b/ui5/eve7/lib/GlViewerRCore.js#L60"">GlViewerRCore.js:60</a></li> <li><a href=""https://github.com/root-project/root/blob/74ab7b0df345dbf4c3aec710a4d01e906862811b/ui5/eve7/lib/GlViewerRCore.js#L277"">GlViewerRCore.js:277</a></li></ul></td>. <td width=""23%""><a href=""https://www.deepcode.ai/app/gh/root-project/root/d356dc18b256eeed3728fc8fc82f4ab840594d37/root-project/root/74ab7b0df345dbf4c3aec710a4d01e906862811b/pr/_/%2Fui5%2Feve7%2Flib%2FGlViewerRCore.js/javascript%2Fdc_interfile_project%2FPromiseNotCaughtGeneral/code/?utm_source=gh_review&"">:wrench: Example fixes</a></td>. </tr>. </tbody>. </table>. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/d356dc18b256eeed3728fc8fc82f4ab840594d37/root-project/root/74ab7b0df345dbf4c3aec710a4d01e906862811b/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8676
https://github.com/root-project/root/pull/8676:1780,integrability,API,API,1780,"7/root-project/root/74ab7b0df345dbf4c3aec710a4d01e906862811b/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) found:. - :information_source: **1** minor issue. :point_down:. ## Top issues. <table>. <thead>. <tr>. <th align=""left"">Description</th>. <th align=""left"">Example fixes</th>. </tr>. </thead>. <tbody>. <tr>. <td width=""77%"">No catch method for promise. This may result in an unhandled promise rejection. Occurrences: <ul><li><a href=""https://github.com/root-project/root/blob/74ab7b0df345dbf4c3aec710a4d01e906862811b/ui5/eve7/lib/GlViewerRCore.js#L54"">GlViewerRCore.js:54</a></li> <li><a href=""https://github.com/root-project/root/blob/74ab7b0df345dbf4c3aec710a4d01e906862811b/ui5/eve7/lib/GlViewerRCore.js#L60"">GlViewerRCore.js:60</a></li> <li><a href=""https://github.com/root-project/root/blob/74ab7b0df345dbf4c3aec710a4d01e906862811b/ui5/eve7/lib/GlViewerRCore.js#L277"">GlViewerRCore.js:277</a></li></ul></td>. <td width=""23%""><a href=""https://www.deepcode.ai/app/gh/root-project/root/d356dc18b256eeed3728fc8fc82f4ab840594d37/root-project/root/74ab7b0df345dbf4c3aec710a4d01e906862811b/pr/_/%2Fui5%2Feve7%2Flib%2FGlViewerRCore.js/javascript%2Fdc_interfile_project%2FPromiseNotCaughtGeneral/code/?utm_source=gh_review&"">:wrench: Example fixes</a></td>. </tr>. </tbody>. </table>. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/d356dc18b256eeed3728fc8fc82f4ab840594d37/root-project/root/74ab7b0df345dbf4c3aec710a4d01e906862811b/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8676
https://github.com/root-project/root/pull/8676:1780,interoperability,API,API,1780,"7/root-project/root/74ab7b0df345dbf4c3aec710a4d01e906862811b/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) found:. - :information_source: **1** minor issue. :point_down:. ## Top issues. <table>. <thead>. <tr>. <th align=""left"">Description</th>. <th align=""left"">Example fixes</th>. </tr>. </thead>. <tbody>. <tr>. <td width=""77%"">No catch method for promise. This may result in an unhandled promise rejection. Occurrences: <ul><li><a href=""https://github.com/root-project/root/blob/74ab7b0df345dbf4c3aec710a4d01e906862811b/ui5/eve7/lib/GlViewerRCore.js#L54"">GlViewerRCore.js:54</a></li> <li><a href=""https://github.com/root-project/root/blob/74ab7b0df345dbf4c3aec710a4d01e906862811b/ui5/eve7/lib/GlViewerRCore.js#L60"">GlViewerRCore.js:60</a></li> <li><a href=""https://github.com/root-project/root/blob/74ab7b0df345dbf4c3aec710a4d01e906862811b/ui5/eve7/lib/GlViewerRCore.js#L277"">GlViewerRCore.js:277</a></li></ul></td>. <td width=""23%""><a href=""https://www.deepcode.ai/app/gh/root-project/root/d356dc18b256eeed3728fc8fc82f4ab840594d37/root-project/root/74ab7b0df345dbf4c3aec710a4d01e906862811b/pr/_/%2Fui5%2Feve7%2Flib%2FGlViewerRCore.js/javascript%2Fdc_interfile_project%2FPromiseNotCaughtGeneral/code/?utm_source=gh_review&"">:wrench: Example fixes</a></td>. </tr>. </tbody>. </table>. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/d356dc18b256eeed3728fc8fc82f4ab840594d37/root-project/root/74ab7b0df345dbf4c3aec710a4d01e906862811b/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8676
https://github.com/root-project/root/pull/8676:1970,interoperability,plug,plugins,1970,"7/root-project/root/74ab7b0df345dbf4c3aec710a4d01e906862811b/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) found:. - :information_source: **1** minor issue. :point_down:. ## Top issues. <table>. <thead>. <tr>. <th align=""left"">Description</th>. <th align=""left"">Example fixes</th>. </tr>. </thead>. <tbody>. <tr>. <td width=""77%"">No catch method for promise. This may result in an unhandled promise rejection. Occurrences: <ul><li><a href=""https://github.com/root-project/root/blob/74ab7b0df345dbf4c3aec710a4d01e906862811b/ui5/eve7/lib/GlViewerRCore.js#L54"">GlViewerRCore.js:54</a></li> <li><a href=""https://github.com/root-project/root/blob/74ab7b0df345dbf4c3aec710a4d01e906862811b/ui5/eve7/lib/GlViewerRCore.js#L60"">GlViewerRCore.js:60</a></li> <li><a href=""https://github.com/root-project/root/blob/74ab7b0df345dbf4c3aec710a4d01e906862811b/ui5/eve7/lib/GlViewerRCore.js#L277"">GlViewerRCore.js:277</a></li></ul></td>. <td width=""23%""><a href=""https://www.deepcode.ai/app/gh/root-project/root/d356dc18b256eeed3728fc8fc82f4ab840594d37/root-project/root/74ab7b0df345dbf4c3aec710a4d01e906862811b/pr/_/%2Fui5%2Feve7%2Flib%2FGlViewerRCore.js/javascript%2Fdc_interfile_project%2FPromiseNotCaughtGeneral/code/?utm_source=gh_review&"">:wrench: Example fixes</a></td>. </tr>. </tbody>. </table>. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/d356dc18b256eeed3728fc8fc82f4ab840594d37/root-project/root/74ab7b0df345dbf4c3aec710a4d01e906862811b/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8676
https://github.com/root-project/root/pull/8676:2042,interoperability,plug,plugin,2042,"7/root-project/root/74ab7b0df345dbf4c3aec710a4d01e906862811b/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) found:. - :information_source: **1** minor issue. :point_down:. ## Top issues. <table>. <thead>. <tr>. <th align=""left"">Description</th>. <th align=""left"">Example fixes</th>. </tr>. </thead>. <tbody>. <tr>. <td width=""77%"">No catch method for promise. This may result in an unhandled promise rejection. Occurrences: <ul><li><a href=""https://github.com/root-project/root/blob/74ab7b0df345dbf4c3aec710a4d01e906862811b/ui5/eve7/lib/GlViewerRCore.js#L54"">GlViewerRCore.js:54</a></li> <li><a href=""https://github.com/root-project/root/blob/74ab7b0df345dbf4c3aec710a4d01e906862811b/ui5/eve7/lib/GlViewerRCore.js#L60"">GlViewerRCore.js:60</a></li> <li><a href=""https://github.com/root-project/root/blob/74ab7b0df345dbf4c3aec710a4d01e906862811b/ui5/eve7/lib/GlViewerRCore.js#L277"">GlViewerRCore.js:277</a></li></ul></td>. <td width=""23%""><a href=""https://www.deepcode.ai/app/gh/root-project/root/d356dc18b256eeed3728fc8fc82f4ab840594d37/root-project/root/74ab7b0df345dbf4c3aec710a4d01e906862811b/pr/_/%2Fui5%2Feve7%2Flib%2FGlViewerRCore.js/javascript%2Fdc_interfile_project%2FPromiseNotCaughtGeneral/code/?utm_source=gh_review&"">:wrench: Example fixes</a></td>. </tr>. </tbody>. </table>. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/d356dc18b256eeed3728fc8fc82f4ab840594d37/root-project/root/74ab7b0df345dbf4c3aec710a4d01e906862811b/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8676
https://github.com/root-project/root/pull/8676:2093,interoperability,plug,plugin,2093,"7/root-project/root/74ab7b0df345dbf4c3aec710a4d01e906862811b/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) found:. - :information_source: **1** minor issue. :point_down:. ## Top issues. <table>. <thead>. <tr>. <th align=""left"">Description</th>. <th align=""left"">Example fixes</th>. </tr>. </thead>. <tbody>. <tr>. <td width=""77%"">No catch method for promise. This may result in an unhandled promise rejection. Occurrences: <ul><li><a href=""https://github.com/root-project/root/blob/74ab7b0df345dbf4c3aec710a4d01e906862811b/ui5/eve7/lib/GlViewerRCore.js#L54"">GlViewerRCore.js:54</a></li> <li><a href=""https://github.com/root-project/root/blob/74ab7b0df345dbf4c3aec710a4d01e906862811b/ui5/eve7/lib/GlViewerRCore.js#L60"">GlViewerRCore.js:60</a></li> <li><a href=""https://github.com/root-project/root/blob/74ab7b0df345dbf4c3aec710a4d01e906862811b/ui5/eve7/lib/GlViewerRCore.js#L277"">GlViewerRCore.js:277</a></li></ul></td>. <td width=""23%""><a href=""https://www.deepcode.ai/app/gh/root-project/root/d356dc18b256eeed3728fc8fc82f4ab840594d37/root-project/root/74ab7b0df345dbf4c3aec710a4d01e906862811b/pr/_/%2Fui5%2Feve7%2Flib%2FGlViewerRCore.js/javascript%2Fdc_interfile_project%2FPromiseNotCaughtGeneral/code/?utm_source=gh_review&"">:wrench: Example fixes</a></td>. </tr>. </tbody>. </table>. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/d356dc18b256eeed3728fc8fc82f4ab840594d37/root-project/root/74ab7b0df345dbf4c3aec710a4d01e906862811b/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8676
https://github.com/root-project/root/pull/8676:1768,modifiability,servic,service,1768,"7/root-project/root/74ab7b0df345dbf4c3aec710a4d01e906862811b/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) found:. - :information_source: **1** minor issue. :point_down:. ## Top issues. <table>. <thead>. <tr>. <th align=""left"">Description</th>. <th align=""left"">Example fixes</th>. </tr>. </thead>. <tbody>. <tr>. <td width=""77%"">No catch method for promise. This may result in an unhandled promise rejection. Occurrences: <ul><li><a href=""https://github.com/root-project/root/blob/74ab7b0df345dbf4c3aec710a4d01e906862811b/ui5/eve7/lib/GlViewerRCore.js#L54"">GlViewerRCore.js:54</a></li> <li><a href=""https://github.com/root-project/root/blob/74ab7b0df345dbf4c3aec710a4d01e906862811b/ui5/eve7/lib/GlViewerRCore.js#L60"">GlViewerRCore.js:60</a></li> <li><a href=""https://github.com/root-project/root/blob/74ab7b0df345dbf4c3aec710a4d01e906862811b/ui5/eve7/lib/GlViewerRCore.js#L277"">GlViewerRCore.js:277</a></li></ul></td>. <td width=""23%""><a href=""https://www.deepcode.ai/app/gh/root-project/root/d356dc18b256eeed3728fc8fc82f4ab840594d37/root-project/root/74ab7b0df345dbf4c3aec710a4d01e906862811b/pr/_/%2Fui5%2Feve7%2Flib%2FGlViewerRCore.js/javascript%2Fdc_interfile_project%2FPromiseNotCaughtGeneral/code/?utm_source=gh_review&"">:wrench: Example fixes</a></td>. </tr>. </tbody>. </table>. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/d356dc18b256eeed3728fc8fc82f4ab840594d37/root-project/root/74ab7b0df345dbf4c3aec710a4d01e906862811b/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8676
https://github.com/root-project/root/pull/8676:509,safety,unhandl,unhandled,509,"## DeepCode's analysis on [#74ab7b](https://www.deepcode.ai/app/gh/root-project/root/d356dc18b256eeed3728fc8fc82f4ab840594d37/root-project/root/74ab7b0df345dbf4c3aec710a4d01e906862811b/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) found:. - :information_source: **1** minor issue. :point_down:. ## Top issues. <table>. <thead>. <tr>. <th align=""left"">Description</th>. <th align=""left"">Example fixes</th>. </tr>. </thead>. <tbody>. <tr>. <td width=""77%"">No catch method for promise. This may result in an unhandled promise rejection. Occurrences: <ul><li><a href=""https://github.com/root-project/root/blob/74ab7b0df345dbf4c3aec710a4d01e906862811b/ui5/eve7/lib/GlViewerRCore.js#L54"">GlViewerRCore.js:54</a></li> <li><a href=""https://github.com/root-project/root/blob/74ab7b0df345dbf4c3aec710a4d01e906862811b/ui5/eve7/lib/GlViewerRCore.js#L60"">GlViewerRCore.js:60</a></li> <li><a href=""https://github.com/root-project/root/blob/74ab7b0df345dbf4c3aec710a4d01e906862811b/ui5/eve7/lib/GlViewerRCore.js#L277"">GlViewerRCore.js:277</a></li></ul></td>. <td width=""23%""><a href=""https://www.deepcode.ai/app/gh/root-project/root/d356dc18b256eeed3728fc8fc82f4ab840594d37/root-project/root/74ab7b0df345dbf4c3aec710a4d01e906862811b/pr/_/%2Fui5%2Feve7%2Flib%2FGlViewerRCore.js/javascript%2Fdc_interfile_project%2FPromiseNotCaughtGeneral/code/?utm_source=gh_review&"">:wrench: Example fixes</a></td>. </tr>. </tbody>. </table>. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/d356dc18b256eeed3728fc8fc82f4ab840594d37/root-project/root/74ab7b0df345dbf4c3aec710a4d01e906862811b/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=1&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interest",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8676
https://github.com/root-project/root/pull/8676:11,deployability,build,build,11,@phsft-bot build,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8676
https://github.com/root-project/root/pull/8676:11,deployability,build,build,11,@phsft-bot build,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8676
https://github.com/root-project/root/pull/8676:104,deployability,automat,automatic,104,"@alja, please use `TString::Format(,,,).Data()` when need to assign to `std::string`. On some platforms automatic conversion `TString` -> `std::string` does not work",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8676
https://github.com/root-project/root/pull/8676:28,interoperability,Format,Format,28,"@alja, please use `TString::Format(,,,).Data()` when need to assign to `std::string`. On some platforms automatic conversion `TString` -> `std::string` does not work",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8676
https://github.com/root-project/root/pull/8676:94,interoperability,platform,platforms,94,"@alja, please use `TString::Format(,,,).Data()` when need to assign to `std::string`. On some platforms automatic conversion `TString` -> `std::string` does not work",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8676
https://github.com/root-project/root/pull/8676:114,interoperability,convers,conversion,114,"@alja, please use `TString::Format(,,,).Data()` when need to assign to `std::string`. On some platforms automatic conversion `TString` -> `std::string` does not work",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8676
https://github.com/root-project/root/pull/8676:152,reliability,doe,does,152,"@alja, please use `TString::Format(,,,).Data()` when need to assign to `std::string`. On some platforms automatic conversion `TString` -> `std::string` does not work",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8676
https://github.com/root-project/root/pull/8676:104,testability,automat,automatic,104,"@alja, please use `TString::Format(,,,).Data()` when need to assign to `std::string`. On some platforms automatic conversion `TString` -> `std::string` does not work",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8676
https://github.com/root-project/root/pull/8676:11,deployability,build,build,11,@phsft-bot build,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8676
https://github.com/root-project/root/pull/8677:11,deployability,build,build,11,@phsft-bot build,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8677
https://github.com/root-project/root/pull/8677:6,deployability,fail,failing,6,Note: failing test is unrelated.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8677
https://github.com/root-project/root/pull/8677:6,reliability,fail,failing,6,Note: failing test is unrelated.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8677
https://github.com/root-project/root/pull/8677:14,safety,test,test,14,Note: failing test is unrelated.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8677
https://github.com/root-project/root/pull/8677:14,testability,test,test,14,Note: failing test is unrelated.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8677
https://github.com/root-project/root/pull/8678:11,deployability,build,build,11,@phsft-bot build,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8678
https://github.com/root-project/root/pull/8678:113,deployability,build,build,113,@veprbl please fork also [roottest](https://github.com/root-project/roottest) to prevent timeouts on the Windows build nodes. Thanks,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8678
https://github.com/root-project/root/pull/8678:89,performance,time,timeouts,89,@veprbl please fork also [roottest](https://github.com/root-project/roottest) to prevent timeouts on the Windows build nodes. Thanks,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8678
https://github.com/root-project/root/pull/8678:81,safety,prevent,prevent,81,@veprbl please fork also [roottest](https://github.com/root-project/roottest) to prevent timeouts on the Windows build nodes. Thanks,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8678
https://github.com/root-project/root/pull/8678:89,safety,timeout,timeouts,89,@veprbl please fork also [roottest](https://github.com/root-project/roottest) to prevent timeouts on the Windows build nodes. Thanks,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8678
https://github.com/root-project/root/pull/8678:81,security,preven,prevent,81,@veprbl please fork also [roottest](https://github.com/root-project/roottest) to prevent timeouts on the Windows build nodes. Thanks,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8678
https://github.com/root-project/root/pull/8678:77,modifiability,pac,package,77,Thanks for the PR. We have created another one which fixes the broken ubuntu package as well -- #8766,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8678
https://github.com/root-project/root/pull/8681:11,deployability,build,build,11,@phsft-bot build,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8681
https://github.com/root-project/root/pull/8681:0,deployability,Fail,Failing,0,Failing test is unrelated (it's happening in other PRs too),MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8681
https://github.com/root-project/root/pull/8681:0,reliability,Fail,Failing,0,Failing test is unrelated (it's happening in other PRs too),MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8681
https://github.com/root-project/root/pull/8681:8,safety,test,test,8,Failing test is unrelated (it's happening in other PRs too),MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8681
https://github.com/root-project/root/pull/8681:8,testability,test,test,8,Failing test is unrelated (it's happening in other PRs too),MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8681
https://github.com/root-project/root/pull/8681:11,deployability,build,build,11,@phsft-bot build.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8681
https://github.com/root-project/root/pull/8681:11,usability,feedback,feedback,11,@couet any feedback on this PR?,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8681
https://github.com/root-project/root/pull/8681:11,deployability,build,build,11,@phsft-bot build,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8681
https://github.com/root-project/root/pull/8684:11,deployability,build,build,11,@phsft-bot build just on ROOT-ubuntu2004/default with flags -Dtmva-sofie=On,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8684
https://github.com/root-project/root/pull/8684:11,deployability,build,build,11,@phsft-bot build just on ROOT-ubuntu2004/default with flags -Dtmva-sofie=On,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8684
https://github.com/root-project/root/pull/8684:11,deployability,build,build,11,@phsft-bot build just on ROOT-ubuntu2004/default with flags -Dtmva-sofie=On,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8684
https://github.com/root-project/root/pull/8684:11,deployability,build,build,11,@phsft-bot build just on ROOT-ubuntu2004/default with flags -Dtmva-sofie=On,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8684
https://github.com/root-project/root/pull/8684:11,deployability,build,build,11,@phsft-bot build!,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8684
https://github.com/root-project/root/issues/8685:5,deployability,depend,depends,5,This depends on https://github.com/root-project/root/issues/6745,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8685
https://github.com/root-project/root/issues/8685:5,integrability,depend,depends,5,This depends on https://github.com/root-project/root/issues/6745,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8685
https://github.com/root-project/root/issues/8685:5,modifiability,depend,depends,5,This depends on https://github.com/root-project/root/issues/6745,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8685
https://github.com/root-project/root/issues/8685:5,safety,depend,depends,5,This depends on https://github.com/root-project/root/issues/6745,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8685
https://github.com/root-project/root/issues/8685:5,testability,depend,depends,5,This depends on https://github.com/root-project/root/issues/6745,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8685
https://github.com/root-project/root/issues/8685:0,usability,Close,Closed,0,Closed by https://github.com/root-project/root/pull/13210,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8685
https://github.com/root-project/root/pull/8688:217,interoperability,share,shared,217,"@pikacic This could be one way for multi-threaded writing. I think we have still some issues regarding the life time of the memory regions used for filling. In this example, `REntry::Get` returns a raw pointer (not a shared pointer like `RNTupleModel::MakeField`). But the basics seems to work. Please let me know if you have comments!",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8688
https://github.com/root-project/root/pull/8688:35,performance,multi-thread,multi-threaded,35,"@pikacic This could be one way for multi-threaded writing. I think we have still some issues regarding the life time of the memory regions used for filling. In this example, `REntry::Get` returns a raw pointer (not a shared pointer like `RNTupleModel::MakeField`). But the basics seems to work. Please let me know if you have comments!",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8688
https://github.com/root-project/root/pull/8688:112,performance,time,time,112,"@pikacic This could be one way for multi-threaded writing. I think we have still some issues regarding the life time of the memory regions used for filling. In this example, `REntry::Get` returns a raw pointer (not a shared pointer like `RNTupleModel::MakeField`). But the basics seems to work. Please let me know if you have comments!",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8688
https://github.com/root-project/root/pull/8688:124,performance,memor,memory,124,"@pikacic This could be one way for multi-threaded writing. I think we have still some issues regarding the life time of the memory regions used for filling. In this example, `REntry::Get` returns a raw pointer (not a shared pointer like `RNTupleModel::MakeField`). But the basics seems to work. Please let me know if you have comments!",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8688
https://github.com/root-project/root/pull/8688:124,usability,memor,memory,124,"@pikacic This could be one way for multi-threaded writing. I think we have still some issues regarding the life time of the memory regions used for filling. In this example, `REntry::Get` returns a raw pointer (not a shared pointer like `RNTupleModel::MakeField`). But the basics seems to work. Please let me know if you have comments!",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8688
https://github.com/root-project/root/pull/8688:217,energy efficiency,model,model,217,"I had a look ant it seems very nice. About the discrepancy between `REntry::Get` and `RNTupleModel::MakeField`, one may argue that `MakeField` might return a raw pointer too as the memory of the field is owned by the model. OTOH, while it's kind of easy to keep track of the lifetime of an `REntry` (I get it, I use it, I drop it), it seems more difficult to keep track of `RNTupleModel` lifetime, which is bound to the lifetime of `RNTupleWriter`. Using raw pointers from `MakeField` is fine if we assume that the writer of the code only accesses the fields within the scope of `RNTupleWriter` (but the fields are accessible before the writer is created... more confusing). My personal opinion is that the `CreateEntry` way is the best option for the single thread case too:. - I define a model. - create a writer based on the model. - get the *buffer* (`REntry`) to write to (one per thread, for example, even if I have only one thread). - commit the *buffer* to the writer. In this way I own the `REntry`, but it has a layout in memory that is directly understood by the serialization process without the need of extra copies. We can also think of a way of constructing the writer from an *inlined* model, without the need for repeated calls to `MakeField`. Something like:. ```cpp. auto ntuple = RNTupleWriter::Recreate({. Field<std::uint32_t>(""id""),. Field<std::vector<float>>(""vpx""),. Field<std::vector<float>>(""vpy""),. Field<std::vector<float>>(""vpz""). }, ""NTuple"", kNTupleFileName);. auto entry = ntuple->CreateEntry();. // cache the pointer for faster access in single thread,. // but I could use TLS (or a framework equivalent) for multithreading. auto& vpx = *entry->Get<std::vector<float>>(""vpx"");. for(auto& event: all_events) {. vpx.clear();. vpx.push_back(42.);. ntuple->Fill(entry); // I would prefer ntuple->Write(entry) but it's a matter of taste. }. ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8688
https://github.com/root-project/root/pull/8688:790,energy efficiency,model,model,790,"I had a look ant it seems very nice. About the discrepancy between `REntry::Get` and `RNTupleModel::MakeField`, one may argue that `MakeField` might return a raw pointer too as the memory of the field is owned by the model. OTOH, while it's kind of easy to keep track of the lifetime of an `REntry` (I get it, I use it, I drop it), it seems more difficult to keep track of `RNTupleModel` lifetime, which is bound to the lifetime of `RNTupleWriter`. Using raw pointers from `MakeField` is fine if we assume that the writer of the code only accesses the fields within the scope of `RNTupleWriter` (but the fields are accessible before the writer is created... more confusing). My personal opinion is that the `CreateEntry` way is the best option for the single thread case too:. - I define a model. - create a writer based on the model. - get the *buffer* (`REntry`) to write to (one per thread, for example, even if I have only one thread). - commit the *buffer* to the writer. In this way I own the `REntry`, but it has a layout in memory that is directly understood by the serialization process without the need of extra copies. We can also think of a way of constructing the writer from an *inlined* model, without the need for repeated calls to `MakeField`. Something like:. ```cpp. auto ntuple = RNTupleWriter::Recreate({. Field<std::uint32_t>(""id""),. Field<std::vector<float>>(""vpx""),. Field<std::vector<float>>(""vpy""),. Field<std::vector<float>>(""vpz""). }, ""NTuple"", kNTupleFileName);. auto entry = ntuple->CreateEntry();. // cache the pointer for faster access in single thread,. // but I could use TLS (or a framework equivalent) for multithreading. auto& vpx = *entry->Get<std::vector<float>>(""vpx"");. for(auto& event: all_events) {. vpx.clear();. vpx.push_back(42.);. ntuple->Fill(entry); // I would prefer ntuple->Write(entry) but it's a matter of taste. }. ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8688
https://github.com/root-project/root/pull/8688:828,energy efficiency,model,model,828,"I had a look ant it seems very nice. About the discrepancy between `REntry::Get` and `RNTupleModel::MakeField`, one may argue that `MakeField` might return a raw pointer too as the memory of the field is owned by the model. OTOH, while it's kind of easy to keep track of the lifetime of an `REntry` (I get it, I use it, I drop it), it seems more difficult to keep track of `RNTupleModel` lifetime, which is bound to the lifetime of `RNTupleWriter`. Using raw pointers from `MakeField` is fine if we assume that the writer of the code only accesses the fields within the scope of `RNTupleWriter` (but the fields are accessible before the writer is created... more confusing). My personal opinion is that the `CreateEntry` way is the best option for the single thread case too:. - I define a model. - create a writer based on the model. - get the *buffer* (`REntry`) to write to (one per thread, for example, even if I have only one thread). - commit the *buffer* to the writer. In this way I own the `REntry`, but it has a layout in memory that is directly understood by the serialization process without the need of extra copies. We can also think of a way of constructing the writer from an *inlined* model, without the need for repeated calls to `MakeField`. Something like:. ```cpp. auto ntuple = RNTupleWriter::Recreate({. Field<std::uint32_t>(""id""),. Field<std::vector<float>>(""vpx""),. Field<std::vector<float>>(""vpy""),. Field<std::vector<float>>(""vpz""). }, ""NTuple"", kNTupleFileName);. auto entry = ntuple->CreateEntry();. // cache the pointer for faster access in single thread,. // but I could use TLS (or a framework equivalent) for multithreading. auto& vpx = *entry->Get<std::vector<float>>(""vpx"");. for(auto& event: all_events) {. vpx.clear();. vpx.push_back(42.);. ntuple->Fill(entry); // I would prefer ntuple->Write(entry) but it's a matter of taste. }. ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8688
https://github.com/root-project/root/pull/8688:1202,energy efficiency,model,model,1202,"I had a look ant it seems very nice. About the discrepancy between `REntry::Get` and `RNTupleModel::MakeField`, one may argue that `MakeField` might return a raw pointer too as the memory of the field is owned by the model. OTOH, while it's kind of easy to keep track of the lifetime of an `REntry` (I get it, I use it, I drop it), it seems more difficult to keep track of `RNTupleModel` lifetime, which is bound to the lifetime of `RNTupleWriter`. Using raw pointers from `MakeField` is fine if we assume that the writer of the code only accesses the fields within the scope of `RNTupleWriter` (but the fields are accessible before the writer is created... more confusing). My personal opinion is that the `CreateEntry` way is the best option for the single thread case too:. - I define a model. - create a writer based on the model. - get the *buffer* (`REntry`) to write to (one per thread, for example, even if I have only one thread). - commit the *buffer* to the writer. In this way I own the `REntry`, but it has a layout in memory that is directly understood by the serialization process without the need of extra copies. We can also think of a way of constructing the writer from an *inlined* model, without the need for repeated calls to `MakeField`. Something like:. ```cpp. auto ntuple = RNTupleWriter::Recreate({. Field<std::uint32_t>(""id""),. Field<std::vector<float>>(""vpx""),. Field<std::vector<float>>(""vpy""),. Field<std::vector<float>>(""vpz""). }, ""NTuple"", kNTupleFileName);. auto entry = ntuple->CreateEntry();. // cache the pointer for faster access in single thread,. // but I could use TLS (or a framework equivalent) for multithreading. auto& vpx = *entry->Get<std::vector<float>>(""vpx"");. for(auto& event: all_events) {. vpx.clear();. vpx.push_back(42.);. ntuple->Fill(entry); // I would prefer ntuple->Write(entry) but it's a matter of taste. }. ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8688
https://github.com/root-project/root/pull/8688:846,integrability,buffer,buffer,846,"I had a look ant it seems very nice. About the discrepancy between `REntry::Get` and `RNTupleModel::MakeField`, one may argue that `MakeField` might return a raw pointer too as the memory of the field is owned by the model. OTOH, while it's kind of easy to keep track of the lifetime of an `REntry` (I get it, I use it, I drop it), it seems more difficult to keep track of `RNTupleModel` lifetime, which is bound to the lifetime of `RNTupleWriter`. Using raw pointers from `MakeField` is fine if we assume that the writer of the code only accesses the fields within the scope of `RNTupleWriter` (but the fields are accessible before the writer is created... more confusing). My personal opinion is that the `CreateEntry` way is the best option for the single thread case too:. - I define a model. - create a writer based on the model. - get the *buffer* (`REntry`) to write to (one per thread, for example, even if I have only one thread). - commit the *buffer* to the writer. In this way I own the `REntry`, but it has a layout in memory that is directly understood by the serialization process without the need of extra copies. We can also think of a way of constructing the writer from an *inlined* model, without the need for repeated calls to `MakeField`. Something like:. ```cpp. auto ntuple = RNTupleWriter::Recreate({. Field<std::uint32_t>(""id""),. Field<std::vector<float>>(""vpx""),. Field<std::vector<float>>(""vpy""),. Field<std::vector<float>>(""vpz""). }, ""NTuple"", kNTupleFileName);. auto entry = ntuple->CreateEntry();. // cache the pointer for faster access in single thread,. // but I could use TLS (or a framework equivalent) for multithreading. auto& vpx = *entry->Get<std::vector<float>>(""vpx"");. for(auto& event: all_events) {. vpx.clear();. vpx.push_back(42.);. ntuple->Fill(entry); // I would prefer ntuple->Write(entry) but it's a matter of taste. }. ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8688
https://github.com/root-project/root/pull/8688:954,integrability,buffer,buffer,954,"I had a look ant it seems very nice. About the discrepancy between `REntry::Get` and `RNTupleModel::MakeField`, one may argue that `MakeField` might return a raw pointer too as the memory of the field is owned by the model. OTOH, while it's kind of easy to keep track of the lifetime of an `REntry` (I get it, I use it, I drop it), it seems more difficult to keep track of `RNTupleModel` lifetime, which is bound to the lifetime of `RNTupleWriter`. Using raw pointers from `MakeField` is fine if we assume that the writer of the code only accesses the fields within the scope of `RNTupleWriter` (but the fields are accessible before the writer is created... more confusing). My personal opinion is that the `CreateEntry` way is the best option for the single thread case too:. - I define a model. - create a writer based on the model. - get the *buffer* (`REntry`) to write to (one per thread, for example, even if I have only one thread). - commit the *buffer* to the writer. In this way I own the `REntry`, but it has a layout in memory that is directly understood by the serialization process without the need of extra copies. We can also think of a way of constructing the writer from an *inlined* model, without the need for repeated calls to `MakeField`. Something like:. ```cpp. auto ntuple = RNTupleWriter::Recreate({. Field<std::uint32_t>(""id""),. Field<std::vector<float>>(""vpx""),. Field<std::vector<float>>(""vpy""),. Field<std::vector<float>>(""vpz""). }, ""NTuple"", kNTupleFileName);. auto entry = ntuple->CreateEntry();. // cache the pointer for faster access in single thread,. // but I could use TLS (or a framework equivalent) for multithreading. auto& vpx = *entry->Get<std::vector<float>>(""vpx"");. for(auto& event: all_events) {. vpx.clear();. vpx.push_back(42.);. ntuple->Fill(entry); // I would prefer ntuple->Write(entry) but it's a matter of taste. }. ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8688
https://github.com/root-project/root/pull/8688:1721,integrability,event,event,1721,"I had a look ant it seems very nice. About the discrepancy between `REntry::Get` and `RNTupleModel::MakeField`, one may argue that `MakeField` might return a raw pointer too as the memory of the field is owned by the model. OTOH, while it's kind of easy to keep track of the lifetime of an `REntry` (I get it, I use it, I drop it), it seems more difficult to keep track of `RNTupleModel` lifetime, which is bound to the lifetime of `RNTupleWriter`. Using raw pointers from `MakeField` is fine if we assume that the writer of the code only accesses the fields within the scope of `RNTupleWriter` (but the fields are accessible before the writer is created... more confusing). My personal opinion is that the `CreateEntry` way is the best option for the single thread case too:. - I define a model. - create a writer based on the model. - get the *buffer* (`REntry`) to write to (one per thread, for example, even if I have only one thread). - commit the *buffer* to the writer. In this way I own the `REntry`, but it has a layout in memory that is directly understood by the serialization process without the need of extra copies. We can also think of a way of constructing the writer from an *inlined* model, without the need for repeated calls to `MakeField`. Something like:. ```cpp. auto ntuple = RNTupleWriter::Recreate({. Field<std::uint32_t>(""id""),. Field<std::vector<float>>(""vpx""),. Field<std::vector<float>>(""vpy""),. Field<std::vector<float>>(""vpz""). }, ""NTuple"", kNTupleFileName);. auto entry = ntuple->CreateEntry();. // cache the pointer for faster access in single thread,. // but I could use TLS (or a framework equivalent) for multithreading. auto& vpx = *entry->Get<std::vector<float>>(""vpx"");. for(auto& event: all_events) {. vpx.clear();. vpx.push_back(42.);. ntuple->Fill(entry); // I would prefer ntuple->Write(entry) but it's a matter of taste. }. ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8688
https://github.com/root-project/root/pull/8688:181,performance,memor,memory,181,"I had a look ant it seems very nice. About the discrepancy between `REntry::Get` and `RNTupleModel::MakeField`, one may argue that `MakeField` might return a raw pointer too as the memory of the field is owned by the model. OTOH, while it's kind of easy to keep track of the lifetime of an `REntry` (I get it, I use it, I drop it), it seems more difficult to keep track of `RNTupleModel` lifetime, which is bound to the lifetime of `RNTupleWriter`. Using raw pointers from `MakeField` is fine if we assume that the writer of the code only accesses the fields within the scope of `RNTupleWriter` (but the fields are accessible before the writer is created... more confusing). My personal opinion is that the `CreateEntry` way is the best option for the single thread case too:. - I define a model. - create a writer based on the model. - get the *buffer* (`REntry`) to write to (one per thread, for example, even if I have only one thread). - commit the *buffer* to the writer. In this way I own the `REntry`, but it has a layout in memory that is directly understood by the serialization process without the need of extra copies. We can also think of a way of constructing the writer from an *inlined* model, without the need for repeated calls to `MakeField`. Something like:. ```cpp. auto ntuple = RNTupleWriter::Recreate({. Field<std::uint32_t>(""id""),. Field<std::vector<float>>(""vpx""),. Field<std::vector<float>>(""vpy""),. Field<std::vector<float>>(""vpz""). }, ""NTuple"", kNTupleFileName);. auto entry = ntuple->CreateEntry();. // cache the pointer for faster access in single thread,. // but I could use TLS (or a framework equivalent) for multithreading. auto& vpx = *entry->Get<std::vector<float>>(""vpx"");. for(auto& event: all_events) {. vpx.clear();. vpx.push_back(42.);. ntuple->Fill(entry); // I would prefer ntuple->Write(entry) but it's a matter of taste. }. ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8688
https://github.com/root-project/root/pull/8688:1032,performance,memor,memory,1032,"I had a look ant it seems very nice. About the discrepancy between `REntry::Get` and `RNTupleModel::MakeField`, one may argue that `MakeField` might return a raw pointer too as the memory of the field is owned by the model. OTOH, while it's kind of easy to keep track of the lifetime of an `REntry` (I get it, I use it, I drop it), it seems more difficult to keep track of `RNTupleModel` lifetime, which is bound to the lifetime of `RNTupleWriter`. Using raw pointers from `MakeField` is fine if we assume that the writer of the code only accesses the fields within the scope of `RNTupleWriter` (but the fields are accessible before the writer is created... more confusing). My personal opinion is that the `CreateEntry` way is the best option for the single thread case too:. - I define a model. - create a writer based on the model. - get the *buffer* (`REntry`) to write to (one per thread, for example, even if I have only one thread). - commit the *buffer* to the writer. In this way I own the `REntry`, but it has a layout in memory that is directly understood by the serialization process without the need of extra copies. We can also think of a way of constructing the writer from an *inlined* model, without the need for repeated calls to `MakeField`. Something like:. ```cpp. auto ntuple = RNTupleWriter::Recreate({. Field<std::uint32_t>(""id""),. Field<std::vector<float>>(""vpx""),. Field<std::vector<float>>(""vpy""),. Field<std::vector<float>>(""vpz""). }, ""NTuple"", kNTupleFileName);. auto entry = ntuple->CreateEntry();. // cache the pointer for faster access in single thread,. // but I could use TLS (or a framework equivalent) for multithreading. auto& vpx = *entry->Get<std::vector<float>>(""vpx"");. for(auto& event: all_events) {. vpx.clear();. vpx.push_back(42.);. ntuple->Fill(entry); // I would prefer ntuple->Write(entry) but it's a matter of taste. }. ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8688
https://github.com/root-project/root/pull/8688:1532,performance,cach,cache,1532,"I had a look ant it seems very nice. About the discrepancy between `REntry::Get` and `RNTupleModel::MakeField`, one may argue that `MakeField` might return a raw pointer too as the memory of the field is owned by the model. OTOH, while it's kind of easy to keep track of the lifetime of an `REntry` (I get it, I use it, I drop it), it seems more difficult to keep track of `RNTupleModel` lifetime, which is bound to the lifetime of `RNTupleWriter`. Using raw pointers from `MakeField` is fine if we assume that the writer of the code only accesses the fields within the scope of `RNTupleWriter` (but the fields are accessible before the writer is created... more confusing). My personal opinion is that the `CreateEntry` way is the best option for the single thread case too:. - I define a model. - create a writer based on the model. - get the *buffer* (`REntry`) to write to (one per thread, for example, even if I have only one thread). - commit the *buffer* to the writer. In this way I own the `REntry`, but it has a layout in memory that is directly understood by the serialization process without the need of extra copies. We can also think of a way of constructing the writer from an *inlined* model, without the need for repeated calls to `MakeField`. Something like:. ```cpp. auto ntuple = RNTupleWriter::Recreate({. Field<std::uint32_t>(""id""),. Field<std::vector<float>>(""vpx""),. Field<std::vector<float>>(""vpy""),. Field<std::vector<float>>(""vpz""). }, ""NTuple"", kNTupleFileName);. auto entry = ntuple->CreateEntry();. // cache the pointer for faster access in single thread,. // but I could use TLS (or a framework equivalent) for multithreading. auto& vpx = *entry->Get<std::vector<float>>(""vpx"");. for(auto& event: all_events) {. vpx.clear();. vpx.push_back(42.);. ntuple->Fill(entry); // I would prefer ntuple->Write(entry) but it's a matter of taste. }. ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8688
https://github.com/root-project/root/pull/8688:217,security,model,model,217,"I had a look ant it seems very nice. About the discrepancy between `REntry::Get` and `RNTupleModel::MakeField`, one may argue that `MakeField` might return a raw pointer too as the memory of the field is owned by the model. OTOH, while it's kind of easy to keep track of the lifetime of an `REntry` (I get it, I use it, I drop it), it seems more difficult to keep track of `RNTupleModel` lifetime, which is bound to the lifetime of `RNTupleWriter`. Using raw pointers from `MakeField` is fine if we assume that the writer of the code only accesses the fields within the scope of `RNTupleWriter` (but the fields are accessible before the writer is created... more confusing). My personal opinion is that the `CreateEntry` way is the best option for the single thread case too:. - I define a model. - create a writer based on the model. - get the *buffer* (`REntry`) to write to (one per thread, for example, even if I have only one thread). - commit the *buffer* to the writer. In this way I own the `REntry`, but it has a layout in memory that is directly understood by the serialization process without the need of extra copies. We can also think of a way of constructing the writer from an *inlined* model, without the need for repeated calls to `MakeField`. Something like:. ```cpp. auto ntuple = RNTupleWriter::Recreate({. Field<std::uint32_t>(""id""),. Field<std::vector<float>>(""vpx""),. Field<std::vector<float>>(""vpy""),. Field<std::vector<float>>(""vpz""). }, ""NTuple"", kNTupleFileName);. auto entry = ntuple->CreateEntry();. // cache the pointer for faster access in single thread,. // but I could use TLS (or a framework equivalent) for multithreading. auto& vpx = *entry->Get<std::vector<float>>(""vpx"");. for(auto& event: all_events) {. vpx.clear();. vpx.push_back(42.);. ntuple->Fill(entry); // I would prefer ntuple->Write(entry) but it's a matter of taste. }. ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8688
https://github.com/root-project/root/pull/8688:539,security,access,accesses,539,"I had a look ant it seems very nice. About the discrepancy between `REntry::Get` and `RNTupleModel::MakeField`, one may argue that `MakeField` might return a raw pointer too as the memory of the field is owned by the model. OTOH, while it's kind of easy to keep track of the lifetime of an `REntry` (I get it, I use it, I drop it), it seems more difficult to keep track of `RNTupleModel` lifetime, which is bound to the lifetime of `RNTupleWriter`. Using raw pointers from `MakeField` is fine if we assume that the writer of the code only accesses the fields within the scope of `RNTupleWriter` (but the fields are accessible before the writer is created... more confusing). My personal opinion is that the `CreateEntry` way is the best option for the single thread case too:. - I define a model. - create a writer based on the model. - get the *buffer* (`REntry`) to write to (one per thread, for example, even if I have only one thread). - commit the *buffer* to the writer. In this way I own the `REntry`, but it has a layout in memory that is directly understood by the serialization process without the need of extra copies. We can also think of a way of constructing the writer from an *inlined* model, without the need for repeated calls to `MakeField`. Something like:. ```cpp. auto ntuple = RNTupleWriter::Recreate({. Field<std::uint32_t>(""id""),. Field<std::vector<float>>(""vpx""),. Field<std::vector<float>>(""vpy""),. Field<std::vector<float>>(""vpz""). }, ""NTuple"", kNTupleFileName);. auto entry = ntuple->CreateEntry();. // cache the pointer for faster access in single thread,. // but I could use TLS (or a framework equivalent) for multithreading. auto& vpx = *entry->Get<std::vector<float>>(""vpx"");. for(auto& event: all_events) {. vpx.clear();. vpx.push_back(42.);. ntuple->Fill(entry); // I would prefer ntuple->Write(entry) but it's a matter of taste. }. ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8688
https://github.com/root-project/root/pull/8688:615,security,access,accessible,615,"I had a look ant it seems very nice. About the discrepancy between `REntry::Get` and `RNTupleModel::MakeField`, one may argue that `MakeField` might return a raw pointer too as the memory of the field is owned by the model. OTOH, while it's kind of easy to keep track of the lifetime of an `REntry` (I get it, I use it, I drop it), it seems more difficult to keep track of `RNTupleModel` lifetime, which is bound to the lifetime of `RNTupleWriter`. Using raw pointers from `MakeField` is fine if we assume that the writer of the code only accesses the fields within the scope of `RNTupleWriter` (but the fields are accessible before the writer is created... more confusing). My personal opinion is that the `CreateEntry` way is the best option for the single thread case too:. - I define a model. - create a writer based on the model. - get the *buffer* (`REntry`) to write to (one per thread, for example, even if I have only one thread). - commit the *buffer* to the writer. In this way I own the `REntry`, but it has a layout in memory that is directly understood by the serialization process without the need of extra copies. We can also think of a way of constructing the writer from an *inlined* model, without the need for repeated calls to `MakeField`. Something like:. ```cpp. auto ntuple = RNTupleWriter::Recreate({. Field<std::uint32_t>(""id""),. Field<std::vector<float>>(""vpx""),. Field<std::vector<float>>(""vpy""),. Field<std::vector<float>>(""vpz""). }, ""NTuple"", kNTupleFileName);. auto entry = ntuple->CreateEntry();. // cache the pointer for faster access in single thread,. // but I could use TLS (or a framework equivalent) for multithreading. auto& vpx = *entry->Get<std::vector<float>>(""vpx"");. for(auto& event: all_events) {. vpx.clear();. vpx.push_back(42.);. ntuple->Fill(entry); // I would prefer ntuple->Write(entry) but it's a matter of taste. }. ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8688
https://github.com/root-project/root/pull/8688:790,security,model,model,790,"I had a look ant it seems very nice. About the discrepancy between `REntry::Get` and `RNTupleModel::MakeField`, one may argue that `MakeField` might return a raw pointer too as the memory of the field is owned by the model. OTOH, while it's kind of easy to keep track of the lifetime of an `REntry` (I get it, I use it, I drop it), it seems more difficult to keep track of `RNTupleModel` lifetime, which is bound to the lifetime of `RNTupleWriter`. Using raw pointers from `MakeField` is fine if we assume that the writer of the code only accesses the fields within the scope of `RNTupleWriter` (but the fields are accessible before the writer is created... more confusing). My personal opinion is that the `CreateEntry` way is the best option for the single thread case too:. - I define a model. - create a writer based on the model. - get the *buffer* (`REntry`) to write to (one per thread, for example, even if I have only one thread). - commit the *buffer* to the writer. In this way I own the `REntry`, but it has a layout in memory that is directly understood by the serialization process without the need of extra copies. We can also think of a way of constructing the writer from an *inlined* model, without the need for repeated calls to `MakeField`. Something like:. ```cpp. auto ntuple = RNTupleWriter::Recreate({. Field<std::uint32_t>(""id""),. Field<std::vector<float>>(""vpx""),. Field<std::vector<float>>(""vpy""),. Field<std::vector<float>>(""vpz""). }, ""NTuple"", kNTupleFileName);. auto entry = ntuple->CreateEntry();. // cache the pointer for faster access in single thread,. // but I could use TLS (or a framework equivalent) for multithreading. auto& vpx = *entry->Get<std::vector<float>>(""vpx"");. for(auto& event: all_events) {. vpx.clear();. vpx.push_back(42.);. ntuple->Fill(entry); // I would prefer ntuple->Write(entry) but it's a matter of taste. }. ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8688
https://github.com/root-project/root/pull/8688:828,security,model,model,828,"I had a look ant it seems very nice. About the discrepancy between `REntry::Get` and `RNTupleModel::MakeField`, one may argue that `MakeField` might return a raw pointer too as the memory of the field is owned by the model. OTOH, while it's kind of easy to keep track of the lifetime of an `REntry` (I get it, I use it, I drop it), it seems more difficult to keep track of `RNTupleModel` lifetime, which is bound to the lifetime of `RNTupleWriter`. Using raw pointers from `MakeField` is fine if we assume that the writer of the code only accesses the fields within the scope of `RNTupleWriter` (but the fields are accessible before the writer is created... more confusing). My personal opinion is that the `CreateEntry` way is the best option for the single thread case too:. - I define a model. - create a writer based on the model. - get the *buffer* (`REntry`) to write to (one per thread, for example, even if I have only one thread). - commit the *buffer* to the writer. In this way I own the `REntry`, but it has a layout in memory that is directly understood by the serialization process without the need of extra copies. We can also think of a way of constructing the writer from an *inlined* model, without the need for repeated calls to `MakeField`. Something like:. ```cpp. auto ntuple = RNTupleWriter::Recreate({. Field<std::uint32_t>(""id""),. Field<std::vector<float>>(""vpx""),. Field<std::vector<float>>(""vpy""),. Field<std::vector<float>>(""vpz""). }, ""NTuple"", kNTupleFileName);. auto entry = ntuple->CreateEntry();. // cache the pointer for faster access in single thread,. // but I could use TLS (or a framework equivalent) for multithreading. auto& vpx = *entry->Get<std::vector<float>>(""vpx"");. for(auto& event: all_events) {. vpx.clear();. vpx.push_back(42.);. ntuple->Fill(entry); // I would prefer ntuple->Write(entry) but it's a matter of taste. }. ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8688
https://github.com/root-project/root/pull/8688:1202,security,model,model,1202,"I had a look ant it seems very nice. About the discrepancy between `REntry::Get` and `RNTupleModel::MakeField`, one may argue that `MakeField` might return a raw pointer too as the memory of the field is owned by the model. OTOH, while it's kind of easy to keep track of the lifetime of an `REntry` (I get it, I use it, I drop it), it seems more difficult to keep track of `RNTupleModel` lifetime, which is bound to the lifetime of `RNTupleWriter`. Using raw pointers from `MakeField` is fine if we assume that the writer of the code only accesses the fields within the scope of `RNTupleWriter` (but the fields are accessible before the writer is created... more confusing). My personal opinion is that the `CreateEntry` way is the best option for the single thread case too:. - I define a model. - create a writer based on the model. - get the *buffer* (`REntry`) to write to (one per thread, for example, even if I have only one thread). - commit the *buffer* to the writer. In this way I own the `REntry`, but it has a layout in memory that is directly understood by the serialization process without the need of extra copies. We can also think of a way of constructing the writer from an *inlined* model, without the need for repeated calls to `MakeField`. Something like:. ```cpp. auto ntuple = RNTupleWriter::Recreate({. Field<std::uint32_t>(""id""),. Field<std::vector<float>>(""vpx""),. Field<std::vector<float>>(""vpy""),. Field<std::vector<float>>(""vpz""). }, ""NTuple"", kNTupleFileName);. auto entry = ntuple->CreateEntry();. // cache the pointer for faster access in single thread,. // but I could use TLS (or a framework equivalent) for multithreading. auto& vpx = *entry->Get<std::vector<float>>(""vpx"");. for(auto& event: all_events) {. vpx.clear();. vpx.push_back(42.);. ntuple->Fill(entry); // I would prefer ntuple->Write(entry) but it's a matter of taste. }. ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8688
https://github.com/root-project/root/pull/8688:1561,security,access,access,1561,"I had a look ant it seems very nice. About the discrepancy between `REntry::Get` and `RNTupleModel::MakeField`, one may argue that `MakeField` might return a raw pointer too as the memory of the field is owned by the model. OTOH, while it's kind of easy to keep track of the lifetime of an `REntry` (I get it, I use it, I drop it), it seems more difficult to keep track of `RNTupleModel` lifetime, which is bound to the lifetime of `RNTupleWriter`. Using raw pointers from `MakeField` is fine if we assume that the writer of the code only accesses the fields within the scope of `RNTupleWriter` (but the fields are accessible before the writer is created... more confusing). My personal opinion is that the `CreateEntry` way is the best option for the single thread case too:. - I define a model. - create a writer based on the model. - get the *buffer* (`REntry`) to write to (one per thread, for example, even if I have only one thread). - commit the *buffer* to the writer. In this way I own the `REntry`, but it has a layout in memory that is directly understood by the serialization process without the need of extra copies. We can also think of a way of constructing the writer from an *inlined* model, without the need for repeated calls to `MakeField`. Something like:. ```cpp. auto ntuple = RNTupleWriter::Recreate({. Field<std::uint32_t>(""id""),. Field<std::vector<float>>(""vpx""),. Field<std::vector<float>>(""vpy""),. Field<std::vector<float>>(""vpz""). }, ""NTuple"", kNTupleFileName);. auto entry = ntuple->CreateEntry();. // cache the pointer for faster access in single thread,. // but I could use TLS (or a framework equivalent) for multithreading. auto& vpx = *entry->Get<std::vector<float>>(""vpx"");. for(auto& event: all_events) {. vpx.clear();. vpx.push_back(42.);. ntuple->Fill(entry); // I would prefer ntuple->Write(entry) but it's a matter of taste. }. ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8688
https://github.com/root-project/root/pull/8688:181,usability,memor,memory,181,"I had a look ant it seems very nice. About the discrepancy between `REntry::Get` and `RNTupleModel::MakeField`, one may argue that `MakeField` might return a raw pointer too as the memory of the field is owned by the model. OTOH, while it's kind of easy to keep track of the lifetime of an `REntry` (I get it, I use it, I drop it), it seems more difficult to keep track of `RNTupleModel` lifetime, which is bound to the lifetime of `RNTupleWriter`. Using raw pointers from `MakeField` is fine if we assume that the writer of the code only accesses the fields within the scope of `RNTupleWriter` (but the fields are accessible before the writer is created... more confusing). My personal opinion is that the `CreateEntry` way is the best option for the single thread case too:. - I define a model. - create a writer based on the model. - get the *buffer* (`REntry`) to write to (one per thread, for example, even if I have only one thread). - commit the *buffer* to the writer. In this way I own the `REntry`, but it has a layout in memory that is directly understood by the serialization process without the need of extra copies. We can also think of a way of constructing the writer from an *inlined* model, without the need for repeated calls to `MakeField`. Something like:. ```cpp. auto ntuple = RNTupleWriter::Recreate({. Field<std::uint32_t>(""id""),. Field<std::vector<float>>(""vpx""),. Field<std::vector<float>>(""vpy""),. Field<std::vector<float>>(""vpz""). }, ""NTuple"", kNTupleFileName);. auto entry = ntuple->CreateEntry();. // cache the pointer for faster access in single thread,. // but I could use TLS (or a framework equivalent) for multithreading. auto& vpx = *entry->Get<std::vector<float>>(""vpx"");. for(auto& event: all_events) {. vpx.clear();. vpx.push_back(42.);. ntuple->Fill(entry); // I would prefer ntuple->Write(entry) but it's a matter of taste. }. ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8688
https://github.com/root-project/root/pull/8688:678,usability,person,personal,678,"I had a look ant it seems very nice. About the discrepancy between `REntry::Get` and `RNTupleModel::MakeField`, one may argue that `MakeField` might return a raw pointer too as the memory of the field is owned by the model. OTOH, while it's kind of easy to keep track of the lifetime of an `REntry` (I get it, I use it, I drop it), it seems more difficult to keep track of `RNTupleModel` lifetime, which is bound to the lifetime of `RNTupleWriter`. Using raw pointers from `MakeField` is fine if we assume that the writer of the code only accesses the fields within the scope of `RNTupleWriter` (but the fields are accessible before the writer is created... more confusing). My personal opinion is that the `CreateEntry` way is the best option for the single thread case too:. - I define a model. - create a writer based on the model. - get the *buffer* (`REntry`) to write to (one per thread, for example, even if I have only one thread). - commit the *buffer* to the writer. In this way I own the `REntry`, but it has a layout in memory that is directly understood by the serialization process without the need of extra copies. We can also think of a way of constructing the writer from an *inlined* model, without the need for repeated calls to `MakeField`. Something like:. ```cpp. auto ntuple = RNTupleWriter::Recreate({. Field<std::uint32_t>(""id""),. Field<std::vector<float>>(""vpx""),. Field<std::vector<float>>(""vpy""),. Field<std::vector<float>>(""vpz""). }, ""NTuple"", kNTupleFileName);. auto entry = ntuple->CreateEntry();. // cache the pointer for faster access in single thread,. // but I could use TLS (or a framework equivalent) for multithreading. auto& vpx = *entry->Get<std::vector<float>>(""vpx"");. for(auto& event: all_events) {. vpx.clear();. vpx.push_back(42.);. ntuple->Fill(entry); // I would prefer ntuple->Write(entry) but it's a matter of taste. }. ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8688
https://github.com/root-project/root/pull/8688:1032,usability,memor,memory,1032,"I had a look ant it seems very nice. About the discrepancy between `REntry::Get` and `RNTupleModel::MakeField`, one may argue that `MakeField` might return a raw pointer too as the memory of the field is owned by the model. OTOH, while it's kind of easy to keep track of the lifetime of an `REntry` (I get it, I use it, I drop it), it seems more difficult to keep track of `RNTupleModel` lifetime, which is bound to the lifetime of `RNTupleWriter`. Using raw pointers from `MakeField` is fine if we assume that the writer of the code only accesses the fields within the scope of `RNTupleWriter` (but the fields are accessible before the writer is created... more confusing). My personal opinion is that the `CreateEntry` way is the best option for the single thread case too:. - I define a model. - create a writer based on the model. - get the *buffer* (`REntry`) to write to (one per thread, for example, even if I have only one thread). - commit the *buffer* to the writer. In this way I own the `REntry`, but it has a layout in memory that is directly understood by the serialization process without the need of extra copies. We can also think of a way of constructing the writer from an *inlined* model, without the need for repeated calls to `MakeField`. Something like:. ```cpp. auto ntuple = RNTupleWriter::Recreate({. Field<std::uint32_t>(""id""),. Field<std::vector<float>>(""vpx""),. Field<std::vector<float>>(""vpy""),. Field<std::vector<float>>(""vpz""). }, ""NTuple"", kNTupleFileName);. auto entry = ntuple->CreateEntry();. // cache the pointer for faster access in single thread,. // but I could use TLS (or a framework equivalent) for multithreading. auto& vpx = *entry->Get<std::vector<float>>(""vpx"");. for(auto& event: all_events) {. vpx.clear();. vpx.push_back(42.);. ntuple->Fill(entry); // I would prefer ntuple->Write(entry) but it's a matter of taste. }. ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8688
https://github.com/root-project/root/pull/8688:1747,usability,clear,clear,1747,"I had a look ant it seems very nice. About the discrepancy between `REntry::Get` and `RNTupleModel::MakeField`, one may argue that `MakeField` might return a raw pointer too as the memory of the field is owned by the model. OTOH, while it's kind of easy to keep track of the lifetime of an `REntry` (I get it, I use it, I drop it), it seems more difficult to keep track of `RNTupleModel` lifetime, which is bound to the lifetime of `RNTupleWriter`. Using raw pointers from `MakeField` is fine if we assume that the writer of the code only accesses the fields within the scope of `RNTupleWriter` (but the fields are accessible before the writer is created... more confusing). My personal opinion is that the `CreateEntry` way is the best option for the single thread case too:. - I define a model. - create a writer based on the model. - get the *buffer* (`REntry`) to write to (one per thread, for example, even if I have only one thread). - commit the *buffer* to the writer. In this way I own the `REntry`, but it has a layout in memory that is directly understood by the serialization process without the need of extra copies. We can also think of a way of constructing the writer from an *inlined* model, without the need for repeated calls to `MakeField`. Something like:. ```cpp. auto ntuple = RNTupleWriter::Recreate({. Field<std::uint32_t>(""id""),. Field<std::vector<float>>(""vpx""),. Field<std::vector<float>>(""vpy""),. Field<std::vector<float>>(""vpz""). }, ""NTuple"", kNTupleFileName);. auto entry = ntuple->CreateEntry();. // cache the pointer for faster access in single thread,. // but I could use TLS (or a framework equivalent) for multithreading. auto& vpx = *entry->Get<std::vector<float>>(""vpx"");. for(auto& event: all_events) {. vpx.clear();. vpx.push_back(42.);. ntuple->Fill(entry); // I would prefer ntuple->Write(entry) but it's a matter of taste. }. ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8688
https://github.com/root-project/root/pull/8688:1810,usability,prefer,prefer,1810,"I had a look ant it seems very nice. About the discrepancy between `REntry::Get` and `RNTupleModel::MakeField`, one may argue that `MakeField` might return a raw pointer too as the memory of the field is owned by the model. OTOH, while it's kind of easy to keep track of the lifetime of an `REntry` (I get it, I use it, I drop it), it seems more difficult to keep track of `RNTupleModel` lifetime, which is bound to the lifetime of `RNTupleWriter`. Using raw pointers from `MakeField` is fine if we assume that the writer of the code only accesses the fields within the scope of `RNTupleWriter` (but the fields are accessible before the writer is created... more confusing). My personal opinion is that the `CreateEntry` way is the best option for the single thread case too:. - I define a model. - create a writer based on the model. - get the *buffer* (`REntry`) to write to (one per thread, for example, even if I have only one thread). - commit the *buffer* to the writer. In this way I own the `REntry`, but it has a layout in memory that is directly understood by the serialization process without the need of extra copies. We can also think of a way of constructing the writer from an *inlined* model, without the need for repeated calls to `MakeField`. Something like:. ```cpp. auto ntuple = RNTupleWriter::Recreate({. Field<std::uint32_t>(""id""),. Field<std::vector<float>>(""vpx""),. Field<std::vector<float>>(""vpy""),. Field<std::vector<float>>(""vpz""). }, ""NTuple"", kNTupleFileName);. auto entry = ntuple->CreateEntry();. // cache the pointer for faster access in single thread,. // but I could use TLS (or a framework equivalent) for multithreading. auto& vpx = *entry->Get<std::vector<float>>(""vpx"");. for(auto& event: all_events) {. vpx.clear();. vpx.push_back(42.);. ntuple->Fill(entry); // I would prefer ntuple->Write(entry) but it's a matter of taste. }. ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8688
https://github.com/root-project/root/pull/8688:274,availability,down,down,274,"I very much like the idea of being able to define a model inline! See #8711 . What the `auto fldXyz = MakeField<type>(""xyz"")` approach is trying to avoid is having to spell the schema more than once. As schemas get large, it can be a little cumbersome to remember and write down all the different types and field names in different places of the program. I do see, however, that the approach complicates reasoning about the lifetimes of the different variables.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8688
https://github.com/root-project/root/pull/8688:52,energy efficiency,model,model,52,"I very much like the idea of being able to define a model inline! See #8711 . What the `auto fldXyz = MakeField<type>(""xyz"")` approach is trying to avoid is having to spell the schema more than once. As schemas get large, it can be a little cumbersome to remember and write down all the different types and field names in different places of the program. I do see, however, that the approach complicates reasoning about the lifetimes of the different variables.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8688
https://github.com/root-project/root/pull/8688:177,integrability,schema,schema,177,"I very much like the idea of being able to define a model inline! See #8711 . What the `auto fldXyz = MakeField<type>(""xyz"")` approach is trying to avoid is having to spell the schema more than once. As schemas get large, it can be a little cumbersome to remember and write down all the different types and field names in different places of the program. I do see, however, that the approach complicates reasoning about the lifetimes of the different variables.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8688
https://github.com/root-project/root/pull/8688:203,integrability,schema,schemas,203,"I very much like the idea of being able to define a model inline! See #8711 . What the `auto fldXyz = MakeField<type>(""xyz"")` approach is trying to avoid is having to spell the schema more than once. As schemas get large, it can be a little cumbersome to remember and write down all the different types and field names in different places of the program. I do see, however, that the approach complicates reasoning about the lifetimes of the different variables.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8688
https://github.com/root-project/root/pull/8688:451,modifiability,variab,variables,451,"I very much like the idea of being able to define a model inline! See #8711 . What the `auto fldXyz = MakeField<type>(""xyz"")` approach is trying to avoid is having to spell the schema more than once. As schemas get large, it can be a little cumbersome to remember and write down all the different types and field names in different places of the program. I do see, however, that the approach complicates reasoning about the lifetimes of the different variables.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8688
https://github.com/root-project/root/pull/8688:148,safety,avoid,avoid,148,"I very much like the idea of being able to define a model inline! See #8711 . What the `auto fldXyz = MakeField<type>(""xyz"")` approach is trying to avoid is having to spell the schema more than once. As schemas get large, it can be a little cumbersome to remember and write down all the different types and field names in different places of the program. I do see, however, that the approach complicates reasoning about the lifetimes of the different variables.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8688
https://github.com/root-project/root/pull/8688:255,safety,reme,remember,255,"I very much like the idea of being able to define a model inline! See #8711 . What the `auto fldXyz = MakeField<type>(""xyz"")` approach is trying to avoid is having to spell the schema more than once. As schemas get large, it can be a little cumbersome to remember and write down all the different types and field names in different places of the program. I do see, however, that the approach complicates reasoning about the lifetimes of the different variables.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8688
https://github.com/root-project/root/pull/8688:392,safety,compl,complicates,392,"I very much like the idea of being able to define a model inline! See #8711 . What the `auto fldXyz = MakeField<type>(""xyz"")` approach is trying to avoid is having to spell the schema more than once. As schemas get large, it can be a little cumbersome to remember and write down all the different types and field names in different places of the program. I do see, however, that the approach complicates reasoning about the lifetimes of the different variables.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8688
https://github.com/root-project/root/pull/8688:52,security,model,model,52,"I very much like the idea of being able to define a model inline! See #8711 . What the `auto fldXyz = MakeField<type>(""xyz"")` approach is trying to avoid is having to spell the schema more than once. As schemas get large, it can be a little cumbersome to remember and write down all the different types and field names in different places of the program. I do see, however, that the approach complicates reasoning about the lifetimes of the different variables.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8688
https://github.com/root-project/root/pull/8688:392,security,compl,complicates,392,"I very much like the idea of being able to define a model inline! See #8711 . What the `auto fldXyz = MakeField<type>(""xyz"")` approach is trying to avoid is having to spell the schema more than once. As schemas get large, it can be a little cumbersome to remember and write down all the different types and field names in different places of the program. I do see, however, that the approach complicates reasoning about the lifetimes of the different variables.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8688
https://github.com/root-project/root/pull/8690:11,deployability,build,build,11,@phsft-bot build,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8690
https://github.com/root-project/root/pull/8690:120,deployability,releas,release,120,"@sazio, please, don't try to open a pull-request targeted at `root-project/stable-branch`. This branch is only used for release purposes. Pull requests usually targe the `master` branch, which is then used to tag new releases. It seems (by the branch name), that your work is also based on some revision of the `latest-stable` branch: GitHub will tell you if there is any merge conflict when you create a PR targeting `master` (quite possible, as `latest-stable` is always behind master). Given all the above, I will close this pull request and you must open a new one, specifying that the content should be merged into `root-project/master`. Cheers,. Javier.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8690
https://github.com/root-project/root/pull/8690:217,deployability,releas,releases,217,"@sazio, please, don't try to open a pull-request targeted at `root-project/stable-branch`. This branch is only used for release purposes. Pull requests usually targe the `master` branch, which is then used to tag new releases. It seems (by the branch name), that your work is also based on some revision of the `latest-stable` branch: GitHub will tell you if there is any merge conflict when you create a PR targeting `master` (quite possible, as `latest-stable` is always behind master). Given all the above, I will close this pull request and you must open a new one, specifying that the content should be merged into `root-project/master`. Cheers,. Javier.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8690
https://github.com/root-project/root/pull/8690:378,interoperability,conflict,conflict,378,"@sazio, please, don't try to open a pull-request targeted at `root-project/stable-branch`. This branch is only used for release purposes. Pull requests usually targe the `master` branch, which is then used to tag new releases. It seems (by the branch name), that your work is also based on some revision of the `latest-stable` branch: GitHub will tell you if there is any merge conflict when you create a PR targeting `master` (quite possible, as `latest-stable` is always behind master). Given all the above, I will close this pull request and you must open a new one, specifying that the content should be merged into `root-project/master`. Cheers,. Javier.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8690
https://github.com/root-project/root/pull/8690:570,interoperability,specif,specifying,570,"@sazio, please, don't try to open a pull-request targeted at `root-project/stable-branch`. This branch is only used for release purposes. Pull requests usually targe the `master` branch, which is then used to tag new releases. It seems (by the branch name), that your work is also based on some revision of the `latest-stable` branch: GitHub will tell you if there is any merge conflict when you create a PR targeting `master` (quite possible, as `latest-stable` is always behind master). Given all the above, I will close this pull request and you must open a new one, specifying that the content should be merged into `root-project/master`. Cheers,. Javier.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8690
https://github.com/root-project/root/pull/8690:590,performance,content,content,590,"@sazio, please, don't try to open a pull-request targeted at `root-project/stable-branch`. This branch is only used for release purposes. Pull requests usually targe the `master` branch, which is then used to tag new releases. It seems (by the branch name), that your work is also based on some revision of the `latest-stable` branch: GitHub will tell you if there is any merge conflict when you create a PR targeting `master` (quite possible, as `latest-stable` is always behind master). Given all the above, I will close this pull request and you must open a new one, specifying that the content should be merged into `root-project/master`. Cheers,. Javier.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8690
https://github.com/root-project/root/pull/8690:517,usability,close,close,517,"@sazio, please, don't try to open a pull-request targeted at `root-project/stable-branch`. This branch is only used for release purposes. Pull requests usually targe the `master` branch, which is then used to tag new releases. It seems (by the branch name), that your work is also based on some revision of the `latest-stable` branch: GitHub will tell you if there is any merge conflict when you create a PR targeting `master` (quite possible, as `latest-stable` is always behind master). Given all the above, I will close this pull request and you must open a new one, specifying that the content should be merged into `root-project/master`. Cheers,. Javier.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8690
https://github.com/root-project/root/pull/8694:27,availability,failur,failures,27,"The last remaining Jenkins failures are not from this PR, so ready for review @lmoneta @guitargeek @hageboeck",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8694
https://github.com/root-project/root/pull/8694:27,deployability,fail,failures,27,"The last remaining Jenkins failures are not from this PR, so ready for review @lmoneta @guitargeek @hageboeck",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8694
https://github.com/root-project/root/pull/8694:27,performance,failur,failures,27,"The last remaining Jenkins failures are not from this PR, so ready for review @lmoneta @guitargeek @hageboeck",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8694
https://github.com/root-project/root/pull/8694:27,reliability,fail,failures,27,"The last remaining Jenkins failures are not from this PR, so ready for review @lmoneta @guitargeek @hageboeck",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8694
https://github.com/root-project/root/pull/8694:71,safety,review,review,71,"The last remaining Jenkins failures are not from this PR, so ready for review @lmoneta @guitargeek @hageboeck",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8694
https://github.com/root-project/root/pull/8694:71,testability,review,review,71,"The last remaining Jenkins failures are not from this PR, so ready for review @lmoneta @guitargeek @hageboeck",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8694
https://github.com/root-project/root/pull/8694:52,availability,error,errored,52,"@phsft-bot build just on mac11.0/cxx17. I think the errored build above timed out before. I'm not sure what happened with the ROOT-debian10-i386/cxx14 build. The build itself was successful, or so it says at the end of the full log.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8694
https://github.com/root-project/root/pull/8694:11,deployability,build,build,11,"@phsft-bot build just on mac11.0/cxx17. I think the errored build above timed out before. I'm not sure what happened with the ROOT-debian10-i386/cxx14 build. The build itself was successful, or so it says at the end of the full log.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8694
https://github.com/root-project/root/pull/8694:60,deployability,build,build,60,"@phsft-bot build just on mac11.0/cxx17. I think the errored build above timed out before. I'm not sure what happened with the ROOT-debian10-i386/cxx14 build. The build itself was successful, or so it says at the end of the full log.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8694
https://github.com/root-project/root/pull/8694:151,deployability,build,build,151,"@phsft-bot build just on mac11.0/cxx17. I think the errored build above timed out before. I'm not sure what happened with the ROOT-debian10-i386/cxx14 build. The build itself was successful, or so it says at the end of the full log.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8694
https://github.com/root-project/root/pull/8694:162,deployability,build,build,162,"@phsft-bot build just on mac11.0/cxx17. I think the errored build above timed out before. I'm not sure what happened with the ROOT-debian10-i386/cxx14 build. The build itself was successful, or so it says at the end of the full log.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8694
https://github.com/root-project/root/pull/8694:228,deployability,log,log,228,"@phsft-bot build just on mac11.0/cxx17. I think the errored build above timed out before. I'm not sure what happened with the ROOT-debian10-i386/cxx14 build. The build itself was successful, or so it says at the end of the full log.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8694
https://github.com/root-project/root/pull/8694:52,performance,error,errored,52,"@phsft-bot build just on mac11.0/cxx17. I think the errored build above timed out before. I'm not sure what happened with the ROOT-debian10-i386/cxx14 build. The build itself was successful, or so it says at the end of the full log.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8694
https://github.com/root-project/root/pull/8694:72,performance,time,timed,72,"@phsft-bot build just on mac11.0/cxx17. I think the errored build above timed out before. I'm not sure what happened with the ROOT-debian10-i386/cxx14 build. The build itself was successful, or so it says at the end of the full log.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8694
https://github.com/root-project/root/pull/8694:52,safety,error,errored,52,"@phsft-bot build just on mac11.0/cxx17. I think the errored build above timed out before. I'm not sure what happened with the ROOT-debian10-i386/cxx14 build. The build itself was successful, or so it says at the end of the full log.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8694
https://github.com/root-project/root/pull/8694:228,safety,log,log,228,"@phsft-bot build just on mac11.0/cxx17. I think the errored build above timed out before. I'm not sure what happened with the ROOT-debian10-i386/cxx14 build. The build itself was successful, or so it says at the end of the full log.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8694
https://github.com/root-project/root/pull/8694:228,security,log,log,228,"@phsft-bot build just on mac11.0/cxx17. I think the errored build above timed out before. I'm not sure what happened with the ROOT-debian10-i386/cxx14 build. The build itself was successful, or so it says at the end of the full log.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8694
https://github.com/root-project/root/pull/8694:228,testability,log,log,228,"@phsft-bot build just on mac11.0/cxx17. I think the errored build above timed out before. I'm not sure what happened with the ROOT-debian10-i386/cxx14 build. The build itself was successful, or so it says at the end of the full log.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8694
https://github.com/root-project/root/pull/8694:52,usability,error,errored,52,"@phsft-bot build just on mac11.0/cxx17. I think the errored build above timed out before. I'm not sure what happened with the ROOT-debian10-i386/cxx14 build. The build itself was successful, or so it says at the end of the full log.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8694
https://github.com/root-project/root/pull/8694:33,safety,review,review,33,"I have opened a PR addressing my review comments, so we don't forget about this.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8694
https://github.com/root-project/root/pull/8694:33,testability,review,review,33,"I have opened a PR addressing my review comments, so we don't forget about this.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8694
https://github.com/root-project/root/issues/8695:440,deployability,build,building,440,"@MarkusFrankATcernch may describe in more detail different use cases for using G4 units with TGeo, he is one of the main authors of this. The main reason was to have the material properties calculated in the same way as Geant4, and also to be able to have a compatible internal representation when coming from GDML without having to go through conversions. . The default units had to be changed back to being ROOT ones in the master, since building geometry (materials) assumes ROOT units even if the default is set to Geant4 ones. The interface for changing the default units was intended to be allowed by explicit unlocking and then locking, to avoid user mistakes. I am aware that the support for units is far from be perfect, it was done like this to accommodate new requirements while kepping the backward compatibility.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8695
https://github.com/root-project/root/issues/8695:536,integrability,interfac,interface,536,"@MarkusFrankATcernch may describe in more detail different use cases for using G4 units with TGeo, he is one of the main authors of this. The main reason was to have the material properties calculated in the same way as Geant4, and also to be able to have a compatible internal representation when coming from GDML without having to go through conversions. . The default units had to be changed back to being ROOT ones in the master, since building geometry (materials) assumes ROOT units even if the default is set to Geant4 ones. The interface for changing the default units was intended to be allowed by explicit unlocking and then locking, to avoid user mistakes. I am aware that the support for units is far from be perfect, it was done like this to accommodate new requirements while kepping the backward compatibility.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8695
https://github.com/root-project/root/issues/8695:258,interoperability,compatib,compatible,258,"@MarkusFrankATcernch may describe in more detail different use cases for using G4 units with TGeo, he is one of the main authors of this. The main reason was to have the material properties calculated in the same way as Geant4, and also to be able to have a compatible internal representation when coming from GDML without having to go through conversions. . The default units had to be changed back to being ROOT ones in the master, since building geometry (materials) assumes ROOT units even if the default is set to Geant4 ones. The interface for changing the default units was intended to be allowed by explicit unlocking and then locking, to avoid user mistakes. I am aware that the support for units is far from be perfect, it was done like this to accommodate new requirements while kepping the backward compatibility.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8695
https://github.com/root-project/root/issues/8695:344,interoperability,convers,conversions,344,"@MarkusFrankATcernch may describe in more detail different use cases for using G4 units with TGeo, he is one of the main authors of this. The main reason was to have the material properties calculated in the same way as Geant4, and also to be able to have a compatible internal representation when coming from GDML without having to go through conversions. . The default units had to be changed back to being ROOT ones in the master, since building geometry (materials) assumes ROOT units even if the default is set to Geant4 ones. The interface for changing the default units was intended to be allowed by explicit unlocking and then locking, to avoid user mistakes. I am aware that the support for units is far from be perfect, it was done like this to accommodate new requirements while kepping the backward compatibility.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8695
https://github.com/root-project/root/issues/8695:536,interoperability,interfac,interface,536,"@MarkusFrankATcernch may describe in more detail different use cases for using G4 units with TGeo, he is one of the main authors of this. The main reason was to have the material properties calculated in the same way as Geant4, and also to be able to have a compatible internal representation when coming from GDML without having to go through conversions. . The default units had to be changed back to being ROOT ones in the master, since building geometry (materials) assumes ROOT units even if the default is set to Geant4 ones. The interface for changing the default units was intended to be allowed by explicit unlocking and then locking, to avoid user mistakes. I am aware that the support for units is far from be perfect, it was done like this to accommodate new requirements while kepping the backward compatibility.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8695
https://github.com/root-project/root/issues/8695:811,interoperability,compatib,compatibility,811,"@MarkusFrankATcernch may describe in more detail different use cases for using G4 units with TGeo, he is one of the main authors of this. The main reason was to have the material properties calculated in the same way as Geant4, and also to be able to have a compatible internal representation when coming from GDML without having to go through conversions. . The default units had to be changed back to being ROOT ones in the master, since building geometry (materials) assumes ROOT units even if the default is set to Geant4 ones. The interface for changing the default units was intended to be allowed by explicit unlocking and then locking, to avoid user mistakes. I am aware that the support for units is far from be perfect, it was done like this to accommodate new requirements while kepping the backward compatibility.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8695
https://github.com/root-project/root/issues/8695:536,modifiability,interfac,interface,536,"@MarkusFrankATcernch may describe in more detail different use cases for using G4 units with TGeo, he is one of the main authors of this. The main reason was to have the material properties calculated in the same way as Geant4, and also to be able to have a compatible internal representation when coming from GDML without having to go through conversions. . The default units had to be changed back to being ROOT ones in the master, since building geometry (materials) assumes ROOT units even if the default is set to Geant4 ones. The interface for changing the default units was intended to be allowed by explicit unlocking and then locking, to avoid user mistakes. I am aware that the support for units is far from be perfect, it was done like this to accommodate new requirements while kepping the backward compatibility.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8695
https://github.com/root-project/root/issues/8695:635,performance,lock,locking,635,"@MarkusFrankATcernch may describe in more detail different use cases for using G4 units with TGeo, he is one of the main authors of this. The main reason was to have the material properties calculated in the same way as Geant4, and also to be able to have a compatible internal representation when coming from GDML without having to go through conversions. . The default units had to be changed back to being ROOT ones in the master, since building geometry (materials) assumes ROOT units even if the default is set to Geant4 ones. The interface for changing the default units was intended to be allowed by explicit unlocking and then locking, to avoid user mistakes. I am aware that the support for units is far from be perfect, it was done like this to accommodate new requirements while kepping the backward compatibility.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8695
https://github.com/root-project/root/issues/8695:647,safety,avoid,avoid,647,"@MarkusFrankATcernch may describe in more detail different use cases for using G4 units with TGeo, he is one of the main authors of this. The main reason was to have the material properties calculated in the same way as Geant4, and also to be able to have a compatible internal representation when coming from GDML without having to go through conversions. . The default units had to be changed back to being ROOT ones in the master, since building geometry (materials) assumes ROOT units even if the default is set to Geant4 ones. The interface for changing the default units was intended to be allowed by explicit unlocking and then locking, to avoid user mistakes. I am aware that the support for units is far from be perfect, it was done like this to accommodate new requirements while kepping the backward compatibility.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8695
https://github.com/root-project/root/issues/8695:121,security,auth,authors,121,"@MarkusFrankATcernch may describe in more detail different use cases for using G4 units with TGeo, he is one of the main authors of this. The main reason was to have the material properties calculated in the same way as Geant4, and also to be able to have a compatible internal representation when coming from GDML without having to go through conversions. . The default units had to be changed back to being ROOT ones in the master, since building geometry (materials) assumes ROOT units even if the default is set to Geant4 ones. The interface for changing the default units was intended to be allowed by explicit unlocking and then locking, to avoid user mistakes. I am aware that the support for units is far from be perfect, it was done like this to accommodate new requirements while kepping the backward compatibility.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8695
https://github.com/root-project/root/issues/8695:635,security,lock,locking,635,"@MarkusFrankATcernch may describe in more detail different use cases for using G4 units with TGeo, he is one of the main authors of this. The main reason was to have the material properties calculated in the same way as Geant4, and also to be able to have a compatible internal representation when coming from GDML without having to go through conversions. . The default units had to be changed back to being ROOT ones in the master, since building geometry (materials) assumes ROOT units even if the default is set to Geant4 ones. The interface for changing the default units was intended to be allowed by explicit unlocking and then locking, to avoid user mistakes. I am aware that the support for units is far from be perfect, it was done like this to accommodate new requirements while kepping the backward compatibility.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8695
https://github.com/root-project/root/issues/8695:82,testability,unit,units,82,"@MarkusFrankATcernch may describe in more detail different use cases for using G4 units with TGeo, he is one of the main authors of this. The main reason was to have the material properties calculated in the same way as Geant4, and also to be able to have a compatible internal representation when coming from GDML without having to go through conversions. . The default units had to be changed back to being ROOT ones in the master, since building geometry (materials) assumes ROOT units even if the default is set to Geant4 ones. The interface for changing the default units was intended to be allowed by explicit unlocking and then locking, to avoid user mistakes. I am aware that the support for units is far from be perfect, it was done like this to accommodate new requirements while kepping the backward compatibility.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8695
https://github.com/root-project/root/issues/8695:371,testability,unit,units,371,"@MarkusFrankATcernch may describe in more detail different use cases for using G4 units with TGeo, he is one of the main authors of this. The main reason was to have the material properties calculated in the same way as Geant4, and also to be able to have a compatible internal representation when coming from GDML without having to go through conversions. . The default units had to be changed back to being ROOT ones in the master, since building geometry (materials) assumes ROOT units even if the default is set to Geant4 ones. The interface for changing the default units was intended to be allowed by explicit unlocking and then locking, to avoid user mistakes. I am aware that the support for units is far from be perfect, it was done like this to accommodate new requirements while kepping the backward compatibility.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8695
https://github.com/root-project/root/issues/8695:483,testability,unit,units,483,"@MarkusFrankATcernch may describe in more detail different use cases for using G4 units with TGeo, he is one of the main authors of this. The main reason was to have the material properties calculated in the same way as Geant4, and also to be able to have a compatible internal representation when coming from GDML without having to go through conversions. . The default units had to be changed back to being ROOT ones in the master, since building geometry (materials) assumes ROOT units even if the default is set to Geant4 ones. The interface for changing the default units was intended to be allowed by explicit unlocking and then locking, to avoid user mistakes. I am aware that the support for units is far from be perfect, it was done like this to accommodate new requirements while kepping the backward compatibility.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8695
https://github.com/root-project/root/issues/8695:571,testability,unit,units,571,"@MarkusFrankATcernch may describe in more detail different use cases for using G4 units with TGeo, he is one of the main authors of this. The main reason was to have the material properties calculated in the same way as Geant4, and also to be able to have a compatible internal representation when coming from GDML without having to go through conversions. . The default units had to be changed back to being ROOT ones in the master, since building geometry (materials) assumes ROOT units even if the default is set to Geant4 ones. The interface for changing the default units was intended to be allowed by explicit unlocking and then locking, to avoid user mistakes. I am aware that the support for units is far from be perfect, it was done like this to accommodate new requirements while kepping the backward compatibility.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8695
https://github.com/root-project/root/issues/8695:700,testability,unit,units,700,"@MarkusFrankATcernch may describe in more detail different use cases for using G4 units with TGeo, he is one of the main authors of this. The main reason was to have the material properties calculated in the same way as Geant4, and also to be able to have a compatible internal representation when coming from GDML without having to go through conversions. . The default units had to be changed back to being ROOT ones in the master, since building geometry (materials) assumes ROOT units even if the default is set to Geant4 ones. The interface for changing the default units was intended to be allowed by explicit unlocking and then locking, to avoid user mistakes. I am aware that the support for units is far from be perfect, it was done like this to accommodate new requirements while kepping the backward compatibility.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8695
https://github.com/root-project/root/issues/8695:653,usability,user,user,653,"@MarkusFrankATcernch may describe in more detail different use cases for using G4 units with TGeo, he is one of the main authors of this. The main reason was to have the material properties calculated in the same way as Geant4, and also to be able to have a compatible internal representation when coming from GDML without having to go through conversions. . The default units had to be changed back to being ROOT ones in the master, since building geometry (materials) assumes ROOT units even if the default is set to Geant4 ones. The interface for changing the default units was intended to be allowed by explicit unlocking and then locking, to avoid user mistakes. I am aware that the support for units is far from be perfect, it was done like this to accommodate new requirements while kepping the backward compatibility.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8695
https://github.com/root-project/root/issues/8695:688,usability,support,support,688,"@MarkusFrankATcernch may describe in more detail different use cases for using G4 units with TGeo, he is one of the main authors of this. The main reason was to have the material properties calculated in the same way as Geant4, and also to be able to have a compatible internal representation when coming from GDML without having to go through conversions. . The default units had to be changed back to being ROOT ones in the master, since building geometry (materials) assumes ROOT units even if the default is set to Geant4 ones. The interface for changing the default units was intended to be allowed by explicit unlocking and then locking, to avoid user mistakes. I am aware that the support for units is far from be perfect, it was done like this to accommodate new requirements while kepping the backward compatibility.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8695
https://github.com/root-project/root/issues/8695:430,interoperability,incompatib,incompatible,430,"There is an intrinsic problem here due to the re-use of the static member . TGeoManager::fgDefaultUnits. This variable is used simultaneously for 2 functionalities:. - The mechanism to change units in TGeo from CGS (ROOT) to G4 units. - The mechanism to switch the GDML between CGS and G4 output (lengths in ""mm""). The problem now arises, that the original default for GDML required fgDefaultUnits to be set to kG4Units, which is incompatible with the intrinsic TGeo CGS unit system. It is not possible to combine both with these constraints using unly one flag of the TGeoManager. The very only way to have both simultaneously would be to separate the two functionalities by defining a ""new"" variable. I can provide something like this if this is required.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8695
https://github.com/root-project/root/issues/8695:110,modifiability,variab,variable,110,"There is an intrinsic problem here due to the re-use of the static member . TGeoManager::fgDefaultUnits. This variable is used simultaneously for 2 functionalities:. - The mechanism to change units in TGeo from CGS (ROOT) to G4 units. - The mechanism to switch the GDML between CGS and G4 output (lengths in ""mm""). The problem now arises, that the original default for GDML required fgDefaultUnits to be set to kG4Units, which is incompatible with the intrinsic TGeo CGS unit system. It is not possible to combine both with these constraints using unly one flag of the TGeoManager. The very only way to have both simultaneously would be to separate the two functionalities by defining a ""new"" variable. I can provide something like this if this is required.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8695
https://github.com/root-project/root/issues/8695:693,modifiability,variab,variable,693,"There is an intrinsic problem here due to the re-use of the static member . TGeoManager::fgDefaultUnits. This variable is used simultaneously for 2 functionalities:. - The mechanism to change units in TGeo from CGS (ROOT) to G4 units. - The mechanism to switch the GDML between CGS and G4 output (lengths in ""mm""). The problem now arises, that the original default for GDML required fgDefaultUnits to be set to kG4Units, which is incompatible with the intrinsic TGeo CGS unit system. It is not possible to combine both with these constraints using unly one flag of the TGeoManager. The very only way to have both simultaneously would be to separate the two functionalities by defining a ""new"" variable. I can provide something like this if this is required.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8695
https://github.com/root-project/root/issues/8695:127,testability,simul,simultaneously,127,"There is an intrinsic problem here due to the re-use of the static member . TGeoManager::fgDefaultUnits. This variable is used simultaneously for 2 functionalities:. - The mechanism to change units in TGeo from CGS (ROOT) to G4 units. - The mechanism to switch the GDML between CGS and G4 output (lengths in ""mm""). The problem now arises, that the original default for GDML required fgDefaultUnits to be set to kG4Units, which is incompatible with the intrinsic TGeo CGS unit system. It is not possible to combine both with these constraints using unly one flag of the TGeoManager. The very only way to have both simultaneously would be to separate the two functionalities by defining a ""new"" variable. I can provide something like this if this is required.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8695
https://github.com/root-project/root/issues/8695:192,testability,unit,units,192,"There is an intrinsic problem here due to the re-use of the static member . TGeoManager::fgDefaultUnits. This variable is used simultaneously for 2 functionalities:. - The mechanism to change units in TGeo from CGS (ROOT) to G4 units. - The mechanism to switch the GDML between CGS and G4 output (lengths in ""mm""). The problem now arises, that the original default for GDML required fgDefaultUnits to be set to kG4Units, which is incompatible with the intrinsic TGeo CGS unit system. It is not possible to combine both with these constraints using unly one flag of the TGeoManager. The very only way to have both simultaneously would be to separate the two functionalities by defining a ""new"" variable. I can provide something like this if this is required.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8695
https://github.com/root-project/root/issues/8695:228,testability,unit,units,228,"There is an intrinsic problem here due to the re-use of the static member . TGeoManager::fgDefaultUnits. This variable is used simultaneously for 2 functionalities:. - The mechanism to change units in TGeo from CGS (ROOT) to G4 units. - The mechanism to switch the GDML between CGS and G4 output (lengths in ""mm""). The problem now arises, that the original default for GDML required fgDefaultUnits to be set to kG4Units, which is incompatible with the intrinsic TGeo CGS unit system. It is not possible to combine both with these constraints using unly one flag of the TGeoManager. The very only way to have both simultaneously would be to separate the two functionalities by defining a ""new"" variable. I can provide something like this if this is required.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8695
https://github.com/root-project/root/issues/8695:471,testability,unit,unit,471,"There is an intrinsic problem here due to the re-use of the static member . TGeoManager::fgDefaultUnits. This variable is used simultaneously for 2 functionalities:. - The mechanism to change units in TGeo from CGS (ROOT) to G4 units. - The mechanism to switch the GDML between CGS and G4 output (lengths in ""mm""). The problem now arises, that the original default for GDML required fgDefaultUnits to be set to kG4Units, which is incompatible with the intrinsic TGeo CGS unit system. It is not possible to combine both with these constraints using unly one flag of the TGeoManager. The very only way to have both simultaneously would be to separate the two functionalities by defining a ""new"" variable. I can provide something like this if this is required.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8695
https://github.com/root-project/root/issues/8695:613,testability,simul,simultaneously,613,"There is an intrinsic problem here due to the re-use of the static member . TGeoManager::fgDefaultUnits. This variable is used simultaneously for 2 functionalities:. - The mechanism to change units in TGeo from CGS (ROOT) to G4 units. - The mechanism to switch the GDML between CGS and G4 output (lengths in ""mm""). The problem now arises, that the original default for GDML required fgDefaultUnits to be set to kG4Units, which is incompatible with the intrinsic TGeo CGS unit system. It is not possible to combine both with these constraints using unly one flag of the TGeoManager. The very only way to have both simultaneously would be to separate the two functionalities by defining a ""new"" variable. I can provide something like this if this is required.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8695
https://github.com/root-project/root/issues/8695:34,energy efficiency,current,current,34,"As the problem extends beyond the current documentation issue, I propose taking the discussion offline. After having an acceptable solution covering all scenarios, we will reflect it in the online TGeo documentation (which we are currently reviewing and improving)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8695
https://github.com/root-project/root/issues/8695:230,energy efficiency,current,currently,230,"As the problem extends beyond the current documentation issue, I propose taking the discussion offline. After having an acceptable solution covering all scenarios, we will reflect it in the online TGeo documentation (which we are currently reviewing and improving)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8695
https://github.com/root-project/root/issues/8695:15,modifiability,exten,extends,15,"As the problem extends beyond the current documentation issue, I propose taking the discussion offline. After having an acceptable solution covering all scenarios, we will reflect it in the online TGeo documentation (which we are currently reviewing and improving)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8695
https://github.com/root-project/root/issues/8695:153,modifiability,scenario,scenarios,153,"As the problem extends beyond the current documentation issue, I propose taking the discussion offline. After having an acceptable solution covering all scenarios, we will reflect it in the online TGeo documentation (which we are currently reviewing and improving)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8695
https://github.com/root-project/root/issues/8695:240,safety,review,reviewing,240,"As the problem extends beyond the current documentation issue, I propose taking the discussion offline. After having an acceptable solution covering all scenarios, we will reflect it in the online TGeo documentation (which we are currently reviewing and improving)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8695
https://github.com/root-project/root/issues/8695:240,testability,review,reviewing,240,"As the problem extends beyond the current documentation issue, I propose taking the discussion offline. After having an acceptable solution covering all scenarios, we will reflect it in the online TGeo documentation (which we are currently reviewing and improving)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8695
https://github.com/root-project/root/issues/8695:42,usability,document,documentation,42,"As the problem extends beyond the current documentation issue, I propose taking the discussion offline. After having an acceptable solution covering all scenarios, we will reflect it in the online TGeo documentation (which we are currently reviewing and improving)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8695
https://github.com/root-project/root/issues/8695:202,usability,document,documentation,202,"As the problem extends beyond the current documentation issue, I propose taking the discussion offline. After having an acceptable solution covering all scenarios, we will reflect it in the online TGeo documentation (which we are currently reviewing and improving)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8695
https://github.com/root-project/root/issues/8695:25,availability,avail,available,25,"There were several fixes available now in the master for having consistent behavior when using Root versus Geant4 units, which can be set now once before defining any elements or materials without having to lock/unlock:. `TGeoManager::SetDefaultUnits(TGeoManager::kG4Units);`. The default units are Root ones (as described in the docs). Please modify this ticket if you observe any other inconsistency.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8695
https://github.com/root-project/root/issues/8695:64,availability,consist,consistent,64,"There were several fixes available now in the master for having consistent behavior when using Root versus Geant4 units, which can be set now once before defining any elements or materials without having to lock/unlock:. `TGeoManager::SetDefaultUnits(TGeoManager::kG4Units);`. The default units are Root ones (as described in the docs). Please modify this ticket if you observe any other inconsistency.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8695
https://github.com/root-project/root/issues/8695:370,deployability,observ,observe,370,"There were several fixes available now in the master for having consistent behavior when using Root versus Geant4 units, which can be set now once before defining any elements or materials without having to lock/unlock:. `TGeoManager::SetDefaultUnits(TGeoManager::kG4Units);`. The default units are Root ones (as described in the docs). Please modify this ticket if you observe any other inconsistency.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8695
https://github.com/root-project/root/issues/8695:207,performance,lock,lock,207,"There were several fixes available now in the master for having consistent behavior when using Root versus Geant4 units, which can be set now once before defining any elements or materials without having to lock/unlock:. `TGeoManager::SetDefaultUnits(TGeoManager::kG4Units);`. The default units are Root ones (as described in the docs). Please modify this ticket if you observe any other inconsistency.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8695
https://github.com/root-project/root/issues/8695:25,reliability,availab,available,25,"There were several fixes available now in the master for having consistent behavior when using Root versus Geant4 units, which can be set now once before defining any elements or materials without having to lock/unlock:. `TGeoManager::SetDefaultUnits(TGeoManager::kG4Units);`. The default units are Root ones (as described in the docs). Please modify this ticket if you observe any other inconsistency.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8695
https://github.com/root-project/root/issues/8695:25,safety,avail,available,25,"There were several fixes available now in the master for having consistent behavior when using Root versus Geant4 units, which can be set now once before defining any elements or materials without having to lock/unlock:. `TGeoManager::SetDefaultUnits(TGeoManager::kG4Units);`. The default units are Root ones (as described in the docs). Please modify this ticket if you observe any other inconsistency.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8695
https://github.com/root-project/root/issues/8695:25,security,availab,available,25,"There were several fixes available now in the master for having consistent behavior when using Root versus Geant4 units, which can be set now once before defining any elements or materials without having to lock/unlock:. `TGeoManager::SetDefaultUnits(TGeoManager::kG4Units);`. The default units are Root ones (as described in the docs). Please modify this ticket if you observe any other inconsistency.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8695
https://github.com/root-project/root/issues/8695:207,security,lock,lock,207,"There were several fixes available now in the master for having consistent behavior when using Root versus Geant4 units, which can be set now once before defining any elements or materials without having to lock/unlock:. `TGeoManager::SetDefaultUnits(TGeoManager::kG4Units);`. The default units are Root ones (as described in the docs). Please modify this ticket if you observe any other inconsistency.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8695
https://github.com/root-project/root/issues/8695:344,security,modif,modify,344,"There were several fixes available now in the master for having consistent behavior when using Root versus Geant4 units, which can be set now once before defining any elements or materials without having to lock/unlock:. `TGeoManager::SetDefaultUnits(TGeoManager::kG4Units);`. The default units are Root ones (as described in the docs). Please modify this ticket if you observe any other inconsistency.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8695
https://github.com/root-project/root/issues/8695:114,testability,unit,units,114,"There were several fixes available now in the master for having consistent behavior when using Root versus Geant4 units, which can be set now once before defining any elements or materials without having to lock/unlock:. `TGeoManager::SetDefaultUnits(TGeoManager::kG4Units);`. The default units are Root ones (as described in the docs). Please modify this ticket if you observe any other inconsistency.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8695
https://github.com/root-project/root/issues/8695:289,testability,unit,units,289,"There were several fixes available now in the master for having consistent behavior when using Root versus Geant4 units, which can be set now once before defining any elements or materials without having to lock/unlock:. `TGeoManager::SetDefaultUnits(TGeoManager::kG4Units);`. The default units are Root ones (as described in the docs). Please modify this ticket if you observe any other inconsistency.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8695
https://github.com/root-project/root/issues/8695:370,testability,observ,observe,370,"There were several fixes available now in the master for having consistent behavior when using Root versus Geant4 units, which can be set now once before defining any elements or materials without having to lock/unlock:. `TGeoManager::SetDefaultUnits(TGeoManager::kG4Units);`. The default units are Root ones (as described in the docs). Please modify this ticket if you observe any other inconsistency.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8695
https://github.com/root-project/root/issues/8695:64,usability,consist,consistent,64,"There were several fixes available now in the master for having consistent behavior when using Root versus Geant4 units, which can be set now once before defining any elements or materials without having to lock/unlock:. `TGeoManager::SetDefaultUnits(TGeoManager::kG4Units);`. The default units are Root ones (as described in the docs). Please modify this ticket if you observe any other inconsistency.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8695
https://github.com/root-project/root/issues/8695:75,usability,behavi,behavior,75,"There were several fixes available now in the master for having consistent behavior when using Root versus Geant4 units, which can be set now once before defining any elements or materials without having to lock/unlock:. `TGeoManager::SetDefaultUnits(TGeoManager::kG4Units);`. The default units are Root ones (as described in the docs). Please modify this ticket if you observe any other inconsistency.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8695
https://github.com/root-project/root/pull/8698:1316,availability,state,state,1316,"@sazio, as I said on the other PR, you originally based your local work on the `latest-stable` branch, which is only used by users to get the latest release, but not to check in new content into ROOT's repository. As you see in the change diff, there are lots of changes that are not meant to be there (that you should also have seen before creating the pull request, in the ""Review changes"" section). Also there are a number of merge conflicts. You must solve this locally before creating the PR. You must rebase your commits on top of `master`, otherwise this is not going to work. Here's what I would recommend, provided that your actual commits are on a local branch called `latest-stable`:. ```bash. $ git branch -m latest-stable tmva-variable-plotter # Renames your local branch. $ git fetch origin master:master. $ git checkout tmva-variable-plotter. # Rebase your 'tmva-variable-plotter' branch onto 'master', provided that the fork point of your branch was 'origin/latest-stable'. # The default editor will open showing a number of 'pick' lines; just verify that those commits are part of your work. You may. # need to fix any merge conflict that might happen here. $ git rebase --interactive --onto master origin/latest-stable. # If you mess up at some point while rebasing, you can revert to the previous state issueing `$ git rebase --abort`. # Then push your changes to GitHub and open a new pull-request. $ git push ... ```. Afterward, you can open a new pull request with origin on `sazio:tmva-variable-plotter` targetting `root-project:master`. Other than that, I will close this pull-request, as it cannot be merged in its current state. Cheers,. Javier.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8698
https://github.com/root-project/root/pull/8698:1648,availability,state,state,1648,"@sazio, as I said on the other PR, you originally based your local work on the `latest-stable` branch, which is only used by users to get the latest release, but not to check in new content into ROOT's repository. As you see in the change diff, there are lots of changes that are not meant to be there (that you should also have seen before creating the pull request, in the ""Review changes"" section). Also there are a number of merge conflicts. You must solve this locally before creating the PR. You must rebase your commits on top of `master`, otherwise this is not going to work. Here's what I would recommend, provided that your actual commits are on a local branch called `latest-stable`:. ```bash. $ git branch -m latest-stable tmva-variable-plotter # Renames your local branch. $ git fetch origin master:master. $ git checkout tmva-variable-plotter. # Rebase your 'tmva-variable-plotter' branch onto 'master', provided that the fork point of your branch was 'origin/latest-stable'. # The default editor will open showing a number of 'pick' lines; just verify that those commits are part of your work. You may. # need to fix any merge conflict that might happen here. $ git rebase --interactive --onto master origin/latest-stable. # If you mess up at some point while rebasing, you can revert to the previous state issueing `$ git rebase --abort`. # Then push your changes to GitHub and open a new pull-request. $ git push ... ```. Afterward, you can open a new pull request with origin on `sazio:tmva-variable-plotter` targetting `root-project:master`. Other than that, I will close this pull-request, as it cannot be merged in its current state. Cheers,. Javier.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8698
https://github.com/root-project/root/pull/8698:149,deployability,releas,release,149,"@sazio, as I said on the other PR, you originally based your local work on the `latest-stable` branch, which is only used by users to get the latest release, but not to check in new content into ROOT's repository. As you see in the change diff, there are lots of changes that are not meant to be there (that you should also have seen before creating the pull request, in the ""Review changes"" section). Also there are a number of merge conflicts. You must solve this locally before creating the PR. You must rebase your commits on top of `master`, otherwise this is not going to work. Here's what I would recommend, provided that your actual commits are on a local branch called `latest-stable`:. ```bash. $ git branch -m latest-stable tmva-variable-plotter # Renames your local branch. $ git fetch origin master:master. $ git checkout tmva-variable-plotter. # Rebase your 'tmva-variable-plotter' branch onto 'master', provided that the fork point of your branch was 'origin/latest-stable'. # The default editor will open showing a number of 'pick' lines; just verify that those commits are part of your work. You may. # need to fix any merge conflict that might happen here. $ git rebase --interactive --onto master origin/latest-stable. # If you mess up at some point while rebasing, you can revert to the previous state issueing `$ git rebase --abort`. # Then push your changes to GitHub and open a new pull-request. $ git push ... ```. Afterward, you can open a new pull request with origin on `sazio:tmva-variable-plotter` targetting `root-project:master`. Other than that, I will close this pull-request, as it cannot be merged in its current state. Cheers,. Javier.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8698
https://github.com/root-project/root/pull/8698:1640,energy efficiency,current,current,1640,"@sazio, as I said on the other PR, you originally based your local work on the `latest-stable` branch, which is only used by users to get the latest release, but not to check in new content into ROOT's repository. As you see in the change diff, there are lots of changes that are not meant to be there (that you should also have seen before creating the pull request, in the ""Review changes"" section). Also there are a number of merge conflicts. You must solve this locally before creating the PR. You must rebase your commits on top of `master`, otherwise this is not going to work. Here's what I would recommend, provided that your actual commits are on a local branch called `latest-stable`:. ```bash. $ git branch -m latest-stable tmva-variable-plotter # Renames your local branch. $ git fetch origin master:master. $ git checkout tmva-variable-plotter. # Rebase your 'tmva-variable-plotter' branch onto 'master', provided that the fork point of your branch was 'origin/latest-stable'. # The default editor will open showing a number of 'pick' lines; just verify that those commits are part of your work. You may. # need to fix any merge conflict that might happen here. $ git rebase --interactive --onto master origin/latest-stable. # If you mess up at some point while rebasing, you can revert to the previous state issueing `$ git rebase --abort`. # Then push your changes to GitHub and open a new pull-request. $ git push ... ```. Afterward, you can open a new pull request with origin on `sazio:tmva-variable-plotter` targetting `root-project:master`. Other than that, I will close this pull-request, as it cannot be merged in its current state. Cheers,. Javier.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8698
https://github.com/root-project/root/pull/8698:202,integrability,repositor,repository,202,"@sazio, as I said on the other PR, you originally based your local work on the `latest-stable` branch, which is only used by users to get the latest release, but not to check in new content into ROOT's repository. As you see in the change diff, there are lots of changes that are not meant to be there (that you should also have seen before creating the pull request, in the ""Review changes"" section). Also there are a number of merge conflicts. You must solve this locally before creating the PR. You must rebase your commits on top of `master`, otherwise this is not going to work. Here's what I would recommend, provided that your actual commits are on a local branch called `latest-stable`:. ```bash. $ git branch -m latest-stable tmva-variable-plotter # Renames your local branch. $ git fetch origin master:master. $ git checkout tmva-variable-plotter. # Rebase your 'tmva-variable-plotter' branch onto 'master', provided that the fork point of your branch was 'origin/latest-stable'. # The default editor will open showing a number of 'pick' lines; just verify that those commits are part of your work. You may. # need to fix any merge conflict that might happen here. $ git rebase --interactive --onto master origin/latest-stable. # If you mess up at some point while rebasing, you can revert to the previous state issueing `$ git rebase --abort`. # Then push your changes to GitHub and open a new pull-request. $ git push ... ```. Afterward, you can open a new pull request with origin on `sazio:tmva-variable-plotter` targetting `root-project:master`. Other than that, I will close this pull-request, as it cannot be merged in its current state. Cheers,. Javier.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8698
https://github.com/root-project/root/pull/8698:1316,integrability,state,state,1316,"@sazio, as I said on the other PR, you originally based your local work on the `latest-stable` branch, which is only used by users to get the latest release, but not to check in new content into ROOT's repository. As you see in the change diff, there are lots of changes that are not meant to be there (that you should also have seen before creating the pull request, in the ""Review changes"" section). Also there are a number of merge conflicts. You must solve this locally before creating the PR. You must rebase your commits on top of `master`, otherwise this is not going to work. Here's what I would recommend, provided that your actual commits are on a local branch called `latest-stable`:. ```bash. $ git branch -m latest-stable tmva-variable-plotter # Renames your local branch. $ git fetch origin master:master. $ git checkout tmva-variable-plotter. # Rebase your 'tmva-variable-plotter' branch onto 'master', provided that the fork point of your branch was 'origin/latest-stable'. # The default editor will open showing a number of 'pick' lines; just verify that those commits are part of your work. You may. # need to fix any merge conflict that might happen here. $ git rebase --interactive --onto master origin/latest-stable. # If you mess up at some point while rebasing, you can revert to the previous state issueing `$ git rebase --abort`. # Then push your changes to GitHub and open a new pull-request. $ git push ... ```. Afterward, you can open a new pull request with origin on `sazio:tmva-variable-plotter` targetting `root-project:master`. Other than that, I will close this pull-request, as it cannot be merged in its current state. Cheers,. Javier.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8698
https://github.com/root-project/root/pull/8698:1648,integrability,state,state,1648,"@sazio, as I said on the other PR, you originally based your local work on the `latest-stable` branch, which is only used by users to get the latest release, but not to check in new content into ROOT's repository. As you see in the change diff, there are lots of changes that are not meant to be there (that you should also have seen before creating the pull request, in the ""Review changes"" section). Also there are a number of merge conflicts. You must solve this locally before creating the PR. You must rebase your commits on top of `master`, otherwise this is not going to work. Here's what I would recommend, provided that your actual commits are on a local branch called `latest-stable`:. ```bash. $ git branch -m latest-stable tmva-variable-plotter # Renames your local branch. $ git fetch origin master:master. $ git checkout tmva-variable-plotter. # Rebase your 'tmva-variable-plotter' branch onto 'master', provided that the fork point of your branch was 'origin/latest-stable'. # The default editor will open showing a number of 'pick' lines; just verify that those commits are part of your work. You may. # need to fix any merge conflict that might happen here. $ git rebase --interactive --onto master origin/latest-stable. # If you mess up at some point while rebasing, you can revert to the previous state issueing `$ git rebase --abort`. # Then push your changes to GitHub and open a new pull-request. $ git push ... ```. Afterward, you can open a new pull request with origin on `sazio:tmva-variable-plotter` targetting `root-project:master`. Other than that, I will close this pull-request, as it cannot be merged in its current state. Cheers,. Javier.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8698
https://github.com/root-project/root/pull/8698:202,interoperability,repositor,repository,202,"@sazio, as I said on the other PR, you originally based your local work on the `latest-stable` branch, which is only used by users to get the latest release, but not to check in new content into ROOT's repository. As you see in the change diff, there are lots of changes that are not meant to be there (that you should also have seen before creating the pull request, in the ""Review changes"" section). Also there are a number of merge conflicts. You must solve this locally before creating the PR. You must rebase your commits on top of `master`, otherwise this is not going to work. Here's what I would recommend, provided that your actual commits are on a local branch called `latest-stable`:. ```bash. $ git branch -m latest-stable tmva-variable-plotter # Renames your local branch. $ git fetch origin master:master. $ git checkout tmva-variable-plotter. # Rebase your 'tmva-variable-plotter' branch onto 'master', provided that the fork point of your branch was 'origin/latest-stable'. # The default editor will open showing a number of 'pick' lines; just verify that those commits are part of your work. You may. # need to fix any merge conflict that might happen here. $ git rebase --interactive --onto master origin/latest-stable. # If you mess up at some point while rebasing, you can revert to the previous state issueing `$ git rebase --abort`. # Then push your changes to GitHub and open a new pull-request. $ git push ... ```. Afterward, you can open a new pull request with origin on `sazio:tmva-variable-plotter` targetting `root-project:master`. Other than that, I will close this pull-request, as it cannot be merged in its current state. Cheers,. Javier.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8698
https://github.com/root-project/root/pull/8698:435,interoperability,conflict,conflicts,435,"@sazio, as I said on the other PR, you originally based your local work on the `latest-stable` branch, which is only used by users to get the latest release, but not to check in new content into ROOT's repository. As you see in the change diff, there are lots of changes that are not meant to be there (that you should also have seen before creating the pull request, in the ""Review changes"" section). Also there are a number of merge conflicts. You must solve this locally before creating the PR. You must rebase your commits on top of `master`, otherwise this is not going to work. Here's what I would recommend, provided that your actual commits are on a local branch called `latest-stable`:. ```bash. $ git branch -m latest-stable tmva-variable-plotter # Renames your local branch. $ git fetch origin master:master. $ git checkout tmva-variable-plotter. # Rebase your 'tmva-variable-plotter' branch onto 'master', provided that the fork point of your branch was 'origin/latest-stable'. # The default editor will open showing a number of 'pick' lines; just verify that those commits are part of your work. You may. # need to fix any merge conflict that might happen here. $ git rebase --interactive --onto master origin/latest-stable. # If you mess up at some point while rebasing, you can revert to the previous state issueing `$ git rebase --abort`. # Then push your changes to GitHub and open a new pull-request. $ git push ... ```. Afterward, you can open a new pull request with origin on `sazio:tmva-variable-plotter` targetting `root-project:master`. Other than that, I will close this pull-request, as it cannot be merged in its current state. Cheers,. Javier.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8698
https://github.com/root-project/root/pull/8698:1142,interoperability,conflict,conflict,1142,"@sazio, as I said on the other PR, you originally based your local work on the `latest-stable` branch, which is only used by users to get the latest release, but not to check in new content into ROOT's repository. As you see in the change diff, there are lots of changes that are not meant to be there (that you should also have seen before creating the pull request, in the ""Review changes"" section). Also there are a number of merge conflicts. You must solve this locally before creating the PR. You must rebase your commits on top of `master`, otherwise this is not going to work. Here's what I would recommend, provided that your actual commits are on a local branch called `latest-stable`:. ```bash. $ git branch -m latest-stable tmva-variable-plotter # Renames your local branch. $ git fetch origin master:master. $ git checkout tmva-variable-plotter. # Rebase your 'tmva-variable-plotter' branch onto 'master', provided that the fork point of your branch was 'origin/latest-stable'. # The default editor will open showing a number of 'pick' lines; just verify that those commits are part of your work. You may. # need to fix any merge conflict that might happen here. $ git rebase --interactive --onto master origin/latest-stable. # If you mess up at some point while rebasing, you can revert to the previous state issueing `$ git rebase --abort`. # Then push your changes to GitHub and open a new pull-request. $ git push ... ```. Afterward, you can open a new pull request with origin on `sazio:tmva-variable-plotter` targetting `root-project:master`. Other than that, I will close this pull-request, as it cannot be merged in its current state. Cheers,. Javier.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8698
https://github.com/root-project/root/pull/8698:740,modifiability,variab,variable-plotter,740,"@sazio, as I said on the other PR, you originally based your local work on the `latest-stable` branch, which is only used by users to get the latest release, but not to check in new content into ROOT's repository. As you see in the change diff, there are lots of changes that are not meant to be there (that you should also have seen before creating the pull request, in the ""Review changes"" section). Also there are a number of merge conflicts. You must solve this locally before creating the PR. You must rebase your commits on top of `master`, otherwise this is not going to work. Here's what I would recommend, provided that your actual commits are on a local branch called `latest-stable`:. ```bash. $ git branch -m latest-stable tmva-variable-plotter # Renames your local branch. $ git fetch origin master:master. $ git checkout tmva-variable-plotter. # Rebase your 'tmva-variable-plotter' branch onto 'master', provided that the fork point of your branch was 'origin/latest-stable'. # The default editor will open showing a number of 'pick' lines; just verify that those commits are part of your work. You may. # need to fix any merge conflict that might happen here. $ git rebase --interactive --onto master origin/latest-stable. # If you mess up at some point while rebasing, you can revert to the previous state issueing `$ git rebase --abort`. # Then push your changes to GitHub and open a new pull-request. $ git push ... ```. Afterward, you can open a new pull request with origin on `sazio:tmva-variable-plotter` targetting `root-project:master`. Other than that, I will close this pull-request, as it cannot be merged in its current state. Cheers,. Javier.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8698
https://github.com/root-project/root/pull/8698:840,modifiability,variab,variable-plotter,840,"@sazio, as I said on the other PR, you originally based your local work on the `latest-stable` branch, which is only used by users to get the latest release, but not to check in new content into ROOT's repository. As you see in the change diff, there are lots of changes that are not meant to be there (that you should also have seen before creating the pull request, in the ""Review changes"" section). Also there are a number of merge conflicts. You must solve this locally before creating the PR. You must rebase your commits on top of `master`, otherwise this is not going to work. Here's what I would recommend, provided that your actual commits are on a local branch called `latest-stable`:. ```bash. $ git branch -m latest-stable tmva-variable-plotter # Renames your local branch. $ git fetch origin master:master. $ git checkout tmva-variable-plotter. # Rebase your 'tmva-variable-plotter' branch onto 'master', provided that the fork point of your branch was 'origin/latest-stable'. # The default editor will open showing a number of 'pick' lines; just verify that those commits are part of your work. You may. # need to fix any merge conflict that might happen here. $ git rebase --interactive --onto master origin/latest-stable. # If you mess up at some point while rebasing, you can revert to the previous state issueing `$ git rebase --abort`. # Then push your changes to GitHub and open a new pull-request. $ git push ... ```. Afterward, you can open a new pull request with origin on `sazio:tmva-variable-plotter` targetting `root-project:master`. Other than that, I will close this pull-request, as it cannot be merged in its current state. Cheers,. Javier.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8698
https://github.com/root-project/root/pull/8698:878,modifiability,variab,variable-plotter,878,"@sazio, as I said on the other PR, you originally based your local work on the `latest-stable` branch, which is only used by users to get the latest release, but not to check in new content into ROOT's repository. As you see in the change diff, there are lots of changes that are not meant to be there (that you should also have seen before creating the pull request, in the ""Review changes"" section). Also there are a number of merge conflicts. You must solve this locally before creating the PR. You must rebase your commits on top of `master`, otherwise this is not going to work. Here's what I would recommend, provided that your actual commits are on a local branch called `latest-stable`:. ```bash. $ git branch -m latest-stable tmva-variable-plotter # Renames your local branch. $ git fetch origin master:master. $ git checkout tmva-variable-plotter. # Rebase your 'tmva-variable-plotter' branch onto 'master', provided that the fork point of your branch was 'origin/latest-stable'. # The default editor will open showing a number of 'pick' lines; just verify that those commits are part of your work. You may. # need to fix any merge conflict that might happen here. $ git rebase --interactive --onto master origin/latest-stable. # If you mess up at some point while rebasing, you can revert to the previous state issueing `$ git rebase --abort`. # Then push your changes to GitHub and open a new pull-request. $ git push ... ```. Afterward, you can open a new pull request with origin on `sazio:tmva-variable-plotter` targetting `root-project:master`. Other than that, I will close this pull-request, as it cannot be merged in its current state. Cheers,. Javier.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8698
https://github.com/root-project/root/pull/8698:1509,modifiability,variab,variable-plotter,1509,"@sazio, as I said on the other PR, you originally based your local work on the `latest-stable` branch, which is only used by users to get the latest release, but not to check in new content into ROOT's repository. As you see in the change diff, there are lots of changes that are not meant to be there (that you should also have seen before creating the pull request, in the ""Review changes"" section). Also there are a number of merge conflicts. You must solve this locally before creating the PR. You must rebase your commits on top of `master`, otherwise this is not going to work. Here's what I would recommend, provided that your actual commits are on a local branch called `latest-stable`:. ```bash. $ git branch -m latest-stable tmva-variable-plotter # Renames your local branch. $ git fetch origin master:master. $ git checkout tmva-variable-plotter. # Rebase your 'tmva-variable-plotter' branch onto 'master', provided that the fork point of your branch was 'origin/latest-stable'. # The default editor will open showing a number of 'pick' lines; just verify that those commits are part of your work. You may. # need to fix any merge conflict that might happen here. $ git rebase --interactive --onto master origin/latest-stable. # If you mess up at some point while rebasing, you can revert to the previous state issueing `$ git rebase --abort`. # Then push your changes to GitHub and open a new pull-request. $ git push ... ```. Afterward, you can open a new pull request with origin on `sazio:tmva-variable-plotter` targetting `root-project:master`. Other than that, I will close this pull-request, as it cannot be merged in its current state. Cheers,. Javier.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8698
https://github.com/root-project/root/pull/8698:182,performance,content,content,182,"@sazio, as I said on the other PR, you originally based your local work on the `latest-stable` branch, which is only used by users to get the latest release, but not to check in new content into ROOT's repository. As you see in the change diff, there are lots of changes that are not meant to be there (that you should also have seen before creating the pull request, in the ""Review changes"" section). Also there are a number of merge conflicts. You must solve this locally before creating the PR. You must rebase your commits on top of `master`, otherwise this is not going to work. Here's what I would recommend, provided that your actual commits are on a local branch called `latest-stable`:. ```bash. $ git branch -m latest-stable tmva-variable-plotter # Renames your local branch. $ git fetch origin master:master. $ git checkout tmva-variable-plotter. # Rebase your 'tmva-variable-plotter' branch onto 'master', provided that the fork point of your branch was 'origin/latest-stable'. # The default editor will open showing a number of 'pick' lines; just verify that those commits are part of your work. You may. # need to fix any merge conflict that might happen here. $ git rebase --interactive --onto master origin/latest-stable. # If you mess up at some point while rebasing, you can revert to the previous state issueing `$ git rebase --abort`. # Then push your changes to GitHub and open a new pull-request. $ git push ... ```. Afterward, you can open a new pull request with origin on `sazio:tmva-variable-plotter` targetting `root-project:master`. Other than that, I will close this pull-request, as it cannot be merged in its current state. Cheers,. Javier.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8698
https://github.com/root-project/root/pull/8698:376,safety,Review,Review,376,"@sazio, as I said on the other PR, you originally based your local work on the `latest-stable` branch, which is only used by users to get the latest release, but not to check in new content into ROOT's repository. As you see in the change diff, there are lots of changes that are not meant to be there (that you should also have seen before creating the pull request, in the ""Review changes"" section). Also there are a number of merge conflicts. You must solve this locally before creating the PR. You must rebase your commits on top of `master`, otherwise this is not going to work. Here's what I would recommend, provided that your actual commits are on a local branch called `latest-stable`:. ```bash. $ git branch -m latest-stable tmva-variable-plotter # Renames your local branch. $ git fetch origin master:master. $ git checkout tmva-variable-plotter. # Rebase your 'tmva-variable-plotter' branch onto 'master', provided that the fork point of your branch was 'origin/latest-stable'. # The default editor will open showing a number of 'pick' lines; just verify that those commits are part of your work. You may. # need to fix any merge conflict that might happen here. $ git rebase --interactive --onto master origin/latest-stable. # If you mess up at some point while rebasing, you can revert to the previous state issueing `$ git rebase --abort`. # Then push your changes to GitHub and open a new pull-request. $ git push ... ```. Afterward, you can open a new pull request with origin on `sazio:tmva-variable-plotter` targetting `root-project:master`. Other than that, I will close this pull-request, as it cannot be merged in its current state. Cheers,. Javier.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8698
https://github.com/root-project/root/pull/8698:376,testability,Review,Review,376,"@sazio, as I said on the other PR, you originally based your local work on the `latest-stable` branch, which is only used by users to get the latest release, but not to check in new content into ROOT's repository. As you see in the change diff, there are lots of changes that are not meant to be there (that you should also have seen before creating the pull request, in the ""Review changes"" section). Also there are a number of merge conflicts. You must solve this locally before creating the PR. You must rebase your commits on top of `master`, otherwise this is not going to work. Here's what I would recommend, provided that your actual commits are on a local branch called `latest-stable`:. ```bash. $ git branch -m latest-stable tmva-variable-plotter # Renames your local branch. $ git fetch origin master:master. $ git checkout tmva-variable-plotter. # Rebase your 'tmva-variable-plotter' branch onto 'master', provided that the fork point of your branch was 'origin/latest-stable'. # The default editor will open showing a number of 'pick' lines; just verify that those commits are part of your work. You may. # need to fix any merge conflict that might happen here. $ git rebase --interactive --onto master origin/latest-stable. # If you mess up at some point while rebasing, you can revert to the previous state issueing `$ git rebase --abort`. # Then push your changes to GitHub and open a new pull-request. $ git push ... ```. Afterward, you can open a new pull request with origin on `sazio:tmva-variable-plotter` targetting `root-project:master`. Other than that, I will close this pull-request, as it cannot be merged in its current state. Cheers,. Javier.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8698
https://github.com/root-project/root/pull/8698:1060,testability,verif,verify,1060,"@sazio, as I said on the other PR, you originally based your local work on the `latest-stable` branch, which is only used by users to get the latest release, but not to check in new content into ROOT's repository. As you see in the change diff, there are lots of changes that are not meant to be there (that you should also have seen before creating the pull request, in the ""Review changes"" section). Also there are a number of merge conflicts. You must solve this locally before creating the PR. You must rebase your commits on top of `master`, otherwise this is not going to work. Here's what I would recommend, provided that your actual commits are on a local branch called `latest-stable`:. ```bash. $ git branch -m latest-stable tmva-variable-plotter # Renames your local branch. $ git fetch origin master:master. $ git checkout tmva-variable-plotter. # Rebase your 'tmva-variable-plotter' branch onto 'master', provided that the fork point of your branch was 'origin/latest-stable'. # The default editor will open showing a number of 'pick' lines; just verify that those commits are part of your work. You may. # need to fix any merge conflict that might happen here. $ git rebase --interactive --onto master origin/latest-stable. # If you mess up at some point while rebasing, you can revert to the previous state issueing `$ git rebase --abort`. # Then push your changes to GitHub and open a new pull-request. $ git push ... ```. Afterward, you can open a new pull request with origin on `sazio:tmva-variable-plotter` targetting `root-project:master`. Other than that, I will close this pull-request, as it cannot be merged in its current state. Cheers,. Javier.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8698
https://github.com/root-project/root/pull/8698:125,usability,user,users,125,"@sazio, as I said on the other PR, you originally based your local work on the `latest-stable` branch, which is only used by users to get the latest release, but not to check in new content into ROOT's repository. As you see in the change diff, there are lots of changes that are not meant to be there (that you should also have seen before creating the pull request, in the ""Review changes"" section). Also there are a number of merge conflicts. You must solve this locally before creating the PR. You must rebase your commits on top of `master`, otherwise this is not going to work. Here's what I would recommend, provided that your actual commits are on a local branch called `latest-stable`:. ```bash. $ git branch -m latest-stable tmva-variable-plotter # Renames your local branch. $ git fetch origin master:master. $ git checkout tmva-variable-plotter. # Rebase your 'tmva-variable-plotter' branch onto 'master', provided that the fork point of your branch was 'origin/latest-stable'. # The default editor will open showing a number of 'pick' lines; just verify that those commits are part of your work. You may. # need to fix any merge conflict that might happen here. $ git rebase --interactive --onto master origin/latest-stable. # If you mess up at some point while rebasing, you can revert to the previous state issueing `$ git rebase --abort`. # Then push your changes to GitHub and open a new pull-request. $ git push ... ```. Afterward, you can open a new pull request with origin on `sazio:tmva-variable-plotter` targetting `root-project:master`. Other than that, I will close this pull-request, as it cannot be merged in its current state. Cheers,. Javier.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8698
https://github.com/root-project/root/pull/8698:1190,usability,interact,interactive,1190,"@sazio, as I said on the other PR, you originally based your local work on the `latest-stable` branch, which is only used by users to get the latest release, but not to check in new content into ROOT's repository. As you see in the change diff, there are lots of changes that are not meant to be there (that you should also have seen before creating the pull request, in the ""Review changes"" section). Also there are a number of merge conflicts. You must solve this locally before creating the PR. You must rebase your commits on top of `master`, otherwise this is not going to work. Here's what I would recommend, provided that your actual commits are on a local branch called `latest-stable`:. ```bash. $ git branch -m latest-stable tmva-variable-plotter # Renames your local branch. $ git fetch origin master:master. $ git checkout tmva-variable-plotter. # Rebase your 'tmva-variable-plotter' branch onto 'master', provided that the fork point of your branch was 'origin/latest-stable'. # The default editor will open showing a number of 'pick' lines; just verify that those commits are part of your work. You may. # need to fix any merge conflict that might happen here. $ git rebase --interactive --onto master origin/latest-stable. # If you mess up at some point while rebasing, you can revert to the previous state issueing `$ git rebase --abort`. # Then push your changes to GitHub and open a new pull-request. $ git push ... ```. Afterward, you can open a new pull request with origin on `sazio:tmva-variable-plotter` targetting `root-project:master`. Other than that, I will close this pull-request, as it cannot be merged in its current state. Cheers,. Javier.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8698
https://github.com/root-project/root/pull/8698:1585,usability,close,close,1585,"@sazio, as I said on the other PR, you originally based your local work on the `latest-stable` branch, which is only used by users to get the latest release, but not to check in new content into ROOT's repository. As you see in the change diff, there are lots of changes that are not meant to be there (that you should also have seen before creating the pull request, in the ""Review changes"" section). Also there are a number of merge conflicts. You must solve this locally before creating the PR. You must rebase your commits on top of `master`, otherwise this is not going to work. Here's what I would recommend, provided that your actual commits are on a local branch called `latest-stable`:. ```bash. $ git branch -m latest-stable tmva-variable-plotter # Renames your local branch. $ git fetch origin master:master. $ git checkout tmva-variable-plotter. # Rebase your 'tmva-variable-plotter' branch onto 'master', provided that the fork point of your branch was 'origin/latest-stable'. # The default editor will open showing a number of 'pick' lines; just verify that those commits are part of your work. You may. # need to fix any merge conflict that might happen here. $ git rebase --interactive --onto master origin/latest-stable. # If you mess up at some point while rebasing, you can revert to the previous state issueing `$ git rebase --abort`. # Then push your changes to GitHub and open a new pull-request. $ git push ... ```. Afterward, you can open a new pull request with origin on `sazio:tmva-variable-plotter` targetting `root-project:master`. Other than that, I will close this pull-request, as it cannot be merged in its current state. Cheers,. Javier.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8698
https://github.com/root-project/root/pull/8698:110,interoperability,conflict,conflicts,110,"Thanks @jalopezg-r00t ! I'm sorry for that, now I have rebased my branch onto master and locally solved merge conflicts",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8698
https://github.com/root-project/root/pull/8699:769,availability,servic,service,769,"<img src=""https://www.deepcode.ai/icons/green_check.svg"" width= ""50px"" align= ""left""/> Congratulations :tada:. DeepCode [analyzed](https://www.deepcode.ai/app/gh/root-project/root/6de8aef9e97adb926e33961825f8aa90de66e4ed/root-project/root/15bb785ee62b71d6e671c7a9c12897fc4aac89bf/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) your code in 3.946 seconds and we found no issues. Enjoy a moment of no bugs :sunny:. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/6de8aef9e97adb926e33961825f8aa90de66e4ed/root-project/root/15bb785ee62b71d6e671c7a9c12897fc4aac89bf/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8699
https://github.com/root-project/root/pull/8699:769,deployability,servic,service,769,"<img src=""https://www.deepcode.ai/icons/green_check.svg"" width= ""50px"" align= ""left""/> Congratulations :tada:. DeepCode [analyzed](https://www.deepcode.ai/app/gh/root-project/root/6de8aef9e97adb926e33961825f8aa90de66e4ed/root-project/root/15bb785ee62b71d6e671c7a9c12897fc4aac89bf/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) your code in 3.946 seconds and we found no issues. Enjoy a moment of no bugs :sunny:. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/6de8aef9e97adb926e33961825f8aa90de66e4ed/root-project/root/15bb785ee62b71d6e671c7a9c12897fc4aac89bf/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8699
https://github.com/root-project/root/pull/8699:781,deployability,API,API,781,"<img src=""https://www.deepcode.ai/icons/green_check.svg"" width= ""50px"" align= ""left""/> Congratulations :tada:. DeepCode [analyzed](https://www.deepcode.ai/app/gh/root-project/root/6de8aef9e97adb926e33961825f8aa90de66e4ed/root-project/root/15bb785ee62b71d6e671c7a9c12897fc4aac89bf/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) your code in 3.946 seconds and we found no issues. Enjoy a moment of no bugs :sunny:. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/6de8aef9e97adb926e33961825f8aa90de66e4ed/root-project/root/15bb785ee62b71d6e671c7a9c12897fc4aac89bf/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8699
https://github.com/root-project/root/pull/8699:769,integrability,servic,service,769,"<img src=""https://www.deepcode.ai/icons/green_check.svg"" width= ""50px"" align= ""left""/> Congratulations :tada:. DeepCode [analyzed](https://www.deepcode.ai/app/gh/root-project/root/6de8aef9e97adb926e33961825f8aa90de66e4ed/root-project/root/15bb785ee62b71d6e671c7a9c12897fc4aac89bf/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) your code in 3.946 seconds and we found no issues. Enjoy a moment of no bugs :sunny:. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/6de8aef9e97adb926e33961825f8aa90de66e4ed/root-project/root/15bb785ee62b71d6e671c7a9c12897fc4aac89bf/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8699
https://github.com/root-project/root/pull/8699:781,integrability,API,API,781,"<img src=""https://www.deepcode.ai/icons/green_check.svg"" width= ""50px"" align= ""left""/> Congratulations :tada:. DeepCode [analyzed](https://www.deepcode.ai/app/gh/root-project/root/6de8aef9e97adb926e33961825f8aa90de66e4ed/root-project/root/15bb785ee62b71d6e671c7a9c12897fc4aac89bf/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) your code in 3.946 seconds and we found no issues. Enjoy a moment of no bugs :sunny:. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/6de8aef9e97adb926e33961825f8aa90de66e4ed/root-project/root/15bb785ee62b71d6e671c7a9c12897fc4aac89bf/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8699
https://github.com/root-project/root/pull/8699:781,interoperability,API,API,781,"<img src=""https://www.deepcode.ai/icons/green_check.svg"" width= ""50px"" align= ""left""/> Congratulations :tada:. DeepCode [analyzed](https://www.deepcode.ai/app/gh/root-project/root/6de8aef9e97adb926e33961825f8aa90de66e4ed/root-project/root/15bb785ee62b71d6e671c7a9c12897fc4aac89bf/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) your code in 3.946 seconds and we found no issues. Enjoy a moment of no bugs :sunny:. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/6de8aef9e97adb926e33961825f8aa90de66e4ed/root-project/root/15bb785ee62b71d6e671c7a9c12897fc4aac89bf/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8699
https://github.com/root-project/root/pull/8699:971,interoperability,plug,plugins,971,"<img src=""https://www.deepcode.ai/icons/green_check.svg"" width= ""50px"" align= ""left""/> Congratulations :tada:. DeepCode [analyzed](https://www.deepcode.ai/app/gh/root-project/root/6de8aef9e97adb926e33961825f8aa90de66e4ed/root-project/root/15bb785ee62b71d6e671c7a9c12897fc4aac89bf/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) your code in 3.946 seconds and we found no issues. Enjoy a moment of no bugs :sunny:. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/6de8aef9e97adb926e33961825f8aa90de66e4ed/root-project/root/15bb785ee62b71d6e671c7a9c12897fc4aac89bf/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8699
https://github.com/root-project/root/pull/8699:1043,interoperability,plug,plugin,1043,"<img src=""https://www.deepcode.ai/icons/green_check.svg"" width= ""50px"" align= ""left""/> Congratulations :tada:. DeepCode [analyzed](https://www.deepcode.ai/app/gh/root-project/root/6de8aef9e97adb926e33961825f8aa90de66e4ed/root-project/root/15bb785ee62b71d6e671c7a9c12897fc4aac89bf/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) your code in 3.946 seconds and we found no issues. Enjoy a moment of no bugs :sunny:. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/6de8aef9e97adb926e33961825f8aa90de66e4ed/root-project/root/15bb785ee62b71d6e671c7a9c12897fc4aac89bf/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8699
https://github.com/root-project/root/pull/8699:1094,interoperability,plug,plugin,1094,"<img src=""https://www.deepcode.ai/icons/green_check.svg"" width= ""50px"" align= ""left""/> Congratulations :tada:. DeepCode [analyzed](https://www.deepcode.ai/app/gh/root-project/root/6de8aef9e97adb926e33961825f8aa90de66e4ed/root-project/root/15bb785ee62b71d6e671c7a9c12897fc4aac89bf/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) your code in 3.946 seconds and we found no issues. Enjoy a moment of no bugs :sunny:. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/6de8aef9e97adb926e33961825f8aa90de66e4ed/root-project/root/15bb785ee62b71d6e671c7a9c12897fc4aac89bf/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8699
https://github.com/root-project/root/pull/8699:769,modifiability,servic,service,769,"<img src=""https://www.deepcode.ai/icons/green_check.svg"" width= ""50px"" align= ""left""/> Congratulations :tada:. DeepCode [analyzed](https://www.deepcode.ai/app/gh/root-project/root/6de8aef9e97adb926e33961825f8aa90de66e4ed/root-project/root/15bb785ee62b71d6e671c7a9c12897fc4aac89bf/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) your code in 3.946 seconds and we found no issues. Enjoy a moment of no bugs :sunny:. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/6de8aef9e97adb926e33961825f8aa90de66e4ed/root-project/root/15bb785ee62b71d6e671c7a9c12897fc4aac89bf/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8699
https://github.com/root-project/root/pull/8699:41,reliability,doe,doesnt,41,I think the problem is that hsimple.root doesnt exist without hsimple.C being run first. I aborted the jenkins.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8699
https://github.com/root-project/root/pull/8699:31,deployability,build,build,31,"It gets created as part of the build: it does exist, in the `tutorials/` subdir of the builddir. [As I said](https://github.com/root-project/root/pull/8699#discussion_r673224852), `gROOT->GetTutoriaalDir()` should help.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8699
https://github.com/root-project/root/pull/8699:87,deployability,build,builddir,87,"It gets created as part of the build: it does exist, in the `tutorials/` subdir of the builddir. [As I said](https://github.com/root-project/root/pull/8699#discussion_r673224852), `gROOT->GetTutoriaalDir()` should help.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8699
https://github.com/root-project/root/pull/8699:73,integrability,sub,subdir,73,"It gets created as part of the build: it does exist, in the `tutorials/` subdir of the builddir. [As I said](https://github.com/root-project/root/pull/8699#discussion_r673224852), `gROOT->GetTutoriaalDir()` should help.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8699
https://github.com/root-project/root/pull/8699:41,reliability,doe,does,41,"It gets created as part of the build: it does exist, in the `tutorials/` subdir of the builddir. [As I said](https://github.com/root-project/root/pull/8699#discussion_r673224852), `gROOT->GetTutoriaalDir()` should help.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8699
https://github.com/root-project/root/pull/8699:214,usability,help,help,214,"It gets created as part of the build: it does exist, in the `tutorials/` subdir of the builddir. [As I said](https://github.com/root-project/root/pull/8699#discussion_r673224852), `gROOT->GetTutoriaalDir()` should help.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8699
https://github.com/root-project/root/pull/8699:33,deployability,build,build,33,"> It gets created as part of the build: it does exist, in the `tutorials/` subdir of the builddir. [As I said](https://github.com/root-project/root/pull/8699#discussion_r673224852), `gROOT->GetTutoriaalDir()` should help. Ahh, ok",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8699
https://github.com/root-project/root/pull/8699:89,deployability,build,builddir,89,"> It gets created as part of the build: it does exist, in the `tutorials/` subdir of the builddir. [As I said](https://github.com/root-project/root/pull/8699#discussion_r673224852), `gROOT->GetTutoriaalDir()` should help. Ahh, ok",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8699
https://github.com/root-project/root/pull/8699:75,integrability,sub,subdir,75,"> It gets created as part of the build: it does exist, in the `tutorials/` subdir of the builddir. [As I said](https://github.com/root-project/root/pull/8699#discussion_r673224852), `gROOT->GetTutoriaalDir()` should help. Ahh, ok",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8699
https://github.com/root-project/root/pull/8699:43,reliability,doe,does,43,"> It gets created as part of the build: it does exist, in the `tutorials/` subdir of the builddir. [As I said](https://github.com/root-project/root/pull/8699#discussion_r673224852), `gROOT->GetTutoriaalDir()` should help. Ahh, ok",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8699
https://github.com/root-project/root/pull/8699:216,usability,help,help,216,"> It gets created as part of the build: it does exist, in the `tutorials/` subdir of the builddir. [As I said](https://github.com/root-project/root/pull/8699#discussion_r673224852), `gROOT->GetTutoriaalDir()` should help. Ahh, ok",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8699
https://github.com/root-project/root/pull/8699:76,deployability,version,version,76,"@Axel-Naumann, @couet pointed out that my read file was basically a simpler version of h1draw.C and that it would be better to just clarify some things in that file.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8699
https://github.com/root-project/root/pull/8699:76,integrability,version,version,76,"@Axel-Naumann, @couet pointed out that my read file was basically a simpler version of h1draw.C and that it would be better to just clarify some things in that file.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8699
https://github.com/root-project/root/pull/8699:76,modifiability,version,version,76,"@Axel-Naumann, @couet pointed out that my read file was basically a simpler version of h1draw.C and that it would be better to just clarify some things in that file.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8699
https://github.com/root-project/root/pull/8699:68,testability,simpl,simpler,68,"@Axel-Naumann, @couet pointed out that my read file was basically a simpler version of h1draw.C and that it would be better to just clarify some things in that file.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8699
https://github.com/root-project/root/pull/8699:68,usability,simpl,simpler,68,"@Axel-Naumann, @couet pointed out that my read file was basically a simpler version of h1draw.C and that it would be better to just clarify some things in that file.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8699
https://github.com/root-project/root/pull/8699:10,reliability,doe,doesnt,10,"This test doesnt seem related to the PR, right?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8699
https://github.com/root-project/root/pull/8699:5,safety,test,test,5,"This test doesnt seem related to the PR, right?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8699
https://github.com/root-project/root/pull/8699:5,testability,test,test,5,"This test doesnt seem related to the PR, right?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8699
https://github.com/root-project/root/pull/8699:12,availability,failur,failure,12,"Indeed, the failure is unrelated (and known).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8699
https://github.com/root-project/root/pull/8699:12,deployability,fail,failure,12,"Indeed, the failure is unrelated (and known).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8699
https://github.com/root-project/root/pull/8699:12,performance,failur,failure,12,"Indeed, the failure is unrelated (and known).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8699
https://github.com/root-project/root/pull/8699:12,reliability,fail,failure,12,"Indeed, the failure is unrelated (and known).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8699
https://github.com/root-project/root/pull/8699:132,deployability,depend,depends,132,"The filename is referred to here:. ```. tutorials/CMakeLists.txt: tutorial-hist-h1draw. tutorials/CMakeLists.txt: set(pyroot-h1draw-depends tutorial-pyroot-hsimple-py). tutorials/CMakeLists.txt: tutorial-pyroot-h1draw-py. tutorials/demos.C: bar->AddButton(""h1draw"", "".x hist/h1draw.C"", ""Drawing Options for 1D Histograms"");. tutorials/legacy/benchmarks.C: summary->AddText("" hist/h1draw.C"");. tutorials/legacy/benchmarks.C: bexec(dir,""hist/h1draw.C"");. tutorials/legacy/pyroot/benchmarks.py: 'fillrandom.py','fit1.py', 'h1draw.py', 'graph.py',. tutorials/pyroot/demo.py:bar.AddButton( 'h1draw', r'TPython::Exec( ""' + to_run.format('h1draw.py') + '"" );', 'Drawing Options for 1D Histograms' ). ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8699
https://github.com/root-project/root/pull/8699:287,energy efficiency,Draw,Drawing,287,"The filename is referred to here:. ```. tutorials/CMakeLists.txt: tutorial-hist-h1draw. tutorials/CMakeLists.txt: set(pyroot-h1draw-depends tutorial-pyroot-hsimple-py). tutorials/CMakeLists.txt: tutorial-pyroot-h1draw-py. tutorials/demos.C: bar->AddButton(""h1draw"", "".x hist/h1draw.C"", ""Drawing Options for 1D Histograms"");. tutorials/legacy/benchmarks.C: summary->AddText("" hist/h1draw.C"");. tutorials/legacy/benchmarks.C: bexec(dir,""hist/h1draw.C"");. tutorials/legacy/pyroot/benchmarks.py: 'fillrandom.py','fit1.py', 'h1draw.py', 'graph.py',. tutorials/pyroot/demo.py:bar.AddButton( 'h1draw', r'TPython::Exec( ""' + to_run.format('h1draw.py') + '"" );', 'Drawing Options for 1D Histograms' ). ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8699
https://github.com/root-project/root/pull/8699:655,energy efficiency,Draw,Drawing,655,"The filename is referred to here:. ```. tutorials/CMakeLists.txt: tutorial-hist-h1draw. tutorials/CMakeLists.txt: set(pyroot-h1draw-depends tutorial-pyroot-hsimple-py). tutorials/CMakeLists.txt: tutorial-pyroot-h1draw-py. tutorials/demos.C: bar->AddButton(""h1draw"", "".x hist/h1draw.C"", ""Drawing Options for 1D Histograms"");. tutorials/legacy/benchmarks.C: summary->AddText("" hist/h1draw.C"");. tutorials/legacy/benchmarks.C: bexec(dir,""hist/h1draw.C"");. tutorials/legacy/pyroot/benchmarks.py: 'fillrandom.py','fit1.py', 'h1draw.py', 'graph.py',. tutorials/pyroot/demo.py:bar.AddButton( 'h1draw', r'TPython::Exec( ""' + to_run.format('h1draw.py') + '"" );', 'Drawing Options for 1D Histograms' ). ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8699
https://github.com/root-project/root/pull/8699:132,integrability,depend,depends,132,"The filename is referred to here:. ```. tutorials/CMakeLists.txt: tutorial-hist-h1draw. tutorials/CMakeLists.txt: set(pyroot-h1draw-depends tutorial-pyroot-hsimple-py). tutorials/CMakeLists.txt: tutorial-pyroot-h1draw-py. tutorials/demos.C: bar->AddButton(""h1draw"", "".x hist/h1draw.C"", ""Drawing Options for 1D Histograms"");. tutorials/legacy/benchmarks.C: summary->AddText("" hist/h1draw.C"");. tutorials/legacy/benchmarks.C: bexec(dir,""hist/h1draw.C"");. tutorials/legacy/pyroot/benchmarks.py: 'fillrandom.py','fit1.py', 'h1draw.py', 'graph.py',. tutorials/pyroot/demo.py:bar.AddButton( 'h1draw', r'TPython::Exec( ""' + to_run.format('h1draw.py') + '"" );', 'Drawing Options for 1D Histograms' ). ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8699
https://github.com/root-project/root/pull/8699:624,interoperability,format,format,624,"The filename is referred to here:. ```. tutorials/CMakeLists.txt: tutorial-hist-h1draw. tutorials/CMakeLists.txt: set(pyroot-h1draw-depends tutorial-pyroot-hsimple-py). tutorials/CMakeLists.txt: tutorial-pyroot-h1draw-py. tutorials/demos.C: bar->AddButton(""h1draw"", "".x hist/h1draw.C"", ""Drawing Options for 1D Histograms"");. tutorials/legacy/benchmarks.C: summary->AddText("" hist/h1draw.C"");. tutorials/legacy/benchmarks.C: bexec(dir,""hist/h1draw.C"");. tutorials/legacy/pyroot/benchmarks.py: 'fillrandom.py','fit1.py', 'h1draw.py', 'graph.py',. tutorials/pyroot/demo.py:bar.AddButton( 'h1draw', r'TPython::Exec( ""' + to_run.format('h1draw.py') + '"" );', 'Drawing Options for 1D Histograms' ). ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8699
https://github.com/root-project/root/pull/8699:132,modifiability,depend,depends,132,"The filename is referred to here:. ```. tutorials/CMakeLists.txt: tutorial-hist-h1draw. tutorials/CMakeLists.txt: set(pyroot-h1draw-depends tutorial-pyroot-hsimple-py). tutorials/CMakeLists.txt: tutorial-pyroot-h1draw-py. tutorials/demos.C: bar->AddButton(""h1draw"", "".x hist/h1draw.C"", ""Drawing Options for 1D Histograms"");. tutorials/legacy/benchmarks.C: summary->AddText("" hist/h1draw.C"");. tutorials/legacy/benchmarks.C: bexec(dir,""hist/h1draw.C"");. tutorials/legacy/pyroot/benchmarks.py: 'fillrandom.py','fit1.py', 'h1draw.py', 'graph.py',. tutorials/pyroot/demo.py:bar.AddButton( 'h1draw', r'TPython::Exec( ""' + to_run.format('h1draw.py') + '"" );', 'Drawing Options for 1D Histograms' ). ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8699
https://github.com/root-project/root/pull/8699:132,safety,depend,depends,132,"The filename is referred to here:. ```. tutorials/CMakeLists.txt: tutorial-hist-h1draw. tutorials/CMakeLists.txt: set(pyroot-h1draw-depends tutorial-pyroot-hsimple-py). tutorials/CMakeLists.txt: tutorial-pyroot-h1draw-py. tutorials/demos.C: bar->AddButton(""h1draw"", "".x hist/h1draw.C"", ""Drawing Options for 1D Histograms"");. tutorials/legacy/benchmarks.C: summary->AddText("" hist/h1draw.C"");. tutorials/legacy/benchmarks.C: bexec(dir,""hist/h1draw.C"");. tutorials/legacy/pyroot/benchmarks.py: 'fillrandom.py','fit1.py', 'h1draw.py', 'graph.py',. tutorials/pyroot/demo.py:bar.AddButton( 'h1draw', r'TPython::Exec( ""' + to_run.format('h1draw.py') + '"" );', 'Drawing Options for 1D Histograms' ). ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8699
https://github.com/root-project/root/pull/8699:132,testability,depend,depends,132,"The filename is referred to here:. ```. tutorials/CMakeLists.txt: tutorial-hist-h1draw. tutorials/CMakeLists.txt: set(pyroot-h1draw-depends tutorial-pyroot-hsimple-py). tutorials/CMakeLists.txt: tutorial-pyroot-h1draw-py. tutorials/demos.C: bar->AddButton(""h1draw"", "".x hist/h1draw.C"", ""Drawing Options for 1D Histograms"");. tutorials/legacy/benchmarks.C: summary->AddText("" hist/h1draw.C"");. tutorials/legacy/benchmarks.C: bexec(dir,""hist/h1draw.C"");. tutorials/legacy/pyroot/benchmarks.py: 'fillrandom.py','fit1.py', 'h1draw.py', 'graph.py',. tutorials/pyroot/demo.py:bar.AddButton( 'h1draw', r'TPython::Exec( ""' + to_run.format('h1draw.py') + '"" );', 'Drawing Options for 1D Histograms' ). ```.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8699
https://github.com/root-project/root/pull/8700:532,deployability,integr,integrate,532,"@hageboeck thanks for another Herculean reviewing effort :) Your comments make total sense; indeed, all the copy-pasted stuff still has to be merged with all the modernization and optimization work that was done in the past two years. I will go through as much of your suggestions as I can before I will be on leave after tomorrow for three weeks. After that, if anyone else has time to work on some of the issues, I'd of course welcome the help. As you know, my time on the project is running out, so I probably won't have time to integrate everything. For instance, Manos' mini computation library I have only learned about in the last few months and don't know it in enough detail to make any kind of sensible estimate of how to do it, let alone how much time this would take. Let's discuss detailed plans in our meeting at 14:00 today.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:180,energy efficiency,optim,optimization,180,"@hageboeck thanks for another Herculean reviewing effort :) Your comments make total sense; indeed, all the copy-pasted stuff still has to be merged with all the modernization and optimization work that was done in the past two years. I will go through as much of your suggestions as I can before I will be on leave after tomorrow for three weeks. After that, if anyone else has time to work on some of the issues, I'd of course welcome the help. As you know, my time on the project is running out, so I probably won't have time to integrate everything. For instance, Manos' mini computation library I have only learned about in the last few months and don't know it in enough detail to make any kind of sensible estimate of how to do it, let alone how much time this would take. Let's discuss detailed plans in our meeting at 14:00 today.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:713,energy efficiency,estimat,estimate,713,"@hageboeck thanks for another Herculean reviewing effort :) Your comments make total sense; indeed, all the copy-pasted stuff still has to be merged with all the modernization and optimization work that was done in the past two years. I will go through as much of your suggestions as I can before I will be on leave after tomorrow for three weeks. After that, if anyone else has time to work on some of the issues, I'd of course welcome the help. As you know, my time on the project is running out, so I probably won't have time to integrate everything. For instance, Manos' mini computation library I have only learned about in the last few months and don't know it in enough detail to make any kind of sensible estimate of how to do it, let alone how much time this would take. Let's discuss detailed plans in our meeting at 14:00 today.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:532,integrability,integr,integrate,532,"@hageboeck thanks for another Herculean reviewing effort :) Your comments make total sense; indeed, all the copy-pasted stuff still has to be merged with all the modernization and optimization work that was done in the past two years. I will go through as much of your suggestions as I can before I will be on leave after tomorrow for three weeks. After that, if anyone else has time to work on some of the issues, I'd of course welcome the help. As you know, my time on the project is running out, so I probably won't have time to integrate everything. For instance, Manos' mini computation library I have only learned about in the last few months and don't know it in enough detail to make any kind of sensible estimate of how to do it, let alone how much time this would take. Let's discuss detailed plans in our meeting at 14:00 today.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:532,interoperability,integr,integrate,532,"@hageboeck thanks for another Herculean reviewing effort :) Your comments make total sense; indeed, all the copy-pasted stuff still has to be merged with all the modernization and optimization work that was done in the past two years. I will go through as much of your suggestions as I can before I will be on leave after tomorrow for three weeks. After that, if anyone else has time to work on some of the issues, I'd of course welcome the help. As you know, my time on the project is running out, so I probably won't have time to integrate everything. For instance, Manos' mini computation library I have only learned about in the last few months and don't know it in enough detail to make any kind of sensible estimate of how to do it, let alone how much time this would take. Let's discuss detailed plans in our meeting at 14:00 today.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:532,modifiability,integr,integrate,532,"@hageboeck thanks for another Herculean reviewing effort :) Your comments make total sense; indeed, all the copy-pasted stuff still has to be merged with all the modernization and optimization work that was done in the past two years. I will go through as much of your suggestions as I can before I will be on leave after tomorrow for three weeks. After that, if anyone else has time to work on some of the issues, I'd of course welcome the help. As you know, my time on the project is running out, so I probably won't have time to integrate everything. For instance, Manos' mini computation library I have only learned about in the last few months and don't know it in enough detail to make any kind of sensible estimate of how to do it, let alone how much time this would take. Let's discuss detailed plans in our meeting at 14:00 today.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:180,performance,optimiz,optimization,180,"@hageboeck thanks for another Herculean reviewing effort :) Your comments make total sense; indeed, all the copy-pasted stuff still has to be merged with all the modernization and optimization work that was done in the past two years. I will go through as much of your suggestions as I can before I will be on leave after tomorrow for three weeks. After that, if anyone else has time to work on some of the issues, I'd of course welcome the help. As you know, my time on the project is running out, so I probably won't have time to integrate everything. For instance, Manos' mini computation library I have only learned about in the last few months and don't know it in enough detail to make any kind of sensible estimate of how to do it, let alone how much time this would take. Let's discuss detailed plans in our meeting at 14:00 today.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:379,performance,time,time,379,"@hageboeck thanks for another Herculean reviewing effort :) Your comments make total sense; indeed, all the copy-pasted stuff still has to be merged with all the modernization and optimization work that was done in the past two years. I will go through as much of your suggestions as I can before I will be on leave after tomorrow for three weeks. After that, if anyone else has time to work on some of the issues, I'd of course welcome the help. As you know, my time on the project is running out, so I probably won't have time to integrate everything. For instance, Manos' mini computation library I have only learned about in the last few months and don't know it in enough detail to make any kind of sensible estimate of how to do it, let alone how much time this would take. Let's discuss detailed plans in our meeting at 14:00 today.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:463,performance,time,time,463,"@hageboeck thanks for another Herculean reviewing effort :) Your comments make total sense; indeed, all the copy-pasted stuff still has to be merged with all the modernization and optimization work that was done in the past two years. I will go through as much of your suggestions as I can before I will be on leave after tomorrow for three weeks. After that, if anyone else has time to work on some of the issues, I'd of course welcome the help. As you know, my time on the project is running out, so I probably won't have time to integrate everything. For instance, Manos' mini computation library I have only learned about in the last few months and don't know it in enough detail to make any kind of sensible estimate of how to do it, let alone how much time this would take. Let's discuss detailed plans in our meeting at 14:00 today.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:524,performance,time,time,524,"@hageboeck thanks for another Herculean reviewing effort :) Your comments make total sense; indeed, all the copy-pasted stuff still has to be merged with all the modernization and optimization work that was done in the past two years. I will go through as much of your suggestions as I can before I will be on leave after tomorrow for three weeks. After that, if anyone else has time to work on some of the issues, I'd of course welcome the help. As you know, my time on the project is running out, so I probably won't have time to integrate everything. For instance, Manos' mini computation library I have only learned about in the last few months and don't know it in enough detail to make any kind of sensible estimate of how to do it, let alone how much time this would take. Let's discuss detailed plans in our meeting at 14:00 today.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:758,performance,time,time,758,"@hageboeck thanks for another Herculean reviewing effort :) Your comments make total sense; indeed, all the copy-pasted stuff still has to be merged with all the modernization and optimization work that was done in the past two years. I will go through as much of your suggestions as I can before I will be on leave after tomorrow for three weeks. After that, if anyone else has time to work on some of the issues, I'd of course welcome the help. As you know, my time on the project is running out, so I probably won't have time to integrate everything. For instance, Manos' mini computation library I have only learned about in the last few months and don't know it in enough detail to make any kind of sensible estimate of how to do it, let alone how much time this would take. Let's discuss detailed plans in our meeting at 14:00 today.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:532,reliability,integr,integrate,532,"@hageboeck thanks for another Herculean reviewing effort :) Your comments make total sense; indeed, all the copy-pasted stuff still has to be merged with all the modernization and optimization work that was done in the past two years. I will go through as much of your suggestions as I can before I will be on leave after tomorrow for three weeks. After that, if anyone else has time to work on some of the issues, I'd of course welcome the help. As you know, my time on the project is running out, so I probably won't have time to integrate everything. For instance, Manos' mini computation library I have only learned about in the last few months and don't know it in enough detail to make any kind of sensible estimate of how to do it, let alone how much time this would take. Let's discuss detailed plans in our meeting at 14:00 today.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:40,safety,review,reviewing,40,"@hageboeck thanks for another Herculean reviewing effort :) Your comments make total sense; indeed, all the copy-pasted stuff still has to be merged with all the modernization and optimization work that was done in the past two years. I will go through as much of your suggestions as I can before I will be on leave after tomorrow for three weeks. After that, if anyone else has time to work on some of the issues, I'd of course welcome the help. As you know, my time on the project is running out, so I probably won't have time to integrate everything. For instance, Manos' mini computation library I have only learned about in the last few months and don't know it in enough detail to make any kind of sensible estimate of how to do it, let alone how much time this would take. Let's discuss detailed plans in our meeting at 14:00 today.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:532,security,integr,integrate,532,"@hageboeck thanks for another Herculean reviewing effort :) Your comments make total sense; indeed, all the copy-pasted stuff still has to be merged with all the modernization and optimization work that was done in the past two years. I will go through as much of your suggestions as I can before I will be on leave after tomorrow for three weeks. After that, if anyone else has time to work on some of the issues, I'd of course welcome the help. As you know, my time on the project is running out, so I probably won't have time to integrate everything. For instance, Manos' mini computation library I have only learned about in the last few months and don't know it in enough detail to make any kind of sensible estimate of how to do it, let alone how much time this would take. Let's discuss detailed plans in our meeting at 14:00 today.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:40,testability,review,reviewing,40,"@hageboeck thanks for another Herculean reviewing effort :) Your comments make total sense; indeed, all the copy-pasted stuff still has to be merged with all the modernization and optimization work that was done in the past two years. I will go through as much of your suggestions as I can before I will be on leave after tomorrow for three weeks. After that, if anyone else has time to work on some of the issues, I'd of course welcome the help. As you know, my time on the project is running out, so I probably won't have time to integrate everything. For instance, Manos' mini computation library I have only learned about in the last few months and don't know it in enough detail to make any kind of sensible estimate of how to do it, let alone how much time this would take. Let's discuss detailed plans in our meeting at 14:00 today.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:532,testability,integr,integrate,532,"@hageboeck thanks for another Herculean reviewing effort :) Your comments make total sense; indeed, all the copy-pasted stuff still has to be merged with all the modernization and optimization work that was done in the past two years. I will go through as much of your suggestions as I can before I will be on leave after tomorrow for three weeks. After that, if anyone else has time to work on some of the issues, I'd of course welcome the help. As you know, my time on the project is running out, so I probably won't have time to integrate everything. For instance, Manos' mini computation library I have only learned about in the last few months and don't know it in enough detail to make any kind of sensible estimate of how to do it, let alone how much time this would take. Let's discuss detailed plans in our meeting at 14:00 today.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:803,testability,plan,plans,803,"@hageboeck thanks for another Herculean reviewing effort :) Your comments make total sense; indeed, all the copy-pasted stuff still has to be merged with all the modernization and optimization work that was done in the past two years. I will go through as much of your suggestions as I can before I will be on leave after tomorrow for three weeks. After that, if anyone else has time to work on some of the issues, I'd of course welcome the help. As you know, my time on the project is running out, so I probably won't have time to integrate everything. For instance, Manos' mini computation library I have only learned about in the last few months and don't know it in enough detail to make any kind of sensible estimate of how to do it, let alone how much time this would take. Let's discuss detailed plans in our meeting at 14:00 today.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:441,usability,help,help,441,"@hageboeck thanks for another Herculean reviewing effort :) Your comments make total sense; indeed, all the copy-pasted stuff still has to be merged with all the modernization and optimization work that was done in the past two years. I will go through as much of your suggestions as I can before I will be on leave after tomorrow for three weeks. After that, if anyone else has time to work on some of the issues, I'd of course welcome the help. As you know, my time on the project is running out, so I probably won't have time to integrate everything. For instance, Manos' mini computation library I have only learned about in the last few months and don't know it in enough detail to make any kind of sensible estimate of how to do it, let alone how much time this would take. Let's discuss detailed plans in our meeting at 14:00 today.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:612,usability,learn,learned,612,"@hageboeck thanks for another Herculean reviewing effort :) Your comments make total sense; indeed, all the copy-pasted stuff still has to be merged with all the modernization and optimization work that was done in the past two years. I will go through as much of your suggestions as I can before I will be on leave after tomorrow for three weeks. After that, if anyone else has time to work on some of the issues, I'd of course welcome the help. As you know, my time on the project is running out, so I probably won't have time to integrate everything. For instance, Manos' mini computation library I have only learned about in the last few months and don't know it in enough detail to make any kind of sensible estimate of how to do it, let alone how much time this would take. Let's discuss detailed plans in our meeting at 14:00 today.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:163,integrability,messag,message,163,"I went through all comments, closed the ones I fixed and left some further comments for follow up discussion. The fixes I made so far are summarized in the commit message. The obvious big things still are `KahanSum` and adding lots of documentation. I will do these (and the other remaining todo's in the first post above this PR, https://github.com/root-project/root/pull/8700#issue-692681999) after my holidays.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:163,interoperability,messag,message,163,"I went through all comments, closed the ones I fixed and left some further comments for follow up discussion. The fixes I made so far are summarized in the commit message. The obvious big things still are `KahanSum` and adding lots of documentation. I will do these (and the other remaining todo's in the first post above this PR, https://github.com/root-project/root/pull/8700#issue-692681999) after my holidays.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:29,usability,close,closed,29,"I went through all comments, closed the ones I fixed and left some further comments for follow up discussion. The fixes I made so far are summarized in the commit message. The obvious big things still are `KahanSum` and adding lots of documentation. I will do these (and the other remaining todo's in the first post above this PR, https://github.com/root-project/root/pull/8700#issue-692681999) after my holidays.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:235,usability,document,documentation,235,"I went through all comments, closed the ones I fixed and left some further comments for follow up discussion. The fixes I made so far are summarized in the commit message. The obvious big things still are `KahanSum` and adding lots of documentation. I will do these (and the other remaining todo's in the first post above this PR, https://github.com/root-project/root/pull/8700#issue-692681999) after my holidays.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:158,deployability,build,builder,158,"During our meeting just now we decided that the highest priority issues for me to work on the coming days are:. 1. Documentation. 2. The top-level likelihood builder interface / PDF analysis (together with @wverkerke). If time permits, after that I will try to harmonize the Kahan sum usages and the other remaining issues, but we may need to postpone these to another PR.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:166,integrability,interfac,interface,166,"During our meeting just now we decided that the highest priority issues for me to work on the coming days are:. 1. Documentation. 2. The top-level likelihood builder interface / PDF analysis (together with @wverkerke). If time permits, after that I will try to harmonize the Kahan sum usages and the other remaining issues, but we may need to postpone these to another PR.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:166,interoperability,interfac,interface,166,"During our meeting just now we decided that the highest priority issues for me to work on the coming days are:. 1. Documentation. 2. The top-level likelihood builder interface / PDF analysis (together with @wverkerke). If time permits, after that I will try to harmonize the Kahan sum usages and the other remaining issues, but we may need to postpone these to another PR.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:166,modifiability,interfac,interface,166,"During our meeting just now we decided that the highest priority issues for me to work on the coming days are:. 1. Documentation. 2. The top-level likelihood builder interface / PDF analysis (together with @wverkerke). If time permits, after that I will try to harmonize the Kahan sum usages and the other remaining issues, but we may need to postpone these to another PR.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:222,performance,time,time,222,"During our meeting just now we decided that the highest priority issues for me to work on the coming days are:. 1. Documentation. 2. The top-level likelihood builder interface / PDF analysis (together with @wverkerke). If time permits, after that I will try to harmonize the Kahan sum usages and the other remaining issues, but we may need to postpone these to another PR.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:115,usability,Document,Documentation,115,"During our meeting just now we decided that the highest priority issues for me to work on the coming days are:. 1. Documentation. 2. The top-level likelihood builder interface / PDF analysis (together with @wverkerke). If time permits, after that I will try to harmonize the Kahan sum usages and the other remaining issues, but we may need to postpone these to another PR.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:25,availability,error,error,25,"I just noticed a compile error for the RooRealL.cxx test file that I introduced in the modernization commit (the second in this PR). I did an interactive rebase to fix this. Also included the RooMinimizer / MinuitFcnGrad circular dependency fix in that modernization/clean-up commit. To clean up this thread, I'll hide all bot messages starting from that previously buggy commit until now.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:230,deployability,depend,dependency,230,"I just noticed a compile error for the RooRealL.cxx test file that I introduced in the modernization commit (the second in this PR). I did an interactive rebase to fix this. Also included the RooMinimizer / MinuitFcnGrad circular dependency fix in that modernization/clean-up commit. To clean up this thread, I'll hide all bot messages starting from that previously buggy commit until now.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:230,integrability,depend,dependency,230,"I just noticed a compile error for the RooRealL.cxx test file that I introduced in the modernization commit (the second in this PR). I did an interactive rebase to fix this. Also included the RooMinimizer / MinuitFcnGrad circular dependency fix in that modernization/clean-up commit. To clean up this thread, I'll hide all bot messages starting from that previously buggy commit until now.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:327,integrability,messag,messages,327,"I just noticed a compile error for the RooRealL.cxx test file that I introduced in the modernization commit (the second in this PR). I did an interactive rebase to fix this. Also included the RooMinimizer / MinuitFcnGrad circular dependency fix in that modernization/clean-up commit. To clean up this thread, I'll hide all bot messages starting from that previously buggy commit until now.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:327,interoperability,messag,messages,327,"I just noticed a compile error for the RooRealL.cxx test file that I introduced in the modernization commit (the second in this PR). I did an interactive rebase to fix this. Also included the RooMinimizer / MinuitFcnGrad circular dependency fix in that modernization/clean-up commit. To clean up this thread, I'll hide all bot messages starting from that previously buggy commit until now.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:230,modifiability,depend,dependency,230,"I just noticed a compile error for the RooRealL.cxx test file that I introduced in the modernization commit (the second in this PR). I did an interactive rebase to fix this. Also included the RooMinimizer / MinuitFcnGrad circular dependency fix in that modernization/clean-up commit. To clean up this thread, I'll hide all bot messages starting from that previously buggy commit until now.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:25,performance,error,error,25,"I just noticed a compile error for the RooRealL.cxx test file that I introduced in the modernization commit (the second in this PR). I did an interactive rebase to fix this. Also included the RooMinimizer / MinuitFcnGrad circular dependency fix in that modernization/clean-up commit. To clean up this thread, I'll hide all bot messages starting from that previously buggy commit until now.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:25,safety,error,error,25,"I just noticed a compile error for the RooRealL.cxx test file that I introduced in the modernization commit (the second in this PR). I did an interactive rebase to fix this. Also included the RooMinimizer / MinuitFcnGrad circular dependency fix in that modernization/clean-up commit. To clean up this thread, I'll hide all bot messages starting from that previously buggy commit until now.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:52,safety,test,test,52,"I just noticed a compile error for the RooRealL.cxx test file that I introduced in the modernization commit (the second in this PR). I did an interactive rebase to fix this. Also included the RooMinimizer / MinuitFcnGrad circular dependency fix in that modernization/clean-up commit. To clean up this thread, I'll hide all bot messages starting from that previously buggy commit until now.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:230,safety,depend,dependency,230,"I just noticed a compile error for the RooRealL.cxx test file that I introduced in the modernization commit (the second in this PR). I did an interactive rebase to fix this. Also included the RooMinimizer / MinuitFcnGrad circular dependency fix in that modernization/clean-up commit. To clean up this thread, I'll hide all bot messages starting from that previously buggy commit until now.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:52,testability,test,test,52,"I just noticed a compile error for the RooRealL.cxx test file that I introduced in the modernization commit (the second in this PR). I did an interactive rebase to fix this. Also included the RooMinimizer / MinuitFcnGrad circular dependency fix in that modernization/clean-up commit. To clean up this thread, I'll hide all bot messages starting from that previously buggy commit until now.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:230,testability,depend,dependency,230,"I just noticed a compile error for the RooRealL.cxx test file that I introduced in the modernization commit (the second in this PR). I did an interactive rebase to fix this. Also included the RooMinimizer / MinuitFcnGrad circular dependency fix in that modernization/clean-up commit. To clean up this thread, I'll hide all bot messages starting from that previously buggy commit until now.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:25,usability,error,error,25,"I just noticed a compile error for the RooRealL.cxx test file that I introduced in the modernization commit (the second in this PR). I did an interactive rebase to fix this. Also included the RooMinimizer / MinuitFcnGrad circular dependency fix in that modernization/clean-up commit. To clean up this thread, I'll hide all bot messages starting from that previously buggy commit until now.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:142,usability,interact,interactive,142,"I just noticed a compile error for the RooRealL.cxx test file that I introduced in the modernization commit (the second in this PR). I did an interactive rebase to fix this. Also included the RooMinimizer / MinuitFcnGrad circular dependency fix in that modernization/clean-up commit. To clean up this thread, I'll hide all bot messages starting from that previously buggy commit until now.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:313,deployability,fail,failing,313,"In dd32e6d, I factored out the unbinned RooNLLVar calculation parts (computeScalar and computeBatched) into static functions inside RooNLLVar. These are now also called from RooUnbinnedL. RooUnbinnedL can now also do batched computation. I also added a test for this in testLikelihoodSerial. However, the test is failing, because it does not give the bit-wise exact equal answer as the old serial code. We should discuss whether this is expected and how to test for it then.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:217,integrability,batch,batched,217,"In dd32e6d, I factored out the unbinned RooNLLVar calculation parts (computeScalar and computeBatched) into static functions inside RooNLLVar. These are now also called from RooUnbinnedL. RooUnbinnedL can now also do batched computation. I also added a test for this in testLikelihoodSerial. However, the test is failing, because it does not give the bit-wise exact equal answer as the old serial code. We should discuss whether this is expected and how to test for it then.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:217,performance,batch,batched,217,"In dd32e6d, I factored out the unbinned RooNLLVar calculation parts (computeScalar and computeBatched) into static functions inside RooNLLVar. These are now also called from RooUnbinnedL. RooUnbinnedL can now also do batched computation. I also added a test for this in testLikelihoodSerial. However, the test is failing, because it does not give the bit-wise exact equal answer as the old serial code. We should discuss whether this is expected and how to test for it then.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:313,reliability,fail,failing,313,"In dd32e6d, I factored out the unbinned RooNLLVar calculation parts (computeScalar and computeBatched) into static functions inside RooNLLVar. These are now also called from RooUnbinnedL. RooUnbinnedL can now also do batched computation. I also added a test for this in testLikelihoodSerial. However, the test is failing, because it does not give the bit-wise exact equal answer as the old serial code. We should discuss whether this is expected and how to test for it then.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:333,reliability,doe,does,333,"In dd32e6d, I factored out the unbinned RooNLLVar calculation parts (computeScalar and computeBatched) into static functions inside RooNLLVar. These are now also called from RooUnbinnedL. RooUnbinnedL can now also do batched computation. I also added a test for this in testLikelihoodSerial. However, the test is failing, because it does not give the bit-wise exact equal answer as the old serial code. We should discuss whether this is expected and how to test for it then.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:253,safety,test,test,253,"In dd32e6d, I factored out the unbinned RooNLLVar calculation parts (computeScalar and computeBatched) into static functions inside RooNLLVar. These are now also called from RooUnbinnedL. RooUnbinnedL can now also do batched computation. I also added a test for this in testLikelihoodSerial. However, the test is failing, because it does not give the bit-wise exact equal answer as the old serial code. We should discuss whether this is expected and how to test for it then.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:270,safety,test,testLikelihoodSerial,270,"In dd32e6d, I factored out the unbinned RooNLLVar calculation parts (computeScalar and computeBatched) into static functions inside RooNLLVar. These are now also called from RooUnbinnedL. RooUnbinnedL can now also do batched computation. I also added a test for this in testLikelihoodSerial. However, the test is failing, because it does not give the bit-wise exact equal answer as the old serial code. We should discuss whether this is expected and how to test for it then.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:305,safety,test,test,305,"In dd32e6d, I factored out the unbinned RooNLLVar calculation parts (computeScalar and computeBatched) into static functions inside RooNLLVar. These are now also called from RooUnbinnedL. RooUnbinnedL can now also do batched computation. I also added a test for this in testLikelihoodSerial. However, the test is failing, because it does not give the bit-wise exact equal answer as the old serial code. We should discuss whether this is expected and how to test for it then.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:457,safety,test,test,457,"In dd32e6d, I factored out the unbinned RooNLLVar calculation parts (computeScalar and computeBatched) into static functions inside RooNLLVar. These are now also called from RooUnbinnedL. RooUnbinnedL can now also do batched computation. I also added a test for this in testLikelihoodSerial. However, the test is failing, because it does not give the bit-wise exact equal answer as the old serial code. We should discuss whether this is expected and how to test for it then.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:253,testability,test,test,253,"In dd32e6d, I factored out the unbinned RooNLLVar calculation parts (computeScalar and computeBatched) into static functions inside RooNLLVar. These are now also called from RooUnbinnedL. RooUnbinnedL can now also do batched computation. I also added a test for this in testLikelihoodSerial. However, the test is failing, because it does not give the bit-wise exact equal answer as the old serial code. We should discuss whether this is expected and how to test for it then.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:270,testability,test,testLikelihoodSerial,270,"In dd32e6d, I factored out the unbinned RooNLLVar calculation parts (computeScalar and computeBatched) into static functions inside RooNLLVar. These are now also called from RooUnbinnedL. RooUnbinnedL can now also do batched computation. I also added a test for this in testLikelihoodSerial. However, the test is failing, because it does not give the bit-wise exact equal answer as the old serial code. We should discuss whether this is expected and how to test for it then.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:305,testability,test,test,305,"In dd32e6d, I factored out the unbinned RooNLLVar calculation parts (computeScalar and computeBatched) into static functions inside RooNLLVar. These are now also called from RooUnbinnedL. RooUnbinnedL can now also do batched computation. I also added a test for this in testLikelihoodSerial. However, the test is failing, because it does not give the bit-wise exact equal answer as the old serial code. We should discuss whether this is expected and how to test for it then.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:457,testability,test,test,457,"In dd32e6d, I factored out the unbinned RooNLLVar calculation parts (computeScalar and computeBatched) into static functions inside RooNLLVar. These are now also called from RooUnbinnedL. RooUnbinnedL can now also do batched computation. I also added a test for this in testLikelihoodSerial. However, the test is failing, because it does not give the bit-wise exact equal answer as the old serial code. We should discuss whether this is expected and how to test for it then.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:209,deployability,version,version,209,"Remaining points and questions arising from @hageboeck's review:. 1. https://github.com/root-project/root/pull/8700#discussion_r672508683 Should RooRealL have I/O? It is currently disabled by setting ClassDef version to 0. 2. https://github.com/root-project/root/pull/8700#discussion_r672509759 RooFit naming convention puts underscores in front of member names. This is discouraged, since underscore prefix is reserved for C++ implementations. I introduced some new members with underscore suffix instead, which is also common practice e.g. in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""Likelihood",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:763,deployability,configurat,configuration,763,"Remaining points and questions arising from @hageboeck's review:. 1. https://github.com/root-project/root/pull/8700#discussion_r672508683 Should RooRealL have I/O? It is currently disabled by setting ClassDef version to 0. 2. https://github.com/root-project/root/pull/8700#discussion_r672509759 RooFit naming convention puts underscores in front of member names. This is discouraged, since underscore prefix is reserved for C++ implementations. I introduced some new members with underscore suffix instead, which is also common practice e.g. in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""Likelihood",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:1042,deployability,fail,fails,1042,"hageboeck's review:. 1. https://github.com/root-project/root/pull/8700#discussion_r672508683 Should RooRealL have I/O? It is currently disabled by setting ClassDef version to 0. 2. https://github.com/root-project/root/pull/8700#discussion_r672509759 RooFit naming convention puts underscores in front of member names. This is discouraged, since underscore prefix is reserved for C++ implementations. I introduced some new members with underscore suffix instead, which is also common practice e.g. in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""LikelihoodGradientSerial"" class (based on RooGradMinimi",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:2070,deployability,integr,integration,2070,". in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""LikelihoodGradientSerial"" class (based on RooGradMinimizerFcn) so we can add an integration test in this PR; B: wait for the integration test in PR 7. 3. Does everybody like the RooNLLVar statics? 4. Offset in the new TestStatistics classes can easily include the RooSubsidiaryL term as well, unlike with RooAbsTestStatistic, where the RooConstraintSum was a separate class that would not be included in the offset. I have now implemented a legacy mode to get bit-wise equal results, but is the new ""full"" mode a good idea at all? Does it make sense?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:2115,deployability,integr,integration,2115,". in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""LikelihoodGradientSerial"" class (based on RooGradMinimizerFcn) so we can add an integration test in this PR; B: wait for the integration test in PR 7. 3. Does everybody like the RooNLLVar statics? 4. Offset in the new TestStatistics classes can easily include the RooSubsidiaryL term as well, unlike with RooAbsTestStatistic, where the RooConstraintSum was a separate class that would not be included in the offset. I have now implemented a legacy mode to get bit-wise equal results, but is the new ""full"" mode a good idea at all? Does it make sense?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:170,energy efficiency,current,currently,170,"Remaining points and questions arising from @hageboeck's review:. 1. https://github.com/root-project/root/pull/8700#discussion_r672508683 Should RooRealL have I/O? It is currently disabled by setting ClassDef version to 0. 2. https://github.com/root-project/root/pull/8700#discussion_r672509759 RooFit naming convention puts underscores in front of member names. This is discouraged, since underscore prefix is reserved for C++ implementations. I introduced some new members with underscore suffix instead, which is also common practice e.g. in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""Likelihood",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:209,integrability,version,version,209,"Remaining points and questions arising from @hageboeck's review:. 1. https://github.com/root-project/root/pull/8700#discussion_r672508683 Should RooRealL have I/O? It is currently disabled by setting ClassDef version to 0. 2. https://github.com/root-project/root/pull/8700#discussion_r672509759 RooFit naming convention puts underscores in front of member names. This is discouraged, since underscore prefix is reserved for C++ implementations. I introduced some new members with underscore suffix instead, which is also common practice e.g. in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""Likelihood",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:763,integrability,configur,configuration,763,"Remaining points and questions arising from @hageboeck's review:. 1. https://github.com/root-project/root/pull/8700#discussion_r672508683 Should RooRealL have I/O? It is currently disabled by setting ClassDef version to 0. 2. https://github.com/root-project/root/pull/8700#discussion_r672509759 RooFit naming convention puts underscores in front of member names. This is discouraged, since underscore prefix is reserved for C++ implementations. I introduced some new members with underscore suffix instead, which is also common practice e.g. in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""Likelihood",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:2070,integrability,integration test,integration test,2070,". in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""LikelihoodGradientSerial"" class (based on RooGradMinimizerFcn) so we can add an integration test in this PR; B: wait for the integration test in PR 7. 3. Does everybody like the RooNLLVar statics? 4. Offset in the new TestStatistics classes can easily include the RooSubsidiaryL term as well, unlike with RooAbsTestStatistic, where the RooConstraintSum was a separate class that would not be included in the offset. I have now implemented a legacy mode to get bit-wise equal results, but is the new ""full"" mode a good idea at all? Does it make sense?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:2115,integrability,integration test,integration test,2115,". in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""LikelihoodGradientSerial"" class (based on RooGradMinimizerFcn) so we can add an integration test in this PR; B: wait for the integration test in PR 7. 3. Does everybody like the RooNLLVar statics? 4. Offset in the new TestStatistics classes can easily include the RooSubsidiaryL term as well, unlike with RooAbsTestStatistic, where the RooConstraintSum was a separate class that would not be included in the offset. I have now implemented a legacy mode to get bit-wise equal results, but is the new ""full"" mode a good idea at all? Does it make sense?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:2070,interoperability,integr,integration,2070,". in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""LikelihoodGradientSerial"" class (based on RooGradMinimizerFcn) so we can add an integration test in this PR; B: wait for the integration test in PR 7. 3. Does everybody like the RooNLLVar statics? 4. Offset in the new TestStatistics classes can easily include the RooSubsidiaryL term as well, unlike with RooAbsTestStatistic, where the RooConstraintSum was a separate class that would not be included in the offset. I have now implemented a legacy mode to get bit-wise equal results, but is the new ""full"" mode a good idea at all? Does it make sense?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:2115,interoperability,integr,integration,2115,". in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""LikelihoodGradientSerial"" class (based on RooGradMinimizerFcn) so we can add an integration test in this PR; B: wait for the integration test in PR 7. 3. Does everybody like the RooNLLVar statics? 4. Offset in the new TestStatistics classes can easily include the RooSubsidiaryL term as well, unlike with RooAbsTestStatistic, where the RooConstraintSum was a separate class that would not be included in the offset. I have now implemented a legacy mode to get bit-wise equal results, but is the new ""full"" mode a good idea at all? Does it make sense?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:209,modifiability,version,version,209,"Remaining points and questions arising from @hageboeck's review:. 1. https://github.com/root-project/root/pull/8700#discussion_r672508683 Should RooRealL have I/O? It is currently disabled by setting ClassDef version to 0. 2. https://github.com/root-project/root/pull/8700#discussion_r672509759 RooFit naming convention puts underscores in front of member names. This is discouraged, since underscore prefix is reserved for C++ implementations. I introduced some new members with underscore suffix instead, which is also common practice e.g. in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""Likelihood",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:763,modifiability,configur,configuration,763,"Remaining points and questions arising from @hageboeck's review:. 1. https://github.com/root-project/root/pull/8700#discussion_r672508683 Should RooRealL have I/O? It is currently disabled by setting ClassDef version to 0. 2. https://github.com/root-project/root/pull/8700#discussion_r672509759 RooFit naming convention puts underscores in front of member names. This is discouraged, since underscore prefix is reserved for C++ implementations. I introduced some new members with underscore suffix instead, which is also common practice e.g. in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""Likelihood",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:777,modifiability,paramet,parameters,777,"Remaining points and questions arising from @hageboeck's review:. 1. https://github.com/root-project/root/pull/8700#discussion_r672508683 Should RooRealL have I/O? It is currently disabled by setting ClassDef version to 0. 2. https://github.com/root-project/root/pull/8700#discussion_r672509759 RooFit naming convention puts underscores in front of member names. This is discouraged, since underscore prefix is reserved for C++ implementations. I introduced some new members with underscore suffix instead, which is also common practice e.g. in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""Likelihood",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:1409,modifiability,reu,reuse,1409,"eserved for C++ implementations. I introduced some new members with underscore suffix instead, which is also common practice e.g. in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""LikelihoodGradientSerial"" class (based on RooGradMinimizerFcn) so we can add an integration test in this PR; B: wait for the integration test in PR 7. 3. Does everybody like the RooNLLVar statics? 4. Offset in the new TestStatistics classes can easily include the RooSubsidiaryL term as well, unlike with RooAbsTestStatistic, where the RooConstraintSum was a separate class that would not be included in the offset. I have",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:2070,modifiability,integr,integration,2070,". in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""LikelihoodGradientSerial"" class (based on RooGradMinimizerFcn) so we can add an integration test in this PR; B: wait for the integration test in PR 7. 3. Does everybody like the RooNLLVar statics? 4. Offset in the new TestStatistics classes can easily include the RooSubsidiaryL term as well, unlike with RooAbsTestStatistic, where the RooConstraintSum was a separate class that would not be included in the offset. I have now implemented a legacy mode to get bit-wise equal results, but is the new ""full"" mode a good idea at all? Does it make sense?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:2115,modifiability,integr,integration,2115,". in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""LikelihoodGradientSerial"" class (based on RooGradMinimizerFcn) so we can add an integration test in this PR; B: wait for the integration test in PR 7. 3. Does everybody like the RooNLLVar statics? 4. Offset in the new TestStatistics classes can easily include the RooSubsidiaryL term as well, unlike with RooAbsTestStatistic, where the RooConstraintSum was a separate class that would not be included in the offset. I have now implemented a legacy mode to get bit-wise equal results, but is the new ""full"" mode a good idea at all? Does it make sense?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:159,performance,I/O,I/O,159,"Remaining points and questions arising from @hageboeck's review:. 1. https://github.com/root-project/root/pull/8700#discussion_r672508683 Should RooRealL have I/O? It is currently disabled by setting ClassDef version to 0. 2. https://github.com/root-project/root/pull/8700#discussion_r672509759 RooFit naming convention puts underscores in front of member names. This is discouraged, since underscore prefix is reserved for C++ implementations. I introduced some new members with underscore suffix instead, which is also common practice e.g. in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""Likelihood",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:627,performance,time,time,627,"Remaining points and questions arising from @hageboeck's review:. 1. https://github.com/root-project/root/pull/8700#discussion_r672508683 Should RooRealL have I/O? It is currently disabled by setting ClassDef version to 0. 2. https://github.com/root-project/root/pull/8700#discussion_r672509759 RooFit naming convention puts underscores in front of member names. This is discouraged, since underscore prefix is reserved for C++ implementations. I introduced some new members with underscore suffix instead, which is also common practice e.g. in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""Likelihood",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:1520,performance,time,time,1520,"ommon practice e.g. in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""LikelihoodGradientSerial"" class (based on RooGradMinimizerFcn) so we can add an integration test in this PR; B: wait for the integration test in PR 7. 3. Does everybody like the RooNLLVar statics? 4. Offset in the new TestStatistics classes can easily include the RooSubsidiaryL term as well, unlike with RooAbsTestStatistic, where the RooConstraintSum was a separate class that would not be included in the offset. I have now implemented a legacy mode to get bit-wise equal results, but is the new ""full"" mode a good idea at all? D",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:1804,performance,parallel,parallel,1804,". in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""LikelihoodGradientSerial"" class (based on RooGradMinimizerFcn) so we can add an integration test in this PR; B: wait for the integration test in PR 7. 3. Does everybody like the RooNLLVar statics? 4. Offset in the new TestStatistics classes can easily include the RooSubsidiaryL term as well, unlike with RooAbsTestStatistic, where the RooConstraintSum was a separate class that would not be included in the offset. I have now implemented a legacy mode to get bit-wise equal results, but is the new ""full"" mode a good idea at all? Does it make sense?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:528,reliability,pra,practice,528,"Remaining points and questions arising from @hageboeck's review:. 1. https://github.com/root-project/root/pull/8700#discussion_r672508683 Should RooRealL have I/O? It is currently disabled by setting ClassDef version to 0. 2. https://github.com/root-project/root/pull/8700#discussion_r672509759 RooFit naming convention puts underscores in front of member names. This is discouraged, since underscore prefix is reserved for C++ implementations. I introduced some new members with underscore suffix instead, which is also common practice e.g. in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""Likelihood",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:1042,reliability,fail,fails,1042,"hageboeck's review:. 1. https://github.com/root-project/root/pull/8700#discussion_r672508683 Should RooRealL have I/O? It is currently disabled by setting ClassDef version to 0. 2. https://github.com/root-project/root/pull/8700#discussion_r672509759 RooFit naming convention puts underscores in front of member names. This is discouraged, since underscore prefix is reserved for C++ implementations. I introduced some new members with underscore suffix instead, which is also common practice e.g. in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""LikelihoodGradientSerial"" class (based on RooGradMinimi",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:1059,reliability,doe,doesn,1059,"w:. 1. https://github.com/root-project/root/pull/8700#discussion_r672508683 Should RooRealL have I/O? It is currently disabled by setting ClassDef version to 0. 2. https://github.com/root-project/root/pull/8700#discussion_r672509759 RooFit naming convention puts underscores in front of member names. This is discouraged, since underscore prefix is reserved for C++ implementations. I introduced some new members with underscore suffix instead, which is also common practice e.g. in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""LikelihoodGradientSerial"" class (based on RooGradMinimizerFcn) so we can",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:2070,reliability,integr,integration,2070,". in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""LikelihoodGradientSerial"" class (based on RooGradMinimizerFcn) so we can add an integration test in this PR; B: wait for the integration test in PR 7. 3. Does everybody like the RooNLLVar statics? 4. Offset in the new TestStatistics classes can easily include the RooSubsidiaryL term as well, unlike with RooAbsTestStatistic, where the RooConstraintSum was a separate class that would not be included in the offset. I have now implemented a legacy mode to get bit-wise equal results, but is the new ""full"" mode a good idea at all? Does it make sense?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:2115,reliability,integr,integration,2115,". in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""LikelihoodGradientSerial"" class (based on RooGradMinimizerFcn) so we can add an integration test in this PR; B: wait for the integration test in PR 7. 3. Does everybody like the RooNLLVar statics? 4. Offset in the new TestStatistics classes can easily include the RooSubsidiaryL term as well, unlike with RooAbsTestStatistic, where the RooConstraintSum was a separate class that would not be included in the offset. I have now implemented a legacy mode to get bit-wise equal results, but is the new ""full"" mode a good idea at all? Does it make sense?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:2144,reliability,Doe,Does,2144,". in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""LikelihoodGradientSerial"" class (based on RooGradMinimizerFcn) so we can add an integration test in this PR; B: wait for the integration test in PR 7. 3. Does everybody like the RooNLLVar statics? 4. Offset in the new TestStatistics classes can easily include the RooSubsidiaryL term as well, unlike with RooAbsTestStatistic, where the RooConstraintSum was a separate class that would not be included in the offset. I have now implemented a legacy mode to get bit-wise equal results, but is the new ""full"" mode a good idea at all? Does it make sense?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:2521,reliability,Doe,Does,2521,". in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""LikelihoodGradientSerial"" class (based on RooGradMinimizerFcn) so we can add an integration test in this PR; B: wait for the integration test in PR 7. 3. Does everybody like the RooNLLVar statics? 4. Offset in the new TestStatistics classes can easily include the RooSubsidiaryL term as well, unlike with RooAbsTestStatistic, where the RooConstraintSum was a separate class that would not be included in the offset. I have now implemented a legacy mode to get bit-wise equal results, but is the new ""full"" mode a good idea at all? Does it make sense?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:57,safety,review,review,57,"Remaining points and questions arising from @hageboeck's review:. 1. https://github.com/root-project/root/pull/8700#discussion_r672508683 Should RooRealL have I/O? It is currently disabled by setting ClassDef version to 0. 2. https://github.com/root-project/root/pull/8700#discussion_r672509759 RooFit naming convention puts underscores in front of member names. This is discouraged, since underscore prefix is reserved for C++ implementations. I introduced some new members with underscore suffix instead, which is also common practice e.g. in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""Likelihood",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:1010,safety,test,testRooRealL,1010," and questions arising from @hageboeck's review:. 1. https://github.com/root-project/root/pull/8700#discussion_r672508683 Should RooRealL have I/O? It is currently disabled by setting ClassDef version to 0. 2. https://github.com/root-project/root/pull/8700#discussion_r672509759 RooFit naming convention puts underscores in front of member names. This is discouraged, since underscore prefix is reserved for C++ implementations. I introduced some new members with underscore suffix instead, which is also common practice e.g. in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""LikelihoodGradientSerial"" ",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:1135,safety,reme,remember,1135,"hould RooRealL have I/O? It is currently disabled by setting ClassDef version to 0. 2. https://github.com/root-project/root/pull/8700#discussion_r672509759 RooFit naming convention puts underscores in front of member names. This is discouraged, since underscore prefix is reserved for C++ implementations. I introduced some new members with underscore suffix instead, which is also common practice e.g. in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""LikelihoodGradientSerial"" class (based on RooGradMinimizerFcn) so we can add an integration test in this PR; B: wait for the integration test in PR 7",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:1273,safety,test,test,1273,"scussion_r672509759 RooFit naming convention puts underscores in front of member names. This is discouraged, since underscore prefix is reserved for C++ implementations. I introduced some new members with underscore suffix instead, which is also common practice e.g. in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""LikelihoodGradientSerial"" class (based on RooGradMinimizerFcn) so we can add an integration test in this PR; B: wait for the integration test in PR 7. 3. Does everybody like the RooNLLVar statics? 4. Offset in the new TestStatistics classes can easily include the RooSubsidiaryL term a",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:1569,safety,test,test,1569,". in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""LikelihoodGradientSerial"" class (based on RooGradMinimizerFcn) so we can add an integration test in this PR; B: wait for the integration test in PR 7. 3. Does everybody like the RooNLLVar statics? 4. Offset in the new TestStatistics classes can easily include the RooSubsidiaryL term as well, unlike with RooAbsTestStatistic, where the RooConstraintSum was a separate class that would not be included in the offset. I have now implemented a legacy mode to get bit-wise equal results, but is the new ""full"" mode a good idea at all? Does it make sense?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:1699,safety,test,test,1699,". in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""LikelihoodGradientSerial"" class (based on RooGradMinimizerFcn) so we can add an integration test in this PR; B: wait for the integration test in PR 7. 3. Does everybody like the RooNLLVar statics? 4. Offset in the new TestStatistics classes can easily include the RooSubsidiaryL term as well, unlike with RooAbsTestStatistic, where the RooConstraintSum was a separate class that would not be included in the offset. I have now implemented a legacy mode to get bit-wise equal results, but is the new ""full"" mode a good idea at all? Does it make sense?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:1852,safety,test,test,1852,". in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""LikelihoodGradientSerial"" class (based on RooGradMinimizerFcn) so we can add an integration test in this PR; B: wait for the integration test in PR 7. 3. Does everybody like the RooNLLVar statics? 4. Offset in the new TestStatistics classes can easily include the RooSubsidiaryL term as well, unlike with RooAbsTestStatistic, where the RooConstraintSum was a separate class that would not be included in the offset. I have now implemented a legacy mode to get bit-wise equal results, but is the new ""full"" mode a good idea at all? Does it make sense?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:1961,safety,except,except,1961,". in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""LikelihoodGradientSerial"" class (based on RooGradMinimizerFcn) so we can add an integration test in this PR; B: wait for the integration test in PR 7. 3. Does everybody like the RooNLLVar statics? 4. Offset in the new TestStatistics classes can easily include the RooSubsidiaryL term as well, unlike with RooAbsTestStatistic, where the RooConstraintSum was a separate class that would not be included in the offset. I have now implemented a legacy mode to get bit-wise equal results, but is the new ""full"" mode a good idea at all? Does it make sense?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:1972,safety,test,testing,1972,". in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""LikelihoodGradientSerial"" class (based on RooGradMinimizerFcn) so we can add an integration test in this PR; B: wait for the integration test in PR 7. 3. Does everybody like the RooNLLVar statics? 4. Offset in the new TestStatistics classes can easily include the RooSubsidiaryL term as well, unlike with RooAbsTestStatistic, where the RooConstraintSum was a separate class that would not be included in the offset. I have now implemented a legacy mode to get bit-wise equal results, but is the new ""full"" mode a good idea at all? Does it make sense?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:2082,safety,test,test,2082,". in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""LikelihoodGradientSerial"" class (based on RooGradMinimizerFcn) so we can add an integration test in this PR; B: wait for the integration test in PR 7. 3. Does everybody like the RooNLLVar statics? 4. Offset in the new TestStatistics classes can easily include the RooSubsidiaryL term as well, unlike with RooAbsTestStatistic, where the RooConstraintSum was a separate class that would not be included in the offset. I have now implemented a legacy mode to get bit-wise equal results, but is the new ""full"" mode a good idea at all? Does it make sense?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:2127,safety,test,test,2127,". in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""LikelihoodGradientSerial"" class (based on RooGradMinimizerFcn) so we can add an integration test in this PR; B: wait for the integration test in PR 7. 3. Does everybody like the RooNLLVar statics? 4. Offset in the new TestStatistics classes can easily include the RooSubsidiaryL term as well, unlike with RooAbsTestStatistic, where the RooConstraintSum was a separate class that would not be included in the offset. I have now implemented a legacy mode to get bit-wise equal results, but is the new ""full"" mode a good idea at all? Does it make sense?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:2208,safety,Test,TestStatistics,2208,". in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""LikelihoodGradientSerial"" class (based on RooGradMinimizerFcn) so we can add an integration test in this PR; B: wait for the integration test in PR 7. 3. Does everybody like the RooNLLVar statics? 4. Offset in the new TestStatistics classes can easily include the RooSubsidiaryL term as well, unlike with RooAbsTestStatistic, where the RooConstraintSum was a separate class that would not be included in the offset. I have now implemented a legacy mode to get bit-wise equal results, but is the new ""full"" mode a good idea at all? Does it make sense?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:763,security,configur,configuration,763,"Remaining points and questions arising from @hageboeck's review:. 1. https://github.com/root-project/root/pull/8700#discussion_r672508683 Should RooRealL have I/O? It is currently disabled by setting ClassDef version to 0. 2. https://github.com/root-project/root/pull/8700#discussion_r672509759 RooFit naming convention puts underscores in front of member names. This is discouraged, since underscore prefix is reserved for C++ implementations. I introduced some new members with underscore suffix instead, which is also common practice e.g. in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""Likelihood",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:2070,security,integr,integration,2070,". in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""LikelihoodGradientSerial"" class (based on RooGradMinimizerFcn) so we can add an integration test in this PR; B: wait for the integration test in PR 7. 3. Does everybody like the RooNLLVar statics? 4. Offset in the new TestStatistics classes can easily include the RooSubsidiaryL term as well, unlike with RooAbsTestStatistic, where the RooConstraintSum was a separate class that would not be included in the offset. I have now implemented a legacy mode to get bit-wise equal results, but is the new ""full"" mode a good idea at all? Does it make sense?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:2115,security,integr,integration,2115,". in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""LikelihoodGradientSerial"" class (based on RooGradMinimizerFcn) so we can add an integration test in this PR; B: wait for the integration test in PR 7. 3. Does everybody like the RooNLLVar statics? 4. Offset in the new TestStatistics classes can easily include the RooSubsidiaryL term as well, unlike with RooAbsTestStatistic, where the RooConstraintSum was a separate class that would not be included in the offset. I have now implemented a legacy mode to get bit-wise equal results, but is the new ""full"" mode a good idea at all? Does it make sense?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:57,testability,review,review,57,"Remaining points and questions arising from @hageboeck's review:. 1. https://github.com/root-project/root/pull/8700#discussion_r672508683 Should RooRealL have I/O? It is currently disabled by setting ClassDef version to 0. 2. https://github.com/root-project/root/pull/8700#discussion_r672509759 RooFit naming convention puts underscores in front of member names. This is discouraged, since underscore prefix is reserved for C++ implementations. I introduced some new members with underscore suffix instead, which is also common practice e.g. in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""Likelihood",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:1010,testability,test,testRooRealL,1010," and questions arising from @hageboeck's review:. 1. https://github.com/root-project/root/pull/8700#discussion_r672508683 Should RooRealL have I/O? It is currently disabled by setting ClassDef version to 0. 2. https://github.com/root-project/root/pull/8700#discussion_r672509759 RooFit naming convention puts underscores in front of member names. This is discouraged, since underscore prefix is reserved for C++ implementations. I introduced some new members with underscore suffix instead, which is also common practice e.g. in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""LikelihoodGradientSerial"" ",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:1273,testability,test,test,1273,"scussion_r672509759 RooFit naming convention puts underscores in front of member names. This is discouraged, since underscore prefix is reserved for C++ implementations. I introduced some new members with underscore suffix instead, which is also common practice e.g. in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""LikelihoodGradientSerial"" class (based on RooGradMinimizerFcn) so we can add an integration test in this PR; B: wait for the integration test in PR 7. 3. Does everybody like the RooNLLVar statics? 4. Offset in the new TestStatistics classes can easily include the RooSubsidiaryL term a",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:1569,testability,test,test,1569,". in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""LikelihoodGradientSerial"" class (based on RooGradMinimizerFcn) so we can add an integration test in this PR; B: wait for the integration test in PR 7. 3. Does everybody like the RooNLLVar statics? 4. Offset in the new TestStatistics classes can easily include the RooSubsidiaryL term as well, unlike with RooAbsTestStatistic, where the RooConstraintSum was a separate class that would not be included in the offset. I have now implemented a legacy mode to get bit-wise equal results, but is the new ""full"" mode a good idea at all? Does it make sense?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:1690,testability,coverag,coverage,1690,". in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""LikelihoodGradientSerial"" class (based on RooGradMinimizerFcn) so we can add an integration test in this PR; B: wait for the integration test in PR 7. 3. Does everybody like the RooNLLVar statics? 4. Offset in the new TestStatistics classes can easily include the RooSubsidiaryL term as well, unlike with RooAbsTestStatistic, where the RooConstraintSum was a separate class that would not be included in the offset. I have now implemented a legacy mode to get bit-wise equal results, but is the new ""full"" mode a good idea at all? Does it make sense?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:1699,testability,test,test,1699,". in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""LikelihoodGradientSerial"" class (based on RooGradMinimizerFcn) so we can add an integration test in this PR; B: wait for the integration test in PR 7. 3. Does everybody like the RooNLLVar statics? 4. Offset in the new TestStatistics classes can easily include the RooSubsidiaryL term as well, unlike with RooAbsTestStatistic, where the RooConstraintSum was a separate class that would not be included in the offset. I have now implemented a legacy mode to get bit-wise equal results, but is the new ""full"" mode a good idea at all? Does it make sense?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:1852,testability,test,test,1852,". in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""LikelihoodGradientSerial"" class (based on RooGradMinimizerFcn) so we can add an integration test in this PR; B: wait for the integration test in PR 7. 3. Does everybody like the RooNLLVar statics? 4. Offset in the new TestStatistics classes can easily include the RooSubsidiaryL term as well, unlike with RooAbsTestStatistic, where the RooConstraintSum was a separate class that would not be included in the offset. I have now implemented a legacy mode to get bit-wise equal results, but is the new ""full"" mode a good idea at all? Does it make sense?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:1972,testability,test,testing,1972,". in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""LikelihoodGradientSerial"" class (based on RooGradMinimizerFcn) so we can add an integration test in this PR; B: wait for the integration test in PR 7. 3. Does everybody like the RooNLLVar statics? 4. Offset in the new TestStatistics classes can easily include the RooSubsidiaryL term as well, unlike with RooAbsTestStatistic, where the RooConstraintSum was a separate class that would not be included in the offset. I have now implemented a legacy mode to get bit-wise equal results, but is the new ""full"" mode a good idea at all? Does it make sense?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:2070,testability,integr,integration,2070,". in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""LikelihoodGradientSerial"" class (based on RooGradMinimizerFcn) so we can add an integration test in this PR; B: wait for the integration test in PR 7. 3. Does everybody like the RooNLLVar statics? 4. Offset in the new TestStatistics classes can easily include the RooSubsidiaryL term as well, unlike with RooAbsTestStatistic, where the RooConstraintSum was a separate class that would not be included in the offset. I have now implemented a legacy mode to get bit-wise equal results, but is the new ""full"" mode a good idea at all? Does it make sense?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:2082,testability,test,test,2082,". in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""LikelihoodGradientSerial"" class (based on RooGradMinimizerFcn) so we can add an integration test in this PR; B: wait for the integration test in PR 7. 3. Does everybody like the RooNLLVar statics? 4. Offset in the new TestStatistics classes can easily include the RooSubsidiaryL term as well, unlike with RooAbsTestStatistic, where the RooConstraintSum was a separate class that would not be included in the offset. I have now implemented a legacy mode to get bit-wise equal results, but is the new ""full"" mode a good idea at all? Does it make sense?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:2115,testability,integr,integration,2115,". in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""LikelihoodGradientSerial"" class (based on RooGradMinimizerFcn) so we can add an integration test in this PR; B: wait for the integration test in PR 7. 3. Does everybody like the RooNLLVar statics? 4. Offset in the new TestStatistics classes can easily include the RooSubsidiaryL term as well, unlike with RooAbsTestStatistic, where the RooConstraintSum was a separate class that would not be included in the offset. I have now implemented a legacy mode to get bit-wise equal results, but is the new ""full"" mode a good idea at all? Does it make sense?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:2127,testability,test,test,2127,". in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""LikelihoodGradientSerial"" class (based on RooGradMinimizerFcn) so we can add an integration test in this PR; B: wait for the integration test in PR 7. 3. Does everybody like the RooNLLVar statics? 4. Offset in the new TestStatistics classes can easily include the RooSubsidiaryL term as well, unlike with RooAbsTestStatistic, where the RooConstraintSum was a separate class that would not be included in the offset. I have now implemented a legacy mode to get bit-wise equal results, but is the new ""full"" mode a good idea at all? Does it make sense?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:2208,testability,Test,TestStatistics,2208,". in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""LikelihoodGradientSerial"" class (based on RooGradMinimizerFcn) so we can add an integration test in this PR; B: wait for the integration test in PR 7. 3. Does everybody like the RooNLLVar statics? 4. Offset in the new TestStatistics classes can easily include the RooSubsidiaryL term as well, unlike with RooAbsTestStatistic, where the RooConstraintSum was a separate class that would not be included in the offset. I have now implemented a legacy mode to get bit-wise equal results, but is the new ""full"" mode a good idea at all? Does it make sense?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:560,usability,guid,guidelines,560,"Remaining points and questions arising from @hageboeck's review:. 1. https://github.com/root-project/root/pull/8700#discussion_r672508683 Should RooRealL have I/O? It is currently disabled by setting ClassDef version to 0. 2. https://github.com/root-project/root/pull/8700#discussion_r672509759 RooFit naming convention puts underscores in front of member names. This is discouraged, since underscore prefix is reserved for C++ implementations. I introduced some new members with underscore suffix instead, which is also common practice e.g. in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""Likelihood",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:1244,usability,help,help,1244,"oot-project/root/pull/8700#discussion_r672509759 RooFit naming convention puts underscores in front of member names. This is discouraged, since underscore prefix is reserved for C++ implementations. I introduced some new members with underscore suffix instead, which is also common practice e.g. in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""LikelihoodGradientSerial"" class (based on RooGradMinimizerFcn) so we can add an integration test in this PR; B: wait for the integration test in PR 7. 3. Does everybody like the RooNLLVar statics? 4. Offset in the new TestStatistics classes can easily incl",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:1644,usability,minim,minimization,1644,". in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""LikelihoodGradientSerial"" class (based on RooGradMinimizerFcn) so we can add an integration test in this PR; B: wait for the integration test in PR 7. 3. Does everybody like the RooNLLVar statics? 4. Offset in the new TestStatistics classes can easily include the RooSubsidiaryL term as well, unlike with RooAbsTestStatistic, where the RooConstraintSum was a separate class that would not be included in the offset. I have now implemented a legacy mode to get bit-wise equal results, but is the new ""full"" mode a good idea at all? Does it make sense?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:1735,usability,minim,minimization,1735,". in the Google C++ guidelines. I think this is not something we should spend too much time on now, but maybe good to keep in mind for the future. 3. https://github.com/root-project/root/pull/8700#discussion_r672519457 The configuration parameters (GlobalObservables etc.): should we unify them or keep like this? 4. https://github.com/root-project/root/pull/8700#discussion_r672538377 This should be checked by Wouter. In addition, some other remaining questions:. 1. `testRooRealL.getValRooAddition` fails because it doesn't know `RooFormulaVar`... I'm not sure what this means, but I vaguely remember that (I think) @hageboeck and @cburgard were talking about this, so I was hoping you may be able to help me out with fixing that test. 2. In an ideal world I would have liked to add a serial implementation of the `LikelihoodGradientWrapper` class. Probably, we can reuse a lot of `RooGradMinimizerFcn` for this and it should be rather straightforward. However, this will take time. The advantage would be that we could add a test to this PR that covers all that is added, basically by just running a minimization. Note that I do have such a full-coverage test, but only for the MultiProcess minimization, where I implemented `LikelihoodGradientWrapper` in the parallel `LikelihoodGradientJob` class. So this test will come in ""PR 7"" (see overview). So, up for discussion, two choices: A: add (probably mostly useless except for testing this PR) ""LikelihoodGradientSerial"" class (based on RooGradMinimizerFcn) so we can add an integration test in this PR; B: wait for the integration test in PR 7. 3. Does everybody like the RooNLLVar statics? 4. Offset in the new TestStatistics classes can easily include the RooSubsidiaryL term as well, unlike with RooAbsTestStatistic, where the RooConstraintSum was a separate class that would not be included in the offset. I have now implemented a legacy mode to get bit-wise equal results, but is the new ""full"" mode a good idea at all? Does it make sense?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:745,deployability,fail,failing,745,"Brief conclusions based on the discussion via Zoom we just had with @lmoneta @guitargeek @manolismih and @wverkerke:. Conclusions from discussion via Zoom:. 1. RooRealL does not need I/O. Basic principle: pdfs and datasets are persistable, test statistics are not. 2. Keep how it is. 3. Keep them and add docstring. 4. Removed TODO, it's fine. Added assert will make sure. Additional items:. 1. Jonas will investigate. Will write in PR. 2. Next PR, time constraints. 3. Yes. Good. Rename to ...Func and pass evalData as reference, not unique_ptr. 4. It's a step in the right direction. Keep like it is. Keep legacy in default to ease transition. Additional todo's:. - Batched test: compare to batched RooNLLVar. - Fix CI issues. - Fix remaining failing tests.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:668,integrability,Batch,Batched,668,"Brief conclusions based on the discussion via Zoom we just had with @lmoneta @guitargeek @manolismih and @wverkerke:. Conclusions from discussion via Zoom:. 1. RooRealL does not need I/O. Basic principle: pdfs and datasets are persistable, test statistics are not. 2. Keep how it is. 3. Keep them and add docstring. 4. Removed TODO, it's fine. Added assert will make sure. Additional items:. 1. Jonas will investigate. Will write in PR. 2. Next PR, time constraints. 3. Yes. Good. Rename to ...Func and pass evalData as reference, not unique_ptr. 4. It's a step in the right direction. Keep like it is. Keep legacy in default to ease transition. Additional todo's:. - Batched test: compare to batched RooNLLVar. - Fix CI issues. - Fix remaining failing tests.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:693,integrability,batch,batched,693,"Brief conclusions based on the discussion via Zoom we just had with @lmoneta @guitargeek @manolismih and @wverkerke:. Conclusions from discussion via Zoom:. 1. RooRealL does not need I/O. Basic principle: pdfs and datasets are persistable, test statistics are not. 2. Keep how it is. 3. Keep them and add docstring. 4. Removed TODO, it's fine. Added assert will make sure. Additional items:. 1. Jonas will investigate. Will write in PR. 2. Next PR, time constraints. 3. Yes. Good. Rename to ...Func and pass evalData as reference, not unique_ptr. 4. It's a step in the right direction. Keep like it is. Keep legacy in default to ease transition. Additional todo's:. - Batched test: compare to batched RooNLLVar. - Fix CI issues. - Fix remaining failing tests.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:183,performance,I/O,I/O,183,"Brief conclusions based on the discussion via Zoom we just had with @lmoneta @guitargeek @manolismih and @wverkerke:. Conclusions from discussion via Zoom:. 1. RooRealL does not need I/O. Basic principle: pdfs and datasets are persistable, test statistics are not. 2. Keep how it is. 3. Keep them and add docstring. 4. Removed TODO, it's fine. Added assert will make sure. Additional items:. 1. Jonas will investigate. Will write in PR. 2. Next PR, time constraints. 3. Yes. Good. Rename to ...Func and pass evalData as reference, not unique_ptr. 4. It's a step in the right direction. Keep like it is. Keep legacy in default to ease transition. Additional todo's:. - Batched test: compare to batched RooNLLVar. - Fix CI issues. - Fix remaining failing tests.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:449,performance,time,time,449,"Brief conclusions based on the discussion via Zoom we just had with @lmoneta @guitargeek @manolismih and @wverkerke:. Conclusions from discussion via Zoom:. 1. RooRealL does not need I/O. Basic principle: pdfs and datasets are persistable, test statistics are not. 2. Keep how it is. 3. Keep them and add docstring. 4. Removed TODO, it's fine. Added assert will make sure. Additional items:. 1. Jonas will investigate. Will write in PR. 2. Next PR, time constraints. 3. Yes. Good. Rename to ...Func and pass evalData as reference, not unique_ptr. 4. It's a step in the right direction. Keep like it is. Keep legacy in default to ease transition. Additional todo's:. - Batched test: compare to batched RooNLLVar. - Fix CI issues. - Fix remaining failing tests.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:668,performance,Batch,Batched,668,"Brief conclusions based on the discussion via Zoom we just had with @lmoneta @guitargeek @manolismih and @wverkerke:. Conclusions from discussion via Zoom:. 1. RooRealL does not need I/O. Basic principle: pdfs and datasets are persistable, test statistics are not. 2. Keep how it is. 3. Keep them and add docstring. 4. Removed TODO, it's fine. Added assert will make sure. Additional items:. 1. Jonas will investigate. Will write in PR. 2. Next PR, time constraints. 3. Yes. Good. Rename to ...Func and pass evalData as reference, not unique_ptr. 4. It's a step in the right direction. Keep like it is. Keep legacy in default to ease transition. Additional todo's:. - Batched test: compare to batched RooNLLVar. - Fix CI issues. - Fix remaining failing tests.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:693,performance,batch,batched,693,"Brief conclusions based on the discussion via Zoom we just had with @lmoneta @guitargeek @manolismih and @wverkerke:. Conclusions from discussion via Zoom:. 1. RooRealL does not need I/O. Basic principle: pdfs and datasets are persistable, test statistics are not. 2. Keep how it is. 3. Keep them and add docstring. 4. Removed TODO, it's fine. Added assert will make sure. Additional items:. 1. Jonas will investigate. Will write in PR. 2. Next PR, time constraints. 3. Yes. Good. Rename to ...Func and pass evalData as reference, not unique_ptr. 4. It's a step in the right direction. Keep like it is. Keep legacy in default to ease transition. Additional todo's:. - Batched test: compare to batched RooNLLVar. - Fix CI issues. - Fix remaining failing tests.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:169,reliability,doe,does,169,"Brief conclusions based on the discussion via Zoom we just had with @lmoneta @guitargeek @manolismih and @wverkerke:. Conclusions from discussion via Zoom:. 1. RooRealL does not need I/O. Basic principle: pdfs and datasets are persistable, test statistics are not. 2. Keep how it is. 3. Keep them and add docstring. 4. Removed TODO, it's fine. Added assert will make sure. Additional items:. 1. Jonas will investigate. Will write in PR. 2. Next PR, time constraints. 3. Yes. Good. Rename to ...Func and pass evalData as reference, not unique_ptr. 4. It's a step in the right direction. Keep like it is. Keep legacy in default to ease transition. Additional todo's:. - Batched test: compare to batched RooNLLVar. - Fix CI issues. - Fix remaining failing tests.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:745,reliability,fail,failing,745,"Brief conclusions based on the discussion via Zoom we just had with @lmoneta @guitargeek @manolismih and @wverkerke:. Conclusions from discussion via Zoom:. 1. RooRealL does not need I/O. Basic principle: pdfs and datasets are persistable, test statistics are not. 2. Keep how it is. 3. Keep them and add docstring. 4. Removed TODO, it's fine. Added assert will make sure. Additional items:. 1. Jonas will investigate. Will write in PR. 2. Next PR, time constraints. 3. Yes. Good. Rename to ...Func and pass evalData as reference, not unique_ptr. 4. It's a step in the right direction. Keep like it is. Keep legacy in default to ease transition. Additional todo's:. - Batched test: compare to batched RooNLLVar. - Fix CI issues. - Fix remaining failing tests.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:240,safety,test,test,240,"Brief conclusions based on the discussion via Zoom we just had with @lmoneta @guitargeek @manolismih and @wverkerke:. Conclusions from discussion via Zoom:. 1. RooRealL does not need I/O. Basic principle: pdfs and datasets are persistable, test statistics are not. 2. Keep how it is. 3. Keep them and add docstring. 4. Removed TODO, it's fine. Added assert will make sure. Additional items:. 1. Jonas will investigate. Will write in PR. 2. Next PR, time constraints. 3. Yes. Good. Rename to ...Func and pass evalData as reference, not unique_ptr. 4. It's a step in the right direction. Keep like it is. Keep legacy in default to ease transition. Additional todo's:. - Batched test: compare to batched RooNLLVar. - Fix CI issues. - Fix remaining failing tests.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:676,safety,test,test,676,"Brief conclusions based on the discussion via Zoom we just had with @lmoneta @guitargeek @manolismih and @wverkerke:. Conclusions from discussion via Zoom:. 1. RooRealL does not need I/O. Basic principle: pdfs and datasets are persistable, test statistics are not. 2. Keep how it is. 3. Keep them and add docstring. 4. Removed TODO, it's fine. Added assert will make sure. Additional items:. 1. Jonas will investigate. Will write in PR. 2. Next PR, time constraints. 3. Yes. Good. Rename to ...Func and pass evalData as reference, not unique_ptr. 4. It's a step in the right direction. Keep like it is. Keep legacy in default to ease transition. Additional todo's:. - Batched test: compare to batched RooNLLVar. - Fix CI issues. - Fix remaining failing tests.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:753,safety,test,tests,753,"Brief conclusions based on the discussion via Zoom we just had with @lmoneta @guitargeek @manolismih and @wverkerke:. Conclusions from discussion via Zoom:. 1. RooRealL does not need I/O. Basic principle: pdfs and datasets are persistable, test statistics are not. 2. Keep how it is. 3. Keep them and add docstring. 4. Removed TODO, it's fine. Added assert will make sure. Additional items:. 1. Jonas will investigate. Will write in PR. 2. Next PR, time constraints. 3. Yes. Good. Rename to ...Func and pass evalData as reference, not unique_ptr. 4. It's a step in the right direction. Keep like it is. Keep legacy in default to ease transition. Additional todo's:. - Batched test: compare to batched RooNLLVar. - Fix CI issues. - Fix remaining failing tests.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:240,testability,test,test,240,"Brief conclusions based on the discussion via Zoom we just had with @lmoneta @guitargeek @manolismih and @wverkerke:. Conclusions from discussion via Zoom:. 1. RooRealL does not need I/O. Basic principle: pdfs and datasets are persistable, test statistics are not. 2. Keep how it is. 3. Keep them and add docstring. 4. Removed TODO, it's fine. Added assert will make sure. Additional items:. 1. Jonas will investigate. Will write in PR. 2. Next PR, time constraints. 3. Yes. Good. Rename to ...Func and pass evalData as reference, not unique_ptr. 4. It's a step in the right direction. Keep like it is. Keep legacy in default to ease transition. Additional todo's:. - Batched test: compare to batched RooNLLVar. - Fix CI issues. - Fix remaining failing tests.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:350,testability,assert,assert,350,"Brief conclusions based on the discussion via Zoom we just had with @lmoneta @guitargeek @manolismih and @wverkerke:. Conclusions from discussion via Zoom:. 1. RooRealL does not need I/O. Basic principle: pdfs and datasets are persistable, test statistics are not. 2. Keep how it is. 3. Keep them and add docstring. 4. Removed TODO, it's fine. Added assert will make sure. Additional items:. 1. Jonas will investigate. Will write in PR. 2. Next PR, time constraints. 3. Yes. Good. Rename to ...Func and pass evalData as reference, not unique_ptr. 4. It's a step in the right direction. Keep like it is. Keep legacy in default to ease transition. Additional todo's:. - Batched test: compare to batched RooNLLVar. - Fix CI issues. - Fix remaining failing tests.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:676,testability,test,test,676,"Brief conclusions based on the discussion via Zoom we just had with @lmoneta @guitargeek @manolismih and @wverkerke:. Conclusions from discussion via Zoom:. 1. RooRealL does not need I/O. Basic principle: pdfs and datasets are persistable, test statistics are not. 2. Keep how it is. 3. Keep them and add docstring. 4. Removed TODO, it's fine. Added assert will make sure. Additional items:. 1. Jonas will investigate. Will write in PR. 2. Next PR, time constraints. 3. Yes. Good. Rename to ...Func and pass evalData as reference, not unique_ptr. 4. It's a step in the right direction. Keep like it is. Keep legacy in default to ease transition. Additional todo's:. - Batched test: compare to batched RooNLLVar. - Fix CI issues. - Fix remaining failing tests.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:753,testability,test,tests,753,"Brief conclusions based on the discussion via Zoom we just had with @lmoneta @guitargeek @manolismih and @wverkerke:. Conclusions from discussion via Zoom:. 1. RooRealL does not need I/O. Basic principle: pdfs and datasets are persistable, test statistics are not. 2. Keep how it is. 3. Keep them and add docstring. 4. Removed TODO, it's fine. Added assert will make sure. Additional items:. 1. Jonas will investigate. Will write in PR. 2. Next PR, time constraints. 3. Yes. Good. Rename to ...Func and pass evalData as reference, not unique_ptr. 4. It's a step in the right direction. Keep like it is. Keep legacy in default to ease transition. Additional todo's:. - Batched test: compare to batched RooNLLVar. - Fix CI issues. - Fix remaining failing tests.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:127,modifiability,paramet,parameter,127,"The latest force push just now implements some of the points we discussed. One thing I omitted is the change of the `evalData` parameter from a `unique_ptr` to a reference. We discussed that a pointer means that the object may be changed and this is indeed what can happen. The function, in fact, initializes the pointer if it is null. I don't know if this was intended or maybe it should be changed, but it was already like this, so I'm hesitant to change it. Remaining todos:. - [x] Add doxygen for the `GlobalObservables` etc parameters. - [x] Fix CI issues. - [ ] Follow @guitargeek's coming comment about how to fix the `RooFormulaVar` bug in testRooRealL.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:529,modifiability,paramet,parameters,529,"The latest force push just now implements some of the points we discussed. One thing I omitted is the change of the `evalData` parameter from a `unique_ptr` to a reference. We discussed that a pointer means that the object may be changed and this is indeed what can happen. The function, in fact, initializes the pointer if it is null. I don't know if this was intended or maybe it should be changed, but it was already like this, so I'm hesitant to change it. Remaining todos:. - [x] Add doxygen for the `GlobalObservables` etc parameters. - [x] Fix CI issues. - [ ] Follow @guitargeek's coming comment about how to fix the `RooFormulaVar` bug in testRooRealL.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:648,safety,test,testRooRealL,648,"The latest force push just now implements some of the points we discussed. One thing I omitted is the change of the `evalData` parameter from a `unique_ptr` to a reference. We discussed that a pointer means that the object may be changed and this is indeed what can happen. The function, in fact, initializes the pointer if it is null. I don't know if this was intended or maybe it should be changed, but it was already like this, so I'm hesitant to change it. Remaining todos:. - [x] Add doxygen for the `GlobalObservables` etc parameters. - [x] Fix CI issues. - [ ] Follow @guitargeek's coming comment about how to fix the `RooFormulaVar` bug in testRooRealL.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:648,testability,test,testRooRealL,648,"The latest force push just now implements some of the points we discussed. One thing I omitted is the change of the `evalData` parameter from a `unique_ptr` to a reference. We discussed that a pointer means that the object may be changed and this is indeed what can happen. The function, in fact, initializes the pointer if it is null. I don't know if this was intended or maybe it should be changed, but it was already like this, so I'm hesitant to change it. Remaining todos:. - [x] Add doxygen for the `GlobalObservables` etc parameters. - [x] Fix CI issues. - [ ] Follow @guitargeek's coming comment about how to fix the `RooFormulaVar` bug in testRooRealL.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:63,availability,error,errors,63,"In the latest force push, I tried to fix all remaining compile errors and warnings that the CI brought up. I used fixup commits with interactive rebase to keep the commit history clean and actually make sure the earlier commits compile as well now. Evidently, I have failed to fix the esoteric error with ambiguous overload in the strongly typed optional parameter classes. There is actually even a paper about this (http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p1008r0.pdf), which I thought should provide a solution, but apparently not on all compilers, so I'll have to try something else.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:294,availability,error,error,294,"In the latest force push, I tried to fix all remaining compile errors and warnings that the CI brought up. I used fixup commits with interactive rebase to keep the commit history clean and actually make sure the earlier commits compile as well now. Evidently, I have failed to fix the esoteric error with ambiguous overload in the strongly typed optional parameter classes. There is actually even a paper about this (http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p1008r0.pdf), which I thought should provide a solution, but apparently not on all compilers, so I'll have to try something else.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:267,deployability,fail,failed,267,"In the latest force push, I tried to fix all remaining compile errors and warnings that the CI brought up. I used fixup commits with interactive rebase to keep the commit history clean and actually make sure the earlier commits compile as well now. Evidently, I have failed to fix the esoteric error with ambiguous overload in the strongly typed optional parameter classes. There is actually even a paper about this (http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p1008r0.pdf), which I thought should provide a solution, but apparently not on all compilers, so I'll have to try something else.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:355,modifiability,paramet,parameter,355,"In the latest force push, I tried to fix all remaining compile errors and warnings that the CI brought up. I used fixup commits with interactive rebase to keep the commit history clean and actually make sure the earlier commits compile as well now. Evidently, I have failed to fix the esoteric error with ambiguous overload in the strongly typed optional parameter classes. There is actually even a paper about this (http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p1008r0.pdf), which I thought should provide a solution, but apparently not on all compilers, so I'll have to try something else.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:63,performance,error,errors,63,"In the latest force push, I tried to fix all remaining compile errors and warnings that the CI brought up. I used fixup commits with interactive rebase to keep the commit history clean and actually make sure the earlier commits compile as well now. Evidently, I have failed to fix the esoteric error with ambiguous overload in the strongly typed optional parameter classes. There is actually even a paper about this (http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p1008r0.pdf), which I thought should provide a solution, but apparently not on all compilers, so I'll have to try something else.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:294,performance,error,error,294,"In the latest force push, I tried to fix all remaining compile errors and warnings that the CI brought up. I used fixup commits with interactive rebase to keep the commit history clean and actually make sure the earlier commits compile as well now. Evidently, I have failed to fix the esoteric error with ambiguous overload in the strongly typed optional parameter classes. There is actually even a paper about this (http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p1008r0.pdf), which I thought should provide a solution, but apparently not on all compilers, so I'll have to try something else.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:267,reliability,fail,failed,267,"In the latest force push, I tried to fix all remaining compile errors and warnings that the CI brought up. I used fixup commits with interactive rebase to keep the commit history clean and actually make sure the earlier commits compile as well now. Evidently, I have failed to fix the esoteric error with ambiguous overload in the strongly typed optional parameter classes. There is actually even a paper about this (http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p1008r0.pdf), which I thought should provide a solution, but apparently not on all compilers, so I'll have to try something else.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:63,safety,error,errors,63,"In the latest force push, I tried to fix all remaining compile errors and warnings that the CI brought up. I used fixup commits with interactive rebase to keep the commit history clean and actually make sure the earlier commits compile as well now. Evidently, I have failed to fix the esoteric error with ambiguous overload in the strongly typed optional parameter classes. There is actually even a paper about this (http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p1008r0.pdf), which I thought should provide a solution, but apparently not on all compilers, so I'll have to try something else.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:294,safety,error,error,294,"In the latest force push, I tried to fix all remaining compile errors and warnings that the CI brought up. I used fixup commits with interactive rebase to keep the commit history clean and actually make sure the earlier commits compile as well now. Evidently, I have failed to fix the esoteric error with ambiguous overload in the strongly typed optional parameter classes. There is actually even a paper about this (http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p1008r0.pdf), which I thought should provide a solution, but apparently not on all compilers, so I'll have to try something else.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:63,usability,error,errors,63,"In the latest force push, I tried to fix all remaining compile errors and warnings that the CI brought up. I used fixup commits with interactive rebase to keep the commit history clean and actually make sure the earlier commits compile as well now. Evidently, I have failed to fix the esoteric error with ambiguous overload in the strongly typed optional parameter classes. There is actually even a paper about this (http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p1008r0.pdf), which I thought should provide a solution, but apparently not on all compilers, so I'll have to try something else.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:133,usability,interact,interactive,133,"In the latest force push, I tried to fix all remaining compile errors and warnings that the CI brought up. I used fixup commits with interactive rebase to keep the commit history clean and actually make sure the earlier commits compile as well now. Evidently, I have failed to fix the esoteric error with ambiguous overload in the strongly typed optional parameter classes. There is actually even a paper about this (http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p1008r0.pdf), which I thought should provide a solution, but apparently not on all compilers, so I'll have to try something else.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:294,usability,error,error,294,"In the latest force push, I tried to fix all remaining compile errors and warnings that the CI brought up. I used fixup commits with interactive rebase to keep the commit history clean and actually make sure the earlier commits compile as well now. Evidently, I have failed to fix the esoteric error with ambiguous overload in the strongly typed optional parameter classes. There is actually even a paper about this (http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p1008r0.pdf), which I thought should provide a solution, but apparently not on all compilers, so I'll have to try something else.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:11,deployability,build,build,11,@phsft-bot build just on windows10/cxx14,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:11,deployability,build,build,11,@phsft-bot build,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:191,availability,state,statement,191,"The latest force push should fix build configuration on Windows, which was the only remaining failing build. The cause seems to have been the extra `m` in `LIBRARIES` in the `ROOT_ADD_GTEST` statement for `testLikelihoodSerial`.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:33,deployability,build,build,33,"The latest force push should fix build configuration on Windows, which was the only remaining failing build. The cause seems to have been the extra `m` in `LIBRARIES` in the `ROOT_ADD_GTEST` statement for `testLikelihoodSerial`.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:39,deployability,configurat,configuration,39,"The latest force push should fix build configuration on Windows, which was the only remaining failing build. The cause seems to have been the extra `m` in `LIBRARIES` in the `ROOT_ADD_GTEST` statement for `testLikelihoodSerial`.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:94,deployability,fail,failing,94,"The latest force push should fix build configuration on Windows, which was the only remaining failing build. The cause seems to have been the extra `m` in `LIBRARIES` in the `ROOT_ADD_GTEST` statement for `testLikelihoodSerial`.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:102,deployability,build,build,102,"The latest force push should fix build configuration on Windows, which was the only remaining failing build. The cause seems to have been the extra `m` in `LIBRARIES` in the `ROOT_ADD_GTEST` statement for `testLikelihoodSerial`.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:39,integrability,configur,configuration,39,"The latest force push should fix build configuration on Windows, which was the only remaining failing build. The cause seems to have been the extra `m` in `LIBRARIES` in the `ROOT_ADD_GTEST` statement for `testLikelihoodSerial`.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:191,integrability,state,statement,191,"The latest force push should fix build configuration on Windows, which was the only remaining failing build. The cause seems to have been the extra `m` in `LIBRARIES` in the `ROOT_ADD_GTEST` statement for `testLikelihoodSerial`.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:39,modifiability,configur,configuration,39,"The latest force push should fix build configuration on Windows, which was the only remaining failing build. The cause seems to have been the extra `m` in `LIBRARIES` in the `ROOT_ADD_GTEST` statement for `testLikelihoodSerial`.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:94,reliability,fail,failing,94,"The latest force push should fix build configuration on Windows, which was the only remaining failing build. The cause seems to have been the extra `m` in `LIBRARIES` in the `ROOT_ADD_GTEST` statement for `testLikelihoodSerial`.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:206,safety,test,testLikelihoodSerial,206,"The latest force push should fix build configuration on Windows, which was the only remaining failing build. The cause seems to have been the extra `m` in `LIBRARIES` in the `ROOT_ADD_GTEST` statement for `testLikelihoodSerial`.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:39,security,configur,configuration,39,"The latest force push should fix build configuration on Windows, which was the only remaining failing build. The cause seems to have been the extra `m` in `LIBRARIES` in the `ROOT_ADD_GTEST` statement for `testLikelihoodSerial`.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:206,testability,test,testLikelihoodSerial,206,"The latest force push should fix build configuration on Windows, which was the only remaining failing build. The cause seems to have been the extra `m` in `LIBRARIES` in the `ROOT_ADD_GTEST` statement for `testLikelihoodSerial`.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:11,deployability,build,build,11,@phsft-bot build,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:11,deployability,build,build,11,@phsft-bot build just on windows10/cxx14,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:11,deployability,build,build,11,@phsft-bot build just on windows10/default,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:77,availability,failur,failures,77,"Fixed the segfault in the RooRealL tests above, now only the `RooFormulaVar` failures remaining.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:77,deployability,fail,failures,77,"Fixed the segfault in the RooRealL tests above, now only the `RooFormulaVar` failures remaining.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:77,performance,failur,failures,77,"Fixed the segfault in the RooRealL tests above, now only the `RooFormulaVar` failures remaining.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:77,reliability,fail,failures,77,"Fixed the segfault in the RooRealL tests above, now only the `RooFormulaVar` failures remaining.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:35,safety,test,tests,35,"Fixed the segfault in the RooRealL tests above, now only the `RooFormulaVar` failures remaining.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:35,testability,test,tests,35,"Fixed the segfault in the RooRealL tests above, now only the `RooFormulaVar` failures remaining.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:44,reliability,doe,doesn,44,Disabled in Windows a test in RooRealL that doesn't work under Windows.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:22,safety,test,test,22,Disabled in Windows a test in RooRealL that doesn't work under Windows.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:22,testability,test,test,22,Disabled in Windows a test in RooRealL that doesn't work under Windows.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:384,integrability,batch,batch,384,"All remaining CI issues are not caused by this PR. If someone wants to review again, feel free. If not, I think we can merge this now. Note that I tried to keep commit history clean and used fixup commits + rebase to also make sure all commits compile and pass tests so it should be possible to merge without squashing. Since I made a few changes in the calculation area (Kahan sums, batch computation) which could affect precision and hence results (and hence tests) it would be nice if the commit history of this PR could be included fully.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:384,performance,batch,batch,384,"All remaining CI issues are not caused by this PR. If someone wants to review again, feel free. If not, I think we can merge this now. Note that I tried to keep commit history clean and used fixup commits + rebase to also make sure all commits compile and pass tests so it should be possible to merge without squashing. Since I made a few changes in the calculation area (Kahan sums, batch computation) which could affect precision and hence results (and hence tests) it would be nice if the commit history of this PR could be included fully.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:71,safety,review,review,71,"All remaining CI issues are not caused by this PR. If someone wants to review again, feel free. If not, I think we can merge this now. Note that I tried to keep commit history clean and used fixup commits + rebase to also make sure all commits compile and pass tests so it should be possible to merge without squashing. Since I made a few changes in the calculation area (Kahan sums, batch computation) which could affect precision and hence results (and hence tests) it would be nice if the commit history of this PR could be included fully.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:261,safety,test,tests,261,"All remaining CI issues are not caused by this PR. If someone wants to review again, feel free. If not, I think we can merge this now. Note that I tried to keep commit history clean and used fixup commits + rebase to also make sure all commits compile and pass tests so it should be possible to merge without squashing. Since I made a few changes in the calculation area (Kahan sums, batch computation) which could affect precision and hence results (and hence tests) it would be nice if the commit history of this PR could be included fully.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:461,safety,test,tests,461,"All remaining CI issues are not caused by this PR. If someone wants to review again, feel free. If not, I think we can merge this now. Note that I tried to keep commit history clean and used fixup commits + rebase to also make sure all commits compile and pass tests so it should be possible to merge without squashing. Since I made a few changes in the calculation area (Kahan sums, batch computation) which could affect precision and hence results (and hence tests) it would be nice if the commit history of this PR could be included fully.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:71,testability,review,review,71,"All remaining CI issues are not caused by this PR. If someone wants to review again, feel free. If not, I think we can merge this now. Note that I tried to keep commit history clean and used fixup commits + rebase to also make sure all commits compile and pass tests so it should be possible to merge without squashing. Since I made a few changes in the calculation area (Kahan sums, batch computation) which could affect precision and hence results (and hence tests) it would be nice if the commit history of this PR could be included fully.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:261,testability,test,tests,261,"All remaining CI issues are not caused by this PR. If someone wants to review again, feel free. If not, I think we can merge this now. Note that I tried to keep commit history clean and used fixup commits + rebase to also make sure all commits compile and pass tests so it should be possible to merge without squashing. Since I made a few changes in the calculation area (Kahan sums, batch computation) which could affect precision and hence results (and hence tests) it would be nice if the commit history of this PR could be included fully.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:461,testability,test,tests,461,"All remaining CI issues are not caused by this PR. If someone wants to review again, feel free. If not, I think we can merge this now. Note that I tried to keep commit history clean and used fixup commits + rebase to also make sure all commits compile and pass tests so it should be possible to merge without squashing. Since I made a few changes in the calculation area (Kahan sums, batch computation) which could affect precision and hence results (and hence tests) it would be nice if the commit history of this PR could be included fully.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:11,deployability,build,build,11,@phsft-bot build with flags -DCTEST_TEST_EXCLUDE_NONE=On,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:26,deployability,updat,update,26,Thank you Patrick for the update. I am testing it now again with all tests and tutorials,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:26,safety,updat,update,26,Thank you Patrick for the update. I am testing it now again with all tests and tutorials,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:39,safety,test,testing,39,Thank you Patrick for the update. I am testing it now again with all tests and tutorials,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:69,safety,test,tests,69,Thank you Patrick for the update. I am testing it now again with all tests and tutorials,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:26,security,updat,update,26,Thank you Patrick for the update. I am testing it now again with all tests and tutorials,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:39,testability,test,testing,39,Thank you Patrick for the update. I am testing it now again with all tests and tutorials,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8700:69,testability,test,tests,69,Thank you Patrick for the update. I am testing it now again with all tests and tutorials,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8700
https://github.com/root-project/root/pull/8703:1413,availability,servic,service,1413,"## DeepCode's analysis on [#9dbf29](https://www.deepcode.ai/app/gh/root-project/root/55a7841e24ac8a65d92e996f85449c0027424b64/root-project/root/9dbf2954c51e5553d077ad04840e0287c4072004/pr/_/%2F/code/?utm_source=gh_review&c=0&w=1&i=0&) found:. - :warning: **1** warning :point_down:. ## Top issues. <table>. <thead>. <tr>. <th align=""left"">Description</th>. <th align=""left"">Example fixes</th>. </tr>. </thead>. <tbody>. <tr>. <td width=""77%"">Potential nullptr dereference. Null flows from nullptr literal. Consider adding a check. Occurrences: <ul><li><a href=""https://github.com/root-project/root/blob/9dbf2954c51e5553d077ad04840e0287c4072004/tree/ntuple/v7/inc/ROOT/RColumn.hxx#L91"">RColumn.hxx:91</a></li></ul></td>. <td width=""23%""><a href=""https://www.deepcode.ai/app/gh/root-project/root/55a7841e24ac8a65d92e996f85449c0027424b64/root-project/root/9dbf2954c51e5553d077ad04840e0287c4072004/pr/_/%2Ftree%2Fntuple%2Fv7%2Finc%2FROOT%2FRColumn.hxx/cpp%2Fdc%2FDerefOfNull/code/?utm_source=gh_review&"">:wrench: Example fixes</a></td>. </tr>. </tbody>. </table>. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/55a7841e24ac8a65d92e996f85449c0027424b64/root-project/root/9dbf2954c51e5553d077ad04840e0287c4072004/pr/_/%2F/code/?utm_source=gh_review&c=0&w=1&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:1413,deployability,servic,service,1413,"## DeepCode's analysis on [#9dbf29](https://www.deepcode.ai/app/gh/root-project/root/55a7841e24ac8a65d92e996f85449c0027424b64/root-project/root/9dbf2954c51e5553d077ad04840e0287c4072004/pr/_/%2F/code/?utm_source=gh_review&c=0&w=1&i=0&) found:. - :warning: **1** warning :point_down:. ## Top issues. <table>. <thead>. <tr>. <th align=""left"">Description</th>. <th align=""left"">Example fixes</th>. </tr>. </thead>. <tbody>. <tr>. <td width=""77%"">Potential nullptr dereference. Null flows from nullptr literal. Consider adding a check. Occurrences: <ul><li><a href=""https://github.com/root-project/root/blob/9dbf2954c51e5553d077ad04840e0287c4072004/tree/ntuple/v7/inc/ROOT/RColumn.hxx#L91"">RColumn.hxx:91</a></li></ul></td>. <td width=""23%""><a href=""https://www.deepcode.ai/app/gh/root-project/root/55a7841e24ac8a65d92e996f85449c0027424b64/root-project/root/9dbf2954c51e5553d077ad04840e0287c4072004/pr/_/%2Ftree%2Fntuple%2Fv7%2Finc%2FROOT%2FRColumn.hxx/cpp%2Fdc%2FDerefOfNull/code/?utm_source=gh_review&"">:wrench: Example fixes</a></td>. </tr>. </tbody>. </table>. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/55a7841e24ac8a65d92e996f85449c0027424b64/root-project/root/9dbf2954c51e5553d077ad04840e0287c4072004/pr/_/%2F/code/?utm_source=gh_review&c=0&w=1&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:1425,deployability,API,API,1425,"## DeepCode's analysis on [#9dbf29](https://www.deepcode.ai/app/gh/root-project/root/55a7841e24ac8a65d92e996f85449c0027424b64/root-project/root/9dbf2954c51e5553d077ad04840e0287c4072004/pr/_/%2F/code/?utm_source=gh_review&c=0&w=1&i=0&) found:. - :warning: **1** warning :point_down:. ## Top issues. <table>. <thead>. <tr>. <th align=""left"">Description</th>. <th align=""left"">Example fixes</th>. </tr>. </thead>. <tbody>. <tr>. <td width=""77%"">Potential nullptr dereference. Null flows from nullptr literal. Consider adding a check. Occurrences: <ul><li><a href=""https://github.com/root-project/root/blob/9dbf2954c51e5553d077ad04840e0287c4072004/tree/ntuple/v7/inc/ROOT/RColumn.hxx#L91"">RColumn.hxx:91</a></li></ul></td>. <td width=""23%""><a href=""https://www.deepcode.ai/app/gh/root-project/root/55a7841e24ac8a65d92e996f85449c0027424b64/root-project/root/9dbf2954c51e5553d077ad04840e0287c4072004/pr/_/%2Ftree%2Fntuple%2Fv7%2Finc%2FROOT%2FRColumn.hxx/cpp%2Fdc%2FDerefOfNull/code/?utm_source=gh_review&"">:wrench: Example fixes</a></td>. </tr>. </tbody>. </table>. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/55a7841e24ac8a65d92e996f85449c0027424b64/root-project/root/9dbf2954c51e5553d077ad04840e0287c4072004/pr/_/%2F/code/?utm_source=gh_review&c=0&w=1&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:1413,integrability,servic,service,1413,"## DeepCode's analysis on [#9dbf29](https://www.deepcode.ai/app/gh/root-project/root/55a7841e24ac8a65d92e996f85449c0027424b64/root-project/root/9dbf2954c51e5553d077ad04840e0287c4072004/pr/_/%2F/code/?utm_source=gh_review&c=0&w=1&i=0&) found:. - :warning: **1** warning :point_down:. ## Top issues. <table>. <thead>. <tr>. <th align=""left"">Description</th>. <th align=""left"">Example fixes</th>. </tr>. </thead>. <tbody>. <tr>. <td width=""77%"">Potential nullptr dereference. Null flows from nullptr literal. Consider adding a check. Occurrences: <ul><li><a href=""https://github.com/root-project/root/blob/9dbf2954c51e5553d077ad04840e0287c4072004/tree/ntuple/v7/inc/ROOT/RColumn.hxx#L91"">RColumn.hxx:91</a></li></ul></td>. <td width=""23%""><a href=""https://www.deepcode.ai/app/gh/root-project/root/55a7841e24ac8a65d92e996f85449c0027424b64/root-project/root/9dbf2954c51e5553d077ad04840e0287c4072004/pr/_/%2Ftree%2Fntuple%2Fv7%2Finc%2FROOT%2FRColumn.hxx/cpp%2Fdc%2FDerefOfNull/code/?utm_source=gh_review&"">:wrench: Example fixes</a></td>. </tr>. </tbody>. </table>. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/55a7841e24ac8a65d92e996f85449c0027424b64/root-project/root/9dbf2954c51e5553d077ad04840e0287c4072004/pr/_/%2F/code/?utm_source=gh_review&c=0&w=1&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:1425,integrability,API,API,1425,"## DeepCode's analysis on [#9dbf29](https://www.deepcode.ai/app/gh/root-project/root/55a7841e24ac8a65d92e996f85449c0027424b64/root-project/root/9dbf2954c51e5553d077ad04840e0287c4072004/pr/_/%2F/code/?utm_source=gh_review&c=0&w=1&i=0&) found:. - :warning: **1** warning :point_down:. ## Top issues. <table>. <thead>. <tr>. <th align=""left"">Description</th>. <th align=""left"">Example fixes</th>. </tr>. </thead>. <tbody>. <tr>. <td width=""77%"">Potential nullptr dereference. Null flows from nullptr literal. Consider adding a check. Occurrences: <ul><li><a href=""https://github.com/root-project/root/blob/9dbf2954c51e5553d077ad04840e0287c4072004/tree/ntuple/v7/inc/ROOT/RColumn.hxx#L91"">RColumn.hxx:91</a></li></ul></td>. <td width=""23%""><a href=""https://www.deepcode.ai/app/gh/root-project/root/55a7841e24ac8a65d92e996f85449c0027424b64/root-project/root/9dbf2954c51e5553d077ad04840e0287c4072004/pr/_/%2Ftree%2Fntuple%2Fv7%2Finc%2FROOT%2FRColumn.hxx/cpp%2Fdc%2FDerefOfNull/code/?utm_source=gh_review&"">:wrench: Example fixes</a></td>. </tr>. </tbody>. </table>. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/55a7841e24ac8a65d92e996f85449c0027424b64/root-project/root/9dbf2954c51e5553d077ad04840e0287c4072004/pr/_/%2F/code/?utm_source=gh_review&c=0&w=1&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:1425,interoperability,API,API,1425,"## DeepCode's analysis on [#9dbf29](https://www.deepcode.ai/app/gh/root-project/root/55a7841e24ac8a65d92e996f85449c0027424b64/root-project/root/9dbf2954c51e5553d077ad04840e0287c4072004/pr/_/%2F/code/?utm_source=gh_review&c=0&w=1&i=0&) found:. - :warning: **1** warning :point_down:. ## Top issues. <table>. <thead>. <tr>. <th align=""left"">Description</th>. <th align=""left"">Example fixes</th>. </tr>. </thead>. <tbody>. <tr>. <td width=""77%"">Potential nullptr dereference. Null flows from nullptr literal. Consider adding a check. Occurrences: <ul><li><a href=""https://github.com/root-project/root/blob/9dbf2954c51e5553d077ad04840e0287c4072004/tree/ntuple/v7/inc/ROOT/RColumn.hxx#L91"">RColumn.hxx:91</a></li></ul></td>. <td width=""23%""><a href=""https://www.deepcode.ai/app/gh/root-project/root/55a7841e24ac8a65d92e996f85449c0027424b64/root-project/root/9dbf2954c51e5553d077ad04840e0287c4072004/pr/_/%2Ftree%2Fntuple%2Fv7%2Finc%2FROOT%2FRColumn.hxx/cpp%2Fdc%2FDerefOfNull/code/?utm_source=gh_review&"">:wrench: Example fixes</a></td>. </tr>. </tbody>. </table>. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/55a7841e24ac8a65d92e996f85449c0027424b64/root-project/root/9dbf2954c51e5553d077ad04840e0287c4072004/pr/_/%2F/code/?utm_source=gh_review&c=0&w=1&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:1615,interoperability,plug,plugins,1615,"## DeepCode's analysis on [#9dbf29](https://www.deepcode.ai/app/gh/root-project/root/55a7841e24ac8a65d92e996f85449c0027424b64/root-project/root/9dbf2954c51e5553d077ad04840e0287c4072004/pr/_/%2F/code/?utm_source=gh_review&c=0&w=1&i=0&) found:. - :warning: **1** warning :point_down:. ## Top issues. <table>. <thead>. <tr>. <th align=""left"">Description</th>. <th align=""left"">Example fixes</th>. </tr>. </thead>. <tbody>. <tr>. <td width=""77%"">Potential nullptr dereference. Null flows from nullptr literal. Consider adding a check. Occurrences: <ul><li><a href=""https://github.com/root-project/root/blob/9dbf2954c51e5553d077ad04840e0287c4072004/tree/ntuple/v7/inc/ROOT/RColumn.hxx#L91"">RColumn.hxx:91</a></li></ul></td>. <td width=""23%""><a href=""https://www.deepcode.ai/app/gh/root-project/root/55a7841e24ac8a65d92e996f85449c0027424b64/root-project/root/9dbf2954c51e5553d077ad04840e0287c4072004/pr/_/%2Ftree%2Fntuple%2Fv7%2Finc%2FROOT%2FRColumn.hxx/cpp%2Fdc%2FDerefOfNull/code/?utm_source=gh_review&"">:wrench: Example fixes</a></td>. </tr>. </tbody>. </table>. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/55a7841e24ac8a65d92e996f85449c0027424b64/root-project/root/9dbf2954c51e5553d077ad04840e0287c4072004/pr/_/%2F/code/?utm_source=gh_review&c=0&w=1&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:1687,interoperability,plug,plugin,1687,"## DeepCode's analysis on [#9dbf29](https://www.deepcode.ai/app/gh/root-project/root/55a7841e24ac8a65d92e996f85449c0027424b64/root-project/root/9dbf2954c51e5553d077ad04840e0287c4072004/pr/_/%2F/code/?utm_source=gh_review&c=0&w=1&i=0&) found:. - :warning: **1** warning :point_down:. ## Top issues. <table>. <thead>. <tr>. <th align=""left"">Description</th>. <th align=""left"">Example fixes</th>. </tr>. </thead>. <tbody>. <tr>. <td width=""77%"">Potential nullptr dereference. Null flows from nullptr literal. Consider adding a check. Occurrences: <ul><li><a href=""https://github.com/root-project/root/blob/9dbf2954c51e5553d077ad04840e0287c4072004/tree/ntuple/v7/inc/ROOT/RColumn.hxx#L91"">RColumn.hxx:91</a></li></ul></td>. <td width=""23%""><a href=""https://www.deepcode.ai/app/gh/root-project/root/55a7841e24ac8a65d92e996f85449c0027424b64/root-project/root/9dbf2954c51e5553d077ad04840e0287c4072004/pr/_/%2Ftree%2Fntuple%2Fv7%2Finc%2FROOT%2FRColumn.hxx/cpp%2Fdc%2FDerefOfNull/code/?utm_source=gh_review&"">:wrench: Example fixes</a></td>. </tr>. </tbody>. </table>. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/55a7841e24ac8a65d92e996f85449c0027424b64/root-project/root/9dbf2954c51e5553d077ad04840e0287c4072004/pr/_/%2F/code/?utm_source=gh_review&c=0&w=1&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:1738,interoperability,plug,plugin,1738,"## DeepCode's analysis on [#9dbf29](https://www.deepcode.ai/app/gh/root-project/root/55a7841e24ac8a65d92e996f85449c0027424b64/root-project/root/9dbf2954c51e5553d077ad04840e0287c4072004/pr/_/%2F/code/?utm_source=gh_review&c=0&w=1&i=0&) found:. - :warning: **1** warning :point_down:. ## Top issues. <table>. <thead>. <tr>. <th align=""left"">Description</th>. <th align=""left"">Example fixes</th>. </tr>. </thead>. <tbody>. <tr>. <td width=""77%"">Potential nullptr dereference. Null flows from nullptr literal. Consider adding a check. Occurrences: <ul><li><a href=""https://github.com/root-project/root/blob/9dbf2954c51e5553d077ad04840e0287c4072004/tree/ntuple/v7/inc/ROOT/RColumn.hxx#L91"">RColumn.hxx:91</a></li></ul></td>. <td width=""23%""><a href=""https://www.deepcode.ai/app/gh/root-project/root/55a7841e24ac8a65d92e996f85449c0027424b64/root-project/root/9dbf2954c51e5553d077ad04840e0287c4072004/pr/_/%2Ftree%2Fntuple%2Fv7%2Finc%2FROOT%2FRColumn.hxx/cpp%2Fdc%2FDerefOfNull/code/?utm_source=gh_review&"">:wrench: Example fixes</a></td>. </tr>. </tbody>. </table>. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/55a7841e24ac8a65d92e996f85449c0027424b64/root-project/root/9dbf2954c51e5553d077ad04840e0287c4072004/pr/_/%2F/code/?utm_source=gh_review&c=0&w=1&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:1413,modifiability,servic,service,1413,"## DeepCode's analysis on [#9dbf29](https://www.deepcode.ai/app/gh/root-project/root/55a7841e24ac8a65d92e996f85449c0027424b64/root-project/root/9dbf2954c51e5553d077ad04840e0287c4072004/pr/_/%2F/code/?utm_source=gh_review&c=0&w=1&i=0&) found:. - :warning: **1** warning :point_down:. ## Top issues. <table>. <thead>. <tr>. <th align=""left"">Description</th>. <th align=""left"">Example fixes</th>. </tr>. </thead>. <tbody>. <tr>. <td width=""77%"">Potential nullptr dereference. Null flows from nullptr literal. Consider adding a check. Occurrences: <ul><li><a href=""https://github.com/root-project/root/blob/9dbf2954c51e5553d077ad04840e0287c4072004/tree/ntuple/v7/inc/ROOT/RColumn.hxx#L91"">RColumn.hxx:91</a></li></ul></td>. <td width=""23%""><a href=""https://www.deepcode.ai/app/gh/root-project/root/55a7841e24ac8a65d92e996f85449c0027424b64/root-project/root/9dbf2954c51e5553d077ad04840e0287c4072004/pr/_/%2Ftree%2Fntuple%2Fv7%2Finc%2FROOT%2FRColumn.hxx/cpp%2Fdc%2FDerefOfNull/code/?utm_source=gh_review&"">:wrench: Example fixes</a></td>. </tr>. </tbody>. </table>. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/55a7841e24ac8a65d92e996f85449c0027424b64/root-project/root/9dbf2954c51e5553d077ad04840e0287c4072004/pr/_/%2F/code/?utm_source=gh_review&c=0&w=1&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:90,availability,cluster,cluster,90,> It would be good to have the PR description as documentation somewhere. Maybe split for cluster and page flushing? Added in [dc4f48d](https://github.com/root-project/root/pull/8703/commits/dc4f48d576d3d3ed7e4753e542f5714eb21ce4ed),MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:90,deployability,cluster,cluster,90,> It would be good to have the PR description as documentation somewhere. Maybe split for cluster and page flushing? Added in [dc4f48d](https://github.com/root-project/root/pull/8703/commits/dc4f48d576d3d3ed7e4753e542f5714eb21ce4ed),MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:49,usability,document,documentation,49,> It would be good to have the PR description as documentation somewhere. Maybe split for cluster and page flushing? Added in [dc4f48d](https://github.com/root-project/root/pull/8703/commits/dc4f48d576d3d3ed7e4753e542f5714eb21ce4ed),MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:67,safety,test,tests,67,Many thanks @Axel-Naumann! I restructured the code a bit and added tests for corner cases.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:67,testability,test,tests,67,Many thanks @Axel-Naumann! I restructured the code a bit and added tests for corner cases.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:16,availability,cluster,clusters,16,> The following clusters use the compression ratio of the last cluster as estimate. This is somewhat 'unstable' (for example if there an alternation of high compression data and low compression data). Did you consider using the average compression (for the column) so far?,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:63,availability,cluster,cluster,63,> The following clusters use the compression ratio of the last cluster as estimate. This is somewhat 'unstable' (for example if there an alternation of high compression data and low compression data). Did you consider using the average compression (for the column) so far?,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:16,deployability,cluster,clusters,16,> The following clusters use the compression ratio of the last cluster as estimate. This is somewhat 'unstable' (for example if there an alternation of high compression data and low compression data). Did you consider using the average compression (for the column) so far?,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:63,deployability,cluster,cluster,63,> The following clusters use the compression ratio of the last cluster as estimate. This is somewhat 'unstable' (for example if there an alternation of high compression data and low compression data). Did you consider using the average compression (for the column) so far?,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:74,energy efficiency,estimat,estimate,74,> The following clusters use the compression ratio of the last cluster as estimate. This is somewhat 'unstable' (for example if there an alternation of high compression data and low compression data). Did you consider using the average compression (for the column) so far?,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:24,integrability,buffer,buffers,24,"> writing uses two page buffers. if I understand correctly, this means that the memory requirement/use is `2 * 64Kib * number_of_columns` (not including the extra scratch space for compression) ... so for large (but not uncommon) data set (10000 columns) this can reach in the GiB, right?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:80,performance,memor,memory,80,"> writing uses two page buffers. if I understand correctly, this means that the memory requirement/use is `2 * 64Kib * number_of_columns` (not including the extra scratch space for compression) ... so for large (but not uncommon) data set (10000 columns) this can reach in the GiB, right?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:38,testability,understand,understand,38,"> writing uses two page buffers. if I understand correctly, this means that the memory requirement/use is `2 * 64Kib * number_of_columns` (not including the extra scratch space for compression) ... so for large (but not uncommon) data set (10000 columns) this can reach in the GiB, right?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:80,usability,memor,memory,80,"> writing uses two page buffers. if I understand correctly, this means that the memory requirement/use is `2 * 64Kib * number_of_columns` (not including the extra scratch space for compression) ... so for large (but not uncommon) data set (10000 columns) this can reach in the GiB, right?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:11,deployability,build,build,11,@phsft-bot build,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:26,integrability,buffer,buffers,26,"> > writing uses two page buffers. > . > if I understand correctly, this means that the memory requirement/use is `2 * 64Kib * number_of_columns` (not including the extra scratch space for compression) ... so for large (but not uncommon) data set (10000 columns) this can reach in the GiB, right? Yes, I think that's correct.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:88,performance,memor,memory,88,"> > writing uses two page buffers. > . > if I understand correctly, this means that the memory requirement/use is `2 * 64Kib * number_of_columns` (not including the extra scratch space for compression) ... so for large (but not uncommon) data set (10000 columns) this can reach in the GiB, right? Yes, I think that's correct.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:46,testability,understand,understand,46,"> > writing uses two page buffers. > . > if I understand correctly, this means that the memory requirement/use is `2 * 64Kib * number_of_columns` (not including the extra scratch space for compression) ... so for large (but not uncommon) data set (10000 columns) this can reach in the GiB, right? Yes, I think that's correct.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:88,usability,memor,memory,88,"> > writing uses two page buffers. > . > if I understand correctly, this means that the memory requirement/use is `2 * 64Kib * number_of_columns` (not including the extra scratch space for compression) ... so for large (but not uncommon) data set (10000 columns) this can reach in the GiB, right? Yes, I think that's correct.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:18,availability,cluster,clusters,18,"> > The following clusters use the compression ratio of the last cluster as estimate. > . > This is somewhat 'unstable' (for example if there an alternation of high compression data and low compression data). Did you consider using the average compression (for the column) so far? You mean keeping the average of the last $x$ MB written per column? That would be possible and more precise but quite some overhead: after each event, we'd need to calculate the total compression ratio from combining all the columns. I think if the compression ratio approximation is a little unstable, it doesn't hurt because we have to make the cluster cut mark at an event boundary, so we'll anyway very rarely be exactly at 50MB. It might be a problem if we have vastly different compression ratios between clusters. If we should expect that, I think we can address it by a larger look-behind window, e.g. we can average the compression ratio over the last 3 or 5 clusters.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:65,availability,cluster,cluster,65,"> > The following clusters use the compression ratio of the last cluster as estimate. > . > This is somewhat 'unstable' (for example if there an alternation of high compression data and low compression data). Did you consider using the average compression (for the column) so far? You mean keeping the average of the last $x$ MB written per column? That would be possible and more precise but quite some overhead: after each event, we'd need to calculate the total compression ratio from combining all the columns. I think if the compression ratio approximation is a little unstable, it doesn't hurt because we have to make the cluster cut mark at an event boundary, so we'll anyway very rarely be exactly at 50MB. It might be a problem if we have vastly different compression ratios between clusters. If we should expect that, I think we can address it by a larger look-behind window, e.g. we can average the compression ratio over the last 3 or 5 clusters.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:628,availability,cluster,cluster,628,"> > The following clusters use the compression ratio of the last cluster as estimate. > . > This is somewhat 'unstable' (for example if there an alternation of high compression data and low compression data). Did you consider using the average compression (for the column) so far? You mean keeping the average of the last $x$ MB written per column? That would be possible and more precise but quite some overhead: after each event, we'd need to calculate the total compression ratio from combining all the columns. I think if the compression ratio approximation is a little unstable, it doesn't hurt because we have to make the cluster cut mark at an event boundary, so we'll anyway very rarely be exactly at 50MB. It might be a problem if we have vastly different compression ratios between clusters. If we should expect that, I think we can address it by a larger look-behind window, e.g. we can average the compression ratio over the last 3 or 5 clusters.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:792,availability,cluster,clusters,792,"> > The following clusters use the compression ratio of the last cluster as estimate. > . > This is somewhat 'unstable' (for example if there an alternation of high compression data and low compression data). Did you consider using the average compression (for the column) so far? You mean keeping the average of the last $x$ MB written per column? That would be possible and more precise but quite some overhead: after each event, we'd need to calculate the total compression ratio from combining all the columns. I think if the compression ratio approximation is a little unstable, it doesn't hurt because we have to make the cluster cut mark at an event boundary, so we'll anyway very rarely be exactly at 50MB. It might be a problem if we have vastly different compression ratios between clusters. If we should expect that, I think we can address it by a larger look-behind window, e.g. we can average the compression ratio over the last 3 or 5 clusters.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:949,availability,cluster,clusters,949,"> > The following clusters use the compression ratio of the last cluster as estimate. > . > This is somewhat 'unstable' (for example if there an alternation of high compression data and low compression data). Did you consider using the average compression (for the column) so far? You mean keeping the average of the last $x$ MB written per column? That would be possible and more precise but quite some overhead: after each event, we'd need to calculate the total compression ratio from combining all the columns. I think if the compression ratio approximation is a little unstable, it doesn't hurt because we have to make the cluster cut mark at an event boundary, so we'll anyway very rarely be exactly at 50MB. It might be a problem if we have vastly different compression ratios between clusters. If we should expect that, I think we can address it by a larger look-behind window, e.g. we can average the compression ratio over the last 3 or 5 clusters.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:18,deployability,cluster,clusters,18,"> > The following clusters use the compression ratio of the last cluster as estimate. > . > This is somewhat 'unstable' (for example if there an alternation of high compression data and low compression data). Did you consider using the average compression (for the column) so far? You mean keeping the average of the last $x$ MB written per column? That would be possible and more precise but quite some overhead: after each event, we'd need to calculate the total compression ratio from combining all the columns. I think if the compression ratio approximation is a little unstable, it doesn't hurt because we have to make the cluster cut mark at an event boundary, so we'll anyway very rarely be exactly at 50MB. It might be a problem if we have vastly different compression ratios between clusters. If we should expect that, I think we can address it by a larger look-behind window, e.g. we can average the compression ratio over the last 3 or 5 clusters.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:65,deployability,cluster,cluster,65,"> > The following clusters use the compression ratio of the last cluster as estimate. > . > This is somewhat 'unstable' (for example if there an alternation of high compression data and low compression data). Did you consider using the average compression (for the column) so far? You mean keeping the average of the last $x$ MB written per column? That would be possible and more precise but quite some overhead: after each event, we'd need to calculate the total compression ratio from combining all the columns. I think if the compression ratio approximation is a little unstable, it doesn't hurt because we have to make the cluster cut mark at an event boundary, so we'll anyway very rarely be exactly at 50MB. It might be a problem if we have vastly different compression ratios between clusters. If we should expect that, I think we can address it by a larger look-behind window, e.g. we can average the compression ratio over the last 3 or 5 clusters.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:628,deployability,cluster,cluster,628,"> > The following clusters use the compression ratio of the last cluster as estimate. > . > This is somewhat 'unstable' (for example if there an alternation of high compression data and low compression data). Did you consider using the average compression (for the column) so far? You mean keeping the average of the last $x$ MB written per column? That would be possible and more precise but quite some overhead: after each event, we'd need to calculate the total compression ratio from combining all the columns. I think if the compression ratio approximation is a little unstable, it doesn't hurt because we have to make the cluster cut mark at an event boundary, so we'll anyway very rarely be exactly at 50MB. It might be a problem if we have vastly different compression ratios between clusters. If we should expect that, I think we can address it by a larger look-behind window, e.g. we can average the compression ratio over the last 3 or 5 clusters.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:792,deployability,cluster,clusters,792,"> > The following clusters use the compression ratio of the last cluster as estimate. > . > This is somewhat 'unstable' (for example if there an alternation of high compression data and low compression data). Did you consider using the average compression (for the column) so far? You mean keeping the average of the last $x$ MB written per column? That would be possible and more precise but quite some overhead: after each event, we'd need to calculate the total compression ratio from combining all the columns. I think if the compression ratio approximation is a little unstable, it doesn't hurt because we have to make the cluster cut mark at an event boundary, so we'll anyway very rarely be exactly at 50MB. It might be a problem if we have vastly different compression ratios between clusters. If we should expect that, I think we can address it by a larger look-behind window, e.g. we can average the compression ratio over the last 3 or 5 clusters.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:949,deployability,cluster,clusters,949,"> > The following clusters use the compression ratio of the last cluster as estimate. > . > This is somewhat 'unstable' (for example if there an alternation of high compression data and low compression data). Did you consider using the average compression (for the column) so far? You mean keeping the average of the last $x$ MB written per column? That would be possible and more precise but quite some overhead: after each event, we'd need to calculate the total compression ratio from combining all the columns. I think if the compression ratio approximation is a little unstable, it doesn't hurt because we have to make the cluster cut mark at an event boundary, so we'll anyway very rarely be exactly at 50MB. It might be a problem if we have vastly different compression ratios between clusters. If we should expect that, I think we can address it by a larger look-behind window, e.g. we can average the compression ratio over the last 3 or 5 clusters.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:76,energy efficiency,estimat,estimate,76,"> > The following clusters use the compression ratio of the last cluster as estimate. > . > This is somewhat 'unstable' (for example if there an alternation of high compression data and low compression data). Did you consider using the average compression (for the column) so far? You mean keeping the average of the last $x$ MB written per column? That would be possible and more precise but quite some overhead: after each event, we'd need to calculate the total compression ratio from combining all the columns. I think if the compression ratio approximation is a little unstable, it doesn't hurt because we have to make the cluster cut mark at an event boundary, so we'll anyway very rarely be exactly at 50MB. It might be a problem if we have vastly different compression ratios between clusters. If we should expect that, I think we can address it by a larger look-behind window, e.g. we can average the compression ratio over the last 3 or 5 clusters.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:425,integrability,event,event,425,"> > The following clusters use the compression ratio of the last cluster as estimate. > . > This is somewhat 'unstable' (for example if there an alternation of high compression data and low compression data). Did you consider using the average compression (for the column) so far? You mean keeping the average of the last $x$ MB written per column? That would be possible and more precise but quite some overhead: after each event, we'd need to calculate the total compression ratio from combining all the columns. I think if the compression ratio approximation is a little unstable, it doesn't hurt because we have to make the cluster cut mark at an event boundary, so we'll anyway very rarely be exactly at 50MB. It might be a problem if we have vastly different compression ratios between clusters. If we should expect that, I think we can address it by a larger look-behind window, e.g. we can average the compression ratio over the last 3 or 5 clusters.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:651,integrability,event,event,651,"> > The following clusters use the compression ratio of the last cluster as estimate. > . > This is somewhat 'unstable' (for example if there an alternation of high compression data and low compression data). Did you consider using the average compression (for the column) so far? You mean keeping the average of the last $x$ MB written per column? That would be possible and more precise but quite some overhead: after each event, we'd need to calculate the total compression ratio from combining all the columns. I think if the compression ratio approximation is a little unstable, it doesn't hurt because we have to make the cluster cut mark at an event boundary, so we'll anyway very rarely be exactly at 50MB. It might be a problem if we have vastly different compression ratios between clusters. If we should expect that, I think we can address it by a larger look-behind window, e.g. we can average the compression ratio over the last 3 or 5 clusters.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:404,performance,overhead,overhead,404,"> > The following clusters use the compression ratio of the last cluster as estimate. > . > This is somewhat 'unstable' (for example if there an alternation of high compression data and low compression data). Did you consider using the average compression (for the column) so far? You mean keeping the average of the last $x$ MB written per column? That would be possible and more precise but quite some overhead: after each event, we'd need to calculate the total compression ratio from combining all the columns. I think if the compression ratio approximation is a little unstable, it doesn't hurt because we have to make the cluster cut mark at an event boundary, so we'll anyway very rarely be exactly at 50MB. It might be a problem if we have vastly different compression ratios between clusters. If we should expect that, I think we can address it by a larger look-behind window, e.g. we can average the compression ratio over the last 3 or 5 clusters.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:587,reliability,doe,doesn,587,"> > The following clusters use the compression ratio of the last cluster as estimate. > . > This is somewhat 'unstable' (for example if there an alternation of high compression data and low compression data). Did you consider using the average compression (for the column) so far? You mean keeping the average of the last $x$ MB written per column? That would be possible and more precise but quite some overhead: after each event, we'd need to calculate the total compression ratio from combining all the columns. I think if the compression ratio approximation is a little unstable, it doesn't hurt because we have to make the cluster cut mark at an event boundary, so we'll anyway very rarely be exactly at 50MB. It might be a problem if we have vastly different compression ratios between clusters. If we should expect that, I think we can address it by a larger look-behind window, e.g. we can average the compression ratio over the last 3 or 5 clusters.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:881,availability,cluster,cluster,881,"In reply to: https://github.com/root-project/root/pull/8703#issuecomment-920044106. > You mean keeping the average of the last $x$ MB written per column? . Not quite, I meant simply an overall average, i.e. simply keeping `total_number_of_bytes` and `total_number_of_compressed_bytes` for each column and then average compression ration is a simple division. But ... actually ... closer to the original proposal is simply the ""total_number_of_bytes_in_all_flushed_clusters"" and ""corresponding_compressed_size"" (i.e. a per RNtuple running total). However this makes me thing of another (potential more noticeable) instability. If there is 2 columns; column A with an constant/average compression ration of 100 and column B with a constant/average compression ration of 1 (or 1.1 :) ) but both columns are collections. If for most of the entries (for more proposal) or only the last cluster (for the PR's proposal), column A has a lot of data and column B has very little data, then the compression ration use for prediction will be 100 (ish). If suddenly the cardinality flip/flop and column A has very little data but column B has a lot of data, then the compression ration of that cluster will be 1 but it would still contains `100 * target_cluster_size_in_compressed_bytes`, namely in this case 100 times larger (on disk) than any other cluster. Obviously this is an extreme example but it points to the benefit of using `per column` compression ratio rather than overall compression ratio (whether it is whole-ntuple or last (few) clusters). The cost for implementation seems to be that after each `Fill`, each column would have to increment a (global-per-RNtuple) counter with `data_written / estimated_compression_ratio` which:. * for collection column should be a small over-head. * for single value column should be a large over-head. however conversely, an accurate estimate is ""only important"" for collection column, so maybe an hybrid solution is a workable compromise?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:1182,availability,cluster,cluster,1182,"In reply to: https://github.com/root-project/root/pull/8703#issuecomment-920044106. > You mean keeping the average of the last $x$ MB written per column? . Not quite, I meant simply an overall average, i.e. simply keeping `total_number_of_bytes` and `total_number_of_compressed_bytes` for each column and then average compression ration is a simple division. But ... actually ... closer to the original proposal is simply the ""total_number_of_bytes_in_all_flushed_clusters"" and ""corresponding_compressed_size"" (i.e. a per RNtuple running total). However this makes me thing of another (potential more noticeable) instability. If there is 2 columns; column A with an constant/average compression ration of 100 and column B with a constant/average compression ration of 1 (or 1.1 :) ) but both columns are collections. If for most of the entries (for more proposal) or only the last cluster (for the PR's proposal), column A has a lot of data and column B has very little data, then the compression ration use for prediction will be 100 (ish). If suddenly the cardinality flip/flop and column A has very little data but column B has a lot of data, then the compression ration of that cluster will be 1 but it would still contains `100 * target_cluster_size_in_compressed_bytes`, namely in this case 100 times larger (on disk) than any other cluster. Obviously this is an extreme example but it points to the benefit of using `per column` compression ratio rather than overall compression ratio (whether it is whole-ntuple or last (few) clusters). The cost for implementation seems to be that after each `Fill`, each column would have to increment a (global-per-RNtuple) counter with `data_written / estimated_compression_ratio` which:. * for collection column should be a small over-head. * for single value column should be a large over-head. however conversely, an accurate estimate is ""only important"" for collection column, so maybe an hybrid solution is a workable compromise?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:1339,availability,cluster,cluster,1339,"In reply to: https://github.com/root-project/root/pull/8703#issuecomment-920044106. > You mean keeping the average of the last $x$ MB written per column? . Not quite, I meant simply an overall average, i.e. simply keeping `total_number_of_bytes` and `total_number_of_compressed_bytes` for each column and then average compression ration is a simple division. But ... actually ... closer to the original proposal is simply the ""total_number_of_bytes_in_all_flushed_clusters"" and ""corresponding_compressed_size"" (i.e. a per RNtuple running total). However this makes me thing of another (potential more noticeable) instability. If there is 2 columns; column A with an constant/average compression ration of 100 and column B with a constant/average compression ration of 1 (or 1.1 :) ) but both columns are collections. If for most of the entries (for more proposal) or only the last cluster (for the PR's proposal), column A has a lot of data and column B has very little data, then the compression ration use for prediction will be 100 (ish). If suddenly the cardinality flip/flop and column A has very little data but column B has a lot of data, then the compression ration of that cluster will be 1 but it would still contains `100 * target_cluster_size_in_compressed_bytes`, namely in this case 100 times larger (on disk) than any other cluster. Obviously this is an extreme example but it points to the benefit of using `per column` compression ratio rather than overall compression ratio (whether it is whole-ntuple or last (few) clusters). The cost for implementation seems to be that after each `Fill`, each column would have to increment a (global-per-RNtuple) counter with `data_written / estimated_compression_ratio` which:. * for collection column should be a small over-head. * for single value column should be a large over-head. however conversely, an accurate estimate is ""only important"" for collection column, so maybe an hybrid solution is a workable compromise?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:1534,availability,cluster,clusters,1534,"In reply to: https://github.com/root-project/root/pull/8703#issuecomment-920044106. > You mean keeping the average of the last $x$ MB written per column? . Not quite, I meant simply an overall average, i.e. simply keeping `total_number_of_bytes` and `total_number_of_compressed_bytes` for each column and then average compression ration is a simple division. But ... actually ... closer to the original proposal is simply the ""total_number_of_bytes_in_all_flushed_clusters"" and ""corresponding_compressed_size"" (i.e. a per RNtuple running total). However this makes me thing of another (potential more noticeable) instability. If there is 2 columns; column A with an constant/average compression ration of 100 and column B with a constant/average compression ration of 1 (or 1.1 :) ) but both columns are collections. If for most of the entries (for more proposal) or only the last cluster (for the PR's proposal), column A has a lot of data and column B has very little data, then the compression ration use for prediction will be 100 (ish). If suddenly the cardinality flip/flop and column A has very little data but column B has a lot of data, then the compression ration of that cluster will be 1 but it would still contains `100 * target_cluster_size_in_compressed_bytes`, namely in this case 100 times larger (on disk) than any other cluster. Obviously this is an extreme example but it points to the benefit of using `per column` compression ratio rather than overall compression ratio (whether it is whole-ntuple or last (few) clusters). The cost for implementation seems to be that after each `Fill`, each column would have to increment a (global-per-RNtuple) counter with `data_written / estimated_compression_ratio` which:. * for collection column should be a small over-head. * for single value column should be a large over-head. however conversely, an accurate estimate is ""only important"" for collection column, so maybe an hybrid solution is a workable compromise?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:881,deployability,cluster,cluster,881,"In reply to: https://github.com/root-project/root/pull/8703#issuecomment-920044106. > You mean keeping the average of the last $x$ MB written per column? . Not quite, I meant simply an overall average, i.e. simply keeping `total_number_of_bytes` and `total_number_of_compressed_bytes` for each column and then average compression ration is a simple division. But ... actually ... closer to the original proposal is simply the ""total_number_of_bytes_in_all_flushed_clusters"" and ""corresponding_compressed_size"" (i.e. a per RNtuple running total). However this makes me thing of another (potential more noticeable) instability. If there is 2 columns; column A with an constant/average compression ration of 100 and column B with a constant/average compression ration of 1 (or 1.1 :) ) but both columns are collections. If for most of the entries (for more proposal) or only the last cluster (for the PR's proposal), column A has a lot of data and column B has very little data, then the compression ration use for prediction will be 100 (ish). If suddenly the cardinality flip/flop and column A has very little data but column B has a lot of data, then the compression ration of that cluster will be 1 but it would still contains `100 * target_cluster_size_in_compressed_bytes`, namely in this case 100 times larger (on disk) than any other cluster. Obviously this is an extreme example but it points to the benefit of using `per column` compression ratio rather than overall compression ratio (whether it is whole-ntuple or last (few) clusters). The cost for implementation seems to be that after each `Fill`, each column would have to increment a (global-per-RNtuple) counter with `data_written / estimated_compression_ratio` which:. * for collection column should be a small over-head. * for single value column should be a large over-head. however conversely, an accurate estimate is ""only important"" for collection column, so maybe an hybrid solution is a workable compromise?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:1182,deployability,cluster,cluster,1182,"In reply to: https://github.com/root-project/root/pull/8703#issuecomment-920044106. > You mean keeping the average of the last $x$ MB written per column? . Not quite, I meant simply an overall average, i.e. simply keeping `total_number_of_bytes` and `total_number_of_compressed_bytes` for each column and then average compression ration is a simple division. But ... actually ... closer to the original proposal is simply the ""total_number_of_bytes_in_all_flushed_clusters"" and ""corresponding_compressed_size"" (i.e. a per RNtuple running total). However this makes me thing of another (potential more noticeable) instability. If there is 2 columns; column A with an constant/average compression ration of 100 and column B with a constant/average compression ration of 1 (or 1.1 :) ) but both columns are collections. If for most of the entries (for more proposal) or only the last cluster (for the PR's proposal), column A has a lot of data and column B has very little data, then the compression ration use for prediction will be 100 (ish). If suddenly the cardinality flip/flop and column A has very little data but column B has a lot of data, then the compression ration of that cluster will be 1 but it would still contains `100 * target_cluster_size_in_compressed_bytes`, namely in this case 100 times larger (on disk) than any other cluster. Obviously this is an extreme example but it points to the benefit of using `per column` compression ratio rather than overall compression ratio (whether it is whole-ntuple or last (few) clusters). The cost for implementation seems to be that after each `Fill`, each column would have to increment a (global-per-RNtuple) counter with `data_written / estimated_compression_ratio` which:. * for collection column should be a small over-head. * for single value column should be a large over-head. however conversely, an accurate estimate is ""only important"" for collection column, so maybe an hybrid solution is a workable compromise?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:1219,deployability,contain,contains,1219,"In reply to: https://github.com/root-project/root/pull/8703#issuecomment-920044106. > You mean keeping the average of the last $x$ MB written per column? . Not quite, I meant simply an overall average, i.e. simply keeping `total_number_of_bytes` and `total_number_of_compressed_bytes` for each column and then average compression ration is a simple division. But ... actually ... closer to the original proposal is simply the ""total_number_of_bytes_in_all_flushed_clusters"" and ""corresponding_compressed_size"" (i.e. a per RNtuple running total). However this makes me thing of another (potential more noticeable) instability. If there is 2 columns; column A with an constant/average compression ration of 100 and column B with a constant/average compression ration of 1 (or 1.1 :) ) but both columns are collections. If for most of the entries (for more proposal) or only the last cluster (for the PR's proposal), column A has a lot of data and column B has very little data, then the compression ration use for prediction will be 100 (ish). If suddenly the cardinality flip/flop and column A has very little data but column B has a lot of data, then the compression ration of that cluster will be 1 but it would still contains `100 * target_cluster_size_in_compressed_bytes`, namely in this case 100 times larger (on disk) than any other cluster. Obviously this is an extreme example but it points to the benefit of using `per column` compression ratio rather than overall compression ratio (whether it is whole-ntuple or last (few) clusters). The cost for implementation seems to be that after each `Fill`, each column would have to increment a (global-per-RNtuple) counter with `data_written / estimated_compression_ratio` which:. * for collection column should be a small over-head. * for single value column should be a large over-head. however conversely, an accurate estimate is ""only important"" for collection column, so maybe an hybrid solution is a workable compromise?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:1339,deployability,cluster,cluster,1339,"In reply to: https://github.com/root-project/root/pull/8703#issuecomment-920044106. > You mean keeping the average of the last $x$ MB written per column? . Not quite, I meant simply an overall average, i.e. simply keeping `total_number_of_bytes` and `total_number_of_compressed_bytes` for each column and then average compression ration is a simple division. But ... actually ... closer to the original proposal is simply the ""total_number_of_bytes_in_all_flushed_clusters"" and ""corresponding_compressed_size"" (i.e. a per RNtuple running total). However this makes me thing of another (potential more noticeable) instability. If there is 2 columns; column A with an constant/average compression ration of 100 and column B with a constant/average compression ration of 1 (or 1.1 :) ) but both columns are collections. If for most of the entries (for more proposal) or only the last cluster (for the PR's proposal), column A has a lot of data and column B has very little data, then the compression ration use for prediction will be 100 (ish). If suddenly the cardinality flip/flop and column A has very little data but column B has a lot of data, then the compression ration of that cluster will be 1 but it would still contains `100 * target_cluster_size_in_compressed_bytes`, namely in this case 100 times larger (on disk) than any other cluster. Obviously this is an extreme example but it points to the benefit of using `per column` compression ratio rather than overall compression ratio (whether it is whole-ntuple or last (few) clusters). The cost for implementation seems to be that after each `Fill`, each column would have to increment a (global-per-RNtuple) counter with `data_written / estimated_compression_ratio` which:. * for collection column should be a small over-head. * for single value column should be a large over-head. however conversely, an accurate estimate is ""only important"" for collection column, so maybe an hybrid solution is a workable compromise?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:1534,deployability,cluster,clusters,1534,"In reply to: https://github.com/root-project/root/pull/8703#issuecomment-920044106. > You mean keeping the average of the last $x$ MB written per column? . Not quite, I meant simply an overall average, i.e. simply keeping `total_number_of_bytes` and `total_number_of_compressed_bytes` for each column and then average compression ration is a simple division. But ... actually ... closer to the original proposal is simply the ""total_number_of_bytes_in_all_flushed_clusters"" and ""corresponding_compressed_size"" (i.e. a per RNtuple running total). However this makes me thing of another (potential more noticeable) instability. If there is 2 columns; column A with an constant/average compression ration of 100 and column B with a constant/average compression ration of 1 (or 1.1 :) ) but both columns are collections. If for most of the entries (for more proposal) or only the last cluster (for the PR's proposal), column A has a lot of data and column B has very little data, then the compression ration use for prediction will be 100 (ish). If suddenly the cardinality flip/flop and column A has very little data but column B has a lot of data, then the compression ration of that cluster will be 1 but it would still contains `100 * target_cluster_size_in_compressed_bytes`, namely in this case 100 times larger (on disk) than any other cluster. Obviously this is an extreme example but it points to the benefit of using `per column` compression ratio rather than overall compression ratio (whether it is whole-ntuple or last (few) clusters). The cost for implementation seems to be that after each `Fill`, each column would have to increment a (global-per-RNtuple) counter with `data_written / estimated_compression_ratio` which:. * for collection column should be a small over-head. * for single value column should be a large over-head. however conversely, an accurate estimate is ""only important"" for collection column, so maybe an hybrid solution is a workable compromise?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:1012,energy efficiency,predict,prediction,1012,"In reply to: https://github.com/root-project/root/pull/8703#issuecomment-920044106. > You mean keeping the average of the last $x$ MB written per column? . Not quite, I meant simply an overall average, i.e. simply keeping `total_number_of_bytes` and `total_number_of_compressed_bytes` for each column and then average compression ration is a simple division. But ... actually ... closer to the original proposal is simply the ""total_number_of_bytes_in_all_flushed_clusters"" and ""corresponding_compressed_size"" (i.e. a per RNtuple running total). However this makes me thing of another (potential more noticeable) instability. If there is 2 columns; column A with an constant/average compression ration of 100 and column B with a constant/average compression ration of 1 (or 1.1 :) ) but both columns are collections. If for most of the entries (for more proposal) or only the last cluster (for the PR's proposal), column A has a lot of data and column B has very little data, then the compression ration use for prediction will be 100 (ish). If suddenly the cardinality flip/flop and column A has very little data but column B has a lot of data, then the compression ration of that cluster will be 1 but it would still contains `100 * target_cluster_size_in_compressed_bytes`, namely in this case 100 times larger (on disk) than any other cluster. Obviously this is an extreme example but it points to the benefit of using `per column` compression ratio rather than overall compression ratio (whether it is whole-ntuple or last (few) clusters). The cost for implementation seems to be that after each `Fill`, each column would have to increment a (global-per-RNtuple) counter with `data_written / estimated_compression_ratio` which:. * for collection column should be a small over-head. * for single value column should be a large over-head. however conversely, an accurate estimate is ""only important"" for collection column, so maybe an hybrid solution is a workable compromise?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:1874,energy efficiency,estimat,estimate,1874,"In reply to: https://github.com/root-project/root/pull/8703#issuecomment-920044106. > You mean keeping the average of the last $x$ MB written per column? . Not quite, I meant simply an overall average, i.e. simply keeping `total_number_of_bytes` and `total_number_of_compressed_bytes` for each column and then average compression ration is a simple division. But ... actually ... closer to the original proposal is simply the ""total_number_of_bytes_in_all_flushed_clusters"" and ""corresponding_compressed_size"" (i.e. a per RNtuple running total). However this makes me thing of another (potential more noticeable) instability. If there is 2 columns; column A with an constant/average compression ration of 100 and column B with a constant/average compression ration of 1 (or 1.1 :) ) but both columns are collections. If for most of the entries (for more proposal) or only the last cluster (for the PR's proposal), column A has a lot of data and column B has very little data, then the compression ration use for prediction will be 100 (ish). If suddenly the cardinality flip/flop and column A has very little data but column B has a lot of data, then the compression ration of that cluster will be 1 but it would still contains `100 * target_cluster_size_in_compressed_bytes`, namely in this case 100 times larger (on disk) than any other cluster. Obviously this is an extreme example but it points to the benefit of using `per column` compression ratio rather than overall compression ratio (whether it is whole-ntuple or last (few) clusters). The cost for implementation seems to be that after each `Fill`, each column would have to increment a (global-per-RNtuple) counter with `data_written / estimated_compression_ratio` which:. * for collection column should be a small over-head. * for single value column should be a large over-head. however conversely, an accurate estimate is ""only important"" for collection column, so maybe an hybrid solution is a workable compromise?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:1850,interoperability,convers,conversely,1850,"In reply to: https://github.com/root-project/root/pull/8703#issuecomment-920044106. > You mean keeping the average of the last $x$ MB written per column? . Not quite, I meant simply an overall average, i.e. simply keeping `total_number_of_bytes` and `total_number_of_compressed_bytes` for each column and then average compression ration is a simple division. But ... actually ... closer to the original proposal is simply the ""total_number_of_bytes_in_all_flushed_clusters"" and ""corresponding_compressed_size"" (i.e. a per RNtuple running total). However this makes me thing of another (potential more noticeable) instability. If there is 2 columns; column A with an constant/average compression ration of 100 and column B with a constant/average compression ration of 1 (or 1.1 :) ) but both columns are collections. If for most of the entries (for more proposal) or only the last cluster (for the PR's proposal), column A has a lot of data and column B has very little data, then the compression ration use for prediction will be 100 (ish). If suddenly the cardinality flip/flop and column A has very little data but column B has a lot of data, then the compression ration of that cluster will be 1 but it would still contains `100 * target_cluster_size_in_compressed_bytes`, namely in this case 100 times larger (on disk) than any other cluster. Obviously this is an extreme example but it points to the benefit of using `per column` compression ratio rather than overall compression ratio (whether it is whole-ntuple or last (few) clusters). The cost for implementation seems to be that after each `Fill`, each column would have to increment a (global-per-RNtuple) counter with `data_written / estimated_compression_ratio` which:. * for collection column should be a small over-head. * for single value column should be a large over-head. however conversely, an accurate estimate is ""only important"" for collection column, so maybe an hybrid solution is a workable compromise?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:1301,performance,time,times,1301,"In reply to: https://github.com/root-project/root/pull/8703#issuecomment-920044106. > You mean keeping the average of the last $x$ MB written per column? . Not quite, I meant simply an overall average, i.e. simply keeping `total_number_of_bytes` and `total_number_of_compressed_bytes` for each column and then average compression ration is a simple division. But ... actually ... closer to the original proposal is simply the ""total_number_of_bytes_in_all_flushed_clusters"" and ""corresponding_compressed_size"" (i.e. a per RNtuple running total). However this makes me thing of another (potential more noticeable) instability. If there is 2 columns; column A with an constant/average compression ration of 100 and column B with a constant/average compression ration of 1 (or 1.1 :) ) but both columns are collections. If for most of the entries (for more proposal) or only the last cluster (for the PR's proposal), column A has a lot of data and column B has very little data, then the compression ration use for prediction will be 100 (ish). If suddenly the cardinality flip/flop and column A has very little data but column B has a lot of data, then the compression ration of that cluster will be 1 but it would still contains `100 * target_cluster_size_in_compressed_bytes`, namely in this case 100 times larger (on disk) than any other cluster. Obviously this is an extreme example but it points to the benefit of using `per column` compression ratio rather than overall compression ratio (whether it is whole-ntuple or last (few) clusters). The cost for implementation seems to be that after each `Fill`, each column would have to increment a (global-per-RNtuple) counter with `data_written / estimated_compression_ratio` which:. * for collection column should be a small over-head. * for single value column should be a large over-head. however conversely, an accurate estimate is ""only important"" for collection column, so maybe an hybrid solution is a workable compromise?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:1318,performance,disk,disk,1318,"In reply to: https://github.com/root-project/root/pull/8703#issuecomment-920044106. > You mean keeping the average of the last $x$ MB written per column? . Not quite, I meant simply an overall average, i.e. simply keeping `total_number_of_bytes` and `total_number_of_compressed_bytes` for each column and then average compression ration is a simple division. But ... actually ... closer to the original proposal is simply the ""total_number_of_bytes_in_all_flushed_clusters"" and ""corresponding_compressed_size"" (i.e. a per RNtuple running total). However this makes me thing of another (potential more noticeable) instability. If there is 2 columns; column A with an constant/average compression ration of 100 and column B with a constant/average compression ration of 1 (or 1.1 :) ) but both columns are collections. If for most of the entries (for more proposal) or only the last cluster (for the PR's proposal), column A has a lot of data and column B has very little data, then the compression ration use for prediction will be 100 (ish). If suddenly the cardinality flip/flop and column A has very little data but column B has a lot of data, then the compression ration of that cluster will be 1 but it would still contains `100 * target_cluster_size_in_compressed_bytes`, namely in this case 100 times larger (on disk) than any other cluster. Obviously this is an extreme example but it points to the benefit of using `per column` compression ratio rather than overall compression ratio (whether it is whole-ntuple or last (few) clusters). The cost for implementation seems to be that after each `Fill`, each column would have to increment a (global-per-RNtuple) counter with `data_written / estimated_compression_ratio` which:. * for collection column should be a small over-head. * for single value column should be a large over-head. however conversely, an accurate estimate is ""only important"" for collection column, so maybe an hybrid solution is a workable compromise?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:1012,safety,predict,prediction,1012,"In reply to: https://github.com/root-project/root/pull/8703#issuecomment-920044106. > You mean keeping the average of the last $x$ MB written per column? . Not quite, I meant simply an overall average, i.e. simply keeping `total_number_of_bytes` and `total_number_of_compressed_bytes` for each column and then average compression ration is a simple division. But ... actually ... closer to the original proposal is simply the ""total_number_of_bytes_in_all_flushed_clusters"" and ""corresponding_compressed_size"" (i.e. a per RNtuple running total). However this makes me thing of another (potential more noticeable) instability. If there is 2 columns; column A with an constant/average compression ration of 100 and column B with a constant/average compression ration of 1 (or 1.1 :) ) but both columns are collections. If for most of the entries (for more proposal) or only the last cluster (for the PR's proposal), column A has a lot of data and column B has very little data, then the compression ration use for prediction will be 100 (ish). If suddenly the cardinality flip/flop and column A has very little data but column B has a lot of data, then the compression ration of that cluster will be 1 but it would still contains `100 * target_cluster_size_in_compressed_bytes`, namely in this case 100 times larger (on disk) than any other cluster. Obviously this is an extreme example but it points to the benefit of using `per column` compression ratio rather than overall compression ratio (whether it is whole-ntuple or last (few) clusters). The cost for implementation seems to be that after each `Fill`, each column would have to increment a (global-per-RNtuple) counter with `data_written / estimated_compression_ratio` which:. * for collection column should be a small over-head. * for single value column should be a large over-head. however conversely, an accurate estimate is ""only important"" for collection column, so maybe an hybrid solution is a workable compromise?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:175,testability,simpl,simply,175,"In reply to: https://github.com/root-project/root/pull/8703#issuecomment-920044106. > You mean keeping the average of the last $x$ MB written per column? . Not quite, I meant simply an overall average, i.e. simply keeping `total_number_of_bytes` and `total_number_of_compressed_bytes` for each column and then average compression ration is a simple division. But ... actually ... closer to the original proposal is simply the ""total_number_of_bytes_in_all_flushed_clusters"" and ""corresponding_compressed_size"" (i.e. a per RNtuple running total). However this makes me thing of another (potential more noticeable) instability. If there is 2 columns; column A with an constant/average compression ration of 100 and column B with a constant/average compression ration of 1 (or 1.1 :) ) but both columns are collections. If for most of the entries (for more proposal) or only the last cluster (for the PR's proposal), column A has a lot of data and column B has very little data, then the compression ration use for prediction will be 100 (ish). If suddenly the cardinality flip/flop and column A has very little data but column B has a lot of data, then the compression ration of that cluster will be 1 but it would still contains `100 * target_cluster_size_in_compressed_bytes`, namely in this case 100 times larger (on disk) than any other cluster. Obviously this is an extreme example but it points to the benefit of using `per column` compression ratio rather than overall compression ratio (whether it is whole-ntuple or last (few) clusters). The cost for implementation seems to be that after each `Fill`, each column would have to increment a (global-per-RNtuple) counter with `data_written / estimated_compression_ratio` which:. * for collection column should be a small over-head. * for single value column should be a large over-head. however conversely, an accurate estimate is ""only important"" for collection column, so maybe an hybrid solution is a workable compromise?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:207,testability,simpl,simply,207,"In reply to: https://github.com/root-project/root/pull/8703#issuecomment-920044106. > You mean keeping the average of the last $x$ MB written per column? . Not quite, I meant simply an overall average, i.e. simply keeping `total_number_of_bytes` and `total_number_of_compressed_bytes` for each column and then average compression ration is a simple division. But ... actually ... closer to the original proposal is simply the ""total_number_of_bytes_in_all_flushed_clusters"" and ""corresponding_compressed_size"" (i.e. a per RNtuple running total). However this makes me thing of another (potential more noticeable) instability. If there is 2 columns; column A with an constant/average compression ration of 100 and column B with a constant/average compression ration of 1 (or 1.1 :) ) but both columns are collections. If for most of the entries (for more proposal) or only the last cluster (for the PR's proposal), column A has a lot of data and column B has very little data, then the compression ration use for prediction will be 100 (ish). If suddenly the cardinality flip/flop and column A has very little data but column B has a lot of data, then the compression ration of that cluster will be 1 but it would still contains `100 * target_cluster_size_in_compressed_bytes`, namely in this case 100 times larger (on disk) than any other cluster. Obviously this is an extreme example but it points to the benefit of using `per column` compression ratio rather than overall compression ratio (whether it is whole-ntuple or last (few) clusters). The cost for implementation seems to be that after each `Fill`, each column would have to increment a (global-per-RNtuple) counter with `data_written / estimated_compression_ratio` which:. * for collection column should be a small over-head. * for single value column should be a large over-head. however conversely, an accurate estimate is ""only important"" for collection column, so maybe an hybrid solution is a workable compromise?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:342,testability,simpl,simple,342,"In reply to: https://github.com/root-project/root/pull/8703#issuecomment-920044106. > You mean keeping the average of the last $x$ MB written per column? . Not quite, I meant simply an overall average, i.e. simply keeping `total_number_of_bytes` and `total_number_of_compressed_bytes` for each column and then average compression ration is a simple division. But ... actually ... closer to the original proposal is simply the ""total_number_of_bytes_in_all_flushed_clusters"" and ""corresponding_compressed_size"" (i.e. a per RNtuple running total). However this makes me thing of another (potential more noticeable) instability. If there is 2 columns; column A with an constant/average compression ration of 100 and column B with a constant/average compression ration of 1 (or 1.1 :) ) but both columns are collections. If for most of the entries (for more proposal) or only the last cluster (for the PR's proposal), column A has a lot of data and column B has very little data, then the compression ration use for prediction will be 100 (ish). If suddenly the cardinality flip/flop and column A has very little data but column B has a lot of data, then the compression ration of that cluster will be 1 but it would still contains `100 * target_cluster_size_in_compressed_bytes`, namely in this case 100 times larger (on disk) than any other cluster. Obviously this is an extreme example but it points to the benefit of using `per column` compression ratio rather than overall compression ratio (whether it is whole-ntuple or last (few) clusters). The cost for implementation seems to be that after each `Fill`, each column would have to increment a (global-per-RNtuple) counter with `data_written / estimated_compression_ratio` which:. * for collection column should be a small over-head. * for single value column should be a large over-head. however conversely, an accurate estimate is ""only important"" for collection column, so maybe an hybrid solution is a workable compromise?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:415,testability,simpl,simply,415,"In reply to: https://github.com/root-project/root/pull/8703#issuecomment-920044106. > You mean keeping the average of the last $x$ MB written per column? . Not quite, I meant simply an overall average, i.e. simply keeping `total_number_of_bytes` and `total_number_of_compressed_bytes` for each column and then average compression ration is a simple division. But ... actually ... closer to the original proposal is simply the ""total_number_of_bytes_in_all_flushed_clusters"" and ""corresponding_compressed_size"" (i.e. a per RNtuple running total). However this makes me thing of another (potential more noticeable) instability. If there is 2 columns; column A with an constant/average compression ration of 100 and column B with a constant/average compression ration of 1 (or 1.1 :) ) but both columns are collections. If for most of the entries (for more proposal) or only the last cluster (for the PR's proposal), column A has a lot of data and column B has very little data, then the compression ration use for prediction will be 100 (ish). If suddenly the cardinality flip/flop and column A has very little data but column B has a lot of data, then the compression ration of that cluster will be 1 but it would still contains `100 * target_cluster_size_in_compressed_bytes`, namely in this case 100 times larger (on disk) than any other cluster. Obviously this is an extreme example but it points to the benefit of using `per column` compression ratio rather than overall compression ratio (whether it is whole-ntuple or last (few) clusters). The cost for implementation seems to be that after each `Fill`, each column would have to increment a (global-per-RNtuple) counter with `data_written / estimated_compression_ratio` which:. * for collection column should be a small over-head. * for single value column should be a large over-head. however conversely, an accurate estimate is ""only important"" for collection column, so maybe an hybrid solution is a workable compromise?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:175,usability,simpl,simply,175,"In reply to: https://github.com/root-project/root/pull/8703#issuecomment-920044106. > You mean keeping the average of the last $x$ MB written per column? . Not quite, I meant simply an overall average, i.e. simply keeping `total_number_of_bytes` and `total_number_of_compressed_bytes` for each column and then average compression ration is a simple division. But ... actually ... closer to the original proposal is simply the ""total_number_of_bytes_in_all_flushed_clusters"" and ""corresponding_compressed_size"" (i.e. a per RNtuple running total). However this makes me thing of another (potential more noticeable) instability. If there is 2 columns; column A with an constant/average compression ration of 100 and column B with a constant/average compression ration of 1 (or 1.1 :) ) but both columns are collections. If for most of the entries (for more proposal) or only the last cluster (for the PR's proposal), column A has a lot of data and column B has very little data, then the compression ration use for prediction will be 100 (ish). If suddenly the cardinality flip/flop and column A has very little data but column B has a lot of data, then the compression ration of that cluster will be 1 but it would still contains `100 * target_cluster_size_in_compressed_bytes`, namely in this case 100 times larger (on disk) than any other cluster. Obviously this is an extreme example but it points to the benefit of using `per column` compression ratio rather than overall compression ratio (whether it is whole-ntuple or last (few) clusters). The cost for implementation seems to be that after each `Fill`, each column would have to increment a (global-per-RNtuple) counter with `data_written / estimated_compression_ratio` which:. * for collection column should be a small over-head. * for single value column should be a large over-head. however conversely, an accurate estimate is ""only important"" for collection column, so maybe an hybrid solution is a workable compromise?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:207,usability,simpl,simply,207,"In reply to: https://github.com/root-project/root/pull/8703#issuecomment-920044106. > You mean keeping the average of the last $x$ MB written per column? . Not quite, I meant simply an overall average, i.e. simply keeping `total_number_of_bytes` and `total_number_of_compressed_bytes` for each column and then average compression ration is a simple division. But ... actually ... closer to the original proposal is simply the ""total_number_of_bytes_in_all_flushed_clusters"" and ""corresponding_compressed_size"" (i.e. a per RNtuple running total). However this makes me thing of another (potential more noticeable) instability. If there is 2 columns; column A with an constant/average compression ration of 100 and column B with a constant/average compression ration of 1 (or 1.1 :) ) but both columns are collections. If for most of the entries (for more proposal) or only the last cluster (for the PR's proposal), column A has a lot of data and column B has very little data, then the compression ration use for prediction will be 100 (ish). If suddenly the cardinality flip/flop and column A has very little data but column B has a lot of data, then the compression ration of that cluster will be 1 but it would still contains `100 * target_cluster_size_in_compressed_bytes`, namely in this case 100 times larger (on disk) than any other cluster. Obviously this is an extreme example but it points to the benefit of using `per column` compression ratio rather than overall compression ratio (whether it is whole-ntuple or last (few) clusters). The cost for implementation seems to be that after each `Fill`, each column would have to increment a (global-per-RNtuple) counter with `data_written / estimated_compression_ratio` which:. * for collection column should be a small over-head. * for single value column should be a large over-head. however conversely, an accurate estimate is ""only important"" for collection column, so maybe an hybrid solution is a workable compromise?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:342,usability,simpl,simple,342,"In reply to: https://github.com/root-project/root/pull/8703#issuecomment-920044106. > You mean keeping the average of the last $x$ MB written per column? . Not quite, I meant simply an overall average, i.e. simply keeping `total_number_of_bytes` and `total_number_of_compressed_bytes` for each column and then average compression ration is a simple division. But ... actually ... closer to the original proposal is simply the ""total_number_of_bytes_in_all_flushed_clusters"" and ""corresponding_compressed_size"" (i.e. a per RNtuple running total). However this makes me thing of another (potential more noticeable) instability. If there is 2 columns; column A with an constant/average compression ration of 100 and column B with a constant/average compression ration of 1 (or 1.1 :) ) but both columns are collections. If for most of the entries (for more proposal) or only the last cluster (for the PR's proposal), column A has a lot of data and column B has very little data, then the compression ration use for prediction will be 100 (ish). If suddenly the cardinality flip/flop and column A has very little data but column B has a lot of data, then the compression ration of that cluster will be 1 but it would still contains `100 * target_cluster_size_in_compressed_bytes`, namely in this case 100 times larger (on disk) than any other cluster. Obviously this is an extreme example but it points to the benefit of using `per column` compression ratio rather than overall compression ratio (whether it is whole-ntuple or last (few) clusters). The cost for implementation seems to be that after each `Fill`, each column would have to increment a (global-per-RNtuple) counter with `data_written / estimated_compression_ratio` which:. * for collection column should be a small over-head. * for single value column should be a large over-head. however conversely, an accurate estimate is ""only important"" for collection column, so maybe an hybrid solution is a workable compromise?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:380,usability,close,closer,380,"In reply to: https://github.com/root-project/root/pull/8703#issuecomment-920044106. > You mean keeping the average of the last $x$ MB written per column? . Not quite, I meant simply an overall average, i.e. simply keeping `total_number_of_bytes` and `total_number_of_compressed_bytes` for each column and then average compression ration is a simple division. But ... actually ... closer to the original proposal is simply the ""total_number_of_bytes_in_all_flushed_clusters"" and ""corresponding_compressed_size"" (i.e. a per RNtuple running total). However this makes me thing of another (potential more noticeable) instability. If there is 2 columns; column A with an constant/average compression ration of 100 and column B with a constant/average compression ration of 1 (or 1.1 :) ) but both columns are collections. If for most of the entries (for more proposal) or only the last cluster (for the PR's proposal), column A has a lot of data and column B has very little data, then the compression ration use for prediction will be 100 (ish). If suddenly the cardinality flip/flop and column A has very little data but column B has a lot of data, then the compression ration of that cluster will be 1 but it would still contains `100 * target_cluster_size_in_compressed_bytes`, namely in this case 100 times larger (on disk) than any other cluster. Obviously this is an extreme example but it points to the benefit of using `per column` compression ratio rather than overall compression ratio (whether it is whole-ntuple or last (few) clusters). The cost for implementation seems to be that after each `Fill`, each column would have to increment a (global-per-RNtuple) counter with `data_written / estimated_compression_ratio` which:. * for collection column should be a small over-head. * for single value column should be a large over-head. however conversely, an accurate estimate is ""only important"" for collection column, so maybe an hybrid solution is a workable compromise?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:415,usability,simpl,simply,415,"In reply to: https://github.com/root-project/root/pull/8703#issuecomment-920044106. > You mean keeping the average of the last $x$ MB written per column? . Not quite, I meant simply an overall average, i.e. simply keeping `total_number_of_bytes` and `total_number_of_compressed_bytes` for each column and then average compression ration is a simple division. But ... actually ... closer to the original proposal is simply the ""total_number_of_bytes_in_all_flushed_clusters"" and ""corresponding_compressed_size"" (i.e. a per RNtuple running total). However this makes me thing of another (potential more noticeable) instability. If there is 2 columns; column A with an constant/average compression ration of 100 and column B with a constant/average compression ration of 1 (or 1.1 :) ) but both columns are collections. If for most of the entries (for more proposal) or only the last cluster (for the PR's proposal), column A has a lot of data and column B has very little data, then the compression ration use for prediction will be 100 (ish). If suddenly the cardinality flip/flop and column A has very little data but column B has a lot of data, then the compression ration of that cluster will be 1 but it would still contains `100 * target_cluster_size_in_compressed_bytes`, namely in this case 100 times larger (on disk) than any other cluster. Obviously this is an extreme example but it points to the benefit of using `per column` compression ratio rather than overall compression ratio (whether it is whole-ntuple or last (few) clusters). The cost for implementation seems to be that after each `Fill`, each column would have to increment a (global-per-RNtuple) counter with `data_written / estimated_compression_ratio` which:. * for collection column should be a small over-head. * for single value column should be a large over-head. however conversely, an accurate estimate is ""only important"" for collection column, so maybe an hybrid solution is a workable compromise?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:55,availability,cluster,cluster,55,"@pcanal As discussed, the estimator for the compressed cluster size is [now](https://github.com/jblomer/root/pull/8703/commits/806637b8a130e4a5a6b599a0b134e9ea0097aae7) the average compression ratio of all so-far written clusters. I also [added](https://github.com/jblomer/root/pull/8703/commits/5a023346d66b58f4c38044544239b71c7856e0b1) a stub checklist for the future RNTuple validation.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:221,availability,cluster,clusters,221,"@pcanal As discussed, the estimator for the compressed cluster size is [now](https://github.com/jblomer/root/pull/8703/commits/806637b8a130e4a5a6b599a0b134e9ea0097aae7) the average compression ratio of all so-far written clusters. I also [added](https://github.com/jblomer/root/pull/8703/commits/5a023346d66b58f4c38044544239b71c7856e0b1) a stub checklist for the future RNTuple validation.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:55,deployability,cluster,cluster,55,"@pcanal As discussed, the estimator for the compressed cluster size is [now](https://github.com/jblomer/root/pull/8703/commits/806637b8a130e4a5a6b599a0b134e9ea0097aae7) the average compression ratio of all so-far written clusters. I also [added](https://github.com/jblomer/root/pull/8703/commits/5a023346d66b58f4c38044544239b71c7856e0b1) a stub checklist for the future RNTuple validation.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:221,deployability,cluster,clusters,221,"@pcanal As discussed, the estimator for the compressed cluster size is [now](https://github.com/jblomer/root/pull/8703/commits/806637b8a130e4a5a6b599a0b134e9ea0097aae7) the average compression ratio of all so-far written clusters. I also [added](https://github.com/jblomer/root/pull/8703/commits/5a023346d66b58f4c38044544239b71c7856e0b1) a stub checklist for the future RNTuple validation.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:26,energy efficiency,estimat,estimator,26,"@pcanal As discussed, the estimator for the compressed cluster size is [now](https://github.com/jblomer/root/pull/8703/commits/806637b8a130e4a5a6b599a0b134e9ea0097aae7) the average compression ratio of all so-far written clusters. I also [added](https://github.com/jblomer/root/pull/8703/commits/5a023346d66b58f4c38044544239b71c7856e0b1) a stub checklist for the future RNTuple validation.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:340,interoperability,stub,stub,340,"@pcanal As discussed, the estimator for the compressed cluster size is [now](https://github.com/jblomer/root/pull/8703/commits/806637b8a130e4a5a6b599a0b134e9ea0097aae7) the average compression ratio of all so-far written clusters. I also [added](https://github.com/jblomer/root/pull/8703/commits/5a023346d66b58f4c38044544239b71c7856e0b1) a stub checklist for the future RNTuple validation.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:378,safety,valid,validation,378,"@pcanal As discussed, the estimator for the compressed cluster size is [now](https://github.com/jblomer/root/pull/8703/commits/806637b8a130e4a5a6b599a0b134e9ea0097aae7) the average compression ratio of all so-far written clusters. I also [added](https://github.com/jblomer/root/pull/8703/commits/5a023346d66b58f4c38044544239b71c7856e0b1) a stub checklist for the future RNTuple validation.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:378,security,validat,validation,378,"@pcanal As discussed, the estimator for the compressed cluster size is [now](https://github.com/jblomer/root/pull/8703/commits/806637b8a130e4a5a6b599a0b134e9ea0097aae7) the average compression ratio of all so-far written clusters. I also [added](https://github.com/jblomer/root/pull/8703/commits/5a023346d66b58f4c38044544239b71c7856e0b1) a stub checklist for the future RNTuple validation.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/pull/8703:340,testability,stub,stub,340,"@pcanal As discussed, the estimator for the compressed cluster size is [now](https://github.com/jblomer/root/pull/8703/commits/806637b8a130e4a5a6b599a0b134e9ea0097aae7) the average compression ratio of all so-far written clusters. I also [added](https://github.com/jblomer/root/pull/8703/commits/5a023346d66b58f4c38044544239b71c7856e0b1) a stub checklist for the future RNTuple validation.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8703
https://github.com/root-project/root/issues/8704:227,energy efficiency,current,current,227,"To clarify, the plan is to have something like a `DefineDefault(""x"", ...)` to provide default values for a branch when it's not present in the dataset. EDIT:. maybe just `Default`? EDIT 2:. after discussion with @vepadulano my current preference is `DefaultFor`",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8704
https://github.com/root-project/root/issues/8704:16,testability,plan,plan,16,"To clarify, the plan is to have something like a `DefineDefault(""x"", ...)` to provide default values for a branch when it's not present in the dataset. EDIT:. maybe just `Default`? EDIT 2:. after discussion with @vepadulano my current preference is `DefaultFor`",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8704
https://github.com/root-project/root/issues/8704:235,usability,prefer,preference,235,"To clarify, the plan is to have something like a `DefineDefault(""x"", ...)` to provide default values for a branch when it's not present in the dataset. EDIT:. maybe just `Default`? EDIT 2:. after discussion with @vepadulano my current preference is `DefaultFor`",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8704
https://github.com/root-project/root/issues/8704:57,integrability,event,events-where-branch-exists,57,Bump https://root-forum.cern.ch/t/only-perform-define-on-events-where-branch-exists/57943/2,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8704
https://github.com/root-project/root/issues/8704:39,performance,perform,perform-define-on-events-where-branch-exists,39,Bump https://root-forum.cern.ch/t/only-perform-define-on-events-where-branch-exists/57943/2,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8704
https://github.com/root-project/root/issues/8704:39,usability,perform,perform-define-on-events-where-branch-exists,39,Bump https://root-forum.cern.ch/t/only-perform-define-on-events-where-branch-exists/57943/2,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8704
https://github.com/root-project/root/issues/8704:1124,availability,sli,slightly,1124,"My bad for not managing to get to this in time. I think most of the logic would go in `GetColumnReader`, in `ColumnReaderUtils`: it would have to create a `TreeColumnReader` if the column is present in the tree and pick the reader for the default value instead. The main complication is dealing with chains in which _some_ trees contain the column and some do not. The most direct way to do it is leveraging TChain's callback mechanism to switch column readers as needed, but a) TChain's callback mechanism is a mess and b) even if unused TTreeReaderValues for missing branches might emit warnings (unless they are completely destroyed maybe?). An more roundabout way to do the same would be to change single-thread RDataFrame so that it runs a separate event loop for each sub-tree and never uses a TChain --> you automatically get a call to `GetColumnReader` that can re-evaluate whether the branch or the default value should be used at the start of every new event loop. Finally, you can implement the default as a `Define` that always returns the same number (and in first approximation that's probably the easiest). A slightly nicer underlying implementation would involve a `ConstReader` that explicitly stores a constant value and always returns that same value, but the advantage is unclear -- a good optimizing compiler would elide the call to the lambda that always returns the same number anyways.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8704
https://github.com/root-project/root/issues/8704:15,deployability,manag,managing,15,"My bad for not managing to get to this in time. I think most of the logic would go in `GetColumnReader`, in `ColumnReaderUtils`: it would have to create a `TreeColumnReader` if the column is present in the tree and pick the reader for the default value instead. The main complication is dealing with chains in which _some_ trees contain the column and some do not. The most direct way to do it is leveraging TChain's callback mechanism to switch column readers as needed, but a) TChain's callback mechanism is a mess and b) even if unused TTreeReaderValues for missing branches might emit warnings (unless they are completely destroyed maybe?). An more roundabout way to do the same would be to change single-thread RDataFrame so that it runs a separate event loop for each sub-tree and never uses a TChain --> you automatically get a call to `GetColumnReader` that can re-evaluate whether the branch or the default value should be used at the start of every new event loop. Finally, you can implement the default as a `Define` that always returns the same number (and in first approximation that's probably the easiest). A slightly nicer underlying implementation would involve a `ConstReader` that explicitly stores a constant value and always returns that same value, but the advantage is unclear -- a good optimizing compiler would elide the call to the lambda that always returns the same number anyways.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8704
https://github.com/root-project/root/issues/8704:68,deployability,log,logic,68,"My bad for not managing to get to this in time. I think most of the logic would go in `GetColumnReader`, in `ColumnReaderUtils`: it would have to create a `TreeColumnReader` if the column is present in the tree and pick the reader for the default value instead. The main complication is dealing with chains in which _some_ trees contain the column and some do not. The most direct way to do it is leveraging TChain's callback mechanism to switch column readers as needed, but a) TChain's callback mechanism is a mess and b) even if unused TTreeReaderValues for missing branches might emit warnings (unless they are completely destroyed maybe?). An more roundabout way to do the same would be to change single-thread RDataFrame so that it runs a separate event loop for each sub-tree and never uses a TChain --> you automatically get a call to `GetColumnReader` that can re-evaluate whether the branch or the default value should be used at the start of every new event loop. Finally, you can implement the default as a `Define` that always returns the same number (and in first approximation that's probably the easiest). A slightly nicer underlying implementation would involve a `ConstReader` that explicitly stores a constant value and always returns that same value, but the advantage is unclear -- a good optimizing compiler would elide the call to the lambda that always returns the same number anyways.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8704
https://github.com/root-project/root/issues/8704:329,deployability,contain,contain,329,"My bad for not managing to get to this in time. I think most of the logic would go in `GetColumnReader`, in `ColumnReaderUtils`: it would have to create a `TreeColumnReader` if the column is present in the tree and pick the reader for the default value instead. The main complication is dealing with chains in which _some_ trees contain the column and some do not. The most direct way to do it is leveraging TChain's callback mechanism to switch column readers as needed, but a) TChain's callback mechanism is a mess and b) even if unused TTreeReaderValues for missing branches might emit warnings (unless they are completely destroyed maybe?). An more roundabout way to do the same would be to change single-thread RDataFrame so that it runs a separate event loop for each sub-tree and never uses a TChain --> you automatically get a call to `GetColumnReader` that can re-evaluate whether the branch or the default value should be used at the start of every new event loop. Finally, you can implement the default as a `Define` that always returns the same number (and in first approximation that's probably the easiest). A slightly nicer underlying implementation would involve a `ConstReader` that explicitly stores a constant value and always returns that same value, but the advantage is unclear -- a good optimizing compiler would elide the call to the lambda that always returns the same number anyways.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8704
https://github.com/root-project/root/issues/8704:815,deployability,automat,automatically,815,"My bad for not managing to get to this in time. I think most of the logic would go in `GetColumnReader`, in `ColumnReaderUtils`: it would have to create a `TreeColumnReader` if the column is present in the tree and pick the reader for the default value instead. The main complication is dealing with chains in which _some_ trees contain the column and some do not. The most direct way to do it is leveraging TChain's callback mechanism to switch column readers as needed, but a) TChain's callback mechanism is a mess and b) even if unused TTreeReaderValues for missing branches might emit warnings (unless they are completely destroyed maybe?). An more roundabout way to do the same would be to change single-thread RDataFrame so that it runs a separate event loop for each sub-tree and never uses a TChain --> you automatically get a call to `GetColumnReader` that can re-evaluate whether the branch or the default value should be used at the start of every new event loop. Finally, you can implement the default as a `Define` that always returns the same number (and in first approximation that's probably the easiest). A slightly nicer underlying implementation would involve a `ConstReader` that explicitly stores a constant value and always returns that same value, but the advantage is unclear -- a good optimizing compiler would elide the call to the lambda that always returns the same number anyways.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8704
https://github.com/root-project/root/issues/8704:15,energy efficiency,manag,managing,15,"My bad for not managing to get to this in time. I think most of the logic would go in `GetColumnReader`, in `ColumnReaderUtils`: it would have to create a `TreeColumnReader` if the column is present in the tree and pick the reader for the default value instead. The main complication is dealing with chains in which _some_ trees contain the column and some do not. The most direct way to do it is leveraging TChain's callback mechanism to switch column readers as needed, but a) TChain's callback mechanism is a mess and b) even if unused TTreeReaderValues for missing branches might emit warnings (unless they are completely destroyed maybe?). An more roundabout way to do the same would be to change single-thread RDataFrame so that it runs a separate event loop for each sub-tree and never uses a TChain --> you automatically get a call to `GetColumnReader` that can re-evaluate whether the branch or the default value should be used at the start of every new event loop. Finally, you can implement the default as a `Define` that always returns the same number (and in first approximation that's probably the easiest). A slightly nicer underlying implementation would involve a `ConstReader` that explicitly stores a constant value and always returns that same value, but the advantage is unclear -- a good optimizing compiler would elide the call to the lambda that always returns the same number anyways.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8704
https://github.com/root-project/root/issues/8704:1310,energy efficiency,optim,optimizing,1310,"My bad for not managing to get to this in time. I think most of the logic would go in `GetColumnReader`, in `ColumnReaderUtils`: it would have to create a `TreeColumnReader` if the column is present in the tree and pick the reader for the default value instead. The main complication is dealing with chains in which _some_ trees contain the column and some do not. The most direct way to do it is leveraging TChain's callback mechanism to switch column readers as needed, but a) TChain's callback mechanism is a mess and b) even if unused TTreeReaderValues for missing branches might emit warnings (unless they are completely destroyed maybe?). An more roundabout way to do the same would be to change single-thread RDataFrame so that it runs a separate event loop for each sub-tree and never uses a TChain --> you automatically get a call to `GetColumnReader` that can re-evaluate whether the branch or the default value should be used at the start of every new event loop. Finally, you can implement the default as a `Define` that always returns the same number (and in first approximation that's probably the easiest). A slightly nicer underlying implementation would involve a `ConstReader` that explicitly stores a constant value and always returns that same value, but the advantage is unclear -- a good optimizing compiler would elide the call to the lambda that always returns the same number anyways.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8704
https://github.com/root-project/root/issues/8704:754,integrability,event,event,754,"My bad for not managing to get to this in time. I think most of the logic would go in `GetColumnReader`, in `ColumnReaderUtils`: it would have to create a `TreeColumnReader` if the column is present in the tree and pick the reader for the default value instead. The main complication is dealing with chains in which _some_ trees contain the column and some do not. The most direct way to do it is leveraging TChain's callback mechanism to switch column readers as needed, but a) TChain's callback mechanism is a mess and b) even if unused TTreeReaderValues for missing branches might emit warnings (unless they are completely destroyed maybe?). An more roundabout way to do the same would be to change single-thread RDataFrame so that it runs a separate event loop for each sub-tree and never uses a TChain --> you automatically get a call to `GetColumnReader` that can re-evaluate whether the branch or the default value should be used at the start of every new event loop. Finally, you can implement the default as a `Define` that always returns the same number (and in first approximation that's probably the easiest). A slightly nicer underlying implementation would involve a `ConstReader` that explicitly stores a constant value and always returns that same value, but the advantage is unclear -- a good optimizing compiler would elide the call to the lambda that always returns the same number anyways.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8704
https://github.com/root-project/root/issues/8704:774,integrability,sub,sub-tree,774,"My bad for not managing to get to this in time. I think most of the logic would go in `GetColumnReader`, in `ColumnReaderUtils`: it would have to create a `TreeColumnReader` if the column is present in the tree and pick the reader for the default value instead. The main complication is dealing with chains in which _some_ trees contain the column and some do not. The most direct way to do it is leveraging TChain's callback mechanism to switch column readers as needed, but a) TChain's callback mechanism is a mess and b) even if unused TTreeReaderValues for missing branches might emit warnings (unless they are completely destroyed maybe?). An more roundabout way to do the same would be to change single-thread RDataFrame so that it runs a separate event loop for each sub-tree and never uses a TChain --> you automatically get a call to `GetColumnReader` that can re-evaluate whether the branch or the default value should be used at the start of every new event loop. Finally, you can implement the default as a `Define` that always returns the same number (and in first approximation that's probably the easiest). A slightly nicer underlying implementation would involve a `ConstReader` that explicitly stores a constant value and always returns that same value, but the advantage is unclear -- a good optimizing compiler would elide the call to the lambda that always returns the same number anyways.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8704
https://github.com/root-project/root/issues/8704:963,integrability,event,event,963,"My bad for not managing to get to this in time. I think most of the logic would go in `GetColumnReader`, in `ColumnReaderUtils`: it would have to create a `TreeColumnReader` if the column is present in the tree and pick the reader for the default value instead. The main complication is dealing with chains in which _some_ trees contain the column and some do not. The most direct way to do it is leveraging TChain's callback mechanism to switch column readers as needed, but a) TChain's callback mechanism is a mess and b) even if unused TTreeReaderValues for missing branches might emit warnings (unless they are completely destroyed maybe?). An more roundabout way to do the same would be to change single-thread RDataFrame so that it runs a separate event loop for each sub-tree and never uses a TChain --> you automatically get a call to `GetColumnReader` that can re-evaluate whether the branch or the default value should be used at the start of every new event loop. Finally, you can implement the default as a `Define` that always returns the same number (and in first approximation that's probably the easiest). A slightly nicer underlying implementation would involve a `ConstReader` that explicitly stores a constant value and always returns that same value, but the advantage is unclear -- a good optimizing compiler would elide the call to the lambda that always returns the same number anyways.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8704
https://github.com/root-project/root/issues/8704:42,performance,time,time,42,"My bad for not managing to get to this in time. I think most of the logic would go in `GetColumnReader`, in `ColumnReaderUtils`: it would have to create a `TreeColumnReader` if the column is present in the tree and pick the reader for the default value instead. The main complication is dealing with chains in which _some_ trees contain the column and some do not. The most direct way to do it is leveraging TChain's callback mechanism to switch column readers as needed, but a) TChain's callback mechanism is a mess and b) even if unused TTreeReaderValues for missing branches might emit warnings (unless they are completely destroyed maybe?). An more roundabout way to do the same would be to change single-thread RDataFrame so that it runs a separate event loop for each sub-tree and never uses a TChain --> you automatically get a call to `GetColumnReader` that can re-evaluate whether the branch or the default value should be used at the start of every new event loop. Finally, you can implement the default as a `Define` that always returns the same number (and in first approximation that's probably the easiest). A slightly nicer underlying implementation would involve a `ConstReader` that explicitly stores a constant value and always returns that same value, but the advantage is unclear -- a good optimizing compiler would elide the call to the lambda that always returns the same number anyways.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8704
https://github.com/root-project/root/issues/8704:1310,performance,optimiz,optimizing,1310,"My bad for not managing to get to this in time. I think most of the logic would go in `GetColumnReader`, in `ColumnReaderUtils`: it would have to create a `TreeColumnReader` if the column is present in the tree and pick the reader for the default value instead. The main complication is dealing with chains in which _some_ trees contain the column and some do not. The most direct way to do it is leveraging TChain's callback mechanism to switch column readers as needed, but a) TChain's callback mechanism is a mess and b) even if unused TTreeReaderValues for missing branches might emit warnings (unless they are completely destroyed maybe?). An more roundabout way to do the same would be to change single-thread RDataFrame so that it runs a separate event loop for each sub-tree and never uses a TChain --> you automatically get a call to `GetColumnReader` that can re-evaluate whether the branch or the default value should be used at the start of every new event loop. Finally, you can implement the default as a `Define` that always returns the same number (and in first approximation that's probably the easiest). A slightly nicer underlying implementation would involve a `ConstReader` that explicitly stores a constant value and always returns that same value, but the advantage is unclear -- a good optimizing compiler would elide the call to the lambda that always returns the same number anyways.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8704
https://github.com/root-project/root/issues/8704:1124,reliability,sli,slightly,1124,"My bad for not managing to get to this in time. I think most of the logic would go in `GetColumnReader`, in `ColumnReaderUtils`: it would have to create a `TreeColumnReader` if the column is present in the tree and pick the reader for the default value instead. The main complication is dealing with chains in which _some_ trees contain the column and some do not. The most direct way to do it is leveraging TChain's callback mechanism to switch column readers as needed, but a) TChain's callback mechanism is a mess and b) even if unused TTreeReaderValues for missing branches might emit warnings (unless they are completely destroyed maybe?). An more roundabout way to do the same would be to change single-thread RDataFrame so that it runs a separate event loop for each sub-tree and never uses a TChain --> you automatically get a call to `GetColumnReader` that can re-evaluate whether the branch or the default value should be used at the start of every new event loop. Finally, you can implement the default as a `Define` that always returns the same number (and in first approximation that's probably the easiest). A slightly nicer underlying implementation would involve a `ConstReader` that explicitly stores a constant value and always returns that same value, but the advantage is unclear -- a good optimizing compiler would elide the call to the lambda that always returns the same number anyways.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8704
https://github.com/root-project/root/issues/8704:15,safety,manag,managing,15,"My bad for not managing to get to this in time. I think most of the logic would go in `GetColumnReader`, in `ColumnReaderUtils`: it would have to create a `TreeColumnReader` if the column is present in the tree and pick the reader for the default value instead. The main complication is dealing with chains in which _some_ trees contain the column and some do not. The most direct way to do it is leveraging TChain's callback mechanism to switch column readers as needed, but a) TChain's callback mechanism is a mess and b) even if unused TTreeReaderValues for missing branches might emit warnings (unless they are completely destroyed maybe?). An more roundabout way to do the same would be to change single-thread RDataFrame so that it runs a separate event loop for each sub-tree and never uses a TChain --> you automatically get a call to `GetColumnReader` that can re-evaluate whether the branch or the default value should be used at the start of every new event loop. Finally, you can implement the default as a `Define` that always returns the same number (and in first approximation that's probably the easiest). A slightly nicer underlying implementation would involve a `ConstReader` that explicitly stores a constant value and always returns that same value, but the advantage is unclear -- a good optimizing compiler would elide the call to the lambda that always returns the same number anyways.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8704
https://github.com/root-project/root/issues/8704:68,safety,log,logic,68,"My bad for not managing to get to this in time. I think most of the logic would go in `GetColumnReader`, in `ColumnReaderUtils`: it would have to create a `TreeColumnReader` if the column is present in the tree and pick the reader for the default value instead. The main complication is dealing with chains in which _some_ trees contain the column and some do not. The most direct way to do it is leveraging TChain's callback mechanism to switch column readers as needed, but a) TChain's callback mechanism is a mess and b) even if unused TTreeReaderValues for missing branches might emit warnings (unless they are completely destroyed maybe?). An more roundabout way to do the same would be to change single-thread RDataFrame so that it runs a separate event loop for each sub-tree and never uses a TChain --> you automatically get a call to `GetColumnReader` that can re-evaluate whether the branch or the default value should be used at the start of every new event loop. Finally, you can implement the default as a `Define` that always returns the same number (and in first approximation that's probably the easiest). A slightly nicer underlying implementation would involve a `ConstReader` that explicitly stores a constant value and always returns that same value, but the advantage is unclear -- a good optimizing compiler would elide the call to the lambda that always returns the same number anyways.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8704
https://github.com/root-project/root/issues/8704:271,safety,compl,complication,271,"My bad for not managing to get to this in time. I think most of the logic would go in `GetColumnReader`, in `ColumnReaderUtils`: it would have to create a `TreeColumnReader` if the column is present in the tree and pick the reader for the default value instead. The main complication is dealing with chains in which _some_ trees contain the column and some do not. The most direct way to do it is leveraging TChain's callback mechanism to switch column readers as needed, but a) TChain's callback mechanism is a mess and b) even if unused TTreeReaderValues for missing branches might emit warnings (unless they are completely destroyed maybe?). An more roundabout way to do the same would be to change single-thread RDataFrame so that it runs a separate event loop for each sub-tree and never uses a TChain --> you automatically get a call to `GetColumnReader` that can re-evaluate whether the branch or the default value should be used at the start of every new event loop. Finally, you can implement the default as a `Define` that always returns the same number (and in first approximation that's probably the easiest). A slightly nicer underlying implementation would involve a `ConstReader` that explicitly stores a constant value and always returns that same value, but the advantage is unclear -- a good optimizing compiler would elide the call to the lambda that always returns the same number anyways.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8704
https://github.com/root-project/root/issues/8704:615,safety,compl,completely,615,"My bad for not managing to get to this in time. I think most of the logic would go in `GetColumnReader`, in `ColumnReaderUtils`: it would have to create a `TreeColumnReader` if the column is present in the tree and pick the reader for the default value instead. The main complication is dealing with chains in which _some_ trees contain the column and some do not. The most direct way to do it is leveraging TChain's callback mechanism to switch column readers as needed, but a) TChain's callback mechanism is a mess and b) even if unused TTreeReaderValues for missing branches might emit warnings (unless they are completely destroyed maybe?). An more roundabout way to do the same would be to change single-thread RDataFrame so that it runs a separate event loop for each sub-tree and never uses a TChain --> you automatically get a call to `GetColumnReader` that can re-evaluate whether the branch or the default value should be used at the start of every new event loop. Finally, you can implement the default as a `Define` that always returns the same number (and in first approximation that's probably the easiest). A slightly nicer underlying implementation would involve a `ConstReader` that explicitly stores a constant value and always returns that same value, but the advantage is unclear -- a good optimizing compiler would elide the call to the lambda that always returns the same number anyways.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8704
https://github.com/root-project/root/issues/8704:68,security,log,logic,68,"My bad for not managing to get to this in time. I think most of the logic would go in `GetColumnReader`, in `ColumnReaderUtils`: it would have to create a `TreeColumnReader` if the column is present in the tree and pick the reader for the default value instead. The main complication is dealing with chains in which _some_ trees contain the column and some do not. The most direct way to do it is leveraging TChain's callback mechanism to switch column readers as needed, but a) TChain's callback mechanism is a mess and b) even if unused TTreeReaderValues for missing branches might emit warnings (unless they are completely destroyed maybe?). An more roundabout way to do the same would be to change single-thread RDataFrame so that it runs a separate event loop for each sub-tree and never uses a TChain --> you automatically get a call to `GetColumnReader` that can re-evaluate whether the branch or the default value should be used at the start of every new event loop. Finally, you can implement the default as a `Define` that always returns the same number (and in first approximation that's probably the easiest). A slightly nicer underlying implementation would involve a `ConstReader` that explicitly stores a constant value and always returns that same value, but the advantage is unclear -- a good optimizing compiler would elide the call to the lambda that always returns the same number anyways.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8704
https://github.com/root-project/root/issues/8704:271,security,compl,complication,271,"My bad for not managing to get to this in time. I think most of the logic would go in `GetColumnReader`, in `ColumnReaderUtils`: it would have to create a `TreeColumnReader` if the column is present in the tree and pick the reader for the default value instead. The main complication is dealing with chains in which _some_ trees contain the column and some do not. The most direct way to do it is leveraging TChain's callback mechanism to switch column readers as needed, but a) TChain's callback mechanism is a mess and b) even if unused TTreeReaderValues for missing branches might emit warnings (unless they are completely destroyed maybe?). An more roundabout way to do the same would be to change single-thread RDataFrame so that it runs a separate event loop for each sub-tree and never uses a TChain --> you automatically get a call to `GetColumnReader` that can re-evaluate whether the branch or the default value should be used at the start of every new event loop. Finally, you can implement the default as a `Define` that always returns the same number (and in first approximation that's probably the easiest). A slightly nicer underlying implementation would involve a `ConstReader` that explicitly stores a constant value and always returns that same value, but the advantage is unclear -- a good optimizing compiler would elide the call to the lambda that always returns the same number anyways.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8704
https://github.com/root-project/root/issues/8704:615,security,compl,completely,615,"My bad for not managing to get to this in time. I think most of the logic would go in `GetColumnReader`, in `ColumnReaderUtils`: it would have to create a `TreeColumnReader` if the column is present in the tree and pick the reader for the default value instead. The main complication is dealing with chains in which _some_ trees contain the column and some do not. The most direct way to do it is leveraging TChain's callback mechanism to switch column readers as needed, but a) TChain's callback mechanism is a mess and b) even if unused TTreeReaderValues for missing branches might emit warnings (unless they are completely destroyed maybe?). An more roundabout way to do the same would be to change single-thread RDataFrame so that it runs a separate event loop for each sub-tree and never uses a TChain --> you automatically get a call to `GetColumnReader` that can re-evaluate whether the branch or the default value should be used at the start of every new event loop. Finally, you can implement the default as a `Define` that always returns the same number (and in first approximation that's probably the easiest). A slightly nicer underlying implementation would involve a `ConstReader` that explicitly stores a constant value and always returns that same value, but the advantage is unclear -- a good optimizing compiler would elide the call to the lambda that always returns the same number anyways.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8704
https://github.com/root-project/root/issues/8704:68,testability,log,logic,68,"My bad for not managing to get to this in time. I think most of the logic would go in `GetColumnReader`, in `ColumnReaderUtils`: it would have to create a `TreeColumnReader` if the column is present in the tree and pick the reader for the default value instead. The main complication is dealing with chains in which _some_ trees contain the column and some do not. The most direct way to do it is leveraging TChain's callback mechanism to switch column readers as needed, but a) TChain's callback mechanism is a mess and b) even if unused TTreeReaderValues for missing branches might emit warnings (unless they are completely destroyed maybe?). An more roundabout way to do the same would be to change single-thread RDataFrame so that it runs a separate event loop for each sub-tree and never uses a TChain --> you automatically get a call to `GetColumnReader` that can re-evaluate whether the branch or the default value should be used at the start of every new event loop. Finally, you can implement the default as a `Define` that always returns the same number (and in first approximation that's probably the easiest). A slightly nicer underlying implementation would involve a `ConstReader` that explicitly stores a constant value and always returns that same value, but the advantage is unclear -- a good optimizing compiler would elide the call to the lambda that always returns the same number anyways.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8704
https://github.com/root-project/root/issues/8704:815,testability,automat,automatically,815,"My bad for not managing to get to this in time. I think most of the logic would go in `GetColumnReader`, in `ColumnReaderUtils`: it would have to create a `TreeColumnReader` if the column is present in the tree and pick the reader for the default value instead. The main complication is dealing with chains in which _some_ trees contain the column and some do not. The most direct way to do it is leveraging TChain's callback mechanism to switch column readers as needed, but a) TChain's callback mechanism is a mess and b) even if unused TTreeReaderValues for missing branches might emit warnings (unless they are completely destroyed maybe?). An more roundabout way to do the same would be to change single-thread RDataFrame so that it runs a separate event loop for each sub-tree and never uses a TChain --> you automatically get a call to `GetColumnReader` that can re-evaluate whether the branch or the default value should be used at the start of every new event loop. Finally, you can implement the default as a `Define` that always returns the same number (and in first approximation that's probably the easiest). A slightly nicer underlying implementation would involve a `ConstReader` that explicitly stores a constant value and always returns that same value, but the advantage is unclear -- a good optimizing compiler would elide the call to the lambda that always returns the same number anyways.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8704
https://github.com/root-project/root/pull/8705:11,deployability,build,build,11,@phsft-bot build,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8705
https://github.com/root-project/root/pull/8705:11,deployability,build,build,11,@phsft-bot build,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8705
https://github.com/root-project/root/pull/8705:11,deployability,build,build,11,@phsft-bot build,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8705
https://github.com/root-project/root/issues/8706:210,deployability,continu,continued,210,"Thanks, @omasanori ! Zenodo and I couldn't find a way to set the authors correctly: I needed to manually enter all authors. Once I know of a way to teach Zenodo about the author list *I* want it to use without continued manual intervention then I'll be happy to have this happen, automatically or not. If you have any pointers on that please let me know!",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8706
https://github.com/root-project/root/issues/8706:280,deployability,automat,automatically,280,"Thanks, @omasanori ! Zenodo and I couldn't find a way to set the authors correctly: I needed to manually enter all authors. Once I know of a way to teach Zenodo about the author list *I* want it to use without continued manual intervention then I'll be happy to have this happen, automatically or not. If you have any pointers on that please let me know!",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8706
https://github.com/root-project/root/issues/8706:65,security,auth,authors,65,"Thanks, @omasanori ! Zenodo and I couldn't find a way to set the authors correctly: I needed to manually enter all authors. Once I know of a way to teach Zenodo about the author list *I* want it to use without continued manual intervention then I'll be happy to have this happen, automatically or not. If you have any pointers on that please let me know!",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8706
https://github.com/root-project/root/issues/8706:115,security,auth,authors,115,"Thanks, @omasanori ! Zenodo and I couldn't find a way to set the authors correctly: I needed to manually enter all authors. Once I know of a way to teach Zenodo about the author list *I* want it to use without continued manual intervention then I'll be happy to have this happen, automatically or not. If you have any pointers on that please let me know!",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8706
https://github.com/root-project/root/issues/8706:171,security,auth,author,171,"Thanks, @omasanori ! Zenodo and I couldn't find a way to set the authors correctly: I needed to manually enter all authors. Once I know of a way to teach Zenodo about the author list *I* want it to use without continued manual intervention then I'll be happy to have this happen, automatically or not. If you have any pointers on that please let me know!",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8706
https://github.com/root-project/root/issues/8706:280,testability,automat,automatically,280,"Thanks, @omasanori ! Zenodo and I couldn't find a way to set the authors correctly: I needed to manually enter all authors. Once I know of a way to teach Zenodo about the author list *I* want it to use without continued manual intervention then I'll be happy to have this happen, automatically or not. If you have any pointers on that please let me know!",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8706
https://github.com/root-project/root/issues/8706:103,deployability,API,API,103,"The canonical list of authors is `README/CREDITS`, right? I am not sure, but at least there is a [REST API](https://developers.zenodo.org/#quickstart-upload) so inserting metadata from that file when uploading records should be possible...",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8706
https://github.com/root-project/root/issues/8706:103,integrability,API,API,103,"The canonical list of authors is `README/CREDITS`, right? I am not sure, but at least there is a [REST API](https://developers.zenodo.org/#quickstart-upload) so inserting metadata from that file when uploading records should be possible...",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8706
https://github.com/root-project/root/issues/8706:103,interoperability,API,API,103,"The canonical list of authors is `README/CREDITS`, right? I am not sure, but at least there is a [REST API](https://developers.zenodo.org/#quickstart-upload) so inserting metadata from that file when uploading records should be possible...",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8706
https://github.com/root-project/root/issues/8706:22,security,auth,authors,22,"The canonical list of authors is `README/CREDITS`, right? I am not sure, but at least there is a [REST API](https://developers.zenodo.org/#quickstart-upload) so inserting metadata from that file when uploading records should be possible...",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8706
https://github.com/root-project/root/issues/8706:182,deployability,automat,automatically,182,"If it helps: I know at least that for instance the [JuliaPlots/Plots](https://github.com/JuliaPlots/Plots.jl) project simply uses a `.zenodo.json` file in their repo which should be automatically parsed, according to [REST API subsection](https://developers.zenodo.org/#add-metadata-to-your-github-repository-release), whenever a new release is created.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8706
https://github.com/root-project/root/issues/8706:223,deployability,API,API,223,"If it helps: I know at least that for instance the [JuliaPlots/Plots](https://github.com/JuliaPlots/Plots.jl) project simply uses a `.zenodo.json` file in their repo which should be automatically parsed, according to [REST API subsection](https://developers.zenodo.org/#add-metadata-to-your-github-repository-release), whenever a new release is created.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8706
https://github.com/root-project/root/issues/8706:309,deployability,releas,release,309,"If it helps: I know at least that for instance the [JuliaPlots/Plots](https://github.com/JuliaPlots/Plots.jl) project simply uses a `.zenodo.json` file in their repo which should be automatically parsed, according to [REST API subsection](https://developers.zenodo.org/#add-metadata-to-your-github-repository-release), whenever a new release is created.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8706
https://github.com/root-project/root/issues/8706:334,deployability,releas,release,334,"If it helps: I know at least that for instance the [JuliaPlots/Plots](https://github.com/JuliaPlots/Plots.jl) project simply uses a `.zenodo.json` file in their repo which should be automatically parsed, according to [REST API subsection](https://developers.zenodo.org/#add-metadata-to-your-github-repository-release), whenever a new release is created.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8706
https://github.com/root-project/root/issues/8706:223,integrability,API,API,223,"If it helps: I know at least that for instance the [JuliaPlots/Plots](https://github.com/JuliaPlots/Plots.jl) project simply uses a `.zenodo.json` file in their repo which should be automatically parsed, according to [REST API subsection](https://developers.zenodo.org/#add-metadata-to-your-github-repository-release), whenever a new release is created.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8706
https://github.com/root-project/root/issues/8706:227,integrability,sub,subsection,227,"If it helps: I know at least that for instance the [JuliaPlots/Plots](https://github.com/JuliaPlots/Plots.jl) project simply uses a `.zenodo.json` file in their repo which should be automatically parsed, according to [REST API subsection](https://developers.zenodo.org/#add-metadata-to-your-github-repository-release), whenever a new release is created.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8706
https://github.com/root-project/root/issues/8706:298,integrability,repositor,repository-release,298,"If it helps: I know at least that for instance the [JuliaPlots/Plots](https://github.com/JuliaPlots/Plots.jl) project simply uses a `.zenodo.json` file in their repo which should be automatically parsed, according to [REST API subsection](https://developers.zenodo.org/#add-metadata-to-your-github-repository-release), whenever a new release is created.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8706
https://github.com/root-project/root/issues/8706:223,interoperability,API,API,223,"If it helps: I know at least that for instance the [JuliaPlots/Plots](https://github.com/JuliaPlots/Plots.jl) project simply uses a `.zenodo.json` file in their repo which should be automatically parsed, according to [REST API subsection](https://developers.zenodo.org/#add-metadata-to-your-github-repository-release), whenever a new release is created.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8706
https://github.com/root-project/root/issues/8706:298,interoperability,repositor,repository-release,298,"If it helps: I know at least that for instance the [JuliaPlots/Plots](https://github.com/JuliaPlots/Plots.jl) project simply uses a `.zenodo.json` file in their repo which should be automatically parsed, according to [REST API subsection](https://developers.zenodo.org/#add-metadata-to-your-github-repository-release), whenever a new release is created.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8706
https://github.com/root-project/root/issues/8706:118,testability,simpl,simply,118,"If it helps: I know at least that for instance the [JuliaPlots/Plots](https://github.com/JuliaPlots/Plots.jl) project simply uses a `.zenodo.json` file in their repo which should be automatically parsed, according to [REST API subsection](https://developers.zenodo.org/#add-metadata-to-your-github-repository-release), whenever a new release is created.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8706
https://github.com/root-project/root/issues/8706:182,testability,automat,automatically,182,"If it helps: I know at least that for instance the [JuliaPlots/Plots](https://github.com/JuliaPlots/Plots.jl) project simply uses a `.zenodo.json` file in their repo which should be automatically parsed, according to [REST API subsection](https://developers.zenodo.org/#add-metadata-to-your-github-repository-release), whenever a new release is created.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8706
https://github.com/root-project/root/issues/8706:6,usability,help,helps,6,"If it helps: I know at least that for instance the [JuliaPlots/Plots](https://github.com/JuliaPlots/Plots.jl) project simply uses a `.zenodo.json` file in their repo which should be automatically parsed, according to [REST API subsection](https://developers.zenodo.org/#add-metadata-to-your-github-repository-release), whenever a new release is created.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8706
https://github.com/root-project/root/issues/8706:118,usability,simpl,simply,118,"If it helps: I know at least that for instance the [JuliaPlots/Plots](https://github.com/JuliaPlots/Plots.jl) project simply uses a `.zenodo.json` file in their repo which should be automatically parsed, according to [REST API subsection](https://developers.zenodo.org/#add-metadata-to-your-github-repository-release), whenever a new release is created.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8706
https://github.com/root-project/root/issues/8706:52,deployability,updat,update,52,"Thanks! Agreed, that's what we figured out as well: update `.zenodo.json` and `CITATION.CFF` (#8626 ) as part of the release baking.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8706
https://github.com/root-project/root/issues/8706:117,deployability,releas,release,117,"Thanks! Agreed, that's what we figured out as well: update `.zenodo.json` and `CITATION.CFF` (#8626 ) as part of the release baking.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8706
https://github.com/root-project/root/issues/8706:52,safety,updat,update,52,"Thanks! Agreed, that's what we figured out as well: update `.zenodo.json` and `CITATION.CFF` (#8626 ) as part of the release baking.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8706
https://github.com/root-project/root/issues/8706:52,security,updat,update,52,"Thanks! Agreed, that's what we figured out as well: update `.zenodo.json` and `CITATION.CFF` (#8626 ) as part of the release baking.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8706
https://github.com/root-project/root/issues/8708:181,availability,error,error,181,"#8709 fixes a part of this. ## What remains to be done:. It is likely that more builtins (or rather `FindXXX` have to be converted to `IMPORTED` targets, so they don't provoke this error again. A broken configuration can be detected by. 1. Having CMake pick up a dependency in some common directory, e.g. install a lot of builtins there. 2. Either. - Place an entire ROOT installation there, where all headers start with `#error This is the wrong header`. or. - Search `compile_commands.json` for `-I/my/include/directory/`. 3. Fix the `FindXXX` for all dependencies that provoke the above error.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8708
https://github.com/root-project/root/issues/8708:423,availability,error,error,423,"#8709 fixes a part of this. ## What remains to be done:. It is likely that more builtins (or rather `FindXXX` have to be converted to `IMPORTED` targets, so they don't provoke this error again. A broken configuration can be detected by. 1. Having CMake pick up a dependency in some common directory, e.g. install a lot of builtins there. 2. Either. - Place an entire ROOT installation there, where all headers start with `#error This is the wrong header`. or. - Search `compile_commands.json` for `-I/my/include/directory/`. 3. Fix the `FindXXX` for all dependencies that provoke the above error.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8708
https://github.com/root-project/root/issues/8708:590,availability,error,error,590,"#8709 fixes a part of this. ## What remains to be done:. It is likely that more builtins (or rather `FindXXX` have to be converted to `IMPORTED` targets, so they don't provoke this error again. A broken configuration can be detected by. 1. Having CMake pick up a dependency in some common directory, e.g. install a lot of builtins there. 2. Either. - Place an entire ROOT installation there, where all headers start with `#error This is the wrong header`. or. - Search `compile_commands.json` for `-I/my/include/directory/`. 3. Fix the `FindXXX` for all dependencies that provoke the above error.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8708
https://github.com/root-project/root/issues/8708:203,deployability,configurat,configuration,203,"#8709 fixes a part of this. ## What remains to be done:. It is likely that more builtins (or rather `FindXXX` have to be converted to `IMPORTED` targets, so they don't provoke this error again. A broken configuration can be detected by. 1. Having CMake pick up a dependency in some common directory, e.g. install a lot of builtins there. 2. Either. - Place an entire ROOT installation there, where all headers start with `#error This is the wrong header`. or. - Search `compile_commands.json` for `-I/my/include/directory/`. 3. Fix the `FindXXX` for all dependencies that provoke the above error.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8708
https://github.com/root-project/root/issues/8708:263,deployability,depend,dependency,263,"#8709 fixes a part of this. ## What remains to be done:. It is likely that more builtins (or rather `FindXXX` have to be converted to `IMPORTED` targets, so they don't provoke this error again. A broken configuration can be detected by. 1. Having CMake pick up a dependency in some common directory, e.g. install a lot of builtins there. 2. Either. - Place an entire ROOT installation there, where all headers start with `#error This is the wrong header`. or. - Search `compile_commands.json` for `-I/my/include/directory/`. 3. Fix the `FindXXX` for all dependencies that provoke the above error.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8708
https://github.com/root-project/root/issues/8708:305,deployability,instal,install,305,"#8709 fixes a part of this. ## What remains to be done:. It is likely that more builtins (or rather `FindXXX` have to be converted to `IMPORTED` targets, so they don't provoke this error again. A broken configuration can be detected by. 1. Having CMake pick up a dependency in some common directory, e.g. install a lot of builtins there. 2. Either. - Place an entire ROOT installation there, where all headers start with `#error This is the wrong header`. or. - Search `compile_commands.json` for `-I/my/include/directory/`. 3. Fix the `FindXXX` for all dependencies that provoke the above error.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8708
https://github.com/root-project/root/issues/8708:372,deployability,instal,installation,372,"#8709 fixes a part of this. ## What remains to be done:. It is likely that more builtins (or rather `FindXXX` have to be converted to `IMPORTED` targets, so they don't provoke this error again. A broken configuration can be detected by. 1. Having CMake pick up a dependency in some common directory, e.g. install a lot of builtins there. 2. Either. - Place an entire ROOT installation there, where all headers start with `#error This is the wrong header`. or. - Search `compile_commands.json` for `-I/my/include/directory/`. 3. Fix the `FindXXX` for all dependencies that provoke the above error.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8708
https://github.com/root-project/root/issues/8708:554,deployability,depend,dependencies,554,"#8709 fixes a part of this. ## What remains to be done:. It is likely that more builtins (or rather `FindXXX` have to be converted to `IMPORTED` targets, so they don't provoke this error again. A broken configuration can be detected by. 1. Having CMake pick up a dependency in some common directory, e.g. install a lot of builtins there. 2. Either. - Place an entire ROOT installation there, where all headers start with `#error This is the wrong header`. or. - Search `compile_commands.json` for `-I/my/include/directory/`. 3. Fix the `FindXXX` for all dependencies that provoke the above error.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8708
https://github.com/root-project/root/issues/8708:203,integrability,configur,configuration,203,"#8709 fixes a part of this. ## What remains to be done:. It is likely that more builtins (or rather `FindXXX` have to be converted to `IMPORTED` targets, so they don't provoke this error again. A broken configuration can be detected by. 1. Having CMake pick up a dependency in some common directory, e.g. install a lot of builtins there. 2. Either. - Place an entire ROOT installation there, where all headers start with `#error This is the wrong header`. or. - Search `compile_commands.json` for `-I/my/include/directory/`. 3. Fix the `FindXXX` for all dependencies that provoke the above error.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8708
https://github.com/root-project/root/issues/8708:263,integrability,depend,dependency,263,"#8709 fixes a part of this. ## What remains to be done:. It is likely that more builtins (or rather `FindXXX` have to be converted to `IMPORTED` targets, so they don't provoke this error again. A broken configuration can be detected by. 1. Having CMake pick up a dependency in some common directory, e.g. install a lot of builtins there. 2. Either. - Place an entire ROOT installation there, where all headers start with `#error This is the wrong header`. or. - Search `compile_commands.json` for `-I/my/include/directory/`. 3. Fix the `FindXXX` for all dependencies that provoke the above error.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8708
https://github.com/root-project/root/issues/8708:554,integrability,depend,dependencies,554,"#8709 fixes a part of this. ## What remains to be done:. It is likely that more builtins (or rather `FindXXX` have to be converted to `IMPORTED` targets, so they don't provoke this error again. A broken configuration can be detected by. 1. Having CMake pick up a dependency in some common directory, e.g. install a lot of builtins there. 2. Either. - Place an entire ROOT installation there, where all headers start with `#error This is the wrong header`. or. - Search `compile_commands.json` for `-I/my/include/directory/`. 3. Fix the `FindXXX` for all dependencies that provoke the above error.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8708
https://github.com/root-project/root/issues/8708:203,modifiability,configur,configuration,203,"#8709 fixes a part of this. ## What remains to be done:. It is likely that more builtins (or rather `FindXXX` have to be converted to `IMPORTED` targets, so they don't provoke this error again. A broken configuration can be detected by. 1. Having CMake pick up a dependency in some common directory, e.g. install a lot of builtins there. 2. Either. - Place an entire ROOT installation there, where all headers start with `#error This is the wrong header`. or. - Search `compile_commands.json` for `-I/my/include/directory/`. 3. Fix the `FindXXX` for all dependencies that provoke the above error.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8708
https://github.com/root-project/root/issues/8708:263,modifiability,depend,dependency,263,"#8709 fixes a part of this. ## What remains to be done:. It is likely that more builtins (or rather `FindXXX` have to be converted to `IMPORTED` targets, so they don't provoke this error again. A broken configuration can be detected by. 1. Having CMake pick up a dependency in some common directory, e.g. install a lot of builtins there. 2. Either. - Place an entire ROOT installation there, where all headers start with `#error This is the wrong header`. or. - Search `compile_commands.json` for `-I/my/include/directory/`. 3. Fix the `FindXXX` for all dependencies that provoke the above error.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8708
https://github.com/root-project/root/issues/8708:554,modifiability,depend,dependencies,554,"#8709 fixes a part of this. ## What remains to be done:. It is likely that more builtins (or rather `FindXXX` have to be converted to `IMPORTED` targets, so they don't provoke this error again. A broken configuration can be detected by. 1. Having CMake pick up a dependency in some common directory, e.g. install a lot of builtins there. 2. Either. - Place an entire ROOT installation there, where all headers start with `#error This is the wrong header`. or. - Search `compile_commands.json` for `-I/my/include/directory/`. 3. Fix the `FindXXX` for all dependencies that provoke the above error.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8708
https://github.com/root-project/root/issues/8708:181,performance,error,error,181,"#8709 fixes a part of this. ## What remains to be done:. It is likely that more builtins (or rather `FindXXX` have to be converted to `IMPORTED` targets, so they don't provoke this error again. A broken configuration can be detected by. 1. Having CMake pick up a dependency in some common directory, e.g. install a lot of builtins there. 2. Either. - Place an entire ROOT installation there, where all headers start with `#error This is the wrong header`. or. - Search `compile_commands.json` for `-I/my/include/directory/`. 3. Fix the `FindXXX` for all dependencies that provoke the above error.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8708
https://github.com/root-project/root/issues/8708:423,performance,error,error,423,"#8709 fixes a part of this. ## What remains to be done:. It is likely that more builtins (or rather `FindXXX` have to be converted to `IMPORTED` targets, so they don't provoke this error again. A broken configuration can be detected by. 1. Having CMake pick up a dependency in some common directory, e.g. install a lot of builtins there. 2. Either. - Place an entire ROOT installation there, where all headers start with `#error This is the wrong header`. or. - Search `compile_commands.json` for `-I/my/include/directory/`. 3. Fix the `FindXXX` for all dependencies that provoke the above error.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8708
https://github.com/root-project/root/issues/8708:590,performance,error,error,590,"#8709 fixes a part of this. ## What remains to be done:. It is likely that more builtins (or rather `FindXXX` have to be converted to `IMPORTED` targets, so they don't provoke this error again. A broken configuration can be detected by. 1. Having CMake pick up a dependency in some common directory, e.g. install a lot of builtins there. 2. Either. - Place an entire ROOT installation there, where all headers start with `#error This is the wrong header`. or. - Search `compile_commands.json` for `-I/my/include/directory/`. 3. Fix the `FindXXX` for all dependencies that provoke the above error.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8708
https://github.com/root-project/root/issues/8708:181,safety,error,error,181,"#8709 fixes a part of this. ## What remains to be done:. It is likely that more builtins (or rather `FindXXX` have to be converted to `IMPORTED` targets, so they don't provoke this error again. A broken configuration can be detected by. 1. Having CMake pick up a dependency in some common directory, e.g. install a lot of builtins there. 2. Either. - Place an entire ROOT installation there, where all headers start with `#error This is the wrong header`. or. - Search `compile_commands.json` for `-I/my/include/directory/`. 3. Fix the `FindXXX` for all dependencies that provoke the above error.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8708
https://github.com/root-project/root/issues/8708:224,safety,detect,detected,224,"#8709 fixes a part of this. ## What remains to be done:. It is likely that more builtins (or rather `FindXXX` have to be converted to `IMPORTED` targets, so they don't provoke this error again. A broken configuration can be detected by. 1. Having CMake pick up a dependency in some common directory, e.g. install a lot of builtins there. 2. Either. - Place an entire ROOT installation there, where all headers start with `#error This is the wrong header`. or. - Search `compile_commands.json` for `-I/my/include/directory/`. 3. Fix the `FindXXX` for all dependencies that provoke the above error.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8708
https://github.com/root-project/root/issues/8708:263,safety,depend,dependency,263,"#8709 fixes a part of this. ## What remains to be done:. It is likely that more builtins (or rather `FindXXX` have to be converted to `IMPORTED` targets, so they don't provoke this error again. A broken configuration can be detected by. 1. Having CMake pick up a dependency in some common directory, e.g. install a lot of builtins there. 2. Either. - Place an entire ROOT installation there, where all headers start with `#error This is the wrong header`. or. - Search `compile_commands.json` for `-I/my/include/directory/`. 3. Fix the `FindXXX` for all dependencies that provoke the above error.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8708
https://github.com/root-project/root/issues/8708:423,safety,error,error,423,"#8709 fixes a part of this. ## What remains to be done:. It is likely that more builtins (or rather `FindXXX` have to be converted to `IMPORTED` targets, so they don't provoke this error again. A broken configuration can be detected by. 1. Having CMake pick up a dependency in some common directory, e.g. install a lot of builtins there. 2. Either. - Place an entire ROOT installation there, where all headers start with `#error This is the wrong header`. or. - Search `compile_commands.json` for `-I/my/include/directory/`. 3. Fix the `FindXXX` for all dependencies that provoke the above error.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8708
https://github.com/root-project/root/issues/8708:554,safety,depend,dependencies,554,"#8709 fixes a part of this. ## What remains to be done:. It is likely that more builtins (or rather `FindXXX` have to be converted to `IMPORTED` targets, so they don't provoke this error again. A broken configuration can be detected by. 1. Having CMake pick up a dependency in some common directory, e.g. install a lot of builtins there. 2. Either. - Place an entire ROOT installation there, where all headers start with `#error This is the wrong header`. or. - Search `compile_commands.json` for `-I/my/include/directory/`. 3. Fix the `FindXXX` for all dependencies that provoke the above error.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8708
https://github.com/root-project/root/issues/8708:590,safety,error,error,590,"#8709 fixes a part of this. ## What remains to be done:. It is likely that more builtins (or rather `FindXXX` have to be converted to `IMPORTED` targets, so they don't provoke this error again. A broken configuration can be detected by. 1. Having CMake pick up a dependency in some common directory, e.g. install a lot of builtins there. 2. Either. - Place an entire ROOT installation there, where all headers start with `#error This is the wrong header`. or. - Search `compile_commands.json` for `-I/my/include/directory/`. 3. Fix the `FindXXX` for all dependencies that provoke the above error.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8708
https://github.com/root-project/root/issues/8708:203,security,configur,configuration,203,"#8709 fixes a part of this. ## What remains to be done:. It is likely that more builtins (or rather `FindXXX` have to be converted to `IMPORTED` targets, so they don't provoke this error again. A broken configuration can be detected by. 1. Having CMake pick up a dependency in some common directory, e.g. install a lot of builtins there. 2. Either. - Place an entire ROOT installation there, where all headers start with `#error This is the wrong header`. or. - Search `compile_commands.json` for `-I/my/include/directory/`. 3. Fix the `FindXXX` for all dependencies that provoke the above error.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8708
https://github.com/root-project/root/issues/8708:224,security,detect,detected,224,"#8709 fixes a part of this. ## What remains to be done:. It is likely that more builtins (or rather `FindXXX` have to be converted to `IMPORTED` targets, so they don't provoke this error again. A broken configuration can be detected by. 1. Having CMake pick up a dependency in some common directory, e.g. install a lot of builtins there. 2. Either. - Place an entire ROOT installation there, where all headers start with `#error This is the wrong header`. or. - Search `compile_commands.json` for `-I/my/include/directory/`. 3. Fix the `FindXXX` for all dependencies that provoke the above error.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8708
https://github.com/root-project/root/issues/8708:263,testability,depend,dependency,263,"#8709 fixes a part of this. ## What remains to be done:. It is likely that more builtins (or rather `FindXXX` have to be converted to `IMPORTED` targets, so they don't provoke this error again. A broken configuration can be detected by. 1. Having CMake pick up a dependency in some common directory, e.g. install a lot of builtins there. 2. Either. - Place an entire ROOT installation there, where all headers start with `#error This is the wrong header`. or. - Search `compile_commands.json` for `-I/my/include/directory/`. 3. Fix the `FindXXX` for all dependencies that provoke the above error.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8708
https://github.com/root-project/root/issues/8708:554,testability,depend,dependencies,554,"#8709 fixes a part of this. ## What remains to be done:. It is likely that more builtins (or rather `FindXXX` have to be converted to `IMPORTED` targets, so they don't provoke this error again. A broken configuration can be detected by. 1. Having CMake pick up a dependency in some common directory, e.g. install a lot of builtins there. 2. Either. - Place an entire ROOT installation there, where all headers start with `#error This is the wrong header`. or. - Search `compile_commands.json` for `-I/my/include/directory/`. 3. Fix the `FindXXX` for all dependencies that provoke the above error.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8708
https://github.com/root-project/root/issues/8708:181,usability,error,error,181,"#8709 fixes a part of this. ## What remains to be done:. It is likely that more builtins (or rather `FindXXX` have to be converted to `IMPORTED` targets, so they don't provoke this error again. A broken configuration can be detected by. 1. Having CMake pick up a dependency in some common directory, e.g. install a lot of builtins there. 2. Either. - Place an entire ROOT installation there, where all headers start with `#error This is the wrong header`. or. - Search `compile_commands.json` for `-I/my/include/directory/`. 3. Fix the `FindXXX` for all dependencies that provoke the above error.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8708
https://github.com/root-project/root/issues/8708:423,usability,error,error,423,"#8709 fixes a part of this. ## What remains to be done:. It is likely that more builtins (or rather `FindXXX` have to be converted to `IMPORTED` targets, so they don't provoke this error again. A broken configuration can be detected by. 1. Having CMake pick up a dependency in some common directory, e.g. install a lot of builtins there. 2. Either. - Place an entire ROOT installation there, where all headers start with `#error This is the wrong header`. or. - Search `compile_commands.json` for `-I/my/include/directory/`. 3. Fix the `FindXXX` for all dependencies that provoke the above error.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8708
https://github.com/root-project/root/issues/8708:590,usability,error,error,590,"#8709 fixes a part of this. ## What remains to be done:. It is likely that more builtins (or rather `FindXXX` have to be converted to `IMPORTED` targets, so they don't provoke this error again. A broken configuration can be detected by. 1. Having CMake pick up a dependency in some common directory, e.g. install a lot of builtins there. 2. Either. - Place an entire ROOT installation there, where all headers start with `#error This is the wrong header`. or. - Search `compile_commands.json` for `-I/my/include/directory/`. 3. Fix the `FindXXX` for all dependencies that provoke the above error.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8708
https://github.com/root-project/root/pull/8709:0,performance,Time,Timeout,0,Timeout - too late for 6.30.00,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8709
https://github.com/root-project/root/pull/8709:0,safety,Timeout,Timeout,0,Timeout - too late for 6.30.00,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8709
https://github.com/root-project/root/pull/8709:78,deployability,releas,release,78,"Removing the 6.32 milestone, as this change is too big for making it into the release. But at least we'll have it for the next one :slightly_smiling_face:",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8709
https://github.com/root-project/root/pull/8709:173,deployability,build,build,173,"Awesome, thanks for commenting! Then I'll stop messing with this PR. Note that I already merged some of your commits, because there were other big changes for XRootD in the build system already:. https://github.com/root-project/root/pull/14901",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8709
https://github.com/root-project/root/pull/8709:42,usability,stop,stop,42,"Awesome, thanks for commenting! Then I'll stop messing with this PR. Note that I already merged some of your commits, because there were other big changes for XRootD in the build system already:. https://github.com/root-project/root/pull/14901",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8709
https://github.com/root-project/root/pull/8709:106,deployability,build,build,106,"> Note that I already merged some of your commits, because there were other big changes for XRootD in the build system already:. OK, no problem, that will solve itself during a rebase!",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8709
https://github.com/root-project/root/issues/8712:69,usability,document,documentation,69,"I am investigating why the Python panels are now coming first in the documentation. I think they came last just ""by chance"" before. Nothing indicates which `\class TFile` should be read first. Which by the way is something also a bit stange ie: twice the same `\class` ... I suspect Doxygen behavior is undefined in that case. I did not find anything mentionning the fact we can have the same `\class` twice... I am investigating ... no solution yet.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8712
https://github.com/root-project/root/issues/8712:140,usability,indicat,indicates,140,"I am investigating why the Python panels are now coming first in the documentation. I think they came last just ""by chance"" before. Nothing indicates which `\class TFile` should be read first. Which by the way is something also a bit stange ie: twice the same `\class` ... I suspect Doxygen behavior is undefined in that case. I did not find anything mentionning the fact we can have the same `\class` twice... I am investigating ... no solution yet.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8712
https://github.com/root-project/root/issues/8712:291,usability,behavi,behavior,291,"I am investigating why the Python panels are now coming first in the documentation. I think they came last just ""by chance"" before. Nothing indicates which `\class TFile` should be read first. Which by the way is something also a bit stange ie: twice the same `\class` ... I suspect Doxygen behavior is undefined in that case. I did not find anything mentionning the fact we can have the same `\class` twice... I am investigating ... no solution yet.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8712
https://github.com/root-project/root/issues/8712:33,integrability,repositor,repository,33,I opened an issue on the doxygen repository asking what the behaviour should be when `\class` is defined twice. https://github.com/doxygen/doxygen/issues/8693,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8712
https://github.com/root-project/root/issues/8712:33,interoperability,repositor,repository,33,I opened an issue on the doxygen repository asking what the behaviour should be when `\class` is defined twice. https://github.com/doxygen/doxygen/issues/8693,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8712
https://github.com/root-project/root/issues/8712:60,usability,behavi,behaviour,60,I opened an issue on the doxygen repository asking what the behaviour should be when `\class` is defined twice. https://github.com/doxygen/doxygen/issues/8693,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8712
https://github.com/root-project/root/pull/8714:161,availability,cluster,cluster,161,"> could we set the cachesize to a better value than the autoflush default of ~30MB? Yes, you could use the `median` of `fClusterSize` (weighted by the number of cluster, which is encoded in `fClusterRangeEnd`, see `TTree::Print` for usage example) [`median` (or something higher) is better here than the average as we want to the size to cover most cluster (the average is likely to be slightly too small)]",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8714
https://github.com/root-project/root/pull/8714:349,availability,cluster,cluster,349,"> could we set the cachesize to a better value than the autoflush default of ~30MB? Yes, you could use the `median` of `fClusterSize` (weighted by the number of cluster, which is encoded in `fClusterRangeEnd`, see `TTree::Print` for usage example) [`median` (or something higher) is better here than the average as we want to the size to cover most cluster (the average is likely to be slightly too small)]",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8714
https://github.com/root-project/root/pull/8714:386,availability,sli,slightly,386,"> could we set the cachesize to a better value than the autoflush default of ~30MB? Yes, you could use the `median` of `fClusterSize` (weighted by the number of cluster, which is encoded in `fClusterRangeEnd`, see `TTree::Print` for usage example) [`median` (or something higher) is better here than the average as we want to the size to cover most cluster (the average is likely to be slightly too small)]",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8714
https://github.com/root-project/root/pull/8714:161,deployability,cluster,cluster,161,"> could we set the cachesize to a better value than the autoflush default of ~30MB? Yes, you could use the `median` of `fClusterSize` (weighted by the number of cluster, which is encoded in `fClusterRangeEnd`, see `TTree::Print` for usage example) [`median` (or something higher) is better here than the average as we want to the size to cover most cluster (the average is likely to be slightly too small)]",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8714
https://github.com/root-project/root/pull/8714:349,deployability,cluster,cluster,349,"> could we set the cachesize to a better value than the autoflush default of ~30MB? Yes, you could use the `median` of `fClusterSize` (weighted by the number of cluster, which is encoded in `fClusterRangeEnd`, see `TTree::Print` for usage example) [`median` (or something higher) is better here than the average as we want to the size to cover most cluster (the average is likely to be slightly too small)]",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8714
https://github.com/root-project/root/pull/8714:19,performance,cach,cachesize,19,"> could we set the cachesize to a better value than the autoflush default of ~30MB? Yes, you could use the `median` of `fClusterSize` (weighted by the number of cluster, which is encoded in `fClusterRangeEnd`, see `TTree::Print` for usage example) [`median` (or something higher) is better here than the average as we want to the size to cover most cluster (the average is likely to be slightly too small)]",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8714
https://github.com/root-project/root/pull/8714:386,reliability,sli,slightly,386,"> could we set the cachesize to a better value than the autoflush default of ~30MB? Yes, you could use the `median` of `fClusterSize` (weighted by the number of cluster, which is encoded in `fClusterRangeEnd`, see `TTree::Print` for usage example) [`median` (or something higher) is better here than the average as we want to the size to cover most cluster (the average is likely to be slightly too small)]",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8714
https://github.com/root-project/root/pull/8714:33,availability,cluster,clusters,33,"The median will still leave many clusters out (it only guarantees that 50% of them will be in, right?). How about the mode?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8714
https://github.com/root-project/root/pull/8714:33,deployability,cluster,clusters,33,"The median will still leave many clusters out (it only guarantees that 50% of them will be in, right?). How about the mode?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8714
https://github.com/root-project/root/pull/8714:115,availability,cluster,cluster,115,"Yes, I agree that the mode, when well defined would be better (As a aside the median an match more than 50% of the cluster, as is the case in the example file you had [where actually the mode and the median are the same]). By ""well defined"", I mean that if a large fraction of the file is not clustered (i.e result of the merge of many unclustered file and a few clustered files) then the 'mode' might not cover the majority of the cluster. . I would say we could use ""the mode or the median which ever cover more clusters and/or the largest fraction of the file"", isn't it?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8714
https://github.com/root-project/root/pull/8714:293,availability,cluster,clustered,293,"Yes, I agree that the mode, when well defined would be better (As a aside the median an match more than 50% of the cluster, as is the case in the example file you had [where actually the mode and the median are the same]). By ""well defined"", I mean that if a large fraction of the file is not clustered (i.e result of the merge of many unclustered file and a few clustered files) then the 'mode' might not cover the majority of the cluster. . I would say we could use ""the mode or the median which ever cover more clusters and/or the largest fraction of the file"", isn't it?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8714
https://github.com/root-project/root/pull/8714:363,availability,cluster,clustered,363,"Yes, I agree that the mode, when well defined would be better (As a aside the median an match more than 50% of the cluster, as is the case in the example file you had [where actually the mode and the median are the same]). By ""well defined"", I mean that if a large fraction of the file is not clustered (i.e result of the merge of many unclustered file and a few clustered files) then the 'mode' might not cover the majority of the cluster. . I would say we could use ""the mode or the median which ever cover more clusters and/or the largest fraction of the file"", isn't it?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8714
https://github.com/root-project/root/pull/8714:432,availability,cluster,cluster,432,"Yes, I agree that the mode, when well defined would be better (As a aside the median an match more than 50% of the cluster, as is the case in the example file you had [where actually the mode and the median are the same]). By ""well defined"", I mean that if a large fraction of the file is not clustered (i.e result of the merge of many unclustered file and a few clustered files) then the 'mode' might not cover the majority of the cluster. . I would say we could use ""the mode or the median which ever cover more clusters and/or the largest fraction of the file"", isn't it?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8714
https://github.com/root-project/root/pull/8714:514,availability,cluster,clusters,514,"Yes, I agree that the mode, when well defined would be better (As a aside the median an match more than 50% of the cluster, as is the case in the example file you had [where actually the mode and the median are the same]). By ""well defined"", I mean that if a large fraction of the file is not clustered (i.e result of the merge of many unclustered file and a few clustered files) then the 'mode' might not cover the majority of the cluster. . I would say we could use ""the mode or the median which ever cover more clusters and/or the largest fraction of the file"", isn't it?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8714
https://github.com/root-project/root/pull/8714:115,deployability,cluster,cluster,115,"Yes, I agree that the mode, when well defined would be better (As a aside the median an match more than 50% of the cluster, as is the case in the example file you had [where actually the mode and the median are the same]). By ""well defined"", I mean that if a large fraction of the file is not clustered (i.e result of the merge of many unclustered file and a few clustered files) then the 'mode' might not cover the majority of the cluster. . I would say we could use ""the mode or the median which ever cover more clusters and/or the largest fraction of the file"", isn't it?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8714
https://github.com/root-project/root/pull/8714:293,deployability,cluster,clustered,293,"Yes, I agree that the mode, when well defined would be better (As a aside the median an match more than 50% of the cluster, as is the case in the example file you had [where actually the mode and the median are the same]). By ""well defined"", I mean that if a large fraction of the file is not clustered (i.e result of the merge of many unclustered file and a few clustered files) then the 'mode' might not cover the majority of the cluster. . I would say we could use ""the mode or the median which ever cover more clusters and/or the largest fraction of the file"", isn't it?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8714
https://github.com/root-project/root/pull/8714:363,deployability,cluster,clustered,363,"Yes, I agree that the mode, when well defined would be better (As a aside the median an match more than 50% of the cluster, as is the case in the example file you had [where actually the mode and the median are the same]). By ""well defined"", I mean that if a large fraction of the file is not clustered (i.e result of the merge of many unclustered file and a few clustered files) then the 'mode' might not cover the majority of the cluster. . I would say we could use ""the mode or the median which ever cover more clusters and/or the largest fraction of the file"", isn't it?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8714
https://github.com/root-project/root/pull/8714:432,deployability,cluster,cluster,432,"Yes, I agree that the mode, when well defined would be better (As a aside the median an match more than 50% of the cluster, as is the case in the example file you had [where actually the mode and the median are the same]). By ""well defined"", I mean that if a large fraction of the file is not clustered (i.e result of the merge of many unclustered file and a few clustered files) then the 'mode' might not cover the majority of the cluster. . I would say we could use ""the mode or the median which ever cover more clusters and/or the largest fraction of the file"", isn't it?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8714
https://github.com/root-project/root/pull/8714:514,deployability,cluster,clusters,514,"Yes, I agree that the mode, when well defined would be better (As a aside the median an match more than 50% of the cluster, as is the case in the example file you had [where actually the mode and the median are the same]). By ""well defined"", I mean that if a large fraction of the file is not clustered (i.e result of the merge of many unclustered file and a few clustered files) then the 'mode' might not cover the majority of the cluster. . I would say we could use ""the mode or the median which ever cover more clusters and/or the largest fraction of the file"", isn't it?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8714
https://github.com/root-project/root/pull/8714:47,availability,cluster,clusters,47,"> the mode or the median which ever cover more clusters and/or the largest fraction of the file. Alright, can do that. But it's not super cheap, so I have two questions. 1. Should I:. a. add a TTree method that evaluates that thing and call it from `TTree::GetCacheAutoSize`. b. add a TTree method that evaluates that thing and add a data member to TTree to cache its value after the first time I compute it. c. evaluate that thing in TTree's constructor and store it in a data member. d. add a free function that takes a TTree and evaluates that thing. e. none of the above, something else. 2. What should we do for TChains? Override that calculation so that it errors out?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8714
https://github.com/root-project/root/pull/8714:663,availability,error,errors,663,"> the mode or the median which ever cover more clusters and/or the largest fraction of the file. Alright, can do that. But it's not super cheap, so I have two questions. 1. Should I:. a. add a TTree method that evaluates that thing and call it from `TTree::GetCacheAutoSize`. b. add a TTree method that evaluates that thing and add a data member to TTree to cache its value after the first time I compute it. c. evaluate that thing in TTree's constructor and store it in a data member. d. add a free function that takes a TTree and evaluates that thing. e. none of the above, something else. 2. What should we do for TChains? Override that calculation so that it errors out?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8714
https://github.com/root-project/root/pull/8714:47,deployability,cluster,clusters,47,"> the mode or the median which ever cover more clusters and/or the largest fraction of the file. Alright, can do that. But it's not super cheap, so I have two questions. 1. Should I:. a. add a TTree method that evaluates that thing and call it from `TTree::GetCacheAutoSize`. b. add a TTree method that evaluates that thing and add a data member to TTree to cache its value after the first time I compute it. c. evaluate that thing in TTree's constructor and store it in a data member. d. add a free function that takes a TTree and evaluates that thing. e. none of the above, something else. 2. What should we do for TChains? Override that calculation so that it errors out?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8714
https://github.com/root-project/root/pull/8714:358,performance,cach,cache,358,"> the mode or the median which ever cover more clusters and/or the largest fraction of the file. Alright, can do that. But it's not super cheap, so I have two questions. 1. Should I:. a. add a TTree method that evaluates that thing and call it from `TTree::GetCacheAutoSize`. b. add a TTree method that evaluates that thing and add a data member to TTree to cache its value after the first time I compute it. c. evaluate that thing in TTree's constructor and store it in a data member. d. add a free function that takes a TTree and evaluates that thing. e. none of the above, something else. 2. What should we do for TChains? Override that calculation so that it errors out?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8714
https://github.com/root-project/root/pull/8714:390,performance,time,time,390,"> the mode or the median which ever cover more clusters and/or the largest fraction of the file. Alright, can do that. But it's not super cheap, so I have two questions. 1. Should I:. a. add a TTree method that evaluates that thing and call it from `TTree::GetCacheAutoSize`. b. add a TTree method that evaluates that thing and add a data member to TTree to cache its value after the first time I compute it. c. evaluate that thing in TTree's constructor and store it in a data member. d. add a free function that takes a TTree and evaluates that thing. e. none of the above, something else. 2. What should we do for TChains? Override that calculation so that it errors out?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8714
https://github.com/root-project/root/pull/8714:663,performance,error,errors,663,"> the mode or the median which ever cover more clusters and/or the largest fraction of the file. Alright, can do that. But it's not super cheap, so I have two questions. 1. Should I:. a. add a TTree method that evaluates that thing and call it from `TTree::GetCacheAutoSize`. b. add a TTree method that evaluates that thing and add a data member to TTree to cache its value after the first time I compute it. c. evaluate that thing in TTree's constructor and store it in a data member. d. add a free function that takes a TTree and evaluates that thing. e. none of the above, something else. 2. What should we do for TChains? Override that calculation so that it errors out?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8714
https://github.com/root-project/root/pull/8714:663,safety,error,errors,663,"> the mode or the median which ever cover more clusters and/or the largest fraction of the file. Alright, can do that. But it's not super cheap, so I have two questions. 1. Should I:. a. add a TTree method that evaluates that thing and call it from `TTree::GetCacheAutoSize`. b. add a TTree method that evaluates that thing and add a data member to TTree to cache its value after the first time I compute it. c. evaluate that thing in TTree's constructor and store it in a data member. d. add a free function that takes a TTree and evaluates that thing. e. none of the above, something else. 2. What should we do for TChains? Override that calculation so that it errors out?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8714
https://github.com/root-project/root/pull/8714:663,usability,error,errors,663,"> the mode or the median which ever cover more clusters and/or the largest fraction of the file. Alright, can do that. But it's not super cheap, so I have two questions. 1. Should I:. a. add a TTree method that evaluates that thing and call it from `TTree::GetCacheAutoSize`. b. add a TTree method that evaluates that thing and add a data member to TTree to cache its value after the first time I compute it. c. evaluate that thing in TTree's constructor and store it in a data member. d. add a free function that takes a TTree and evaluates that thing. e. none of the above, something else. 2. What should we do for TChains? Override that calculation so that it errors out?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8714
https://github.com/root-project/root/pull/8714:70,availability,error,errors,70,"> What should we do for TChains? Override that calculation so that it errors out? Humm .. i think it somehow get delegated to the underlying TTree. > But it's not super cheap, . it should be a one time (per TTree) cost in the case where the user did not specify the cache size and fAutoFlush is to zero (because after that the CacheSize is set). > add a TTree method that evaluates that thing and call it from TTree::GetCacheAutoSize. I would do that. Thanks.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8714
https://github.com/root-project/root/pull/8714:254,interoperability,specif,specify,254,"> What should we do for TChains? Override that calculation so that it errors out? Humm .. i think it somehow get delegated to the underlying TTree. > But it's not super cheap, . it should be a one time (per TTree) cost in the case where the user did not specify the cache size and fAutoFlush is to zero (because after that the CacheSize is set). > add a TTree method that evaluates that thing and call it from TTree::GetCacheAutoSize. I would do that. Thanks.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8714
https://github.com/root-project/root/pull/8714:70,performance,error,errors,70,"> What should we do for TChains? Override that calculation so that it errors out? Humm .. i think it somehow get delegated to the underlying TTree. > But it's not super cheap, . it should be a one time (per TTree) cost in the case where the user did not specify the cache size and fAutoFlush is to zero (because after that the CacheSize is set). > add a TTree method that evaluates that thing and call it from TTree::GetCacheAutoSize. I would do that. Thanks.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8714
https://github.com/root-project/root/pull/8714:197,performance,time,time,197,"> What should we do for TChains? Override that calculation so that it errors out? Humm .. i think it somehow get delegated to the underlying TTree. > But it's not super cheap, . it should be a one time (per TTree) cost in the case where the user did not specify the cache size and fAutoFlush is to zero (because after that the CacheSize is set). > add a TTree method that evaluates that thing and call it from TTree::GetCacheAutoSize. I would do that. Thanks.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8714
https://github.com/root-project/root/pull/8714:266,performance,cach,cache,266,"> What should we do for TChains? Override that calculation so that it errors out? Humm .. i think it somehow get delegated to the underlying TTree. > But it's not super cheap, . it should be a one time (per TTree) cost in the case where the user did not specify the cache size and fAutoFlush is to zero (because after that the CacheSize is set). > add a TTree method that evaluates that thing and call it from TTree::GetCacheAutoSize. I would do that. Thanks.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8714
https://github.com/root-project/root/pull/8714:327,performance,Cach,CacheSize,327,"> What should we do for TChains? Override that calculation so that it errors out? Humm .. i think it somehow get delegated to the underlying TTree. > But it's not super cheap, . it should be a one time (per TTree) cost in the case where the user did not specify the cache size and fAutoFlush is to zero (because after that the CacheSize is set). > add a TTree method that evaluates that thing and call it from TTree::GetCacheAutoSize. I would do that. Thanks.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8714
https://github.com/root-project/root/pull/8714:70,safety,error,errors,70,"> What should we do for TChains? Override that calculation so that it errors out? Humm .. i think it somehow get delegated to the underlying TTree. > But it's not super cheap, . it should be a one time (per TTree) cost in the case where the user did not specify the cache size and fAutoFlush is to zero (because after that the CacheSize is set). > add a TTree method that evaluates that thing and call it from TTree::GetCacheAutoSize. I would do that. Thanks.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8714
https://github.com/root-project/root/pull/8714:70,usability,error,errors,70,"> What should we do for TChains? Override that calculation so that it errors out? Humm .. i think it somehow get delegated to the underlying TTree. > But it's not super cheap, . it should be a one time (per TTree) cost in the case where the user did not specify the cache size and fAutoFlush is to zero (because after that the CacheSize is set). > add a TTree method that evaluates that thing and call it from TTree::GetCacheAutoSize. I would do that. Thanks.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8714
https://github.com/root-project/root/pull/8714:241,usability,user,user,241,"> What should we do for TChains? Override that calculation so that it errors out? Humm .. i think it somehow get delegated to the underlying TTree. > But it's not super cheap, . it should be a one time (per TTree) cost in the case where the user did not specify the cache size and fAutoFlush is to zero (because after that the CacheSize is set). > add a TTree method that evaluates that thing and call it from TTree::GetCacheAutoSize. I would do that. Thanks.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8714
https://github.com/root-project/root/pull/8714:113,availability,cluster,cluster,113,"Latest hurdle: `fClusterSize` is a number of entries, the cache size (which we want to set to the median/mode of cluster sizes) has to be in bytes (compressed or uncompressed, I'm not sure) -- how do I go from median/mode of the number of entries in a cluster to the desired size in bytes?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8714
https://github.com/root-project/root/pull/8714:252,availability,cluster,cluster,252,"Latest hurdle: `fClusterSize` is a number of entries, the cache size (which we want to set to the median/mode of cluster sizes) has to be in bytes (compressed or uncompressed, I'm not sure) -- how do I go from median/mode of the number of entries in a cluster to the desired size in bytes?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8714
https://github.com/root-project/root/pull/8714:113,deployability,cluster,cluster,113,"Latest hurdle: `fClusterSize` is a number of entries, the cache size (which we want to set to the median/mode of cluster sizes) has to be in bytes (compressed or uncompressed, I'm not sure) -- how do I go from median/mode of the number of entries in a cluster to the desired size in bytes?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8714
https://github.com/root-project/root/pull/8714:252,deployability,cluster,cluster,252,"Latest hurdle: `fClusterSize` is a number of entries, the cache size (which we want to set to the median/mode of cluster sizes) has to be in bytes (compressed or uncompressed, I'm not sure) -- how do I go from median/mode of the number of entries in a cluster to the desired size in bytes?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8714
https://github.com/root-project/root/pull/8714:58,performance,cach,cache,58,"Latest hurdle: `fClusterSize` is a number of entries, the cache size (which we want to set to the median/mode of cluster sizes) has to be in bytes (compressed or uncompressed, I'm not sure) -- how do I go from median/mode of the number of entries in a cluster to the desired size in bytes?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8714
https://github.com/root-project/root/pull/8714:183,availability,cluster,cluster,183,"Now, if `fAutoFlush == 0`, we are setting the cache size to `1.5 * medianClusterSize * GetZipBytes() / (fEntries + 1)` (mimicking the logic in `GetCacheAutoSize` but using the median cluster size instead of `fAutoFlush`). When calculating the median, cluster ranges for which `fClusterSize == 0` are ignored.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8714
https://github.com/root-project/root/pull/8714:251,availability,cluster,cluster,251,"Now, if `fAutoFlush == 0`, we are setting the cache size to `1.5 * medianClusterSize * GetZipBytes() / (fEntries + 1)` (mimicking the logic in `GetCacheAutoSize` but using the median cluster size instead of `fAutoFlush`). When calculating the median, cluster ranges for which `fClusterSize == 0` are ignored.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8714
https://github.com/root-project/root/pull/8714:134,deployability,log,logic,134,"Now, if `fAutoFlush == 0`, we are setting the cache size to `1.5 * medianClusterSize * GetZipBytes() / (fEntries + 1)` (mimicking the logic in `GetCacheAutoSize` but using the median cluster size instead of `fAutoFlush`). When calculating the median, cluster ranges for which `fClusterSize == 0` are ignored.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8714
https://github.com/root-project/root/pull/8714:183,deployability,cluster,cluster,183,"Now, if `fAutoFlush == 0`, we are setting the cache size to `1.5 * medianClusterSize * GetZipBytes() / (fEntries + 1)` (mimicking the logic in `GetCacheAutoSize` but using the median cluster size instead of `fAutoFlush`). When calculating the median, cluster ranges for which `fClusterSize == 0` are ignored.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8714
https://github.com/root-project/root/pull/8714:251,deployability,cluster,cluster,251,"Now, if `fAutoFlush == 0`, we are setting the cache size to `1.5 * medianClusterSize * GetZipBytes() / (fEntries + 1)` (mimicking the logic in `GetCacheAutoSize` but using the median cluster size instead of `fAutoFlush`). When calculating the median, cluster ranges for which `fClusterSize == 0` are ignored.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8714
https://github.com/root-project/root/pull/8714:46,performance,cach,cache,46,"Now, if `fAutoFlush == 0`, we are setting the cache size to `1.5 * medianClusterSize * GetZipBytes() / (fEntries + 1)` (mimicking the logic in `GetCacheAutoSize` but using the median cluster size instead of `fAutoFlush`). When calculating the median, cluster ranges for which `fClusterSize == 0` are ignored.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8714
https://github.com/root-project/root/pull/8714:134,safety,log,logic,134,"Now, if `fAutoFlush == 0`, we are setting the cache size to `1.5 * medianClusterSize * GetZipBytes() / (fEntries + 1)` (mimicking the logic in `GetCacheAutoSize` but using the median cluster size instead of `fAutoFlush`). When calculating the median, cluster ranges for which `fClusterSize == 0` are ignored.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8714
https://github.com/root-project/root/pull/8714:134,security,log,logic,134,"Now, if `fAutoFlush == 0`, we are setting the cache size to `1.5 * medianClusterSize * GetZipBytes() / (fEntries + 1)` (mimicking the logic in `GetCacheAutoSize` but using the median cluster size instead of `fAutoFlush`). When calculating the median, cluster ranges for which `fClusterSize == 0` are ignored.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8714
https://github.com/root-project/root/pull/8714:134,testability,log,logic,134,"Now, if `fAutoFlush == 0`, we are setting the cache size to `1.5 * medianClusterSize * GetZipBytes() / (fEntries + 1)` (mimicking the logic in `GetCacheAutoSize` but using the median cluster size instead of `fAutoFlush`). When calculating the median, cluster ranges for which `fClusterSize == 0` are ignored.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8714
https://github.com/root-project/root/pull/8715:83,integrability,buffer,buffers,83,":+1: perfect for concurrent histogram filling, where the merging happens from fill buffers handed over from the filling threads to the merging thread, which flushes them into the target histogram. IIUC that's a use case for exactly this kind of queue, right?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8715
https://github.com/root-project/root/pull/8715:245,integrability,queue,queue,245,":+1: perfect for concurrent histogram filling, where the merging happens from fill buffers handed over from the filling threads to the merging thread, which flushes them into the target histogram. IIUC that's a use case for exactly this kind of queue, right?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8715
https://github.com/root-project/root/pull/8715:17,performance,concurren,concurrent,17,":+1: perfect for concurrent histogram filling, where the merging happens from fill buffers handed over from the filling threads to the merging thread, which flushes them into the target histogram. IIUC that's a use case for exactly this kind of queue, right?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8715
https://github.com/root-project/root/pull/8715:245,performance,queue,queue,245,":+1: perfect for concurrent histogram filling, where the merging happens from fill buffers handed over from the filling threads to the merging thread, which flushes them into the target histogram. IIUC that's a use case for exactly this kind of queue, right?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8715
https://github.com/root-project/root/pull/8715:124,usability,close,close,124,"@Axel-Naumann pointed out that we already have `concurrent_bounded_queue` from TBB. From my point of view, we can therefore close this.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8715
https://github.com/root-project/root/pull/8715:94,deployability,depend,dependency,94,I thought we could not freely use stuff from TBB? EDIT: it's tricky not to introduce a header dependency on TBB when using template classes,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8715
https://github.com/root-project/root/pull/8715:94,integrability,depend,dependency,94,I thought we could not freely use stuff from TBB? EDIT: it's tricky not to introduce a header dependency on TBB when using template classes,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8715
https://github.com/root-project/root/pull/8715:94,modifiability,depend,dependency,94,I thought we could not freely use stuff from TBB? EDIT: it's tricky not to introduce a header dependency on TBB when using template classes,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8715
https://github.com/root-project/root/pull/8715:94,safety,depend,dependency,94,I thought we could not freely use stuff from TBB? EDIT: it's tricky not to introduce a header dependency on TBB when using template classes,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8715
https://github.com/root-project/root/pull/8715:94,testability,depend,dependency,94,I thought we could not freely use stuff from TBB? EDIT: it's tricky not to introduce a header dependency on TBB when using template classes,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8715
https://github.com/root-project/root/pull/8715:39,integrability,interfac,interface,39,"@eguiraud As long as we don't leak the interface to users, I guess TBB is fine.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8715
https://github.com/root-project/root/pull/8715:39,interoperability,interfac,interface,39,"@eguiraud As long as we don't leak the interface to users, I guess TBB is fine.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8715
https://github.com/root-project/root/pull/8715:39,modifiability,interfac,interface,39,"@eguiraud As long as we don't leak the interface to users, I guess TBB is fine.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8715
https://github.com/root-project/root/pull/8715:52,usability,user,users,52,"@eguiraud As long as we don't leak the interface to users, I guess TBB is fine.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8715
https://github.com/root-project/root/pull/8715:41,integrability,interfac,interface,41,"> @eguiraud As long as we don't leak the interface to users, I guess TBB is fine. @jblomer @Axel-Naumann does that mean we can include TBB headers in our headers? I thought that wasn't the case. If we cannot it will be impossible (or at least very awkward) to use TBB's concurrent queue e.g. in template code.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8715
https://github.com/root-project/root/pull/8715:281,integrability,queue,queue,281,"> @eguiraud As long as we don't leak the interface to users, I guess TBB is fine. @jblomer @Axel-Naumann does that mean we can include TBB headers in our headers? I thought that wasn't the case. If we cannot it will be impossible (or at least very awkward) to use TBB's concurrent queue e.g. in template code.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8715
https://github.com/root-project/root/pull/8715:41,interoperability,interfac,interface,41,"> @eguiraud As long as we don't leak the interface to users, I guess TBB is fine. @jblomer @Axel-Naumann does that mean we can include TBB headers in our headers? I thought that wasn't the case. If we cannot it will be impossible (or at least very awkward) to use TBB's concurrent queue e.g. in template code.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8715
https://github.com/root-project/root/pull/8715:41,modifiability,interfac,interface,41,"> @eguiraud As long as we don't leak the interface to users, I guess TBB is fine. @jblomer @Axel-Naumann does that mean we can include TBB headers in our headers? I thought that wasn't the case. If we cannot it will be impossible (or at least very awkward) to use TBB's concurrent queue e.g. in template code.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8715
https://github.com/root-project/root/pull/8715:270,performance,concurren,concurrent,270,"> @eguiraud As long as we don't leak the interface to users, I guess TBB is fine. @jblomer @Axel-Naumann does that mean we can include TBB headers in our headers? I thought that wasn't the case. If we cannot it will be impossible (or at least very awkward) to use TBB's concurrent queue e.g. in template code.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8715
https://github.com/root-project/root/pull/8715:281,performance,queue,queue,281,"> @eguiraud As long as we don't leak the interface to users, I guess TBB is fine. @jblomer @Axel-Naumann does that mean we can include TBB headers in our headers? I thought that wasn't the case. If we cannot it will be impossible (or at least very awkward) to use TBB's concurrent queue e.g. in template code.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8715
https://github.com/root-project/root/pull/8715:105,reliability,doe,does,105,"> @eguiraud As long as we don't leak the interface to users, I guess TBB is fine. @jblomer @Axel-Naumann does that mean we can include TBB headers in our headers? I thought that wasn't the case. If we cannot it will be impossible (or at least very awkward) to use TBB's concurrent queue e.g. in template code.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8715
https://github.com/root-project/root/pull/8715:54,usability,user,users,54,"> @eguiraud As long as we don't leak the interface to users, I guess TBB is fine. @jblomer @Axel-Naumann does that mean we can include TBB headers in our headers? I thought that wasn't the case. If we cannot it will be impossible (or at least very awkward) to use TBB's concurrent queue e.g. in template code.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8715
https://github.com/root-project/root/pull/8715:112,integrability,interfac,interface,112,"Indeed, we cannot do that; that's I suppose what @jblomer tried to express with. > As long as we don't leak the interface to users. But until we have a usecase for such a queue that cannot live in `src/` I guess we can survive with tbb.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8715
https://github.com/root-project/root/pull/8715:171,integrability,queue,queue,171,"Indeed, we cannot do that; that's I suppose what @jblomer tried to express with. > As long as we don't leak the interface to users. But until we have a usecase for such a queue that cannot live in `src/` I guess we can survive with tbb.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8715
https://github.com/root-project/root/pull/8715:112,interoperability,interfac,interface,112,"Indeed, we cannot do that; that's I suppose what @jblomer tried to express with. > As long as we don't leak the interface to users. But until we have a usecase for such a queue that cannot live in `src/` I guess we can survive with tbb.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8715
https://github.com/root-project/root/pull/8715:112,modifiability,interfac,interface,112,"Indeed, we cannot do that; that's I suppose what @jblomer tried to express with. > As long as we don't leak the interface to users. But until we have a usecase for such a queue that cannot live in `src/` I guess we can survive with tbb.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8715
https://github.com/root-project/root/pull/8715:171,performance,queue,queue,171,"Indeed, we cannot do that; that's I suppose what @jblomer tried to express with. > As long as we don't leak the interface to users. But until we have a usecase for such a queue that cannot live in `src/` I guess we can survive with tbb.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8715
https://github.com/root-project/root/pull/8715:125,usability,user,users,125,"Indeed, we cannot do that; that's I suppose what @jblomer tried to express with. > As long as we don't leak the interface to users. But until we have a usecase for such a queue that cannot live in `src/` I guess we can survive with tbb.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8715
https://github.com/root-project/root/issues/8716:22,integrability,interfac,interface,22,"The `CheckGradient()` interface was removed, since it had no effect as you said:. https://github.com/root-project/root/commit/94549da66304cd9a94a256620abb09e5fbd1079d. It's also on our radar that numeric gradients are still used for things like the seeding step. We're working towards making Minuit 2 ""numeric gradient free"" as well. If you think this should be tracked separately, feel free to re-open a new issue about this specific subject! Thanks for your input.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8716
https://github.com/root-project/root/issues/8716:435,integrability,sub,subject,435,"The `CheckGradient()` interface was removed, since it had no effect as you said:. https://github.com/root-project/root/commit/94549da66304cd9a94a256620abb09e5fbd1079d. It's also on our radar that numeric gradients are still used for things like the seeding step. We're working towards making Minuit 2 ""numeric gradient free"" as well. If you think this should be tracked separately, feel free to re-open a new issue about this specific subject! Thanks for your input.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8716
https://github.com/root-project/root/issues/8716:22,interoperability,interfac,interface,22,"The `CheckGradient()` interface was removed, since it had no effect as you said:. https://github.com/root-project/root/commit/94549da66304cd9a94a256620abb09e5fbd1079d. It's also on our radar that numeric gradients are still used for things like the seeding step. We're working towards making Minuit 2 ""numeric gradient free"" as well. If you think this should be tracked separately, feel free to re-open a new issue about this specific subject! Thanks for your input.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8716
https://github.com/root-project/root/issues/8716:426,interoperability,specif,specific,426,"The `CheckGradient()` interface was removed, since it had no effect as you said:. https://github.com/root-project/root/commit/94549da66304cd9a94a256620abb09e5fbd1079d. It's also on our radar that numeric gradients are still used for things like the seeding step. We're working towards making Minuit 2 ""numeric gradient free"" as well. If you think this should be tracked separately, feel free to re-open a new issue about this specific subject! Thanks for your input.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8716
https://github.com/root-project/root/issues/8716:22,modifiability,interfac,interface,22,"The `CheckGradient()` interface was removed, since it had no effect as you said:. https://github.com/root-project/root/commit/94549da66304cd9a94a256620abb09e5fbd1079d. It's also on our radar that numeric gradients are still used for things like the seeding step. We're working towards making Minuit 2 ""numeric gradient free"" as well. If you think this should be tracked separately, feel free to re-open a new issue about this specific subject! Thanks for your input.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8716
https://github.com/root-project/root/issues/8716:460,safety,input,input,460,"The `CheckGradient()` interface was removed, since it had no effect as you said:. https://github.com/root-project/root/commit/94549da66304cd9a94a256620abb09e5fbd1079d. It's also on our radar that numeric gradients are still used for things like the seeding step. We're working towards making Minuit 2 ""numeric gradient free"" as well. If you think this should be tracked separately, feel free to re-open a new issue about this specific subject! Thanks for your input.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8716
https://github.com/root-project/root/issues/8716:460,usability,input,input,460,"The `CheckGradient()` interface was removed, since it had no effect as you said:. https://github.com/root-project/root/commit/94549da66304cd9a94a256620abb09e5fbd1079d. It's also on our radar that numeric gradients are still used for things like the seeding step. We're working towards making Minuit 2 ""numeric gradient free"" as well. If you think this should be tracked separately, feel free to re-open a new issue about this specific subject! Thanks for your input.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8716
https://github.com/root-project/root/pull/8717:11,deployability,build,build,11,@phsft-bot build!,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:176,availability,servic,services,176,"> Build failed on ROOT-debian10-i386/cxx14. > Running on pcepsft11.dyndns.cern.ch:/home/sftnight/build/workspace/root-pullrequests-build. > [See console output](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/console). > ### Failing tests:. > . > * [projectroot.roottest.python.cpp.roottest_python_cpp_cpp](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/testReport/projectroot.roottest.python/cpp/roottest_python_cpp_cpp/). @vgvassilev , I don't have this test in my checkout:. ```pgras@xps13:~/git.d/root.grasph/mybuild$ ctest -R projectroot.roottest.python.cpp.roottest_python_cpp_cpp . Test project /home/pgras/git.d/root.grasph/mybuild. No tests were found!!! ```. Do you have an idea why? I ran cmake with the following options as described in [1]:. ```cmake -DCMAKE_BUILD_TYPE=Debug -Dtesting=ON -Droottest=ON```. Do I need more options? A `pyroottest=ON` ? Philippe. [1] https://root.cern/for_developers/run_the_tests/",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:354,availability,servic,services,354,"> Build failed on ROOT-debian10-i386/cxx14. > Running on pcepsft11.dyndns.cern.ch:/home/sftnight/build/workspace/root-pullrequests-build. > [See console output](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/console). > ### Failing tests:. > . > * [projectroot.roottest.python.cpp.roottest_python_cpp_cpp](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/testReport/projectroot.roottest.python/cpp/roottest_python_cpp_cpp/). @vgvassilev , I don't have this test in my checkout:. ```pgras@xps13:~/git.d/root.grasph/mybuild$ ctest -R projectroot.roottest.python.cpp.roottest_python_cpp_cpp . Test project /home/pgras/git.d/root.grasph/mybuild. No tests were found!!! ```. Do you have an idea why? I ran cmake with the following options as described in [1]:. ```cmake -DCMAKE_BUILD_TYPE=Debug -Dtesting=ON -Droottest=ON```. Do I need more options? A `pyroottest=ON` ? Philippe. [1] https://root.cern/for_developers/run_the_tests/",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:2,deployability,Build,Build,2,"> Build failed on ROOT-debian10-i386/cxx14. > Running on pcepsft11.dyndns.cern.ch:/home/sftnight/build/workspace/root-pullrequests-build. > [See console output](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/console). > ### Failing tests:. > . > * [projectroot.roottest.python.cpp.roottest_python_cpp_cpp](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/testReport/projectroot.roottest.python/cpp/roottest_python_cpp_cpp/). @vgvassilev , I don't have this test in my checkout:. ```pgras@xps13:~/git.d/root.grasph/mybuild$ ctest -R projectroot.roottest.python.cpp.roottest_python_cpp_cpp . Test project /home/pgras/git.d/root.grasph/mybuild. No tests were found!!! ```. Do you have an idea why? I ran cmake with the following options as described in [1]:. ```cmake -DCMAKE_BUILD_TYPE=Debug -Dtesting=ON -Droottest=ON```. Do I need more options? A `pyroottest=ON` ? Philippe. [1] https://root.cern/for_developers/run_the_tests/",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:8,deployability,fail,failed,8,"> Build failed on ROOT-debian10-i386/cxx14. > Running on pcepsft11.dyndns.cern.ch:/home/sftnight/build/workspace/root-pullrequests-build. > [See console output](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/console). > ### Failing tests:. > . > * [projectroot.roottest.python.cpp.roottest_python_cpp_cpp](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/testReport/projectroot.roottest.python/cpp/roottest_python_cpp_cpp/). @vgvassilev , I don't have this test in my checkout:. ```pgras@xps13:~/git.d/root.grasph/mybuild$ ctest -R projectroot.roottest.python.cpp.roottest_python_cpp_cpp . Test project /home/pgras/git.d/root.grasph/mybuild. No tests were found!!! ```. Do you have an idea why? I ran cmake with the following options as described in [1]:. ```cmake -DCMAKE_BUILD_TYPE=Debug -Dtesting=ON -Droottest=ON```. Do I need more options? A `pyroottest=ON` ? Philippe. [1] https://root.cern/for_developers/run_the_tests/",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:97,deployability,build,build,97,"> Build failed on ROOT-debian10-i386/cxx14. > Running on pcepsft11.dyndns.cern.ch:/home/sftnight/build/workspace/root-pullrequests-build. > [See console output](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/console). > ### Failing tests:. > . > * [projectroot.roottest.python.cpp.roottest_python_cpp_cpp](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/testReport/projectroot.roottest.python/cpp/roottest_python_cpp_cpp/). @vgvassilev , I don't have this test in my checkout:. ```pgras@xps13:~/git.d/root.grasph/mybuild$ ctest -R projectroot.roottest.python.cpp.roottest_python_cpp_cpp . Test project /home/pgras/git.d/root.grasph/mybuild. No tests were found!!! ```. Do you have an idea why? I ran cmake with the following options as described in [1]:. ```cmake -DCMAKE_BUILD_TYPE=Debug -Dtesting=ON -Droottest=ON```. Do I need more options? A `pyroottest=ON` ? Philippe. [1] https://root.cern/for_developers/run_the_tests/",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:131,deployability,build,build,131,"> Build failed on ROOT-debian10-i386/cxx14. > Running on pcepsft11.dyndns.cern.ch:/home/sftnight/build/workspace/root-pullrequests-build. > [See console output](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/console). > ### Failing tests:. > . > * [projectroot.roottest.python.cpp.roottest_python_cpp_cpp](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/testReport/projectroot.roottest.python/cpp/roottest_python_cpp_cpp/). @vgvassilev , I don't have this test in my checkout:. ```pgras@xps13:~/git.d/root.grasph/mybuild$ ctest -R projectroot.roottest.python.cpp.roottest_python_cpp_cpp . Test project /home/pgras/git.d/root.grasph/mybuild. No tests were found!!! ```. Do you have an idea why? I ran cmake with the following options as described in [1]:. ```cmake -DCMAKE_BUILD_TYPE=Debug -Dtesting=ON -Droottest=ON```. Do I need more options? A `pyroottest=ON` ? Philippe. [1] https://root.cern/for_developers/run_the_tests/",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:176,deployability,servic,services,176,"> Build failed on ROOT-debian10-i386/cxx14. > Running on pcepsft11.dyndns.cern.ch:/home/sftnight/build/workspace/root-pullrequests-build. > [See console output](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/console). > ### Failing tests:. > . > * [projectroot.roottest.python.cpp.roottest_python_cpp_cpp](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/testReport/projectroot.roottest.python/cpp/roottest_python_cpp_cpp/). @vgvassilev , I don't have this test in my checkout:. ```pgras@xps13:~/git.d/root.grasph/mybuild$ ctest -R projectroot.roottest.python.cpp.roottest_python_cpp_cpp . Test project /home/pgras/git.d/root.grasph/mybuild. No tests were found!!! ```. Do you have an idea why? I ran cmake with the following options as described in [1]:. ```cmake -DCMAKE_BUILD_TYPE=Debug -Dtesting=ON -Droottest=ON```. Do I need more options? A `pyroottest=ON` ? Philippe. [1] https://root.cern/for_developers/run_the_tests/",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:228,deployability,build,build,228,"> Build failed on ROOT-debian10-i386/cxx14. > Running on pcepsft11.dyndns.cern.ch:/home/sftnight/build/workspace/root-pullrequests-build. > [See console output](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/console). > ### Failing tests:. > . > * [projectroot.roottest.python.cpp.roottest_python_cpp_cpp](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/testReport/projectroot.roottest.python/cpp/roottest_python_cpp_cpp/). @vgvassilev , I don't have this test in my checkout:. ```pgras@xps13:~/git.d/root.grasph/mybuild$ ctest -R projectroot.roottest.python.cpp.roottest_python_cpp_cpp . Test project /home/pgras/git.d/root.grasph/mybuild. No tests were found!!! ```. Do you have an idea why? I ran cmake with the following options as described in [1]:. ```cmake -DCMAKE_BUILD_TYPE=Debug -Dtesting=ON -Droottest=ON```. Do I need more options? A `pyroottest=ON` ? Philippe. [1] https://root.cern/for_developers/run_the_tests/",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:257,deployability,Fail,Failing,257,"> Build failed on ROOT-debian10-i386/cxx14. > Running on pcepsft11.dyndns.cern.ch:/home/sftnight/build/workspace/root-pullrequests-build. > [See console output](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/console). > ### Failing tests:. > . > * [projectroot.roottest.python.cpp.roottest_python_cpp_cpp](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/testReport/projectroot.roottest.python/cpp/roottest_python_cpp_cpp/). @vgvassilev , I don't have this test in my checkout:. ```pgras@xps13:~/git.d/root.grasph/mybuild$ ctest -R projectroot.roottest.python.cpp.roottest_python_cpp_cpp . Test project /home/pgras/git.d/root.grasph/mybuild. No tests were found!!! ```. Do you have an idea why? I ran cmake with the following options as described in [1]:. ```cmake -DCMAKE_BUILD_TYPE=Debug -Dtesting=ON -Droottest=ON```. Do I need more options? A `pyroottest=ON` ? Philippe. [1] https://root.cern/for_developers/run_the_tests/",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:354,deployability,servic,services,354,"> Build failed on ROOT-debian10-i386/cxx14. > Running on pcepsft11.dyndns.cern.ch:/home/sftnight/build/workspace/root-pullrequests-build. > [See console output](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/console). > ### Failing tests:. > . > * [projectroot.roottest.python.cpp.roottest_python_cpp_cpp](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/testReport/projectroot.roottest.python/cpp/roottest_python_cpp_cpp/). @vgvassilev , I don't have this test in my checkout:. ```pgras@xps13:~/git.d/root.grasph/mybuild$ ctest -R projectroot.roottest.python.cpp.roottest_python_cpp_cpp . Test project /home/pgras/git.d/root.grasph/mybuild. No tests were found!!! ```. Do you have an idea why? I ran cmake with the following options as described in [1]:. ```cmake -DCMAKE_BUILD_TYPE=Debug -Dtesting=ON -Droottest=ON```. Do I need more options? A `pyroottest=ON` ? Philippe. [1] https://root.cern/for_developers/run_the_tests/",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:406,deployability,build,build,406,"> Build failed on ROOT-debian10-i386/cxx14. > Running on pcepsft11.dyndns.cern.ch:/home/sftnight/build/workspace/root-pullrequests-build. > [See console output](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/console). > ### Failing tests:. > . > * [projectroot.roottest.python.cpp.roottest_python_cpp_cpp](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/testReport/projectroot.roottest.python/cpp/roottest_python_cpp_cpp/). @vgvassilev , I don't have this test in my checkout:. ```pgras@xps13:~/git.d/root.grasph/mybuild$ ctest -R projectroot.roottest.python.cpp.roottest_python_cpp_cpp . Test project /home/pgras/git.d/root.grasph/mybuild. No tests were found!!! ```. Do you have an idea why? I ran cmake with the following options as described in [1]:. ```cmake -DCMAKE_BUILD_TYPE=Debug -Dtesting=ON -Droottest=ON```. Do I need more options? A `pyroottest=ON` ? Philippe. [1] https://root.cern/for_developers/run_the_tests/",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:176,integrability,servic,services,176,"> Build failed on ROOT-debian10-i386/cxx14. > Running on pcepsft11.dyndns.cern.ch:/home/sftnight/build/workspace/root-pullrequests-build. > [See console output](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/console). > ### Failing tests:. > . > * [projectroot.roottest.python.cpp.roottest_python_cpp_cpp](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/testReport/projectroot.roottest.python/cpp/roottest_python_cpp_cpp/). @vgvassilev , I don't have this test in my checkout:. ```pgras@xps13:~/git.d/root.grasph/mybuild$ ctest -R projectroot.roottest.python.cpp.roottest_python_cpp_cpp . Test project /home/pgras/git.d/root.grasph/mybuild. No tests were found!!! ```. Do you have an idea why? I ran cmake with the following options as described in [1]:. ```cmake -DCMAKE_BUILD_TYPE=Debug -Dtesting=ON -Droottest=ON```. Do I need more options? A `pyroottest=ON` ? Philippe. [1] https://root.cern/for_developers/run_the_tests/",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:354,integrability,servic,services,354,"> Build failed on ROOT-debian10-i386/cxx14. > Running on pcepsft11.dyndns.cern.ch:/home/sftnight/build/workspace/root-pullrequests-build. > [See console output](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/console). > ### Failing tests:. > . > * [projectroot.roottest.python.cpp.roottest_python_cpp_cpp](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/testReport/projectroot.roottest.python/cpp/roottest_python_cpp_cpp/). @vgvassilev , I don't have this test in my checkout:. ```pgras@xps13:~/git.d/root.grasph/mybuild$ ctest -R projectroot.roottest.python.cpp.roottest_python_cpp_cpp . Test project /home/pgras/git.d/root.grasph/mybuild. No tests were found!!! ```. Do you have an idea why? I ran cmake with the following options as described in [1]:. ```cmake -DCMAKE_BUILD_TYPE=Debug -Dtesting=ON -Droottest=ON```. Do I need more options? A `pyroottest=ON` ? Philippe. [1] https://root.cern/for_developers/run_the_tests/",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:176,modifiability,servic,services,176,"> Build failed on ROOT-debian10-i386/cxx14. > Running on pcepsft11.dyndns.cern.ch:/home/sftnight/build/workspace/root-pullrequests-build. > [See console output](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/console). > ### Failing tests:. > . > * [projectroot.roottest.python.cpp.roottest_python_cpp_cpp](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/testReport/projectroot.roottest.python/cpp/roottest_python_cpp_cpp/). @vgvassilev , I don't have this test in my checkout:. ```pgras@xps13:~/git.d/root.grasph/mybuild$ ctest -R projectroot.roottest.python.cpp.roottest_python_cpp_cpp . Test project /home/pgras/git.d/root.grasph/mybuild. No tests were found!!! ```. Do you have an idea why? I ran cmake with the following options as described in [1]:. ```cmake -DCMAKE_BUILD_TYPE=Debug -Dtesting=ON -Droottest=ON```. Do I need more options? A `pyroottest=ON` ? Philippe. [1] https://root.cern/for_developers/run_the_tests/",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:354,modifiability,servic,services,354,"> Build failed on ROOT-debian10-i386/cxx14. > Running on pcepsft11.dyndns.cern.ch:/home/sftnight/build/workspace/root-pullrequests-build. > [See console output](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/console). > ### Failing tests:. > . > * [projectroot.roottest.python.cpp.roottest_python_cpp_cpp](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/testReport/projectroot.roottest.python/cpp/roottest_python_cpp_cpp/). @vgvassilev , I don't have this test in my checkout:. ```pgras@xps13:~/git.d/root.grasph/mybuild$ ctest -R projectroot.roottest.python.cpp.roottest_python_cpp_cpp . Test project /home/pgras/git.d/root.grasph/mybuild. No tests were found!!! ```. Do you have an idea why? I ran cmake with the following options as described in [1]:. ```cmake -DCMAKE_BUILD_TYPE=Debug -Dtesting=ON -Droottest=ON```. Do I need more options? A `pyroottest=ON` ? Philippe. [1] https://root.cern/for_developers/run_the_tests/",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:8,reliability,fail,failed,8,"> Build failed on ROOT-debian10-i386/cxx14. > Running on pcepsft11.dyndns.cern.ch:/home/sftnight/build/workspace/root-pullrequests-build. > [See console output](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/console). > ### Failing tests:. > . > * [projectroot.roottest.python.cpp.roottest_python_cpp_cpp](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/testReport/projectroot.roottest.python/cpp/roottest_python_cpp_cpp/). @vgvassilev , I don't have this test in my checkout:. ```pgras@xps13:~/git.d/root.grasph/mybuild$ ctest -R projectroot.roottest.python.cpp.roottest_python_cpp_cpp . Test project /home/pgras/git.d/root.grasph/mybuild. No tests were found!!! ```. Do you have an idea why? I ran cmake with the following options as described in [1]:. ```cmake -DCMAKE_BUILD_TYPE=Debug -Dtesting=ON -Droottest=ON```. Do I need more options? A `pyroottest=ON` ? Philippe. [1] https://root.cern/for_developers/run_the_tests/",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:257,reliability,Fail,Failing,257,"> Build failed on ROOT-debian10-i386/cxx14. > Running on pcepsft11.dyndns.cern.ch:/home/sftnight/build/workspace/root-pullrequests-build. > [See console output](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/console). > ### Failing tests:. > . > * [projectroot.roottest.python.cpp.roottest_python_cpp_cpp](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/testReport/projectroot.roottest.python/cpp/roottest_python_cpp_cpp/). @vgvassilev , I don't have this test in my checkout:. ```pgras@xps13:~/git.d/root.grasph/mybuild$ ctest -R projectroot.roottest.python.cpp.roottest_python_cpp_cpp . Test project /home/pgras/git.d/root.grasph/mybuild. No tests were found!!! ```. Do you have an idea why? I ran cmake with the following options as described in [1]:. ```cmake -DCMAKE_BUILD_TYPE=Debug -Dtesting=ON -Droottest=ON```. Do I need more options? A `pyroottest=ON` ? Philippe. [1] https://root.cern/for_developers/run_the_tests/",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:265,safety,test,tests,265,"> Build failed on ROOT-debian10-i386/cxx14. > Running on pcepsft11.dyndns.cern.ch:/home/sftnight/build/workspace/root-pullrequests-build. > [See console output](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/console). > ### Failing tests:. > . > * [projectroot.roottest.python.cpp.roottest_python_cpp_cpp](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/testReport/projectroot.roottest.python/cpp/roottest_python_cpp_cpp/). @vgvassilev , I don't have this test in my checkout:. ```pgras@xps13:~/git.d/root.grasph/mybuild$ ctest -R projectroot.roottest.python.cpp.roottest_python_cpp_cpp . Test project /home/pgras/git.d/root.grasph/mybuild. No tests were found!!! ```. Do you have an idea why? I ran cmake with the following options as described in [1]:. ```cmake -DCMAKE_BUILD_TYPE=Debug -Dtesting=ON -Droottest=ON```. Do I need more options? A `pyroottest=ON` ? Philippe. [1] https://root.cern/for_developers/run_the_tests/",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:419,safety,test,testReport,419,"> Build failed on ROOT-debian10-i386/cxx14. > Running on pcepsft11.dyndns.cern.ch:/home/sftnight/build/workspace/root-pullrequests-build. > [See console output](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/console). > ### Failing tests:. > . > * [projectroot.roottest.python.cpp.roottest_python_cpp_cpp](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/testReport/projectroot.roottest.python/cpp/roottest_python_cpp_cpp/). @vgvassilev , I don't have this test in my checkout:. ```pgras@xps13:~/git.d/root.grasph/mybuild$ ctest -R projectroot.roottest.python.cpp.roottest_python_cpp_cpp . Test project /home/pgras/git.d/root.grasph/mybuild. No tests were found!!! ```. Do you have an idea why? I ran cmake with the following options as described in [1]:. ```cmake -DCMAKE_BUILD_TYPE=Debug -Dtesting=ON -Droottest=ON```. Do I need more options? A `pyroottest=ON` ? Philippe. [1] https://root.cern/for_developers/run_the_tests/",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:521,safety,test,test,521,"> Build failed on ROOT-debian10-i386/cxx14. > Running on pcepsft11.dyndns.cern.ch:/home/sftnight/build/workspace/root-pullrequests-build. > [See console output](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/console). > ### Failing tests:. > . > * [projectroot.roottest.python.cpp.roottest_python_cpp_cpp](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/testReport/projectroot.roottest.python/cpp/roottest_python_cpp_cpp/). @vgvassilev , I don't have this test in my checkout:. ```pgras@xps13:~/git.d/root.grasph/mybuild$ ctest -R projectroot.roottest.python.cpp.roottest_python_cpp_cpp . Test project /home/pgras/git.d/root.grasph/mybuild. No tests were found!!! ```. Do you have an idea why? I ran cmake with the following options as described in [1]:. ```cmake -DCMAKE_BUILD_TYPE=Debug -Dtesting=ON -Droottest=ON```. Do I need more options? A `pyroottest=ON` ? Philippe. [1] https://root.cern/for_developers/run_the_tests/",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:654,safety,Test,Test,654,"> Build failed on ROOT-debian10-i386/cxx14. > Running on pcepsft11.dyndns.cern.ch:/home/sftnight/build/workspace/root-pullrequests-build. > [See console output](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/console). > ### Failing tests:. > . > * [projectroot.roottest.python.cpp.roottest_python_cpp_cpp](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/testReport/projectroot.roottest.python/cpp/roottest_python_cpp_cpp/). @vgvassilev , I don't have this test in my checkout:. ```pgras@xps13:~/git.d/root.grasph/mybuild$ ctest -R projectroot.roottest.python.cpp.roottest_python_cpp_cpp . Test project /home/pgras/git.d/root.grasph/mybuild. No tests were found!!! ```. Do you have an idea why? I ran cmake with the following options as described in [1]:. ```cmake -DCMAKE_BUILD_TYPE=Debug -Dtesting=ON -Droottest=ON```. Do I need more options? A `pyroottest=ON` ? Philippe. [1] https://root.cern/for_developers/run_the_tests/",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:709,safety,test,tests,709,"> Build failed on ROOT-debian10-i386/cxx14. > Running on pcepsft11.dyndns.cern.ch:/home/sftnight/build/workspace/root-pullrequests-build. > [See console output](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/console). > ### Failing tests:. > . > * [projectroot.roottest.python.cpp.roottest_python_cpp_cpp](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/testReport/projectroot.roottest.python/cpp/roottest_python_cpp_cpp/). @vgvassilev , I don't have this test in my checkout:. ```pgras@xps13:~/git.d/root.grasph/mybuild$ ctest -R projectroot.roottest.python.cpp.roottest_python_cpp_cpp . Test project /home/pgras/git.d/root.grasph/mybuild. No tests were found!!! ```. Do you have an idea why? I ran cmake with the following options as described in [1]:. ```cmake -DCMAKE_BUILD_TYPE=Debug -Dtesting=ON -Droottest=ON```. Do I need more options? A `pyroottest=ON` ? Philippe. [1] https://root.cern/for_developers/run_the_tests/",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:265,testability,test,tests,265,"> Build failed on ROOT-debian10-i386/cxx14. > Running on pcepsft11.dyndns.cern.ch:/home/sftnight/build/workspace/root-pullrequests-build. > [See console output](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/console). > ### Failing tests:. > . > * [projectroot.roottest.python.cpp.roottest_python_cpp_cpp](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/testReport/projectroot.roottest.python/cpp/roottest_python_cpp_cpp/). @vgvassilev , I don't have this test in my checkout:. ```pgras@xps13:~/git.d/root.grasph/mybuild$ ctest -R projectroot.roottest.python.cpp.roottest_python_cpp_cpp . Test project /home/pgras/git.d/root.grasph/mybuild. No tests were found!!! ```. Do you have an idea why? I ran cmake with the following options as described in [1]:. ```cmake -DCMAKE_BUILD_TYPE=Debug -Dtesting=ON -Droottest=ON```. Do I need more options? A `pyroottest=ON` ? Philippe. [1] https://root.cern/for_developers/run_the_tests/",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:419,testability,test,testReport,419,"> Build failed on ROOT-debian10-i386/cxx14. > Running on pcepsft11.dyndns.cern.ch:/home/sftnight/build/workspace/root-pullrequests-build. > [See console output](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/console). > ### Failing tests:. > . > * [projectroot.roottest.python.cpp.roottest_python_cpp_cpp](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/testReport/projectroot.roottest.python/cpp/roottest_python_cpp_cpp/). @vgvassilev , I don't have this test in my checkout:. ```pgras@xps13:~/git.d/root.grasph/mybuild$ ctest -R projectroot.roottest.python.cpp.roottest_python_cpp_cpp . Test project /home/pgras/git.d/root.grasph/mybuild. No tests were found!!! ```. Do you have an idea why? I ran cmake with the following options as described in [1]:. ```cmake -DCMAKE_BUILD_TYPE=Debug -Dtesting=ON -Droottest=ON```. Do I need more options? A `pyroottest=ON` ? Philippe. [1] https://root.cern/for_developers/run_the_tests/",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:521,testability,test,test,521,"> Build failed on ROOT-debian10-i386/cxx14. > Running on pcepsft11.dyndns.cern.ch:/home/sftnight/build/workspace/root-pullrequests-build. > [See console output](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/console). > ### Failing tests:. > . > * [projectroot.roottest.python.cpp.roottest_python_cpp_cpp](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/testReport/projectroot.roottest.python/cpp/roottest_python_cpp_cpp/). @vgvassilev , I don't have this test in my checkout:. ```pgras@xps13:~/git.d/root.grasph/mybuild$ ctest -R projectroot.roottest.python.cpp.roottest_python_cpp_cpp . Test project /home/pgras/git.d/root.grasph/mybuild. No tests were found!!! ```. Do you have an idea why? I ran cmake with the following options as described in [1]:. ```cmake -DCMAKE_BUILD_TYPE=Debug -Dtesting=ON -Droottest=ON```. Do I need more options? A `pyroottest=ON` ? Philippe. [1] https://root.cern/for_developers/run_the_tests/",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:654,testability,Test,Test,654,"> Build failed on ROOT-debian10-i386/cxx14. > Running on pcepsft11.dyndns.cern.ch:/home/sftnight/build/workspace/root-pullrequests-build. > [See console output](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/console). > ### Failing tests:. > . > * [projectroot.roottest.python.cpp.roottest_python_cpp_cpp](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/testReport/projectroot.roottest.python/cpp/roottest_python_cpp_cpp/). @vgvassilev , I don't have this test in my checkout:. ```pgras@xps13:~/git.d/root.grasph/mybuild$ ctest -R projectroot.roottest.python.cpp.roottest_python_cpp_cpp . Test project /home/pgras/git.d/root.grasph/mybuild. No tests were found!!! ```. Do you have an idea why? I ran cmake with the following options as described in [1]:. ```cmake -DCMAKE_BUILD_TYPE=Debug -Dtesting=ON -Droottest=ON```. Do I need more options? A `pyroottest=ON` ? Philippe. [1] https://root.cern/for_developers/run_the_tests/",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:709,testability,test,tests,709,"> Build failed on ROOT-debian10-i386/cxx14. > Running on pcepsft11.dyndns.cern.ch:/home/sftnight/build/workspace/root-pullrequests-build. > [See console output](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/console). > ### Failing tests:. > . > * [projectroot.roottest.python.cpp.roottest_python_cpp_cpp](https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/123133/testReport/projectroot.roottest.python/cpp/roottest_python_cpp_cpp/). @vgvassilev , I don't have this test in my checkout:. ```pgras@xps13:~/git.d/root.grasph/mybuild$ ctest -R projectroot.roottest.python.cpp.roottest_python_cpp_cpp . Test project /home/pgras/git.d/root.grasph/mybuild. No tests were found!!! ```. Do you have an idea why? I ran cmake with the following options as described in [1]:. ```cmake -DCMAKE_BUILD_TYPE=Debug -Dtesting=ON -Droottest=ON```. Do I need more options? A `pyroottest=ON` ? Philippe. [1] https://root.cern/for_developers/run_the_tests/",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:27,availability,failur,failure,27,"@grasph, that is a regular failure and it is not caused by your PR.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:27,deployability,fail,failure,27,"@grasph, that is a regular failure and it is not caused by your PR.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:27,performance,failur,failure,27,"@grasph, that is a regular failure and it is not caused by your PR.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:27,reliability,fail,failure,27,"@grasph, that is a regular failure and it is not caused by your PR.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:299,deployability,build,build,299,We're already adding `-fno-semantic-interposition` since commit https://github.com/root-project/root/commit/e564c8a04feb2b6fa18a2f429d8fb5a103825a34 / PR #8204 which I *think* should already set the visibility such that LLVM symbols are hidden from other libraries. Did you recently test a `master` build or 6.24/02?,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:27,interoperability,semant,semantic-interposition,27,We're already adding `-fno-semantic-interposition` since commit https://github.com/root-project/root/commit/e564c8a04feb2b6fa18a2f429d8fb5a103825a34 / PR #8204 which I *think* should already set the visibility such that LLVM symbols are hidden from other libraries. Did you recently test a `master` build or 6.24/02?,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:283,safety,test,test,283,We're already adding `-fno-semantic-interposition` since commit https://github.com/root-project/root/commit/e564c8a04feb2b6fa18a2f429d8fb5a103825a34 / PR #8204 which I *think* should already set the visibility such that LLVM symbols are hidden from other libraries. Did you recently test a `master` build or 6.24/02?,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:283,testability,test,test,283,We're already adding `-fno-semantic-interposition` since commit https://github.com/root-project/root/commit/e564c8a04feb2b6fa18a2f429d8fb5a103825a34 / PR #8204 which I *think* should already set the visibility such that LLVM symbols are hidden from other libraries. Did you recently test a `master` build or 6.24/02?,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:14,deployability,patch,patch,14,"@hahnjo, this patch is useful for cppyy and it sounds like -Bsymbolic covers more than VISIBILITY_INLINES_HIDDEN while also reducing the PLTs. In particular (ROOT master):. $ nm -CD libCling.so | grep llvm | grep ' [Ww] ' | wc -l. 379",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:124,energy efficiency,reduc,reducing,124,"@hahnjo, this patch is useful for cppyy and it sounds like -Bsymbolic covers more than VISIBILITY_INLINES_HIDDEN while also reducing the PLTs. In particular (ROOT master):. $ nm -CD libCling.so | grep llvm | grep ' [Ww] ' | wc -l. 379",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:14,safety,patch,patch,14,"@hahnjo, this patch is useful for cppyy and it sounds like -Bsymbolic covers more than VISIBILITY_INLINES_HIDDEN while also reducing the PLTs. In particular (ROOT master):. $ nm -CD libCling.so | grep llvm | grep ' [Ww] ' | wc -l. 379",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:14,security,patch,patch,14,"@hahnjo, this patch is useful for cppyy and it sounds like -Bsymbolic covers more than VISIBILITY_INLINES_HIDDEN while also reducing the PLTs. In particular (ROOT master):. $ nm -CD libCling.so | grep llvm | grep ' [Ww] ' | wc -l. 379",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:1272,availability,reliab,reliable,1272,"> We're already adding `-fno-semantic-interposition` since commit [e564c8a](https://github.com/root-project/root/commit/e564c8a04feb2b6fa18a2f429d8fb5a103825a34) / PR #8204 which I _think_ should already set the visibility such that LLVM symbols are hidden from other libraries. Did you recently test a `master` build or 6.24/02? Interesting. From my understanding of g++ document, this flag with accidentally solves the problem if the compiler optimize the code by in-lining it but it is not guarranted. In addition it can create confusing situations where the problem disappears as soon as you try to debug it using the debugged version of the code. That said, we are in a confusing situation, as I'm not able to reproduce the problem as soon as I recompile ROOT. Nevertheless, I recompiled many time ROOT last week, and then the problem was systematic. It could be that it is due to the ROOT code version I used. The problem is there for LCG100 which used ROOT 6.24/00, but not for `/cvmfs/sft.cern.ch/lcg/app/releases/ROOT/6.24.02/x86_64-centos7-gcc48-opt/`. So it would confirm that `-fno-semantic-interposition` has solved the problem if it has happened between the two versions. Nevertheless, as I wrote before, in my understanding, it's accidental and won't be as reliable as the -Bsymbolic option. Philippe.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:312,deployability,build,build,312,"> We're already adding `-fno-semantic-interposition` since commit [e564c8a](https://github.com/root-project/root/commit/e564c8a04feb2b6fa18a2f429d8fb5a103825a34) / PR #8204 which I _think_ should already set the visibility such that LLVM symbols are hidden from other libraries. Did you recently test a `master` build or 6.24/02? Interesting. From my understanding of g++ document, this flag with accidentally solves the problem if the compiler optimize the code by in-lining it but it is not guarranted. In addition it can create confusing situations where the problem disappears as soon as you try to debug it using the debugged version of the code. That said, we are in a confusing situation, as I'm not able to reproduce the problem as soon as I recompile ROOT. Nevertheless, I recompiled many time ROOT last week, and then the problem was systematic. It could be that it is due to the ROOT code version I used. The problem is there for LCG100 which used ROOT 6.24/00, but not for `/cvmfs/sft.cern.ch/lcg/app/releases/ROOT/6.24.02/x86_64-centos7-gcc48-opt/`. So it would confirm that `-fno-semantic-interposition` has solved the problem if it has happened between the two versions. Nevertheless, as I wrote before, in my understanding, it's accidental and won't be as reliable as the -Bsymbolic option. Philippe.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:631,deployability,version,version,631,"> We're already adding `-fno-semantic-interposition` since commit [e564c8a](https://github.com/root-project/root/commit/e564c8a04feb2b6fa18a2f429d8fb5a103825a34) / PR #8204 which I _think_ should already set the visibility such that LLVM symbols are hidden from other libraries. Did you recently test a `master` build or 6.24/02? Interesting. From my understanding of g++ document, this flag with accidentally solves the problem if the compiler optimize the code by in-lining it but it is not guarranted. In addition it can create confusing situations where the problem disappears as soon as you try to debug it using the debugged version of the code. That said, we are in a confusing situation, as I'm not able to reproduce the problem as soon as I recompile ROOT. Nevertheless, I recompiled many time ROOT last week, and then the problem was systematic. It could be that it is due to the ROOT code version I used. The problem is there for LCG100 which used ROOT 6.24/00, but not for `/cvmfs/sft.cern.ch/lcg/app/releases/ROOT/6.24.02/x86_64-centos7-gcc48-opt/`. So it would confirm that `-fno-semantic-interposition` has solved the problem if it has happened between the two versions. Nevertheless, as I wrote before, in my understanding, it's accidental and won't be as reliable as the -Bsymbolic option. Philippe.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:900,deployability,version,version,900,"> We're already adding `-fno-semantic-interposition` since commit [e564c8a](https://github.com/root-project/root/commit/e564c8a04feb2b6fa18a2f429d8fb5a103825a34) / PR #8204 which I _think_ should already set the visibility such that LLVM symbols are hidden from other libraries. Did you recently test a `master` build or 6.24/02? Interesting. From my understanding of g++ document, this flag with accidentally solves the problem if the compiler optimize the code by in-lining it but it is not guarranted. In addition it can create confusing situations where the problem disappears as soon as you try to debug it using the debugged version of the code. That said, we are in a confusing situation, as I'm not able to reproduce the problem as soon as I recompile ROOT. Nevertheless, I recompiled many time ROOT last week, and then the problem was systematic. It could be that it is due to the ROOT code version I used. The problem is there for LCG100 which used ROOT 6.24/00, but not for `/cvmfs/sft.cern.ch/lcg/app/releases/ROOT/6.24.02/x86_64-centos7-gcc48-opt/`. So it would confirm that `-fno-semantic-interposition` has solved the problem if it has happened between the two versions. Nevertheless, as I wrote before, in my understanding, it's accidental and won't be as reliable as the -Bsymbolic option. Philippe.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:1013,deployability,releas,releases,1013,"> We're already adding `-fno-semantic-interposition` since commit [e564c8a](https://github.com/root-project/root/commit/e564c8a04feb2b6fa18a2f429d8fb5a103825a34) / PR #8204 which I _think_ should already set the visibility such that LLVM symbols are hidden from other libraries. Did you recently test a `master` build or 6.24/02? Interesting. From my understanding of g++ document, this flag with accidentally solves the problem if the compiler optimize the code by in-lining it but it is not guarranted. In addition it can create confusing situations where the problem disappears as soon as you try to debug it using the debugged version of the code. That said, we are in a confusing situation, as I'm not able to reproduce the problem as soon as I recompile ROOT. Nevertheless, I recompiled many time ROOT last week, and then the problem was systematic. It could be that it is due to the ROOT code version I used. The problem is there for LCG100 which used ROOT 6.24/00, but not for `/cvmfs/sft.cern.ch/lcg/app/releases/ROOT/6.24.02/x86_64-centos7-gcc48-opt/`. So it would confirm that `-fno-semantic-interposition` has solved the problem if it has happened between the two versions. Nevertheless, as I wrote before, in my understanding, it's accidental and won't be as reliable as the -Bsymbolic option. Philippe.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:1176,deployability,version,versions,1176,"> We're already adding `-fno-semantic-interposition` since commit [e564c8a](https://github.com/root-project/root/commit/e564c8a04feb2b6fa18a2f429d8fb5a103825a34) / PR #8204 which I _think_ should already set the visibility such that LLVM symbols are hidden from other libraries. Did you recently test a `master` build or 6.24/02? Interesting. From my understanding of g++ document, this flag with accidentally solves the problem if the compiler optimize the code by in-lining it but it is not guarranted. In addition it can create confusing situations where the problem disappears as soon as you try to debug it using the debugged version of the code. That said, we are in a confusing situation, as I'm not able to reproduce the problem as soon as I recompile ROOT. Nevertheless, I recompiled many time ROOT last week, and then the problem was systematic. It could be that it is due to the ROOT code version I used. The problem is there for LCG100 which used ROOT 6.24/00, but not for `/cvmfs/sft.cern.ch/lcg/app/releases/ROOT/6.24.02/x86_64-centos7-gcc48-opt/`. So it would confirm that `-fno-semantic-interposition` has solved the problem if it has happened between the two versions. Nevertheless, as I wrote before, in my understanding, it's accidental and won't be as reliable as the -Bsymbolic option. Philippe.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:445,energy efficiency,optim,optimize,445,"> We're already adding `-fno-semantic-interposition` since commit [e564c8a](https://github.com/root-project/root/commit/e564c8a04feb2b6fa18a2f429d8fb5a103825a34) / PR #8204 which I _think_ should already set the visibility such that LLVM symbols are hidden from other libraries. Did you recently test a `master` build or 6.24/02? Interesting. From my understanding of g++ document, this flag with accidentally solves the problem if the compiler optimize the code by in-lining it but it is not guarranted. In addition it can create confusing situations where the problem disappears as soon as you try to debug it using the debugged version of the code. That said, we are in a confusing situation, as I'm not able to reproduce the problem as soon as I recompile ROOT. Nevertheless, I recompiled many time ROOT last week, and then the problem was systematic. It could be that it is due to the ROOT code version I used. The problem is there for LCG100 which used ROOT 6.24/00, but not for `/cvmfs/sft.cern.ch/lcg/app/releases/ROOT/6.24.02/x86_64-centos7-gcc48-opt/`. So it would confirm that `-fno-semantic-interposition` has solved the problem if it has happened between the two versions. Nevertheless, as I wrote before, in my understanding, it's accidental and won't be as reliable as the -Bsymbolic option. Philippe.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:631,integrability,version,version,631,"> We're already adding `-fno-semantic-interposition` since commit [e564c8a](https://github.com/root-project/root/commit/e564c8a04feb2b6fa18a2f429d8fb5a103825a34) / PR #8204 which I _think_ should already set the visibility such that LLVM symbols are hidden from other libraries. Did you recently test a `master` build or 6.24/02? Interesting. From my understanding of g++ document, this flag with accidentally solves the problem if the compiler optimize the code by in-lining it but it is not guarranted. In addition it can create confusing situations where the problem disappears as soon as you try to debug it using the debugged version of the code. That said, we are in a confusing situation, as I'm not able to reproduce the problem as soon as I recompile ROOT. Nevertheless, I recompiled many time ROOT last week, and then the problem was systematic. It could be that it is due to the ROOT code version I used. The problem is there for LCG100 which used ROOT 6.24/00, but not for `/cvmfs/sft.cern.ch/lcg/app/releases/ROOT/6.24.02/x86_64-centos7-gcc48-opt/`. So it would confirm that `-fno-semantic-interposition` has solved the problem if it has happened between the two versions. Nevertheless, as I wrote before, in my understanding, it's accidental and won't be as reliable as the -Bsymbolic option. Philippe.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:900,integrability,version,version,900,"> We're already adding `-fno-semantic-interposition` since commit [e564c8a](https://github.com/root-project/root/commit/e564c8a04feb2b6fa18a2f429d8fb5a103825a34) / PR #8204 which I _think_ should already set the visibility such that LLVM symbols are hidden from other libraries. Did you recently test a `master` build or 6.24/02? Interesting. From my understanding of g++ document, this flag with accidentally solves the problem if the compiler optimize the code by in-lining it but it is not guarranted. In addition it can create confusing situations where the problem disappears as soon as you try to debug it using the debugged version of the code. That said, we are in a confusing situation, as I'm not able to reproduce the problem as soon as I recompile ROOT. Nevertheless, I recompiled many time ROOT last week, and then the problem was systematic. It could be that it is due to the ROOT code version I used. The problem is there for LCG100 which used ROOT 6.24/00, but not for `/cvmfs/sft.cern.ch/lcg/app/releases/ROOT/6.24.02/x86_64-centos7-gcc48-opt/`. So it would confirm that `-fno-semantic-interposition` has solved the problem if it has happened between the two versions. Nevertheless, as I wrote before, in my understanding, it's accidental and won't be as reliable as the -Bsymbolic option. Philippe.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:1176,integrability,version,versions,1176,"> We're already adding `-fno-semantic-interposition` since commit [e564c8a](https://github.com/root-project/root/commit/e564c8a04feb2b6fa18a2f429d8fb5a103825a34) / PR #8204 which I _think_ should already set the visibility such that LLVM symbols are hidden from other libraries. Did you recently test a `master` build or 6.24/02? Interesting. From my understanding of g++ document, this flag with accidentally solves the problem if the compiler optimize the code by in-lining it but it is not guarranted. In addition it can create confusing situations where the problem disappears as soon as you try to debug it using the debugged version of the code. That said, we are in a confusing situation, as I'm not able to reproduce the problem as soon as I recompile ROOT. Nevertheless, I recompiled many time ROOT last week, and then the problem was systematic. It could be that it is due to the ROOT code version I used. The problem is there for LCG100 which used ROOT 6.24/00, but not for `/cvmfs/sft.cern.ch/lcg/app/releases/ROOT/6.24.02/x86_64-centos7-gcc48-opt/`. So it would confirm that `-fno-semantic-interposition` has solved the problem if it has happened between the two versions. Nevertheless, as I wrote before, in my understanding, it's accidental and won't be as reliable as the -Bsymbolic option. Philippe.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:29,interoperability,semant,semantic-interposition,29,"> We're already adding `-fno-semantic-interposition` since commit [e564c8a](https://github.com/root-project/root/commit/e564c8a04feb2b6fa18a2f429d8fb5a103825a34) / PR #8204 which I _think_ should already set the visibility such that LLVM symbols are hidden from other libraries. Did you recently test a `master` build or 6.24/02? Interesting. From my understanding of g++ document, this flag with accidentally solves the problem if the compiler optimize the code by in-lining it but it is not guarranted. In addition it can create confusing situations where the problem disappears as soon as you try to debug it using the debugged version of the code. That said, we are in a confusing situation, as I'm not able to reproduce the problem as soon as I recompile ROOT. Nevertheless, I recompiled many time ROOT last week, and then the problem was systematic. It could be that it is due to the ROOT code version I used. The problem is there for LCG100 which used ROOT 6.24/00, but not for `/cvmfs/sft.cern.ch/lcg/app/releases/ROOT/6.24.02/x86_64-centos7-gcc48-opt/`. So it would confirm that `-fno-semantic-interposition` has solved the problem if it has happened between the two versions. Nevertheless, as I wrote before, in my understanding, it's accidental and won't be as reliable as the -Bsymbolic option. Philippe.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:1094,interoperability,semant,semantic-interposition,1094,"> We're already adding `-fno-semantic-interposition` since commit [e564c8a](https://github.com/root-project/root/commit/e564c8a04feb2b6fa18a2f429d8fb5a103825a34) / PR #8204 which I _think_ should already set the visibility such that LLVM symbols are hidden from other libraries. Did you recently test a `master` build or 6.24/02? Interesting. From my understanding of g++ document, this flag with accidentally solves the problem if the compiler optimize the code by in-lining it but it is not guarranted. In addition it can create confusing situations where the problem disappears as soon as you try to debug it using the debugged version of the code. That said, we are in a confusing situation, as I'm not able to reproduce the problem as soon as I recompile ROOT. Nevertheless, I recompiled many time ROOT last week, and then the problem was systematic. It could be that it is due to the ROOT code version I used. The problem is there for LCG100 which used ROOT 6.24/00, but not for `/cvmfs/sft.cern.ch/lcg/app/releases/ROOT/6.24.02/x86_64-centos7-gcc48-opt/`. So it would confirm that `-fno-semantic-interposition` has solved the problem if it has happened between the two versions. Nevertheless, as I wrote before, in my understanding, it's accidental and won't be as reliable as the -Bsymbolic option. Philippe.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:631,modifiability,version,version,631,"> We're already adding `-fno-semantic-interposition` since commit [e564c8a](https://github.com/root-project/root/commit/e564c8a04feb2b6fa18a2f429d8fb5a103825a34) / PR #8204 which I _think_ should already set the visibility such that LLVM symbols are hidden from other libraries. Did you recently test a `master` build or 6.24/02? Interesting. From my understanding of g++ document, this flag with accidentally solves the problem if the compiler optimize the code by in-lining it but it is not guarranted. In addition it can create confusing situations where the problem disappears as soon as you try to debug it using the debugged version of the code. That said, we are in a confusing situation, as I'm not able to reproduce the problem as soon as I recompile ROOT. Nevertheless, I recompiled many time ROOT last week, and then the problem was systematic. It could be that it is due to the ROOT code version I used. The problem is there for LCG100 which used ROOT 6.24/00, but not for `/cvmfs/sft.cern.ch/lcg/app/releases/ROOT/6.24.02/x86_64-centos7-gcc48-opt/`. So it would confirm that `-fno-semantic-interposition` has solved the problem if it has happened between the two versions. Nevertheless, as I wrote before, in my understanding, it's accidental and won't be as reliable as the -Bsymbolic option. Philippe.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:900,modifiability,version,version,900,"> We're already adding `-fno-semantic-interposition` since commit [e564c8a](https://github.com/root-project/root/commit/e564c8a04feb2b6fa18a2f429d8fb5a103825a34) / PR #8204 which I _think_ should already set the visibility such that LLVM symbols are hidden from other libraries. Did you recently test a `master` build or 6.24/02? Interesting. From my understanding of g++ document, this flag with accidentally solves the problem if the compiler optimize the code by in-lining it but it is not guarranted. In addition it can create confusing situations where the problem disappears as soon as you try to debug it using the debugged version of the code. That said, we are in a confusing situation, as I'm not able to reproduce the problem as soon as I recompile ROOT. Nevertheless, I recompiled many time ROOT last week, and then the problem was systematic. It could be that it is due to the ROOT code version I used. The problem is there for LCG100 which used ROOT 6.24/00, but not for `/cvmfs/sft.cern.ch/lcg/app/releases/ROOT/6.24.02/x86_64-centos7-gcc48-opt/`. So it would confirm that `-fno-semantic-interposition` has solved the problem if it has happened between the two versions. Nevertheless, as I wrote before, in my understanding, it's accidental and won't be as reliable as the -Bsymbolic option. Philippe.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:1176,modifiability,version,versions,1176,"> We're already adding `-fno-semantic-interposition` since commit [e564c8a](https://github.com/root-project/root/commit/e564c8a04feb2b6fa18a2f429d8fb5a103825a34) / PR #8204 which I _think_ should already set the visibility such that LLVM symbols are hidden from other libraries. Did you recently test a `master` build or 6.24/02? Interesting. From my understanding of g++ document, this flag with accidentally solves the problem if the compiler optimize the code by in-lining it but it is not guarranted. In addition it can create confusing situations where the problem disappears as soon as you try to debug it using the debugged version of the code. That said, we are in a confusing situation, as I'm not able to reproduce the problem as soon as I recompile ROOT. Nevertheless, I recompiled many time ROOT last week, and then the problem was systematic. It could be that it is due to the ROOT code version I used. The problem is there for LCG100 which used ROOT 6.24/00, but not for `/cvmfs/sft.cern.ch/lcg/app/releases/ROOT/6.24.02/x86_64-centos7-gcc48-opt/`. So it would confirm that `-fno-semantic-interposition` has solved the problem if it has happened between the two versions. Nevertheless, as I wrote before, in my understanding, it's accidental and won't be as reliable as the -Bsymbolic option. Philippe.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:445,performance,optimiz,optimize,445,"> We're already adding `-fno-semantic-interposition` since commit [e564c8a](https://github.com/root-project/root/commit/e564c8a04feb2b6fa18a2f429d8fb5a103825a34) / PR #8204 which I _think_ should already set the visibility such that LLVM symbols are hidden from other libraries. Did you recently test a `master` build or 6.24/02? Interesting. From my understanding of g++ document, this flag with accidentally solves the problem if the compiler optimize the code by in-lining it but it is not guarranted. In addition it can create confusing situations where the problem disappears as soon as you try to debug it using the debugged version of the code. That said, we are in a confusing situation, as I'm not able to reproduce the problem as soon as I recompile ROOT. Nevertheless, I recompiled many time ROOT last week, and then the problem was systematic. It could be that it is due to the ROOT code version I used. The problem is there for LCG100 which used ROOT 6.24/00, but not for `/cvmfs/sft.cern.ch/lcg/app/releases/ROOT/6.24.02/x86_64-centos7-gcc48-opt/`. So it would confirm that `-fno-semantic-interposition` has solved the problem if it has happened between the two versions. Nevertheless, as I wrote before, in my understanding, it's accidental and won't be as reliable as the -Bsymbolic option. Philippe.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:798,performance,time,time,798,"> We're already adding `-fno-semantic-interposition` since commit [e564c8a](https://github.com/root-project/root/commit/e564c8a04feb2b6fa18a2f429d8fb5a103825a34) / PR #8204 which I _think_ should already set the visibility such that LLVM symbols are hidden from other libraries. Did you recently test a `master` build or 6.24/02? Interesting. From my understanding of g++ document, this flag with accidentally solves the problem if the compiler optimize the code by in-lining it but it is not guarranted. In addition it can create confusing situations where the problem disappears as soon as you try to debug it using the debugged version of the code. That said, we are in a confusing situation, as I'm not able to reproduce the problem as soon as I recompile ROOT. Nevertheless, I recompiled many time ROOT last week, and then the problem was systematic. It could be that it is due to the ROOT code version I used. The problem is there for LCG100 which used ROOT 6.24/00, but not for `/cvmfs/sft.cern.ch/lcg/app/releases/ROOT/6.24.02/x86_64-centos7-gcc48-opt/`. So it would confirm that `-fno-semantic-interposition` has solved the problem if it has happened between the two versions. Nevertheless, as I wrote before, in my understanding, it's accidental and won't be as reliable as the -Bsymbolic option. Philippe.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:1272,reliability,reliab,reliable,1272,"> We're already adding `-fno-semantic-interposition` since commit [e564c8a](https://github.com/root-project/root/commit/e564c8a04feb2b6fa18a2f429d8fb5a103825a34) / PR #8204 which I _think_ should already set the visibility such that LLVM symbols are hidden from other libraries. Did you recently test a `master` build or 6.24/02? Interesting. From my understanding of g++ document, this flag with accidentally solves the problem if the compiler optimize the code by in-lining it but it is not guarranted. In addition it can create confusing situations where the problem disappears as soon as you try to debug it using the debugged version of the code. That said, we are in a confusing situation, as I'm not able to reproduce the problem as soon as I recompile ROOT. Nevertheless, I recompiled many time ROOT last week, and then the problem was systematic. It could be that it is due to the ROOT code version I used. The problem is there for LCG100 which used ROOT 6.24/00, but not for `/cvmfs/sft.cern.ch/lcg/app/releases/ROOT/6.24.02/x86_64-centos7-gcc48-opt/`. So it would confirm that `-fno-semantic-interposition` has solved the problem if it has happened between the two versions. Nevertheless, as I wrote before, in my understanding, it's accidental and won't be as reliable as the -Bsymbolic option. Philippe.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:296,safety,test,test,296,"> We're already adding `-fno-semantic-interposition` since commit [e564c8a](https://github.com/root-project/root/commit/e564c8a04feb2b6fa18a2f429d8fb5a103825a34) / PR #8204 which I _think_ should already set the visibility such that LLVM symbols are hidden from other libraries. Did you recently test a `master` build or 6.24/02? Interesting. From my understanding of g++ document, this flag with accidentally solves the problem if the compiler optimize the code by in-lining it but it is not guarranted. In addition it can create confusing situations where the problem disappears as soon as you try to debug it using the debugged version of the code. That said, we are in a confusing situation, as I'm not able to reproduce the problem as soon as I recompile ROOT. Nevertheless, I recompiled many time ROOT last week, and then the problem was systematic. It could be that it is due to the ROOT code version I used. The problem is there for LCG100 which used ROOT 6.24/00, but not for `/cvmfs/sft.cern.ch/lcg/app/releases/ROOT/6.24.02/x86_64-centos7-gcc48-opt/`. So it would confirm that `-fno-semantic-interposition` has solved the problem if it has happened between the two versions. Nevertheless, as I wrote before, in my understanding, it's accidental and won't be as reliable as the -Bsymbolic option. Philippe.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:397,safety,accid,accidentally,397,"> We're already adding `-fno-semantic-interposition` since commit [e564c8a](https://github.com/root-project/root/commit/e564c8a04feb2b6fa18a2f429d8fb5a103825a34) / PR #8204 which I _think_ should already set the visibility such that LLVM symbols are hidden from other libraries. Did you recently test a `master` build or 6.24/02? Interesting. From my understanding of g++ document, this flag with accidentally solves the problem if the compiler optimize the code by in-lining it but it is not guarranted. In addition it can create confusing situations where the problem disappears as soon as you try to debug it using the debugged version of the code. That said, we are in a confusing situation, as I'm not able to reproduce the problem as soon as I recompile ROOT. Nevertheless, I recompiled many time ROOT last week, and then the problem was systematic. It could be that it is due to the ROOT code version I used. The problem is there for LCG100 which used ROOT 6.24/00, but not for `/cvmfs/sft.cern.ch/lcg/app/releases/ROOT/6.24.02/x86_64-centos7-gcc48-opt/`. So it would confirm that `-fno-semantic-interposition` has solved the problem if it has happened between the two versions. Nevertheless, as I wrote before, in my understanding, it's accidental and won't be as reliable as the -Bsymbolic option. Philippe.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:1245,safety,accid,accidental,1245,"> We're already adding `-fno-semantic-interposition` since commit [e564c8a](https://github.com/root-project/root/commit/e564c8a04feb2b6fa18a2f429d8fb5a103825a34) / PR #8204 which I _think_ should already set the visibility such that LLVM symbols are hidden from other libraries. Did you recently test a `master` build or 6.24/02? Interesting. From my understanding of g++ document, this flag with accidentally solves the problem if the compiler optimize the code by in-lining it but it is not guarranted. In addition it can create confusing situations where the problem disappears as soon as you try to debug it using the debugged version of the code. That said, we are in a confusing situation, as I'm not able to reproduce the problem as soon as I recompile ROOT. Nevertheless, I recompiled many time ROOT last week, and then the problem was systematic. It could be that it is due to the ROOT code version I used. The problem is there for LCG100 which used ROOT 6.24/00, but not for `/cvmfs/sft.cern.ch/lcg/app/releases/ROOT/6.24.02/x86_64-centos7-gcc48-opt/`. So it would confirm that `-fno-semantic-interposition` has solved the problem if it has happened between the two versions. Nevertheless, as I wrote before, in my understanding, it's accidental and won't be as reliable as the -Bsymbolic option. Philippe.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:296,testability,test,test,296,"> We're already adding `-fno-semantic-interposition` since commit [e564c8a](https://github.com/root-project/root/commit/e564c8a04feb2b6fa18a2f429d8fb5a103825a34) / PR #8204 which I _think_ should already set the visibility such that LLVM symbols are hidden from other libraries. Did you recently test a `master` build or 6.24/02? Interesting. From my understanding of g++ document, this flag with accidentally solves the problem if the compiler optimize the code by in-lining it but it is not guarranted. In addition it can create confusing situations where the problem disappears as soon as you try to debug it using the debugged version of the code. That said, we are in a confusing situation, as I'm not able to reproduce the problem as soon as I recompile ROOT. Nevertheless, I recompiled many time ROOT last week, and then the problem was systematic. It could be that it is due to the ROOT code version I used. The problem is there for LCG100 which used ROOT 6.24/00, but not for `/cvmfs/sft.cern.ch/lcg/app/releases/ROOT/6.24.02/x86_64-centos7-gcc48-opt/`. So it would confirm that `-fno-semantic-interposition` has solved the problem if it has happened between the two versions. Nevertheless, as I wrote before, in my understanding, it's accidental and won't be as reliable as the -Bsymbolic option. Philippe.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:351,testability,understand,understanding,351,"> We're already adding `-fno-semantic-interposition` since commit [e564c8a](https://github.com/root-project/root/commit/e564c8a04feb2b6fa18a2f429d8fb5a103825a34) / PR #8204 which I _think_ should already set the visibility such that LLVM symbols are hidden from other libraries. Did you recently test a `master` build or 6.24/02? Interesting. From my understanding of g++ document, this flag with accidentally solves the problem if the compiler optimize the code by in-lining it but it is not guarranted. In addition it can create confusing situations where the problem disappears as soon as you try to debug it using the debugged version of the code. That said, we are in a confusing situation, as I'm not able to reproduce the problem as soon as I recompile ROOT. Nevertheless, I recompiled many time ROOT last week, and then the problem was systematic. It could be that it is due to the ROOT code version I used. The problem is there for LCG100 which used ROOT 6.24/00, but not for `/cvmfs/sft.cern.ch/lcg/app/releases/ROOT/6.24.02/x86_64-centos7-gcc48-opt/`. So it would confirm that `-fno-semantic-interposition` has solved the problem if it has happened between the two versions. Nevertheless, as I wrote before, in my understanding, it's accidental and won't be as reliable as the -Bsymbolic option. Philippe.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:1225,testability,understand,understanding,1225,"> We're already adding `-fno-semantic-interposition` since commit [e564c8a](https://github.com/root-project/root/commit/e564c8a04feb2b6fa18a2f429d8fb5a103825a34) / PR #8204 which I _think_ should already set the visibility such that LLVM symbols are hidden from other libraries. Did you recently test a `master` build or 6.24/02? Interesting. From my understanding of g++ document, this flag with accidentally solves the problem if the compiler optimize the code by in-lining it but it is not guarranted. In addition it can create confusing situations where the problem disappears as soon as you try to debug it using the debugged version of the code. That said, we are in a confusing situation, as I'm not able to reproduce the problem as soon as I recompile ROOT. Nevertheless, I recompiled many time ROOT last week, and then the problem was systematic. It could be that it is due to the ROOT code version I used. The problem is there for LCG100 which used ROOT 6.24/00, but not for `/cvmfs/sft.cern.ch/lcg/app/releases/ROOT/6.24.02/x86_64-centos7-gcc48-opt/`. So it would confirm that `-fno-semantic-interposition` has solved the problem if it has happened between the two versions. Nevertheless, as I wrote before, in my understanding, it's accidental and won't be as reliable as the -Bsymbolic option. Philippe.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:372,usability,document,document,372,"> We're already adding `-fno-semantic-interposition` since commit [e564c8a](https://github.com/root-project/root/commit/e564c8a04feb2b6fa18a2f429d8fb5a103825a34) / PR #8204 which I _think_ should already set the visibility such that LLVM symbols are hidden from other libraries. Did you recently test a `master` build or 6.24/02? Interesting. From my understanding of g++ document, this flag with accidentally solves the problem if the compiler optimize the code by in-lining it but it is not guarranted. In addition it can create confusing situations where the problem disappears as soon as you try to debug it using the debugged version of the code. That said, we are in a confusing situation, as I'm not able to reproduce the problem as soon as I recompile ROOT. Nevertheless, I recompiled many time ROOT last week, and then the problem was systematic. It could be that it is due to the ROOT code version I used. The problem is there for LCG100 which used ROOT 6.24/00, but not for `/cvmfs/sft.cern.ch/lcg/app/releases/ROOT/6.24.02/x86_64-centos7-gcc48-opt/`. So it would confirm that `-fno-semantic-interposition` has solved the problem if it has happened between the two versions. Nevertheless, as I wrote before, in my understanding, it's accidental and won't be as reliable as the -Bsymbolic option. Philippe.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:1075,usability,confirm,confirm,1075,"> We're already adding `-fno-semantic-interposition` since commit [e564c8a](https://github.com/root-project/root/commit/e564c8a04feb2b6fa18a2f429d8fb5a103825a34) / PR #8204 which I _think_ should already set the visibility such that LLVM symbols are hidden from other libraries. Did you recently test a `master` build or 6.24/02? Interesting. From my understanding of g++ document, this flag with accidentally solves the problem if the compiler optimize the code by in-lining it but it is not guarranted. In addition it can create confusing situations where the problem disappears as soon as you try to debug it using the debugged version of the code. That said, we are in a confusing situation, as I'm not able to reproduce the problem as soon as I recompile ROOT. Nevertheless, I recompiled many time ROOT last week, and then the problem was systematic. It could be that it is due to the ROOT code version I used. The problem is there for LCG100 which used ROOT 6.24/00, but not for `/cvmfs/sft.cern.ch/lcg/app/releases/ROOT/6.24.02/x86_64-centos7-gcc48-opt/`. So it would confirm that `-fno-semantic-interposition` has solved the problem if it has happened between the two versions. Nevertheless, as I wrote before, in my understanding, it's accidental and won't be as reliable as the -Bsymbolic option. Philippe.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:148,availability,error,error,148,"How is it accidental? It was totally on purpose, solving other occurrences of this exact same issue. I suppose, because i have not seen any issue / error message / backtrace, so all I can do is guess and assume :-) Can you please provide some details on the issue you try to fix? Can you please rebase this branch onto master? We do not allow merge commits (unless fast forward).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:154,integrability,messag,message,154,"How is it accidental? It was totally on purpose, solving other occurrences of this exact same issue. I suppose, because i have not seen any issue / error message / backtrace, so all I can do is guess and assume :-) Can you please provide some details on the issue you try to fix? Can you please rebase this branch onto master? We do not allow merge commits (unless fast forward).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:154,interoperability,messag,message,154,"How is it accidental? It was totally on purpose, solving other occurrences of this exact same issue. I suppose, because i have not seen any issue / error message / backtrace, so all I can do is guess and assume :-) Can you please provide some details on the issue you try to fix? Can you please rebase this branch onto master? We do not allow merge commits (unless fast forward).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:148,performance,error,error,148,"How is it accidental? It was totally on purpose, solving other occurrences of this exact same issue. I suppose, because i have not seen any issue / error message / backtrace, so all I can do is guess and assume :-) Can you please provide some details on the issue you try to fix? Can you please rebase this branch onto master? We do not allow merge commits (unless fast forward).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:10,safety,accid,accidental,10,"How is it accidental? It was totally on purpose, solving other occurrences of this exact same issue. I suppose, because i have not seen any issue / error message / backtrace, so all I can do is guess and assume :-) Can you please provide some details on the issue you try to fix? Can you please rebase this branch onto master? We do not allow merge commits (unless fast forward).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:148,safety,error,error,148,"How is it accidental? It was totally on purpose, solving other occurrences of this exact same issue. I suppose, because i have not seen any issue / error message / backtrace, so all I can do is guess and assume :-) Can you please provide some details on the issue you try to fix? Can you please rebase this branch onto master? We do not allow merge commits (unless fast forward).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:148,usability,error,error,148,"How is it accidental? It was totally on purpose, solving other occurrences of this exact same issue. I suppose, because i have not seen any issue / error message / backtrace, so all I can do is guess and assume :-) Can you please provide some details on the issue you try to fix? Can you please rebase this branch onto master? We do not allow merge commits (unless fast forward).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:13,availability,mask,maskray,13,Iiuc https://maskray.me/blog/2021-05-16-elf-interposition-and-bsymbolic argues that -Bsymbolic should be fine for libCling and is superior to no symbolic interposition.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:15,availability,mask,maskray,15,"> Iiuc https://maskray.me/blog/2021-05-16-elf-interposition-and-bsymbolic argues that -Bsymbolic should be fine for libCling and is superior to no symbolic interposition. Thanks, Axel, for the link to this interesting article. By ""accidental"", I meant that, if we assume the gcc manpage description of the flag correct and complete (*), then we rely on a side-effect of compiler optimizations. The article you quoted mention another optimization that should make it work for all calls to functions defined in the same compilation unit (**), while I wrote it will be limited to inline functions. The -Bsymbolic, which is an option of the linker, instead of compiler, addresses directly what we target. The ld manpage description reads as ""When creating a shared library, bind references to global symbols to the definition within the shared library, if any. "". Philippe. (*) ""with -fno-semantic-interposition the compiler assumes that if interposition happens for functions the overwriting function will have precisely the same semantics (and side effects). "". (**) Although the statement on LD_PRELOAD confuses me.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:1078,availability,state,statement,1078,"> Iiuc https://maskray.me/blog/2021-05-16-elf-interposition-and-bsymbolic argues that -Bsymbolic should be fine for libCling and is superior to no symbolic interposition. Thanks, Axel, for the link to this interesting article. By ""accidental"", I meant that, if we assume the gcc manpage description of the flag correct and complete (*), then we rely on a side-effect of compiler optimizations. The article you quoted mention another optimization that should make it work for all calls to functions defined in the same compilation unit (**), while I wrote it will be limited to inline functions. The -Bsymbolic, which is an option of the linker, instead of compiler, addresses directly what we target. The ld manpage description reads as ""When creating a shared library, bind references to global symbols to the definition within the shared library, if any. "". Philippe. (*) ""with -fno-semantic-interposition the compiler assumes that if interposition happens for functions the overwriting function will have precisely the same semantics (and side effects). "". (**) Although the statement on LD_PRELOAD confuses me.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:379,energy efficiency,optim,optimizations,379,"> Iiuc https://maskray.me/blog/2021-05-16-elf-interposition-and-bsymbolic argues that -Bsymbolic should be fine for libCling and is superior to no symbolic interposition. Thanks, Axel, for the link to this interesting article. By ""accidental"", I meant that, if we assume the gcc manpage description of the flag correct and complete (*), then we rely on a side-effect of compiler optimizations. The article you quoted mention another optimization that should make it work for all calls to functions defined in the same compilation unit (**), while I wrote it will be limited to inline functions. The -Bsymbolic, which is an option of the linker, instead of compiler, addresses directly what we target. The ld manpage description reads as ""When creating a shared library, bind references to global symbols to the definition within the shared library, if any. "". Philippe. (*) ""with -fno-semantic-interposition the compiler assumes that if interposition happens for functions the overwriting function will have precisely the same semantics (and side effects). "". (**) Although the statement on LD_PRELOAD confuses me.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:433,energy efficiency,optim,optimization,433,"> Iiuc https://maskray.me/blog/2021-05-16-elf-interposition-and-bsymbolic argues that -Bsymbolic should be fine for libCling and is superior to no symbolic interposition. Thanks, Axel, for the link to this interesting article. By ""accidental"", I meant that, if we assume the gcc manpage description of the flag correct and complete (*), then we rely on a side-effect of compiler optimizations. The article you quoted mention another optimization that should make it work for all calls to functions defined in the same compilation unit (**), while I wrote it will be limited to inline functions. The -Bsymbolic, which is an option of the linker, instead of compiler, addresses directly what we target. The ld manpage description reads as ""When creating a shared library, bind references to global symbols to the definition within the shared library, if any. "". Philippe. (*) ""with -fno-semantic-interposition the compiler assumes that if interposition happens for functions the overwriting function will have precisely the same semantics (and side effects). "". (**) Although the statement on LD_PRELOAD confuses me.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:1078,integrability,state,statement,1078,"> Iiuc https://maskray.me/blog/2021-05-16-elf-interposition-and-bsymbolic argues that -Bsymbolic should be fine for libCling and is superior to no symbolic interposition. Thanks, Axel, for the link to this interesting article. By ""accidental"", I meant that, if we assume the gcc manpage description of the flag correct and complete (*), then we rely on a side-effect of compiler optimizations. The article you quoted mention another optimization that should make it work for all calls to functions defined in the same compilation unit (**), while I wrote it will be limited to inline functions. The -Bsymbolic, which is an option of the linker, instead of compiler, addresses directly what we target. The ld manpage description reads as ""When creating a shared library, bind references to global symbols to the definition within the shared library, if any. "". Philippe. (*) ""with -fno-semantic-interposition the compiler assumes that if interposition happens for functions the overwriting function will have precisely the same semantics (and side effects). "". (**) Although the statement on LD_PRELOAD confuses me.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:754,interoperability,share,shared,754,"> Iiuc https://maskray.me/blog/2021-05-16-elf-interposition-and-bsymbolic argues that -Bsymbolic should be fine for libCling and is superior to no symbolic interposition. Thanks, Axel, for the link to this interesting article. By ""accidental"", I meant that, if we assume the gcc manpage description of the flag correct and complete (*), then we rely on a side-effect of compiler optimizations. The article you quoted mention another optimization that should make it work for all calls to functions defined in the same compilation unit (**), while I wrote it will be limited to inline functions. The -Bsymbolic, which is an option of the linker, instead of compiler, addresses directly what we target. The ld manpage description reads as ""When creating a shared library, bind references to global symbols to the definition within the shared library, if any. "". Philippe. (*) ""with -fno-semantic-interposition the compiler assumes that if interposition happens for functions the overwriting function will have precisely the same semantics (and side effects). "". (**) Although the statement on LD_PRELOAD confuses me.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:770,interoperability,bind,bind,770,"> Iiuc https://maskray.me/blog/2021-05-16-elf-interposition-and-bsymbolic argues that -Bsymbolic should be fine for libCling and is superior to no symbolic interposition. Thanks, Axel, for the link to this interesting article. By ""accidental"", I meant that, if we assume the gcc manpage description of the flag correct and complete (*), then we rely on a side-effect of compiler optimizations. The article you quoted mention another optimization that should make it work for all calls to functions defined in the same compilation unit (**), while I wrote it will be limited to inline functions. The -Bsymbolic, which is an option of the linker, instead of compiler, addresses directly what we target. The ld manpage description reads as ""When creating a shared library, bind references to global symbols to the definition within the shared library, if any. "". Philippe. (*) ""with -fno-semantic-interposition the compiler assumes that if interposition happens for functions the overwriting function will have precisely the same semantics (and side effects). "". (**) Although the statement on LD_PRELOAD confuses me.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:833,interoperability,share,shared,833,"> Iiuc https://maskray.me/blog/2021-05-16-elf-interposition-and-bsymbolic argues that -Bsymbolic should be fine for libCling and is superior to no symbolic interposition. Thanks, Axel, for the link to this interesting article. By ""accidental"", I meant that, if we assume the gcc manpage description of the flag correct and complete (*), then we rely on a side-effect of compiler optimizations. The article you quoted mention another optimization that should make it work for all calls to functions defined in the same compilation unit (**), while I wrote it will be limited to inline functions. The -Bsymbolic, which is an option of the linker, instead of compiler, addresses directly what we target. The ld manpage description reads as ""When creating a shared library, bind references to global symbols to the definition within the shared library, if any. "". Philippe. (*) ""with -fno-semantic-interposition the compiler assumes that if interposition happens for functions the overwriting function will have precisely the same semantics (and side effects). "". (**) Although the statement on LD_PRELOAD confuses me.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:885,interoperability,semant,semantic-interposition,885,"> Iiuc https://maskray.me/blog/2021-05-16-elf-interposition-and-bsymbolic argues that -Bsymbolic should be fine for libCling and is superior to no symbolic interposition. Thanks, Axel, for the link to this interesting article. By ""accidental"", I meant that, if we assume the gcc manpage description of the flag correct and complete (*), then we rely on a side-effect of compiler optimizations. The article you quoted mention another optimization that should make it work for all calls to functions defined in the same compilation unit (**), while I wrote it will be limited to inline functions. The -Bsymbolic, which is an option of the linker, instead of compiler, addresses directly what we target. The ld manpage description reads as ""When creating a shared library, bind references to global symbols to the definition within the shared library, if any. "". Philippe. (*) ""with -fno-semantic-interposition the compiler assumes that if interposition happens for functions the overwriting function will have precisely the same semantics (and side effects). "". (**) Although the statement on LD_PRELOAD confuses me.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:1027,interoperability,semant,semantics,1027,"> Iiuc https://maskray.me/blog/2021-05-16-elf-interposition-and-bsymbolic argues that -Bsymbolic should be fine for libCling and is superior to no symbolic interposition. Thanks, Axel, for the link to this interesting article. By ""accidental"", I meant that, if we assume the gcc manpage description of the flag correct and complete (*), then we rely on a side-effect of compiler optimizations. The article you quoted mention another optimization that should make it work for all calls to functions defined in the same compilation unit (**), while I wrote it will be limited to inline functions. The -Bsymbolic, which is an option of the linker, instead of compiler, addresses directly what we target. The ld manpage description reads as ""When creating a shared library, bind references to global symbols to the definition within the shared library, if any. "". Philippe. (*) ""with -fno-semantic-interposition the compiler assumes that if interposition happens for functions the overwriting function will have precisely the same semantics (and side effects). "". (**) Although the statement on LD_PRELOAD confuses me.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:770,modifiability,bind,bind,770,"> Iiuc https://maskray.me/blog/2021-05-16-elf-interposition-and-bsymbolic argues that -Bsymbolic should be fine for libCling and is superior to no symbolic interposition. Thanks, Axel, for the link to this interesting article. By ""accidental"", I meant that, if we assume the gcc manpage description of the flag correct and complete (*), then we rely on a side-effect of compiler optimizations. The article you quoted mention another optimization that should make it work for all calls to functions defined in the same compilation unit (**), while I wrote it will be limited to inline functions. The -Bsymbolic, which is an option of the linker, instead of compiler, addresses directly what we target. The ld manpage description reads as ""When creating a shared library, bind references to global symbols to the definition within the shared library, if any. "". Philippe. (*) ""with -fno-semantic-interposition the compiler assumes that if interposition happens for functions the overwriting function will have precisely the same semantics (and side effects). "". (**) Although the statement on LD_PRELOAD confuses me.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:379,performance,optimiz,optimizations,379,"> Iiuc https://maskray.me/blog/2021-05-16-elf-interposition-and-bsymbolic argues that -Bsymbolic should be fine for libCling and is superior to no symbolic interposition. Thanks, Axel, for the link to this interesting article. By ""accidental"", I meant that, if we assume the gcc manpage description of the flag correct and complete (*), then we rely on a side-effect of compiler optimizations. The article you quoted mention another optimization that should make it work for all calls to functions defined in the same compilation unit (**), while I wrote it will be limited to inline functions. The -Bsymbolic, which is an option of the linker, instead of compiler, addresses directly what we target. The ld manpage description reads as ""When creating a shared library, bind references to global symbols to the definition within the shared library, if any. "". Philippe. (*) ""with -fno-semantic-interposition the compiler assumes that if interposition happens for functions the overwriting function will have precisely the same semantics (and side effects). "". (**) Although the statement on LD_PRELOAD confuses me.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:433,performance,optimiz,optimization,433,"> Iiuc https://maskray.me/blog/2021-05-16-elf-interposition-and-bsymbolic argues that -Bsymbolic should be fine for libCling and is superior to no symbolic interposition. Thanks, Axel, for the link to this interesting article. By ""accidental"", I meant that, if we assume the gcc manpage description of the flag correct and complete (*), then we rely on a side-effect of compiler optimizations. The article you quoted mention another optimization that should make it work for all calls to functions defined in the same compilation unit (**), while I wrote it will be limited to inline functions. The -Bsymbolic, which is an option of the linker, instead of compiler, addresses directly what we target. The ld manpage description reads as ""When creating a shared library, bind references to global symbols to the definition within the shared library, if any. "". Philippe. (*) ""with -fno-semantic-interposition the compiler assumes that if interposition happens for functions the overwriting function will have precisely the same semantics (and side effects). "". (**) Although the statement on LD_PRELOAD confuses me.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:231,safety,accid,accidental,231,"> Iiuc https://maskray.me/blog/2021-05-16-elf-interposition-and-bsymbolic argues that -Bsymbolic should be fine for libCling and is superior to no symbolic interposition. Thanks, Axel, for the link to this interesting article. By ""accidental"", I meant that, if we assume the gcc manpage description of the flag correct and complete (*), then we rely on a side-effect of compiler optimizations. The article you quoted mention another optimization that should make it work for all calls to functions defined in the same compilation unit (**), while I wrote it will be limited to inline functions. The -Bsymbolic, which is an option of the linker, instead of compiler, addresses directly what we target. The ld manpage description reads as ""When creating a shared library, bind references to global symbols to the definition within the shared library, if any. "". Philippe. (*) ""with -fno-semantic-interposition the compiler assumes that if interposition happens for functions the overwriting function will have precisely the same semantics (and side effects). "". (**) Although the statement on LD_PRELOAD confuses me.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:323,safety,compl,complete,323,"> Iiuc https://maskray.me/blog/2021-05-16-elf-interposition-and-bsymbolic argues that -Bsymbolic should be fine for libCling and is superior to no symbolic interposition. Thanks, Axel, for the link to this interesting article. By ""accidental"", I meant that, if we assume the gcc manpage description of the flag correct and complete (*), then we rely on a side-effect of compiler optimizations. The article you quoted mention another optimization that should make it work for all calls to functions defined in the same compilation unit (**), while I wrote it will be limited to inline functions. The -Bsymbolic, which is an option of the linker, instead of compiler, addresses directly what we target. The ld manpage description reads as ""When creating a shared library, bind references to global symbols to the definition within the shared library, if any. "". Philippe. (*) ""with -fno-semantic-interposition the compiler assumes that if interposition happens for functions the overwriting function will have precisely the same semantics (and side effects). "". (**) Although the statement on LD_PRELOAD confuses me.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:323,security,compl,complete,323,"> Iiuc https://maskray.me/blog/2021-05-16-elf-interposition-and-bsymbolic argues that -Bsymbolic should be fine for libCling and is superior to no symbolic interposition. Thanks, Axel, for the link to this interesting article. By ""accidental"", I meant that, if we assume the gcc manpage description of the flag correct and complete (*), then we rely on a side-effect of compiler optimizations. The article you quoted mention another optimization that should make it work for all calls to functions defined in the same compilation unit (**), while I wrote it will be limited to inline functions. The -Bsymbolic, which is an option of the linker, instead of compiler, addresses directly what we target. The ld manpage description reads as ""When creating a shared library, bind references to global symbols to the definition within the shared library, if any. "". Philippe. (*) ""with -fno-semantic-interposition the compiler assumes that if interposition happens for functions the overwriting function will have precisely the same semantics (and side effects). "". (**) Although the statement on LD_PRELOAD confuses me.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:530,testability,unit,unit,530,"> Iiuc https://maskray.me/blog/2021-05-16-elf-interposition-and-bsymbolic argues that -Bsymbolic should be fine for libCling and is superior to no symbolic interposition. Thanks, Axel, for the link to this interesting article. By ""accidental"", I meant that, if we assume the gcc manpage description of the flag correct and complete (*), then we rely on a side-effect of compiler optimizations. The article you quoted mention another optimization that should make it work for all calls to functions defined in the same compilation unit (**), while I wrote it will be limited to inline functions. The -Bsymbolic, which is an option of the linker, instead of compiler, addresses directly what we target. The ld manpage description reads as ""When creating a shared library, bind references to global symbols to the definition within the shared library, if any. "". Philippe. (*) ""with -fno-semantic-interposition the compiler assumes that if interposition happens for functions the overwriting function will have precisely the same semantics (and side effects). "". (**) Although the statement on LD_PRELOAD confuses me.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:113,deployability,build,build,113,@grasph please fork also [roottest](https://github.com/root-project/roottest) to prevent timeouts on the Windows build nodes. Thanks,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:89,performance,time,timeouts,89,@grasph please fork also [roottest](https://github.com/root-project/roottest) to prevent timeouts on the Windows build nodes. Thanks,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:81,safety,prevent,prevent,81,@grasph please fork also [roottest](https://github.com/root-project/roottest) to prevent timeouts on the Windows build nodes. Thanks,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:89,safety,timeout,timeouts,89,@grasph please fork also [roottest](https://github.com/root-project/roottest) to prevent timeouts on the Windows build nodes. Thanks,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:81,security,preven,prevent,81,@grasph please fork also [roottest](https://github.com/root-project/roottest) to prevent timeouts on the Windows build nodes. Thanks,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:115,deployability,build,build,115,> @grasph please fork also [roottest](https://github.com/root-project/roottest) to prevent timeouts on the Windows build nodes. Thanks. Done. Do I need to create a branch with the same name ?,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:91,performance,time,timeouts,91,> @grasph please fork also [roottest](https://github.com/root-project/roottest) to prevent timeouts on the Windows build nodes. Thanks. Done. Do I need to create a branch with the same name ?,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:83,safety,prevent,prevent,83,> @grasph please fork also [roottest](https://github.com/root-project/roottest) to prevent timeouts on the Windows build nodes. Thanks. Done. Do I need to create a branch with the same name ?,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:91,safety,timeout,timeouts,91,> @grasph please fork also [roottest](https://github.com/root-project/roottest) to prevent timeouts on the Windows build nodes. Thanks. Done. Do I need to create a branch with the same name ?,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:83,security,preven,prevent,83,> @grasph please fork also [roottest](https://github.com/root-project/roottest) to prevent timeouts on the Windows build nodes. Thanks. Done. Do I need to create a branch with the same name ?,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:117,deployability,build,build,117,"> > @grasph please fork also [roottest](https://github.com/root-project/roottest) to prevent timeouts on the Windows build nodes. Thanks. > . > Done. Do I need to create a branch with the same name ? Thanks. And no, simply forking is good enough",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:93,performance,time,timeouts,93,"> > @grasph please fork also [roottest](https://github.com/root-project/roottest) to prevent timeouts on the Windows build nodes. Thanks. > . > Done. Do I need to create a branch with the same name ? Thanks. And no, simply forking is good enough",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:85,safety,prevent,prevent,85,"> > @grasph please fork also [roottest](https://github.com/root-project/roottest) to prevent timeouts on the Windows build nodes. Thanks. > . > Done. Do I need to create a branch with the same name ? Thanks. And no, simply forking is good enough",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:93,safety,timeout,timeouts,93,"> > @grasph please fork also [roottest](https://github.com/root-project/roottest) to prevent timeouts on the Windows build nodes. Thanks. > . > Done. Do I need to create a branch with the same name ? Thanks. And no, simply forking is good enough",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:85,security,preven,prevent,85,"> > @grasph please fork also [roottest](https://github.com/root-project/roottest) to prevent timeouts on the Windows build nodes. Thanks. > . > Done. Do I need to create a branch with the same name ? Thanks. And no, simply forking is good enough",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:216,testability,simpl,simply,216,"> > @grasph please fork also [roottest](https://github.com/root-project/roottest) to prevent timeouts on the Windows build nodes. Thanks. > . > Done. Do I need to create a branch with the same name ? Thanks. And no, simply forking is good enough",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:216,usability,simpl,simply,216,"> > @grasph please fork also [roottest](https://github.com/root-project/roottest) to prevent timeouts on the Windows build nodes. Thanks. > . > Done. Do I need to create a branch with the same name ? Thanks. And no, simply forking is good enough",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:220,usability,close,close,220,@bellenot I'm not sure why the PR is blocked. Is it an issue with deepcode-ci-bot or is github that did not get that I made the changed @Axel-Naumann requested? I'm leaving for holidays on Wednesday. It would be good to close the PR by then.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:11,deployability,build,build,11,@phsft-bot build!,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:8,deployability,build,build,8,Jenkins build failed to merge with new master. Rebased done in commit 387418 and that should fix the issue.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:14,deployability,fail,failed,14,Jenkins build failed to merge with new master. Rebased done in commit 387418 and that should fix the issue.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:14,reliability,fail,failed,14,Jenkins build failed to merge with new master. Rebased done in commit 387418 and that should fix the issue.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:307,safety,review,reviewer,307,> @bellenot I'm not sure why the PR is blocked. Is it an issue with deepcode-ci-bot or is github that did not get that I made the changed @Axel-Naumann requested? I'm leaving for holidays on Wednesday. It would be good to close the PR by then. It is blocked because it needs an approval from (at least) one reviewer,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:307,testability,review,reviewer,307,> @bellenot I'm not sure why the PR is blocked. Is it an issue with deepcode-ci-bot or is github that did not get that I made the changed @Axel-Naumann requested? I'm leaving for holidays on Wednesday. It would be good to close the PR by then. It is blocked because it needs an approval from (at least) one reviewer,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:222,usability,close,close,222,> @bellenot I'm not sure why the PR is blocked. Is it an issue with deepcode-ci-bot or is github that did not get that I made the changed @Axel-Naumann requested? I'm leaving for holidays on Wednesday. It would be good to close the PR by then. It is blocked because it needs an approval from (at least) one reviewer,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:11,deployability,build,build,11,@phsft-bot build,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:276,deployability,releas,release,276,"FYI: You might want to check `-Bsymbolic-non-weak-functions` in the future as well. It keeps the weird ELF interposition behavior for weak symbols, which can be vital for things like emulated TLS (see last comment in the review). Initial support comes with the current LLD 13 release: https://reviews.llvm.org/D102570#2915489",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:261,energy efficiency,current,current,261,"FYI: You might want to check `-Bsymbolic-non-weak-functions` in the future as well. It keeps the weird ELF interposition behavior for weak symbols, which can be vital for things like emulated TLS (see last comment in the review). Initial support comes with the current LLD 13 release: https://reviews.llvm.org/D102570#2915489",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:221,safety,review,review,221,"FYI: You might want to check `-Bsymbolic-non-weak-functions` in the future as well. It keeps the weird ELF interposition behavior for weak symbols, which can be vital for things like emulated TLS (see last comment in the review). Initial support comes with the current LLD 13 release: https://reviews.llvm.org/D102570#2915489",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:293,safety,review,reviews,293,"FYI: You might want to check `-Bsymbolic-non-weak-functions` in the future as well. It keeps the weird ELF interposition behavior for weak symbols, which can be vital for things like emulated TLS (see last comment in the review). Initial support comes with the current LLD 13 release: https://reviews.llvm.org/D102570#2915489",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:183,testability,emul,emulated,183,"FYI: You might want to check `-Bsymbolic-non-weak-functions` in the future as well. It keeps the weird ELF interposition behavior for weak symbols, which can be vital for things like emulated TLS (see last comment in the review). Initial support comes with the current LLD 13 release: https://reviews.llvm.org/D102570#2915489",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:221,testability,review,review,221,"FYI: You might want to check `-Bsymbolic-non-weak-functions` in the future as well. It keeps the weird ELF interposition behavior for weak symbols, which can be vital for things like emulated TLS (see last comment in the review). Initial support comes with the current LLD 13 release: https://reviews.llvm.org/D102570#2915489",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:293,testability,review,reviews,293,"FYI: You might want to check `-Bsymbolic-non-weak-functions` in the future as well. It keeps the weird ELF interposition behavior for weak symbols, which can be vital for things like emulated TLS (see last comment in the review). Initial support comes with the current LLD 13 release: https://reviews.llvm.org/D102570#2915489",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:121,usability,behavi,behavior,121,"FYI: You might want to check `-Bsymbolic-non-weak-functions` in the future as well. It keeps the weird ELF interposition behavior for weak symbols, which can be vital for things like emulated TLS (see last comment in the review). Initial support comes with the current LLD 13 release: https://reviews.llvm.org/D102570#2915489",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:238,usability,support,support,238,"FYI: You might want to check `-Bsymbolic-non-weak-functions` in the future as well. It keeps the weird ELF interposition behavior for weak symbols, which can be vital for things like emulated TLS (see last comment in the review). Initial support comes with the current LLD 13 release: https://reviews.llvm.org/D102570#2915489",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:11,deployability,build,build,11,@phsft-bot build,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:97,availability,Error,Error,97,macOS 12 messes up some system lib's internal (?) llvm symbols with those of cling: `CommandLine Error: Option 'h' registered more than once!`. Does anyone know why the default namespacing on macOS might fail here? I bet we're missing a similar flag now also for macOS :-/,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:204,deployability,fail,fail,204,macOS 12 messes up some system lib's internal (?) llvm symbols with those of cling: `CommandLine Error: Option 'h' registered more than once!`. Does anyone know why the default namespacing on macOS might fail here? I bet we're missing a similar flag now also for macOS :-/,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:97,performance,Error,Error,97,macOS 12 messes up some system lib's internal (?) llvm symbols with those of cling: `CommandLine Error: Option 'h' registered more than once!`. Does anyone know why the default namespacing on macOS might fail here? I bet we're missing a similar flag now also for macOS :-/,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:144,reliability,Doe,Does,144,macOS 12 messes up some system lib's internal (?) llvm symbols with those of cling: `CommandLine Error: Option 'h' registered more than once!`. Does anyone know why the default namespacing on macOS might fail here? I bet we're missing a similar flag now also for macOS :-/,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:204,reliability,fail,fail,204,macOS 12 messes up some system lib's internal (?) llvm symbols with those of cling: `CommandLine Error: Option 'h' registered more than once!`. Does anyone know why the default namespacing on macOS might fail here? I bet we're missing a similar flag now also for macOS :-/,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:97,safety,Error,Error,97,macOS 12 messes up some system lib's internal (?) llvm symbols with those of cling: `CommandLine Error: Option 'h' registered more than once!`. Does anyone know why the default namespacing on macOS might fail here? I bet we're missing a similar flag now also for macOS :-/,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:85,usability,Command,CommandLine,85,macOS 12 messes up some system lib's internal (?) llvm symbols with those of cling: `CommandLine Error: Option 'h' registered more than once!`. Does anyone know why the default namespacing on macOS might fail here? I bet we're missing a similar flag now also for macOS :-/,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:97,usability,Error,Error,97,macOS 12 messes up some system lib's internal (?) llvm symbols with those of cling: `CommandLine Error: Option 'h' registered more than once!`. Does anyone know why the default namespacing on macOS might fail here? I bet we're missing a similar flag now also for macOS :-/,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:0,availability,Failur,Failures,0,Failures are unrelated. Thanks for the contribution!,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:0,deployability,Fail,Failures,0,Failures are unrelated. Thanks for the contribution!,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:0,performance,Failur,Failures,0,Failures are unrelated. Thanks for the contribution!,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:0,reliability,Fail,Failures,0,Failures are unrelated. Thanks for the contribution!,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:18,deployability,releas,release,18,Which future ROOT release will this (likely) be included in?,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:20,deployability,releas,release,20,"> Which future ROOT release will this (likely) be included in? Hello @oschulz , I let Axel answer to your release question. I expect you are interested by the interfacing with Julia. This is already working starting from version 6.24/02 (explained in a previous comment of the PR discussion). This pull request is only consolidating a code correction which was included in the release between 6.24/00 and 6.24/02). Philippe.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:106,deployability,releas,release,106,"> Which future ROOT release will this (likely) be included in? Hello @oschulz , I let Axel answer to your release question. I expect you are interested by the interfacing with Julia. This is already working starting from version 6.24/02 (explained in a previous comment of the PR discussion). This pull request is only consolidating a code correction which was included in the release between 6.24/00 and 6.24/02). Philippe.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:221,deployability,version,version,221,"> Which future ROOT release will this (likely) be included in? Hello @oschulz , I let Axel answer to your release question. I expect you are interested by the interfacing with Julia. This is already working starting from version 6.24/02 (explained in a previous comment of the PR discussion). This pull request is only consolidating a code correction which was included in the release between 6.24/00 and 6.24/02). Philippe.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:377,deployability,releas,release,377,"> Which future ROOT release will this (likely) be included in? Hello @oschulz , I let Axel answer to your release question. I expect you are interested by the interfacing with Julia. This is already working starting from version 6.24/02 (explained in a previous comment of the PR discussion). This pull request is only consolidating a code correction which was included in the release between 6.24/00 and 6.24/02). Philippe.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:159,integrability,interfac,interfacing,159,"> Which future ROOT release will this (likely) be included in? Hello @oschulz , I let Axel answer to your release question. I expect you are interested by the interfacing with Julia. This is already working starting from version 6.24/02 (explained in a previous comment of the PR discussion). This pull request is only consolidating a code correction which was included in the release between 6.24/00 and 6.24/02). Philippe.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:221,integrability,version,version,221,"> Which future ROOT release will this (likely) be included in? Hello @oschulz , I let Axel answer to your release question. I expect you are interested by the interfacing with Julia. This is already working starting from version 6.24/02 (explained in a previous comment of the PR discussion). This pull request is only consolidating a code correction which was included in the release between 6.24/00 and 6.24/02). Philippe.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:159,interoperability,interfac,interfacing,159,"> Which future ROOT release will this (likely) be included in? Hello @oschulz , I let Axel answer to your release question. I expect you are interested by the interfacing with Julia. This is already working starting from version 6.24/02 (explained in a previous comment of the PR discussion). This pull request is only consolidating a code correction which was included in the release between 6.24/00 and 6.24/02). Philippe.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:159,modifiability,interfac,interfacing,159,"> Which future ROOT release will this (likely) be included in? Hello @oschulz , I let Axel answer to your release question. I expect you are interested by the interfacing with Julia. This is already working starting from version 6.24/02 (explained in a previous comment of the PR discussion). This pull request is only consolidating a code correction which was included in the release between 6.24/00 and 6.24/02). Philippe.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:221,modifiability,version,version,221,"> Which future ROOT release will this (likely) be included in? Hello @oschulz , I let Axel answer to your release question. I expect you are interested by the interfacing with Julia. This is already working starting from version 6.24/02 (explained in a previous comment of the PR discussion). This pull request is only consolidating a code correction which was included in the release between 6.24/00 and 6.24/02). Philippe.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:37,integrability,interfac,interfacing,37,> I expect you are interested by the interfacing with Julia. Yes - thanks @grasph !,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:37,interoperability,interfac,interfacing,37,> I expect you are interested by the interfacing with Julia. Yes - thanks @grasph !,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8717:37,modifiability,interfac,interfacing,37,> I expect you are interested by the interfacing with Julia. Yes - thanks @grasph !,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8717
https://github.com/root-project/root/pull/8718:9,testability,understand,understand,9,I do not understand the purpose of this tutorial. There is no description.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8718
https://github.com/root-project/root/pull/8718:11,testability,understand,understand,11,> I do not understand the purpose of this tutorial. There is no description. I will redo this as we discussed.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8718
https://github.com/root-project/root/pull/8719:15,deployability,patch,patch,15,"Thanks for the patch, Advait! My thoughts: If the function is added to ROOT as is, I'd implement it in the header and simply `return a==b`. The C/C++ cast from `bool` to `int` is effectively a Kronecker Delta. That said, I'm not sure if the function as such does in practice what users expect. Floating point values are hardly ever exactly equal, but usually one tests for them to be equal within +/ epsilon. I'll assign to @lmoneta.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8719
https://github.com/root-project/root/pull/8719:258,reliability,doe,does,258,"Thanks for the patch, Advait! My thoughts: If the function is added to ROOT as is, I'd implement it in the header and simply `return a==b`. The C/C++ cast from `bool` to `int` is effectively a Kronecker Delta. That said, I'm not sure if the function as such does in practice what users expect. Floating point values are hardly ever exactly equal, but usually one tests for them to be equal within +/ epsilon. I'll assign to @lmoneta.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8719
https://github.com/root-project/root/pull/8719:266,reliability,pra,practice,266,"Thanks for the patch, Advait! My thoughts: If the function is added to ROOT as is, I'd implement it in the header and simply `return a==b`. The C/C++ cast from `bool` to `int` is effectively a Kronecker Delta. That said, I'm not sure if the function as such does in practice what users expect. Floating point values are hardly ever exactly equal, but usually one tests for them to be equal within +/ epsilon. I'll assign to @lmoneta.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8719
https://github.com/root-project/root/pull/8719:15,safety,patch,patch,15,"Thanks for the patch, Advait! My thoughts: If the function is added to ROOT as is, I'd implement it in the header and simply `return a==b`. The C/C++ cast from `bool` to `int` is effectively a Kronecker Delta. That said, I'm not sure if the function as such does in practice what users expect. Floating point values are hardly ever exactly equal, but usually one tests for them to be equal within +/ epsilon. I'll assign to @lmoneta.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8719
https://github.com/root-project/root/pull/8719:363,safety,test,tests,363,"Thanks for the patch, Advait! My thoughts: If the function is added to ROOT as is, I'd implement it in the header and simply `return a==b`. The C/C++ cast from `bool` to `int` is effectively a Kronecker Delta. That said, I'm not sure if the function as such does in practice what users expect. Floating point values are hardly ever exactly equal, but usually one tests for them to be equal within +/ epsilon. I'll assign to @lmoneta.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8719
https://github.com/root-project/root/pull/8719:15,security,patch,patch,15,"Thanks for the patch, Advait! My thoughts: If the function is added to ROOT as is, I'd implement it in the header and simply `return a==b`. The C/C++ cast from `bool` to `int` is effectively a Kronecker Delta. That said, I'm not sure if the function as such does in practice what users expect. Floating point values are hardly ever exactly equal, but usually one tests for them to be equal within +/ epsilon. I'll assign to @lmoneta.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8719
https://github.com/root-project/root/pull/8719:118,testability,simpl,simply,118,"Thanks for the patch, Advait! My thoughts: If the function is added to ROOT as is, I'd implement it in the header and simply `return a==b`. The C/C++ cast from `bool` to `int` is effectively a Kronecker Delta. That said, I'm not sure if the function as such does in practice what users expect. Floating point values are hardly ever exactly equal, but usually one tests for them to be equal within +/ epsilon. I'll assign to @lmoneta.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8719
https://github.com/root-project/root/pull/8719:363,testability,test,tests,363,"Thanks for the patch, Advait! My thoughts: If the function is added to ROOT as is, I'd implement it in the header and simply `return a==b`. The C/C++ cast from `bool` to `int` is effectively a Kronecker Delta. That said, I'm not sure if the function as such does in practice what users expect. Floating point values are hardly ever exactly equal, but usually one tests for them to be equal within +/ epsilon. I'll assign to @lmoneta.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8719
https://github.com/root-project/root/pull/8719:118,usability,simpl,simply,118,"Thanks for the patch, Advait! My thoughts: If the function is added to ROOT as is, I'd implement it in the header and simply `return a==b`. The C/C++ cast from `bool` to `int` is effectively a Kronecker Delta. That said, I'm not sure if the function as such does in practice what users expect. Floating point values are hardly ever exactly equal, but usually one tests for them to be equal within +/ epsilon. I'll assign to @lmoneta.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8719
https://github.com/root-project/root/pull/8719:179,usability,effectiv,effectively,179,"Thanks for the patch, Advait! My thoughts: If the function is added to ROOT as is, I'd implement it in the header and simply `return a==b`. The C/C++ cast from `bool` to `int` is effectively a Kronecker Delta. That said, I'm not sure if the function as such does in practice what users expect. Floating point values are hardly ever exactly equal, but usually one tests for them to be equal within +/ epsilon. I'll assign to @lmoneta.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8719
https://github.com/root-project/root/pull/8719:280,usability,user,users,280,"Thanks for the patch, Advait! My thoughts: If the function is added to ROOT as is, I'd implement it in the header and simply `return a==b`. The C/C++ cast from `bool` to `int` is effectively a Kronecker Delta. That said, I'm not sure if the function as such does in practice what users expect. Floating point values are hardly ever exactly equal, but usually one tests for them to be equal within +/ epsilon. I'll assign to @lmoneta.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8719
https://github.com/root-project/root/pull/8719:17,deployability,patch,patch,17,"> Thanks for the patch, Advait! > . > My thoughts: If the function is added to ROOT as is, I'd implement it in the header and simply `return a==b`. The C/C++ cast from `bool` to `int` is effectively a Kronecker Delta. > . > That said, I'm not sure if the function as such does in practice what users expect. Floating point values are hardly ever exactly equal, but usually one tests for them to be equal within +/ epsilon. > . > I'll assign to @lmoneta. Ahh yes , I see what you mean.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8719
https://github.com/root-project/root/pull/8719:272,reliability,doe,does,272,"> Thanks for the patch, Advait! > . > My thoughts: If the function is added to ROOT as is, I'd implement it in the header and simply `return a==b`. The C/C++ cast from `bool` to `int` is effectively a Kronecker Delta. > . > That said, I'm not sure if the function as such does in practice what users expect. Floating point values are hardly ever exactly equal, but usually one tests for them to be equal within +/ epsilon. > . > I'll assign to @lmoneta. Ahh yes , I see what you mean.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8719
https://github.com/root-project/root/pull/8719:280,reliability,pra,practice,280,"> Thanks for the patch, Advait! > . > My thoughts: If the function is added to ROOT as is, I'd implement it in the header and simply `return a==b`. The C/C++ cast from `bool` to `int` is effectively a Kronecker Delta. > . > That said, I'm not sure if the function as such does in practice what users expect. Floating point values are hardly ever exactly equal, but usually one tests for them to be equal within +/ epsilon. > . > I'll assign to @lmoneta. Ahh yes , I see what you mean.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8719
https://github.com/root-project/root/pull/8719:17,safety,patch,patch,17,"> Thanks for the patch, Advait! > . > My thoughts: If the function is added to ROOT as is, I'd implement it in the header and simply `return a==b`. The C/C++ cast from `bool` to `int` is effectively a Kronecker Delta. > . > That said, I'm not sure if the function as such does in practice what users expect. Floating point values are hardly ever exactly equal, but usually one tests for them to be equal within +/ epsilon. > . > I'll assign to @lmoneta. Ahh yes , I see what you mean.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8719
https://github.com/root-project/root/pull/8719:377,safety,test,tests,377,"> Thanks for the patch, Advait! > . > My thoughts: If the function is added to ROOT as is, I'd implement it in the header and simply `return a==b`. The C/C++ cast from `bool` to `int` is effectively a Kronecker Delta. > . > That said, I'm not sure if the function as such does in practice what users expect. Floating point values are hardly ever exactly equal, but usually one tests for them to be equal within +/ epsilon. > . > I'll assign to @lmoneta. Ahh yes , I see what you mean.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8719
https://github.com/root-project/root/pull/8719:17,security,patch,patch,17,"> Thanks for the patch, Advait! > . > My thoughts: If the function is added to ROOT as is, I'd implement it in the header and simply `return a==b`. The C/C++ cast from `bool` to `int` is effectively a Kronecker Delta. > . > That said, I'm not sure if the function as such does in practice what users expect. Floating point values are hardly ever exactly equal, but usually one tests for them to be equal within +/ epsilon. > . > I'll assign to @lmoneta. Ahh yes , I see what you mean.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8719
https://github.com/root-project/root/pull/8719:126,testability,simpl,simply,126,"> Thanks for the patch, Advait! > . > My thoughts: If the function is added to ROOT as is, I'd implement it in the header and simply `return a==b`. The C/C++ cast from `bool` to `int` is effectively a Kronecker Delta. > . > That said, I'm not sure if the function as such does in practice what users expect. Floating point values are hardly ever exactly equal, but usually one tests for them to be equal within +/ epsilon. > . > I'll assign to @lmoneta. Ahh yes , I see what you mean.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8719
https://github.com/root-project/root/pull/8719:377,testability,test,tests,377,"> Thanks for the patch, Advait! > . > My thoughts: If the function is added to ROOT as is, I'd implement it in the header and simply `return a==b`. The C/C++ cast from `bool` to `int` is effectively a Kronecker Delta. > . > That said, I'm not sure if the function as such does in practice what users expect. Floating point values are hardly ever exactly equal, but usually one tests for them to be equal within +/ epsilon. > . > I'll assign to @lmoneta. Ahh yes , I see what you mean.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8719
https://github.com/root-project/root/pull/8719:126,usability,simpl,simply,126,"> Thanks for the patch, Advait! > . > My thoughts: If the function is added to ROOT as is, I'd implement it in the header and simply `return a==b`. The C/C++ cast from `bool` to `int` is effectively a Kronecker Delta. > . > That said, I'm not sure if the function as such does in practice what users expect. Floating point values are hardly ever exactly equal, but usually one tests for them to be equal within +/ epsilon. > . > I'll assign to @lmoneta. Ahh yes , I see what you mean.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8719
https://github.com/root-project/root/pull/8719:187,usability,effectiv,effectively,187,"> Thanks for the patch, Advait! > . > My thoughts: If the function is added to ROOT as is, I'd implement it in the header and simply `return a==b`. The C/C++ cast from `bool` to `int` is effectively a Kronecker Delta. > . > That said, I'm not sure if the function as such does in practice what users expect. Floating point values are hardly ever exactly equal, but usually one tests for them to be equal within +/ epsilon. > . > I'll assign to @lmoneta. Ahh yes , I see what you mean.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8719
https://github.com/root-project/root/pull/8719:294,usability,user,users,294,"> Thanks for the patch, Advait! > . > My thoughts: If the function is added to ROOT as is, I'd implement it in the header and simply `return a==b`. The C/C++ cast from `bool` to `int` is effectively a Kronecker Delta. > . > That said, I'm not sure if the function as such does in practice what users expect. Floating point values are hardly ever exactly equal, but usually one tests for them to be equal within +/ epsilon. > . > I'll assign to @lmoneta. Ahh yes , I see what you mean.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8719
https://github.com/root-project/root/issues/8721:34,deployability,stack,stack,34,This is most likely a OS/Compiler stack size limit. You can either try to increase this limit or use something like:. ```. float *arr1 = new float[n];. ```,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8721
https://github.com/root-project/root/issues/8721:1019,deployability,stack,stack,1019,"Hi @pcanal,. yes, I can confirm, that working on the heap. ```. int testvec() {. 	TTree *t = new TTree(""tree"",""tree"");. 	const int n = 1000000;. 	unsigned int myN = n;. 	float *arr1 = new float[n];. 	float *arr2 = new float[n];. 	float *arr3 = new float[n];. 	float *arr4 = new float[n];. 	float *arr5 = new float[n];. 	float *arr6 = new float[n];. 	float *arr7 = new float[n];. 	float *arr8 = new float[n];. 	float *arr9 = new float[n];. 	float *arr10 = new float[n];. 	float *arr11 = new float[n];. 	. 	t->Branch(""n"",&myN);. 	. 	t->Branch(""arr1"",arr1,""arr1[n]"");. 	t->Branch(""arr2"",arr2,""arr2[n]"");. 	t->Branch(""arr3"",arr3,""arr3[n]"");. 	t->Branch(""arr4"",arr4,""arr4[n]"");. 	t->Branch(""arr5"",arr5,""arr5[n]"");. 	t->Branch(""arr6"",arr6,""arr6[n]"");. 	t->Branch(""arr7"",arr7,""arr7[n]"");. 	t->Branch(""arr8"",arr8,""arr8[n]"");. 	t->Branch(""arr9"",arr9,""arr9[n]"");. 	t->Branch(""arr10"",arr10,""arr10[n]"");. 	t->Branch(""arr11"",arr11,""arr11[n]"");. 	return 0;. }. ```. works! Thank you. Only for my own interest - how do I increase the stack size limit?? Georg.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8721
https://github.com/root-project/root/issues/8721:68,safety,test,testvec,68,"Hi @pcanal,. yes, I can confirm, that working on the heap. ```. int testvec() {. 	TTree *t = new TTree(""tree"",""tree"");. 	const int n = 1000000;. 	unsigned int myN = n;. 	float *arr1 = new float[n];. 	float *arr2 = new float[n];. 	float *arr3 = new float[n];. 	float *arr4 = new float[n];. 	float *arr5 = new float[n];. 	float *arr6 = new float[n];. 	float *arr7 = new float[n];. 	float *arr8 = new float[n];. 	float *arr9 = new float[n];. 	float *arr10 = new float[n];. 	float *arr11 = new float[n];. 	. 	t->Branch(""n"",&myN);. 	. 	t->Branch(""arr1"",arr1,""arr1[n]"");. 	t->Branch(""arr2"",arr2,""arr2[n]"");. 	t->Branch(""arr3"",arr3,""arr3[n]"");. 	t->Branch(""arr4"",arr4,""arr4[n]"");. 	t->Branch(""arr5"",arr5,""arr5[n]"");. 	t->Branch(""arr6"",arr6,""arr6[n]"");. 	t->Branch(""arr7"",arr7,""arr7[n]"");. 	t->Branch(""arr8"",arr8,""arr8[n]"");. 	t->Branch(""arr9"",arr9,""arr9[n]"");. 	t->Branch(""arr10"",arr10,""arr10[n]"");. 	t->Branch(""arr11"",arr11,""arr11[n]"");. 	return 0;. }. ```. works! Thank you. Only for my own interest - how do I increase the stack size limit?? Georg.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8721
https://github.com/root-project/root/issues/8721:68,testability,test,testvec,68,"Hi @pcanal,. yes, I can confirm, that working on the heap. ```. int testvec() {. 	TTree *t = new TTree(""tree"",""tree"");. 	const int n = 1000000;. 	unsigned int myN = n;. 	float *arr1 = new float[n];. 	float *arr2 = new float[n];. 	float *arr3 = new float[n];. 	float *arr4 = new float[n];. 	float *arr5 = new float[n];. 	float *arr6 = new float[n];. 	float *arr7 = new float[n];. 	float *arr8 = new float[n];. 	float *arr9 = new float[n];. 	float *arr10 = new float[n];. 	float *arr11 = new float[n];. 	. 	t->Branch(""n"",&myN);. 	. 	t->Branch(""arr1"",arr1,""arr1[n]"");. 	t->Branch(""arr2"",arr2,""arr2[n]"");. 	t->Branch(""arr3"",arr3,""arr3[n]"");. 	t->Branch(""arr4"",arr4,""arr4[n]"");. 	t->Branch(""arr5"",arr5,""arr5[n]"");. 	t->Branch(""arr6"",arr6,""arr6[n]"");. 	t->Branch(""arr7"",arr7,""arr7[n]"");. 	t->Branch(""arr8"",arr8,""arr8[n]"");. 	t->Branch(""arr9"",arr9,""arr9[n]"");. 	t->Branch(""arr10"",arr10,""arr10[n]"");. 	t->Branch(""arr11"",arr11,""arr11[n]"");. 	return 0;. }. ```. works! Thank you. Only for my own interest - how do I increase the stack size limit?? Georg.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8721
https://github.com/root-project/root/issues/8721:24,usability,confirm,confirm,24,"Hi @pcanal,. yes, I can confirm, that working on the heap. ```. int testvec() {. 	TTree *t = new TTree(""tree"",""tree"");. 	const int n = 1000000;. 	unsigned int myN = n;. 	float *arr1 = new float[n];. 	float *arr2 = new float[n];. 	float *arr3 = new float[n];. 	float *arr4 = new float[n];. 	float *arr5 = new float[n];. 	float *arr6 = new float[n];. 	float *arr7 = new float[n];. 	float *arr8 = new float[n];. 	float *arr9 = new float[n];. 	float *arr10 = new float[n];. 	float *arr11 = new float[n];. 	. 	t->Branch(""n"",&myN);. 	. 	t->Branch(""arr1"",arr1,""arr1[n]"");. 	t->Branch(""arr2"",arr2,""arr2[n]"");. 	t->Branch(""arr3"",arr3,""arr3[n]"");. 	t->Branch(""arr4"",arr4,""arr4[n]"");. 	t->Branch(""arr5"",arr5,""arr5[n]"");. 	t->Branch(""arr6"",arr6,""arr6[n]"");. 	t->Branch(""arr7"",arr7,""arr7[n]"");. 	t->Branch(""arr8"",arr8,""arr8[n]"");. 	t->Branch(""arr9"",arr9,""arr9[n]"");. 	t->Branch(""arr10"",arr10,""arr10[n]"");. 	t->Branch(""arr11"",arr11,""arr11[n]"");. 	return 0;. }. ```. works! Thank you. Only for my own interest - how do I increase the stack size limit?? Georg.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8721
https://github.com/root-project/root/issues/8721:22,usability,command,command,22,You can use the shell command `ulimit` usually with the `-s` option.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8721
https://github.com/root-project/root/pull/8722:14,safety,test,test,14,Can you add a test file with that fixture to show people how they should be using it?,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8722
https://github.com/root-project/root/pull/8722:14,testability,test,test,14,Can you add a test file with that fixture to show people how they should be using it?,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8722
https://github.com/root-project/root/pull/8722:86,integrability,messag,message,86,"sure @vgvassilev, do you mean a example file in the repo or just an example in the PR message?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8722
https://github.com/root-project/root/pull/8722:86,interoperability,messag,message,86,"sure @vgvassilev, do you mean a example file in the repo or just an example in the PR message?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8722
https://github.com/root-project/root/pull/8722:88,integrability,messag,message,88,"> sure @vgvassilev, do you mean a example file in the repo or just an example in the PR message? It'd be best to have an example test with a fixture. Maybe convert an already existing test to a fixture if appropriate?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8722
https://github.com/root-project/root/pull/8722:88,interoperability,messag,message,88,"> sure @vgvassilev, do you mean a example file in the repo or just an example in the PR message? It'd be best to have an example test with a fixture. Maybe convert an already existing test to a fixture if appropriate?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8722
https://github.com/root-project/root/pull/8722:129,safety,test,test,129,"> sure @vgvassilev, do you mean a example file in the repo or just an example in the PR message? It'd be best to have an example test with a fixture. Maybe convert an already existing test to a fixture if appropriate?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8722
https://github.com/root-project/root/pull/8722:184,safety,test,test,184,"> sure @vgvassilev, do you mean a example file in the repo or just an example in the PR message? It'd be best to have an example test with a fixture. Maybe convert an already existing test to a fixture if appropriate?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8722
https://github.com/root-project/root/pull/8722:129,testability,test,test,129,"> sure @vgvassilev, do you mean a example file in the repo or just an example in the PR message? It'd be best to have an example test with a fixture. Maybe convert an already existing test to a fixture if appropriate?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8722
https://github.com/root-project/root/pull/8722:184,testability,test,test,184,"> sure @vgvassilev, do you mean a example file in the repo or just an example in the PR message? It'd be best to have an example test with a fixture. Maybe convert an already existing test to a fixture if appropriate?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8722
https://github.com/root-project/root/pull/8722:51,safety,test,test,51,Ah yes. Well I've implemented this specially for a test! It will be merged soon too :+1:,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8722
https://github.com/root-project/root/pull/8722:51,testability,test,test,51,Ah yes. Well I've implemented this specially for a test! It will be merged soon too :+1:,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8722
https://github.com/root-project/root/pull/8723:11,deployability,build,build,11,@phsft-bot build,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8723
https://github.com/root-project/root/pull/8723:1608,availability,servic,service,1608,"## DeepCode's analysis on [#10aee9](https://www.deepcode.ai/app/gh/root-project/root/293a52b457c64492cb4893872389ddb3c271e58e/root-project/root/10aee91a545f762c6933819860cc14d4150700c8/pr/_/%2F/code/?utm_source=gh_review&c=0&w=1&i=0&) found:. - :warning: **1** warning :point_down:. ## Top issues. <table>. <thead>. <tr>. <th align=""left"">Description</th>. <th align=""left"">Example fixes</th>. </tr>. </thead>. <tbody>. <tr>. <td width=""77%"">Leaking memory. TLegend is allocated on the heap and never freed. In function DrawMultiROCCurve. Occurrences: <ul><li><a href=""https://github.com/root-project/root/blob/10aee91a545f762c6933819860cc14d4150700c8/tmva/tmvagui/src/RVariablePlotter.cxx#L269"">RVariablePlotter.cxx:269</a></li> <li><a href=""https://github.com/root-project/root/blob/10aee91a545f762c6933819860cc14d4150700c8/tmva/tmvagui/src/RVariablePlotter.cxx#L394"">RVariablePlotter.cxx:394</a></li></ul></td>. <td width=""23%""><a href=""https://www.deepcode.ai/app/gh/root-project/root/293a52b457c64492cb4893872389ddb3c271e58e/root-project/root/10aee91a545f762c6933819860cc14d4150700c8/pr/_/%2Ftmva%2Ftmvagui%2Fsrc%2FRVariablePlotter.cxx/cpp%2Fdc%2FCppMemoryLeak/code/?utm_source=gh_review&"">:wrench: Example fixes</a></td>. </tr>. </tbody>. </table>. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/293a52b457c64492cb4893872389ddb3c271e58e/root-project/root/10aee91a545f762c6933819860cc14d4150700c8/pr/_/%2F/code/?utm_source=gh_review&c=0&w=1&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8723
https://github.com/root-project/root/pull/8723:1608,deployability,servic,service,1608,"## DeepCode's analysis on [#10aee9](https://www.deepcode.ai/app/gh/root-project/root/293a52b457c64492cb4893872389ddb3c271e58e/root-project/root/10aee91a545f762c6933819860cc14d4150700c8/pr/_/%2F/code/?utm_source=gh_review&c=0&w=1&i=0&) found:. - :warning: **1** warning :point_down:. ## Top issues. <table>. <thead>. <tr>. <th align=""left"">Description</th>. <th align=""left"">Example fixes</th>. </tr>. </thead>. <tbody>. <tr>. <td width=""77%"">Leaking memory. TLegend is allocated on the heap and never freed. In function DrawMultiROCCurve. Occurrences: <ul><li><a href=""https://github.com/root-project/root/blob/10aee91a545f762c6933819860cc14d4150700c8/tmva/tmvagui/src/RVariablePlotter.cxx#L269"">RVariablePlotter.cxx:269</a></li> <li><a href=""https://github.com/root-project/root/blob/10aee91a545f762c6933819860cc14d4150700c8/tmva/tmvagui/src/RVariablePlotter.cxx#L394"">RVariablePlotter.cxx:394</a></li></ul></td>. <td width=""23%""><a href=""https://www.deepcode.ai/app/gh/root-project/root/293a52b457c64492cb4893872389ddb3c271e58e/root-project/root/10aee91a545f762c6933819860cc14d4150700c8/pr/_/%2Ftmva%2Ftmvagui%2Fsrc%2FRVariablePlotter.cxx/cpp%2Fdc%2FCppMemoryLeak/code/?utm_source=gh_review&"">:wrench: Example fixes</a></td>. </tr>. </tbody>. </table>. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/293a52b457c64492cb4893872389ddb3c271e58e/root-project/root/10aee91a545f762c6933819860cc14d4150700c8/pr/_/%2F/code/?utm_source=gh_review&c=0&w=1&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8723
https://github.com/root-project/root/pull/8723:1620,deployability,API,API,1620,"## DeepCode's analysis on [#10aee9](https://www.deepcode.ai/app/gh/root-project/root/293a52b457c64492cb4893872389ddb3c271e58e/root-project/root/10aee91a545f762c6933819860cc14d4150700c8/pr/_/%2F/code/?utm_source=gh_review&c=0&w=1&i=0&) found:. - :warning: **1** warning :point_down:. ## Top issues. <table>. <thead>. <tr>. <th align=""left"">Description</th>. <th align=""left"">Example fixes</th>. </tr>. </thead>. <tbody>. <tr>. <td width=""77%"">Leaking memory. TLegend is allocated on the heap and never freed. In function DrawMultiROCCurve. Occurrences: <ul><li><a href=""https://github.com/root-project/root/blob/10aee91a545f762c6933819860cc14d4150700c8/tmva/tmvagui/src/RVariablePlotter.cxx#L269"">RVariablePlotter.cxx:269</a></li> <li><a href=""https://github.com/root-project/root/blob/10aee91a545f762c6933819860cc14d4150700c8/tmva/tmvagui/src/RVariablePlotter.cxx#L394"">RVariablePlotter.cxx:394</a></li></ul></td>. <td width=""23%""><a href=""https://www.deepcode.ai/app/gh/root-project/root/293a52b457c64492cb4893872389ddb3c271e58e/root-project/root/10aee91a545f762c6933819860cc14d4150700c8/pr/_/%2Ftmva%2Ftmvagui%2Fsrc%2FRVariablePlotter.cxx/cpp%2Fdc%2FCppMemoryLeak/code/?utm_source=gh_review&"">:wrench: Example fixes</a></td>. </tr>. </tbody>. </table>. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/293a52b457c64492cb4893872389ddb3c271e58e/root-project/root/10aee91a545f762c6933819860cc14d4150700c8/pr/_/%2F/code/?utm_source=gh_review&c=0&w=1&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8723
https://github.com/root-project/root/pull/8723:469,energy efficiency,alloc,allocated,469,"## DeepCode's analysis on [#10aee9](https://www.deepcode.ai/app/gh/root-project/root/293a52b457c64492cb4893872389ddb3c271e58e/root-project/root/10aee91a545f762c6933819860cc14d4150700c8/pr/_/%2F/code/?utm_source=gh_review&c=0&w=1&i=0&) found:. - :warning: **1** warning :point_down:. ## Top issues. <table>. <thead>. <tr>. <th align=""left"">Description</th>. <th align=""left"">Example fixes</th>. </tr>. </thead>. <tbody>. <tr>. <td width=""77%"">Leaking memory. TLegend is allocated on the heap and never freed. In function DrawMultiROCCurve. Occurrences: <ul><li><a href=""https://github.com/root-project/root/blob/10aee91a545f762c6933819860cc14d4150700c8/tmva/tmvagui/src/RVariablePlotter.cxx#L269"">RVariablePlotter.cxx:269</a></li> <li><a href=""https://github.com/root-project/root/blob/10aee91a545f762c6933819860cc14d4150700c8/tmva/tmvagui/src/RVariablePlotter.cxx#L394"">RVariablePlotter.cxx:394</a></li></ul></td>. <td width=""23%""><a href=""https://www.deepcode.ai/app/gh/root-project/root/293a52b457c64492cb4893872389ddb3c271e58e/root-project/root/10aee91a545f762c6933819860cc14d4150700c8/pr/_/%2Ftmva%2Ftmvagui%2Fsrc%2FRVariablePlotter.cxx/cpp%2Fdc%2FCppMemoryLeak/code/?utm_source=gh_review&"">:wrench: Example fixes</a></td>. </tr>. </tbody>. </table>. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/293a52b457c64492cb4893872389ddb3c271e58e/root-project/root/10aee91a545f762c6933819860cc14d4150700c8/pr/_/%2F/code/?utm_source=gh_review&c=0&w=1&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8723
https://github.com/root-project/root/pull/8723:520,energy efficiency,Draw,DrawMultiROCCurve,520,"## DeepCode's analysis on [#10aee9](https://www.deepcode.ai/app/gh/root-project/root/293a52b457c64492cb4893872389ddb3c271e58e/root-project/root/10aee91a545f762c6933819860cc14d4150700c8/pr/_/%2F/code/?utm_source=gh_review&c=0&w=1&i=0&) found:. - :warning: **1** warning :point_down:. ## Top issues. <table>. <thead>. <tr>. <th align=""left"">Description</th>. <th align=""left"">Example fixes</th>. </tr>. </thead>. <tbody>. <tr>. <td width=""77%"">Leaking memory. TLegend is allocated on the heap and never freed. In function DrawMultiROCCurve. Occurrences: <ul><li><a href=""https://github.com/root-project/root/blob/10aee91a545f762c6933819860cc14d4150700c8/tmva/tmvagui/src/RVariablePlotter.cxx#L269"">RVariablePlotter.cxx:269</a></li> <li><a href=""https://github.com/root-project/root/blob/10aee91a545f762c6933819860cc14d4150700c8/tmva/tmvagui/src/RVariablePlotter.cxx#L394"">RVariablePlotter.cxx:394</a></li></ul></td>. <td width=""23%""><a href=""https://www.deepcode.ai/app/gh/root-project/root/293a52b457c64492cb4893872389ddb3c271e58e/root-project/root/10aee91a545f762c6933819860cc14d4150700c8/pr/_/%2Ftmva%2Ftmvagui%2Fsrc%2FRVariablePlotter.cxx/cpp%2Fdc%2FCppMemoryLeak/code/?utm_source=gh_review&"">:wrench: Example fixes</a></td>. </tr>. </tbody>. </table>. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/293a52b457c64492cb4893872389ddb3c271e58e/root-project/root/10aee91a545f762c6933819860cc14d4150700c8/pr/_/%2F/code/?utm_source=gh_review&c=0&w=1&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8723
https://github.com/root-project/root/pull/8723:1608,integrability,servic,service,1608,"## DeepCode's analysis on [#10aee9](https://www.deepcode.ai/app/gh/root-project/root/293a52b457c64492cb4893872389ddb3c271e58e/root-project/root/10aee91a545f762c6933819860cc14d4150700c8/pr/_/%2F/code/?utm_source=gh_review&c=0&w=1&i=0&) found:. - :warning: **1** warning :point_down:. ## Top issues. <table>. <thead>. <tr>. <th align=""left"">Description</th>. <th align=""left"">Example fixes</th>. </tr>. </thead>. <tbody>. <tr>. <td width=""77%"">Leaking memory. TLegend is allocated on the heap and never freed. In function DrawMultiROCCurve. Occurrences: <ul><li><a href=""https://github.com/root-project/root/blob/10aee91a545f762c6933819860cc14d4150700c8/tmva/tmvagui/src/RVariablePlotter.cxx#L269"">RVariablePlotter.cxx:269</a></li> <li><a href=""https://github.com/root-project/root/blob/10aee91a545f762c6933819860cc14d4150700c8/tmva/tmvagui/src/RVariablePlotter.cxx#L394"">RVariablePlotter.cxx:394</a></li></ul></td>. <td width=""23%""><a href=""https://www.deepcode.ai/app/gh/root-project/root/293a52b457c64492cb4893872389ddb3c271e58e/root-project/root/10aee91a545f762c6933819860cc14d4150700c8/pr/_/%2Ftmva%2Ftmvagui%2Fsrc%2FRVariablePlotter.cxx/cpp%2Fdc%2FCppMemoryLeak/code/?utm_source=gh_review&"">:wrench: Example fixes</a></td>. </tr>. </tbody>. </table>. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/293a52b457c64492cb4893872389ddb3c271e58e/root-project/root/10aee91a545f762c6933819860cc14d4150700c8/pr/_/%2F/code/?utm_source=gh_review&c=0&w=1&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8723
https://github.com/root-project/root/pull/8723:1620,integrability,API,API,1620,"## DeepCode's analysis on [#10aee9](https://www.deepcode.ai/app/gh/root-project/root/293a52b457c64492cb4893872389ddb3c271e58e/root-project/root/10aee91a545f762c6933819860cc14d4150700c8/pr/_/%2F/code/?utm_source=gh_review&c=0&w=1&i=0&) found:. - :warning: **1** warning :point_down:. ## Top issues. <table>. <thead>. <tr>. <th align=""left"">Description</th>. <th align=""left"">Example fixes</th>. </tr>. </thead>. <tbody>. <tr>. <td width=""77%"">Leaking memory. TLegend is allocated on the heap and never freed. In function DrawMultiROCCurve. Occurrences: <ul><li><a href=""https://github.com/root-project/root/blob/10aee91a545f762c6933819860cc14d4150700c8/tmva/tmvagui/src/RVariablePlotter.cxx#L269"">RVariablePlotter.cxx:269</a></li> <li><a href=""https://github.com/root-project/root/blob/10aee91a545f762c6933819860cc14d4150700c8/tmva/tmvagui/src/RVariablePlotter.cxx#L394"">RVariablePlotter.cxx:394</a></li></ul></td>. <td width=""23%""><a href=""https://www.deepcode.ai/app/gh/root-project/root/293a52b457c64492cb4893872389ddb3c271e58e/root-project/root/10aee91a545f762c6933819860cc14d4150700c8/pr/_/%2Ftmva%2Ftmvagui%2Fsrc%2FRVariablePlotter.cxx/cpp%2Fdc%2FCppMemoryLeak/code/?utm_source=gh_review&"">:wrench: Example fixes</a></td>. </tr>. </tbody>. </table>. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/293a52b457c64492cb4893872389ddb3c271e58e/root-project/root/10aee91a545f762c6933819860cc14d4150700c8/pr/_/%2F/code/?utm_source=gh_review&c=0&w=1&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8723
https://github.com/root-project/root/pull/8723:1620,interoperability,API,API,1620,"## DeepCode's analysis on [#10aee9](https://www.deepcode.ai/app/gh/root-project/root/293a52b457c64492cb4893872389ddb3c271e58e/root-project/root/10aee91a545f762c6933819860cc14d4150700c8/pr/_/%2F/code/?utm_source=gh_review&c=0&w=1&i=0&) found:. - :warning: **1** warning :point_down:. ## Top issues. <table>. <thead>. <tr>. <th align=""left"">Description</th>. <th align=""left"">Example fixes</th>. </tr>. </thead>. <tbody>. <tr>. <td width=""77%"">Leaking memory. TLegend is allocated on the heap and never freed. In function DrawMultiROCCurve. Occurrences: <ul><li><a href=""https://github.com/root-project/root/blob/10aee91a545f762c6933819860cc14d4150700c8/tmva/tmvagui/src/RVariablePlotter.cxx#L269"">RVariablePlotter.cxx:269</a></li> <li><a href=""https://github.com/root-project/root/blob/10aee91a545f762c6933819860cc14d4150700c8/tmva/tmvagui/src/RVariablePlotter.cxx#L394"">RVariablePlotter.cxx:394</a></li></ul></td>. <td width=""23%""><a href=""https://www.deepcode.ai/app/gh/root-project/root/293a52b457c64492cb4893872389ddb3c271e58e/root-project/root/10aee91a545f762c6933819860cc14d4150700c8/pr/_/%2Ftmva%2Ftmvagui%2Fsrc%2FRVariablePlotter.cxx/cpp%2Fdc%2FCppMemoryLeak/code/?utm_source=gh_review&"">:wrench: Example fixes</a></td>. </tr>. </tbody>. </table>. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/293a52b457c64492cb4893872389ddb3c271e58e/root-project/root/10aee91a545f762c6933819860cc14d4150700c8/pr/_/%2F/code/?utm_source=gh_review&c=0&w=1&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8723
https://github.com/root-project/root/pull/8723:1810,interoperability,plug,plugins,1810,"## DeepCode's analysis on [#10aee9](https://www.deepcode.ai/app/gh/root-project/root/293a52b457c64492cb4893872389ddb3c271e58e/root-project/root/10aee91a545f762c6933819860cc14d4150700c8/pr/_/%2F/code/?utm_source=gh_review&c=0&w=1&i=0&) found:. - :warning: **1** warning :point_down:. ## Top issues. <table>. <thead>. <tr>. <th align=""left"">Description</th>. <th align=""left"">Example fixes</th>. </tr>. </thead>. <tbody>. <tr>. <td width=""77%"">Leaking memory. TLegend is allocated on the heap and never freed. In function DrawMultiROCCurve. Occurrences: <ul><li><a href=""https://github.com/root-project/root/blob/10aee91a545f762c6933819860cc14d4150700c8/tmva/tmvagui/src/RVariablePlotter.cxx#L269"">RVariablePlotter.cxx:269</a></li> <li><a href=""https://github.com/root-project/root/blob/10aee91a545f762c6933819860cc14d4150700c8/tmva/tmvagui/src/RVariablePlotter.cxx#L394"">RVariablePlotter.cxx:394</a></li></ul></td>. <td width=""23%""><a href=""https://www.deepcode.ai/app/gh/root-project/root/293a52b457c64492cb4893872389ddb3c271e58e/root-project/root/10aee91a545f762c6933819860cc14d4150700c8/pr/_/%2Ftmva%2Ftmvagui%2Fsrc%2FRVariablePlotter.cxx/cpp%2Fdc%2FCppMemoryLeak/code/?utm_source=gh_review&"">:wrench: Example fixes</a></td>. </tr>. </tbody>. </table>. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/293a52b457c64492cb4893872389ddb3c271e58e/root-project/root/10aee91a545f762c6933819860cc14d4150700c8/pr/_/%2F/code/?utm_source=gh_review&c=0&w=1&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8723
https://github.com/root-project/root/pull/8723:1882,interoperability,plug,plugin,1882,"## DeepCode's analysis on [#10aee9](https://www.deepcode.ai/app/gh/root-project/root/293a52b457c64492cb4893872389ddb3c271e58e/root-project/root/10aee91a545f762c6933819860cc14d4150700c8/pr/_/%2F/code/?utm_source=gh_review&c=0&w=1&i=0&) found:. - :warning: **1** warning :point_down:. ## Top issues. <table>. <thead>. <tr>. <th align=""left"">Description</th>. <th align=""left"">Example fixes</th>. </tr>. </thead>. <tbody>. <tr>. <td width=""77%"">Leaking memory. TLegend is allocated on the heap and never freed. In function DrawMultiROCCurve. Occurrences: <ul><li><a href=""https://github.com/root-project/root/blob/10aee91a545f762c6933819860cc14d4150700c8/tmva/tmvagui/src/RVariablePlotter.cxx#L269"">RVariablePlotter.cxx:269</a></li> <li><a href=""https://github.com/root-project/root/blob/10aee91a545f762c6933819860cc14d4150700c8/tmva/tmvagui/src/RVariablePlotter.cxx#L394"">RVariablePlotter.cxx:394</a></li></ul></td>. <td width=""23%""><a href=""https://www.deepcode.ai/app/gh/root-project/root/293a52b457c64492cb4893872389ddb3c271e58e/root-project/root/10aee91a545f762c6933819860cc14d4150700c8/pr/_/%2Ftmva%2Ftmvagui%2Fsrc%2FRVariablePlotter.cxx/cpp%2Fdc%2FCppMemoryLeak/code/?utm_source=gh_review&"">:wrench: Example fixes</a></td>. </tr>. </tbody>. </table>. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/293a52b457c64492cb4893872389ddb3c271e58e/root-project/root/10aee91a545f762c6933819860cc14d4150700c8/pr/_/%2F/code/?utm_source=gh_review&c=0&w=1&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8723
https://github.com/root-project/root/pull/8723:1933,interoperability,plug,plugin,1933,"## DeepCode's analysis on [#10aee9](https://www.deepcode.ai/app/gh/root-project/root/293a52b457c64492cb4893872389ddb3c271e58e/root-project/root/10aee91a545f762c6933819860cc14d4150700c8/pr/_/%2F/code/?utm_source=gh_review&c=0&w=1&i=0&) found:. - :warning: **1** warning :point_down:. ## Top issues. <table>. <thead>. <tr>. <th align=""left"">Description</th>. <th align=""left"">Example fixes</th>. </tr>. </thead>. <tbody>. <tr>. <td width=""77%"">Leaking memory. TLegend is allocated on the heap and never freed. In function DrawMultiROCCurve. Occurrences: <ul><li><a href=""https://github.com/root-project/root/blob/10aee91a545f762c6933819860cc14d4150700c8/tmva/tmvagui/src/RVariablePlotter.cxx#L269"">RVariablePlotter.cxx:269</a></li> <li><a href=""https://github.com/root-project/root/blob/10aee91a545f762c6933819860cc14d4150700c8/tmva/tmvagui/src/RVariablePlotter.cxx#L394"">RVariablePlotter.cxx:394</a></li></ul></td>. <td width=""23%""><a href=""https://www.deepcode.ai/app/gh/root-project/root/293a52b457c64492cb4893872389ddb3c271e58e/root-project/root/10aee91a545f762c6933819860cc14d4150700c8/pr/_/%2Ftmva%2Ftmvagui%2Fsrc%2FRVariablePlotter.cxx/cpp%2Fdc%2FCppMemoryLeak/code/?utm_source=gh_review&"">:wrench: Example fixes</a></td>. </tr>. </tbody>. </table>. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/293a52b457c64492cb4893872389ddb3c271e58e/root-project/root/10aee91a545f762c6933819860cc14d4150700c8/pr/_/%2F/code/?utm_source=gh_review&c=0&w=1&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8723
https://github.com/root-project/root/pull/8723:1608,modifiability,servic,service,1608,"## DeepCode's analysis on [#10aee9](https://www.deepcode.ai/app/gh/root-project/root/293a52b457c64492cb4893872389ddb3c271e58e/root-project/root/10aee91a545f762c6933819860cc14d4150700c8/pr/_/%2F/code/?utm_source=gh_review&c=0&w=1&i=0&) found:. - :warning: **1** warning :point_down:. ## Top issues. <table>. <thead>. <tr>. <th align=""left"">Description</th>. <th align=""left"">Example fixes</th>. </tr>. </thead>. <tbody>. <tr>. <td width=""77%"">Leaking memory. TLegend is allocated on the heap and never freed. In function DrawMultiROCCurve. Occurrences: <ul><li><a href=""https://github.com/root-project/root/blob/10aee91a545f762c6933819860cc14d4150700c8/tmva/tmvagui/src/RVariablePlotter.cxx#L269"">RVariablePlotter.cxx:269</a></li> <li><a href=""https://github.com/root-project/root/blob/10aee91a545f762c6933819860cc14d4150700c8/tmva/tmvagui/src/RVariablePlotter.cxx#L394"">RVariablePlotter.cxx:394</a></li></ul></td>. <td width=""23%""><a href=""https://www.deepcode.ai/app/gh/root-project/root/293a52b457c64492cb4893872389ddb3c271e58e/root-project/root/10aee91a545f762c6933819860cc14d4150700c8/pr/_/%2Ftmva%2Ftmvagui%2Fsrc%2FRVariablePlotter.cxx/cpp%2Fdc%2FCppMemoryLeak/code/?utm_source=gh_review&"">:wrench: Example fixes</a></td>. </tr>. </tbody>. </table>. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/293a52b457c64492cb4893872389ddb3c271e58e/root-project/root/10aee91a545f762c6933819860cc14d4150700c8/pr/_/%2F/code/?utm_source=gh_review&c=0&w=1&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8723
https://github.com/root-project/root/pull/8723:450,performance,memor,memory,450,"## DeepCode's analysis on [#10aee9](https://www.deepcode.ai/app/gh/root-project/root/293a52b457c64492cb4893872389ddb3c271e58e/root-project/root/10aee91a545f762c6933819860cc14d4150700c8/pr/_/%2F/code/?utm_source=gh_review&c=0&w=1&i=0&) found:. - :warning: **1** warning :point_down:. ## Top issues. <table>. <thead>. <tr>. <th align=""left"">Description</th>. <th align=""left"">Example fixes</th>. </tr>. </thead>. <tbody>. <tr>. <td width=""77%"">Leaking memory. TLegend is allocated on the heap and never freed. In function DrawMultiROCCurve. Occurrences: <ul><li><a href=""https://github.com/root-project/root/blob/10aee91a545f762c6933819860cc14d4150700c8/tmva/tmvagui/src/RVariablePlotter.cxx#L269"">RVariablePlotter.cxx:269</a></li> <li><a href=""https://github.com/root-project/root/blob/10aee91a545f762c6933819860cc14d4150700c8/tmva/tmvagui/src/RVariablePlotter.cxx#L394"">RVariablePlotter.cxx:394</a></li></ul></td>. <td width=""23%""><a href=""https://www.deepcode.ai/app/gh/root-project/root/293a52b457c64492cb4893872389ddb3c271e58e/root-project/root/10aee91a545f762c6933819860cc14d4150700c8/pr/_/%2Ftmva%2Ftmvagui%2Fsrc%2FRVariablePlotter.cxx/cpp%2Fdc%2FCppMemoryLeak/code/?utm_source=gh_review&"">:wrench: Example fixes</a></td>. </tr>. </tbody>. </table>. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/293a52b457c64492cb4893872389ddb3c271e58e/root-project/root/10aee91a545f762c6933819860cc14d4150700c8/pr/_/%2F/code/?utm_source=gh_review&c=0&w=1&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8723
https://github.com/root-project/root/pull/8723:450,usability,memor,memory,450,"## DeepCode's analysis on [#10aee9](https://www.deepcode.ai/app/gh/root-project/root/293a52b457c64492cb4893872389ddb3c271e58e/root-project/root/10aee91a545f762c6933819860cc14d4150700c8/pr/_/%2F/code/?utm_source=gh_review&c=0&w=1&i=0&) found:. - :warning: **1** warning :point_down:. ## Top issues. <table>. <thead>. <tr>. <th align=""left"">Description</th>. <th align=""left"">Example fixes</th>. </tr>. </thead>. <tbody>. <tr>. <td width=""77%"">Leaking memory. TLegend is allocated on the heap and never freed. In function DrawMultiROCCurve. Occurrences: <ul><li><a href=""https://github.com/root-project/root/blob/10aee91a545f762c6933819860cc14d4150700c8/tmva/tmvagui/src/RVariablePlotter.cxx#L269"">RVariablePlotter.cxx:269</a></li> <li><a href=""https://github.com/root-project/root/blob/10aee91a545f762c6933819860cc14d4150700c8/tmva/tmvagui/src/RVariablePlotter.cxx#L394"">RVariablePlotter.cxx:394</a></li></ul></td>. <td width=""23%""><a href=""https://www.deepcode.ai/app/gh/root-project/root/293a52b457c64492cb4893872389ddb3c271e58e/root-project/root/10aee91a545f762c6933819860cc14d4150700c8/pr/_/%2Ftmva%2Ftmvagui%2Fsrc%2FRVariablePlotter.cxx/cpp%2Fdc%2FCppMemoryLeak/code/?utm_source=gh_review&"">:wrench: Example fixes</a></td>. </tr>. </tbody>. </table>. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/293a52b457c64492cb4893872389ddb3c271e58e/root-project/root/10aee91a545f762c6933819860cc14d4150700c8/pr/_/%2F/code/?utm_source=gh_review&c=0&w=1&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8723
https://github.com/root-project/root/pull/8728:44,integrability,schema,schema,44,"> Nice try, but you have to be careful with schema evolution. > . > 1. Go back, write a class that uses those proxies to a file, commit the file to git, and implement a read test. > . > 2. Change the classes, and implement the proper typedefs, so no other code in RF has to be touched. > . > 3. Create schema evolution rules like those:. > https://github.com/root-project/root/blob/05d10d2127282c5ec11b1330763dc0b43a93a6df/roofit/roofitcore/inc/LinkDef.h#L185-L192. Hi @hageboeck, thanks for the review! The proper typedefs were already implemented, and now I also implemented schema evolution rules and implemented a read test.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8728
https://github.com/root-project/root/pull/8728:302,integrability,schema,schema,302,"> Nice try, but you have to be careful with schema evolution. > . > 1. Go back, write a class that uses those proxies to a file, commit the file to git, and implement a read test. > . > 2. Change the classes, and implement the proper typedefs, so no other code in RF has to be touched. > . > 3. Create schema evolution rules like those:. > https://github.com/root-project/root/blob/05d10d2127282c5ec11b1330763dc0b43a93a6df/roofit/roofitcore/inc/LinkDef.h#L185-L192. Hi @hageboeck, thanks for the review! The proper typedefs were already implemented, and now I also implemented schema evolution rules and implemented a read test.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8728
https://github.com/root-project/root/pull/8728:577,integrability,schema,schema,577,"> Nice try, but you have to be careful with schema evolution. > . > 1. Go back, write a class that uses those proxies to a file, commit the file to git, and implement a read test. > . > 2. Change the classes, and implement the proper typedefs, so no other code in RF has to be touched. > . > 3. Create schema evolution rules like those:. > https://github.com/root-project/root/blob/05d10d2127282c5ec11b1330763dc0b43a93a6df/roofit/roofitcore/inc/LinkDef.h#L185-L192. Hi @hageboeck, thanks for the review! The proper typedefs were already implemented, and now I also implemented schema evolution rules and implemented a read test.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8728
https://github.com/root-project/root/pull/8728:110,interoperability,prox,proxies,110,"> Nice try, but you have to be careful with schema evolution. > . > 1. Go back, write a class that uses those proxies to a file, commit the file to git, and implement a read test. > . > 2. Change the classes, and implement the proper typedefs, so no other code in RF has to be touched. > . > 3. Create schema evolution rules like those:. > https://github.com/root-project/root/blob/05d10d2127282c5ec11b1330763dc0b43a93a6df/roofit/roofitcore/inc/LinkDef.h#L185-L192. Hi @hageboeck, thanks for the review! The proper typedefs were already implemented, and now I also implemented schema evolution rules and implemented a read test.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8728
https://github.com/root-project/root/pull/8728:174,safety,test,test,174,"> Nice try, but you have to be careful with schema evolution. > . > 1. Go back, write a class that uses those proxies to a file, commit the file to git, and implement a read test. > . > 2. Change the classes, and implement the proper typedefs, so no other code in RF has to be touched. > . > 3. Create schema evolution rules like those:. > https://github.com/root-project/root/blob/05d10d2127282c5ec11b1330763dc0b43a93a6df/roofit/roofitcore/inc/LinkDef.h#L185-L192. Hi @hageboeck, thanks for the review! The proper typedefs were already implemented, and now I also implemented schema evolution rules and implemented a read test.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8728
https://github.com/root-project/root/pull/8728:496,safety,review,review,496,"> Nice try, but you have to be careful with schema evolution. > . > 1. Go back, write a class that uses those proxies to a file, commit the file to git, and implement a read test. > . > 2. Change the classes, and implement the proper typedefs, so no other code in RF has to be touched. > . > 3. Create schema evolution rules like those:. > https://github.com/root-project/root/blob/05d10d2127282c5ec11b1330763dc0b43a93a6df/roofit/roofitcore/inc/LinkDef.h#L185-L192. Hi @hageboeck, thanks for the review! The proper typedefs were already implemented, and now I also implemented schema evolution rules and implemented a read test.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8728
https://github.com/root-project/root/pull/8728:623,safety,test,test,623,"> Nice try, but you have to be careful with schema evolution. > . > 1. Go back, write a class that uses those proxies to a file, commit the file to git, and implement a read test. > . > 2. Change the classes, and implement the proper typedefs, so no other code in RF has to be touched. > . > 3. Create schema evolution rules like those:. > https://github.com/root-project/root/blob/05d10d2127282c5ec11b1330763dc0b43a93a6df/roofit/roofitcore/inc/LinkDef.h#L185-L192. Hi @hageboeck, thanks for the review! The proper typedefs were already implemented, and now I also implemented schema evolution rules and implemented a read test.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8728
https://github.com/root-project/root/pull/8728:174,testability,test,test,174,"> Nice try, but you have to be careful with schema evolution. > . > 1. Go back, write a class that uses those proxies to a file, commit the file to git, and implement a read test. > . > 2. Change the classes, and implement the proper typedefs, so no other code in RF has to be touched. > . > 3. Create schema evolution rules like those:. > https://github.com/root-project/root/blob/05d10d2127282c5ec11b1330763dc0b43a93a6df/roofit/roofitcore/inc/LinkDef.h#L185-L192. Hi @hageboeck, thanks for the review! The proper typedefs were already implemented, and now I also implemented schema evolution rules and implemented a read test.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8728
https://github.com/root-project/root/pull/8728:496,testability,review,review,496,"> Nice try, but you have to be careful with schema evolution. > . > 1. Go back, write a class that uses those proxies to a file, commit the file to git, and implement a read test. > . > 2. Change the classes, and implement the proper typedefs, so no other code in RF has to be touched. > . > 3. Create schema evolution rules like those:. > https://github.com/root-project/root/blob/05d10d2127282c5ec11b1330763dc0b43a93a6df/roofit/roofitcore/inc/LinkDef.h#L185-L192. Hi @hageboeck, thanks for the review! The proper typedefs were already implemented, and now I also implemented schema evolution rules and implemented a read test.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8728
https://github.com/root-project/root/pull/8728:623,testability,test,test,623,"> Nice try, but you have to be careful with schema evolution. > . > 1. Go back, write a class that uses those proxies to a file, commit the file to git, and implement a read test. > . > 2. Change the classes, and implement the proper typedefs, so no other code in RF has to be touched. > . > 3. Create schema evolution rules like those:. > https://github.com/root-project/root/blob/05d10d2127282c5ec11b1330763dc0b43a93a6df/roofit/roofitcore/inc/LinkDef.h#L185-L192. Hi @hageboeck, thanks for the review! The proper typedefs were already implemented, and now I also implemented schema evolution rules and implemented a read test.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8728
https://github.com/root-project/root/pull/8732:109,modifiability,pac,packages,109,"> I don't think it's good enough. Some header files have to be found by ROOT, and there are quite a few more packages than those two IIRC... Yeah, this wont be as simple as I thought it would be üòÖ . I'll come up with a solution.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8732
https://github.com/root-project/root/pull/8732:163,testability,simpl,simple,163,"> I don't think it's good enough. Some header files have to be found by ROOT, and there are quite a few more packages than those two IIRC... Yeah, this wont be as simple as I thought it would be üòÖ . I'll come up with a solution.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8732
https://github.com/root-project/root/pull/8732:163,usability,simpl,simple,163,"> I don't think it's good enough. Some header files have to be found by ROOT, and there are quite a few more packages than those two IIRC... Yeah, this wont be as simple as I thought it would be üòÖ . I'll come up with a solution.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8732
https://github.com/root-project/root/pull/8732:21,availability,error,error,21,After some trial and error I've realized that this task is too hard for me üòÖ,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8732
https://github.com/root-project/root/pull/8732:21,performance,error,error,21,After some trial and error I've realized that this task is too hard for me üòÖ,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8732
https://github.com/root-project/root/pull/8732:21,safety,error,error,21,After some trial and error I've realized that this task is too hard for me üòÖ,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8732
https://github.com/root-project/root/pull/8732:21,usability,error,error,21,After some trial and error I've realized that this task is too hard for me üòÖ,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8732
https://github.com/root-project/root/issues/8735:187,modifiability,variab,variable,187,"One solution for a general derivative function could be to simply pass ""Function->Derivative(x)"" in the second TF1. But there still needs to be a way to differentiate with respect to any variable.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8735
https://github.com/root-project/root/issues/8735:59,testability,simpl,simply,59,"One solution for a general derivative function could be to simply pass ""Function->Derivative(x)"" in the second TF1. But there still needs to be a way to differentiate with respect to any variable.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8735
https://github.com/root-project/root/issues/8735:59,usability,simpl,simply,59,"One solution for a general derivative function could be to simply pass ""Function->Derivative(x)"" in the second TF1. But there still needs to be a way to differentiate with respect to any variable.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8735
https://github.com/root-project/root/issues/8735:17,modifiability,variab,variable,17,TF1 only has one variable.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8735
https://github.com/root-project/root/issues/8735:12,safety,compl,completely,12,"This is not completely correct since TF1 is a base class and also represents generic functions in the N-dimension. . I think it would not be bad to have also a Gradient function as we have GradientPar. On the other hand, we have the Derivator class, which can be easily used.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8735
https://github.com/root-project/root/issues/8735:12,security,compl,completely,12,"This is not completely correct since TF1 is a base class and also represents generic functions in the N-dimension. . I think it would not be bad to have also a Gradient function as we have GradientPar. On the other hand, we have the Derivator class, which can be easily used.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8735
https://github.com/root-project/root/pull/8736:11,deployability,build,build,11,@phsft-bot build,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8736
https://github.com/root-project/root/pull/8737:134,reliability,doe,doesn,134,"@Axel-Naumann, @vgvassilev, given that I will be off in August, feel free to merge this PR and the sibling PR in roottest if the code doesn't need changes after the review.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8737
https://github.com/root-project/root/pull/8737:165,safety,review,review,165,"@Axel-Naumann, @vgvassilev, given that I will be off in August, feel free to merge this PR and the sibling PR in roottest if the code doesn't need changes after the review.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8737
https://github.com/root-project/root/pull/8737:165,testability,review,review,165,"@Axel-Naumann, @vgvassilev, given that I will be off in August, feel free to merge this PR and the sibling PR in roottest if the code doesn't need changes after the review.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8737
https://github.com/root-project/root/pull/8737:11,deployability,build,build,11,@phsft-bot build just on ROOT-debian10-i386/cxx14,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8737
https://github.com/root-project/root/pull/8737:11,deployability,build,build,11,@phsft-bot build,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8737
https://github.com/root-project/root/pull/8737:11,deployability,build,build,11,@phsft-bot build just on ROOT-debian10-i386/cxx14,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8737
https://github.com/root-project/root/pull/8738:11,deployability,build,build,11,@phsft-bot build,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8738
https://github.com/root-project/root/issues/8739:5,usability,close,close,5,Will close after backporting,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8739
https://github.com/root-project/root/pull/8740:264,deployability,modul,module,264,"> Unrelated, but I hope you don't really need the double ROOT.ROOT to call this from PyROOT, I think that has been fixed years ago. I thought so too, but currently. ```py. >>> ROOT.Detail.EnterRange. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. AttributeError: <namespace cppyy.gbl.Detail at 0x56507072a2f0> has no attribute 'EnterRange'. Full details:. type object 'Detail' has no attribute 'EnterRange'. 'Detail::EnterRange' is not a known C++ class. 'EnterRange' is not a known C++ template. 'EnterRange' is not a known C++ enum. ```. So maybe it's something that must be ""activated"" somehow. I'd need to see how it's done with other classes/functions",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8740
https://github.com/root-project/root/pull/8740:154,energy efficiency,current,currently,154,"> Unrelated, but I hope you don't really need the double ROOT.ROOT to call this from PyROOT, I think that has been fixed years ago. I thought so too, but currently. ```py. >>> ROOT.Detail.EnterRange. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. AttributeError: <namespace cppyy.gbl.Detail at 0x56507072a2f0> has no attribute 'EnterRange'. Full details:. type object 'Detail' has no attribute 'EnterRange'. 'Detail::EnterRange' is not a known C++ class. 'EnterRange' is not a known C++ template. 'EnterRange' is not a known C++ enum. ```. So maybe it's something that must be ""activated"" somehow. I'd need to see how it's done with other classes/functions",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8740
https://github.com/root-project/root/pull/8740:264,modifiability,modul,module,264,"> Unrelated, but I hope you don't really need the double ROOT.ROOT to call this from PyROOT, I think that has been fixed years ago. I thought so too, but currently. ```py. >>> ROOT.Detail.EnterRange. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. AttributeError: <namespace cppyy.gbl.Detail at 0x56507072a2f0> has no attribute 'EnterRange'. Full details:. type object 'Detail' has no attribute 'EnterRange'. 'Detail::EnterRange' is not a known C++ class. 'EnterRange' is not a known C++ template. 'EnterRange' is not a known C++ enum. ```. So maybe it's something that must be ""activated"" somehow. I'd need to see how it's done with other classes/functions",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8740
https://github.com/root-project/root/pull/8740:264,safety,modul,module,264,"> Unrelated, but I hope you don't really need the double ROOT.ROOT to call this from PyROOT, I think that has been fixed years ago. I thought so too, but currently. ```py. >>> ROOT.Detail.EnterRange. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. AttributeError: <namespace cppyy.gbl.Detail at 0x56507072a2f0> has no attribute 'EnterRange'. Full details:. type object 'Detail' has no attribute 'EnterRange'. 'Detail::EnterRange' is not a known C++ class. 'EnterRange' is not a known C++ template. 'EnterRange' is not a known C++ enum. ```. So maybe it's something that must be ""activated"" somehow. I'd need to see how it's done with other classes/functions",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8740
https://github.com/root-project/root/pull/8740:200,testability,Trace,Traceback,200,"> Unrelated, but I hope you don't really need the double ROOT.ROOT to call this from PyROOT, I think that has been fixed years ago. I thought so too, but currently. ```py. >>> ROOT.Detail.EnterRange. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. AttributeError: <namespace cppyy.gbl.Detail at 0x56507072a2f0> has no attribute 'EnterRange'. Full details:. type object 'Detail' has no attribute 'EnterRange'. 'Detail::EnterRange' is not a known C++ class. 'EnterRange' is not a known C++ template. 'EnterRange' is not a known C++ enum. ```. So maybe it's something that must be ""activated"" somehow. I'd need to see how it's done with other classes/functions",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8740
https://github.com/root-project/root/pull/8740:211,deployability,log,logically,211,"> Also maybe I'd put the tree argument before the step argument because I don't expect the step argument to be used much? (but maybe that's also the case for tree?). I put the step first because I felt it went ""logically"" after the other two integer arguments. I really wouldn't know what could be used more between that and the tree argument",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8740
https://github.com/root-project/root/pull/8740:211,safety,log,logically,211,"> Also maybe I'd put the tree argument before the step argument because I don't expect the step argument to be used much? (but maybe that's also the case for tree?). I put the step first because I felt it went ""logically"" after the other two integer arguments. I really wouldn't know what could be used more between that and the tree argument",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8740
https://github.com/root-project/root/pull/8740:211,security,log,logically,211,"> Also maybe I'd put the tree argument before the step argument because I don't expect the step argument to be used much? (but maybe that's also the case for tree?). I put the step first because I felt it went ""logically"" after the other two integer arguments. I really wouldn't know what could be used more between that and the tree argument",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8740
https://github.com/root-project/root/pull/8740:211,testability,log,logically,211,"> Also maybe I'd put the tree argument before the step argument because I don't expect the step argument to be used much? (but maybe that's also the case for tree?). I put the step first because I felt it went ""logically"" after the other two integer arguments. I really wouldn't know what could be used more between that and the tree argument",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8740
https://github.com/root-project/root/pull/8740:9,availability,failur,failures,9,"The test failures report the file created during the test does not exist, but this node ran the test fine https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/124359/testReport/projectroot.tree.tree/test/gtest_tree_tree_test_entrylist_enterrange/ . Probably using the same filename for multiple tests leads to a recreation/deletion of the file that is then not usable by the new test?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8740
https://github.com/root-project/root/pull/8740:121,availability,servic,services,121,"The test failures report the file created during the test does not exist, but this node ran the test fine https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/124359/testReport/projectroot.tree.tree/test/gtest_tree_tree_test_entrylist_enterrange/ . Probably using the same filename for multiple tests leads to a recreation/deletion of the file that is then not usable by the new test?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8740
https://github.com/root-project/root/pull/8740:9,deployability,fail,failures,9,"The test failures report the file created during the test does not exist, but this node ran the test fine https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/124359/testReport/projectroot.tree.tree/test/gtest_tree_tree_test_entrylist_enterrange/ . Probably using the same filename for multiple tests leads to a recreation/deletion of the file that is then not usable by the new test?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8740
https://github.com/root-project/root/pull/8740:121,deployability,servic,services,121,"The test failures report the file created during the test does not exist, but this node ran the test fine https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/124359/testReport/projectroot.tree.tree/test/gtest_tree_tree_test_entrylist_enterrange/ . Probably using the same filename for multiple tests leads to a recreation/deletion of the file that is then not usable by the new test?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8740
https://github.com/root-project/root/pull/8740:173,deployability,build,build,173,"The test failures report the file created during the test does not exist, but this node ran the test fine https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/124359/testReport/projectroot.tree.tree/test/gtest_tree_tree_test_entrylist_enterrange/ . Probably using the same filename for multiple tests leads to a recreation/deletion of the file that is then not usable by the new test?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8740
https://github.com/root-project/root/pull/8740:121,integrability,servic,services,121,"The test failures report the file created during the test does not exist, but this node ran the test fine https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/124359/testReport/projectroot.tree.tree/test/gtest_tree_tree_test_entrylist_enterrange/ . Probably using the same filename for multiple tests leads to a recreation/deletion of the file that is then not usable by the new test?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8740
https://github.com/root-project/root/pull/8740:121,modifiability,servic,services,121,"The test failures report the file created during the test does not exist, but this node ran the test fine https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/124359/testReport/projectroot.tree.tree/test/gtest_tree_tree_test_entrylist_enterrange/ . Probably using the same filename for multiple tests leads to a recreation/deletion of the file that is then not usable by the new test?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8740
https://github.com/root-project/root/pull/8740:9,performance,failur,failures,9,"The test failures report the file created during the test does not exist, but this node ran the test fine https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/124359/testReport/projectroot.tree.tree/test/gtest_tree_tree_test_entrylist_enterrange/ . Probably using the same filename for multiple tests leads to a recreation/deletion of the file that is then not usable by the new test?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8740
https://github.com/root-project/root/pull/8740:9,reliability,fail,failures,9,"The test failures report the file created during the test does not exist, but this node ran the test fine https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/124359/testReport/projectroot.tree.tree/test/gtest_tree_tree_test_entrylist_enterrange/ . Probably using the same filename for multiple tests leads to a recreation/deletion of the file that is then not usable by the new test?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8740
https://github.com/root-project/root/pull/8740:58,reliability,doe,does,58,"The test failures report the file created during the test does not exist, but this node ran the test fine https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/124359/testReport/projectroot.tree.tree/test/gtest_tree_tree_test_entrylist_enterrange/ . Probably using the same filename for multiple tests leads to a recreation/deletion of the file that is then not usable by the new test?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8740
https://github.com/root-project/root/pull/8740:4,safety,test,test,4,"The test failures report the file created during the test does not exist, but this node ran the test fine https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/124359/testReport/projectroot.tree.tree/test/gtest_tree_tree_test_entrylist_enterrange/ . Probably using the same filename for multiple tests leads to a recreation/deletion of the file that is then not usable by the new test?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8740
https://github.com/root-project/root/pull/8740:53,safety,test,test,53,"The test failures report the file created during the test does not exist, but this node ran the test fine https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/124359/testReport/projectroot.tree.tree/test/gtest_tree_tree_test_entrylist_enterrange/ . Probably using the same filename for multiple tests leads to a recreation/deletion of the file that is then not usable by the new test?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8740
https://github.com/root-project/root/pull/8740:96,safety,test,test,96,"The test failures report the file created during the test does not exist, but this node ran the test fine https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/124359/testReport/projectroot.tree.tree/test/gtest_tree_tree_test_entrylist_enterrange/ . Probably using the same filename for multiple tests leads to a recreation/deletion of the file that is then not usable by the new test?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8740
https://github.com/root-project/root/pull/8740:186,safety,test,testReport,186,"The test failures report the file created during the test does not exist, but this node ran the test fine https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/124359/testReport/projectroot.tree.tree/test/gtest_tree_tree_test_entrylist_enterrange/ . Probably using the same filename for multiple tests leads to a recreation/deletion of the file that is then not usable by the new test?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8740
https://github.com/root-project/root/pull/8740:219,safety,test,test,219,"The test failures report the file created during the test does not exist, but this node ran the test fine https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/124359/testReport/projectroot.tree.tree/test/gtest_tree_tree_test_entrylist_enterrange/ . Probably using the same filename for multiple tests leads to a recreation/deletion of the file that is then not usable by the new test?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8740
https://github.com/root-project/root/pull/8740:315,safety,test,tests,315,"The test failures report the file created during the test does not exist, but this node ran the test fine https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/124359/testReport/projectroot.tree.tree/test/gtest_tree_tree_test_entrylist_enterrange/ . Probably using the same filename for multiple tests leads to a recreation/deletion of the file that is then not usable by the new test?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8740
https://github.com/root-project/root/pull/8740:399,safety,test,test,399,"The test failures report the file created during the test does not exist, but this node ran the test fine https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/124359/testReport/projectroot.tree.tree/test/gtest_tree_tree_test_entrylist_enterrange/ . Probably using the same filename for multiple tests leads to a recreation/deletion of the file that is then not usable by the new test?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8740
https://github.com/root-project/root/pull/8740:4,testability,test,test,4,"The test failures report the file created during the test does not exist, but this node ran the test fine https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/124359/testReport/projectroot.tree.tree/test/gtest_tree_tree_test_entrylist_enterrange/ . Probably using the same filename for multiple tests leads to a recreation/deletion of the file that is then not usable by the new test?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8740
https://github.com/root-project/root/pull/8740:53,testability,test,test,53,"The test failures report the file created during the test does not exist, but this node ran the test fine https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/124359/testReport/projectroot.tree.tree/test/gtest_tree_tree_test_entrylist_enterrange/ . Probably using the same filename for multiple tests leads to a recreation/deletion of the file that is then not usable by the new test?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8740
https://github.com/root-project/root/pull/8740:96,testability,test,test,96,"The test failures report the file created during the test does not exist, but this node ran the test fine https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/124359/testReport/projectroot.tree.tree/test/gtest_tree_tree_test_entrylist_enterrange/ . Probably using the same filename for multiple tests leads to a recreation/deletion of the file that is then not usable by the new test?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8740
https://github.com/root-project/root/pull/8740:186,testability,test,testReport,186,"The test failures report the file created during the test does not exist, but this node ran the test fine https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/124359/testReport/projectroot.tree.tree/test/gtest_tree_tree_test_entrylist_enterrange/ . Probably using the same filename for multiple tests leads to a recreation/deletion of the file that is then not usable by the new test?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8740
https://github.com/root-project/root/pull/8740:219,testability,test,test,219,"The test failures report the file created during the test does not exist, but this node ran the test fine https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/124359/testReport/projectroot.tree.tree/test/gtest_tree_tree_test_entrylist_enterrange/ . Probably using the same filename for multiple tests leads to a recreation/deletion of the file that is then not usable by the new test?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8740
https://github.com/root-project/root/pull/8740:315,testability,test,tests,315,"The test failures report the file created during the test does not exist, but this node ran the test fine https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/124359/testReport/projectroot.tree.tree/test/gtest_tree_tree_test_entrylist_enterrange/ . Probably using the same filename for multiple tests leads to a recreation/deletion of the file that is then not usable by the new test?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8740
https://github.com/root-project/root/pull/8740:399,testability,test,test,399,"The test failures report the file created during the test does not exist, but this node ran the test fine https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/124359/testReport/projectroot.tree.tree/test/gtest_tree_tree_test_entrylist_enterrange/ . Probably using the same filename for multiple tests leads to a recreation/deletion of the file that is then not usable by the new test?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8740
https://github.com/root-project/root/pull/8740:381,usability,usab,usable,381,"The test failures report the file created during the test does not exist, but this node ran the test fine https://lcgapp-services.cern.ch/root-jenkins/job/root-pullrequests-build/124359/testReport/projectroot.tree.tree/test/gtest_tree_tree_test_entrylist_enterrange/ . Probably using the same filename for multiple tests leads to a recreation/deletion of the file that is then not usable by the new test?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8740
https://github.com/root-project/root/pull/8740:68,performance,concurren,concurrently,68,"Using the same file in multiple tests is never good, they might run concurrently",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8740
https://github.com/root-project/root/pull/8740:32,safety,test,tests,32,"Using the same file in multiple tests is never good, they might run concurrently",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8740
https://github.com/root-project/root/pull/8740:32,testability,test,tests,32,"Using the same file in multiple tests is never good, they might run concurrently",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8740
https://github.com/root-project/root/pull/8740:70,performance,concurren,concurrently,70,"> Using the same file in multiple tests is never good, they might run concurrently. You are right, I changed the filename in the latest commit :+1:",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8740
https://github.com/root-project/root/pull/8740:34,safety,test,tests,34,"> Using the same file in multiple tests is never good, they might run concurrently. You are right, I changed the filename in the latest commit :+1:",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8740
https://github.com/root-project/root/pull/8740:34,testability,test,tests,34,"> Using the same file in multiple tests is never good, they might run concurrently. You are right, I changed the filename in the latest commit :+1:",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8740
https://github.com/root-project/root/pull/8740:13,integrability,messag,message,13,Added commit message and rebased. Will merge if tests pass,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8740
https://github.com/root-project/root/pull/8740:13,interoperability,messag,message,13,Added commit message and rebased. Will merge if tests pass,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8740
https://github.com/root-project/root/pull/8740:48,safety,test,tests,48,Added commit message and rebased. Will merge if tests pass,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8740
https://github.com/root-project/root/pull/8740:48,testability,test,tests,48,Added commit message and rebased. Will merge if tests pass,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8740
https://github.com/root-project/root/pull/8741:27,usability,behavi,behaviour,27,"I'm a bit confused by this behaviour. ```cpp. root [1] ROOT::RDataFrame(10).Define(""x"", ""rdfentry_"").Snapshot<ULong64_t>(""entries"", ""10entries.root"", {""x""}). (ROOT::RDF::RResultPtr<ROOT::RDF::RInterface<ROOT::RDF::RInterface<ROOT::Detail::RDF::RLoopManager, void>::RLoopManager> >) @0x55f76d0. root [2] TChain c{""entries""};. root [3] c.Add(""10entries.root?query#entries"");. root [4] c.Print(). ******************************************************************************. *Chain :entries : 10entries.root?query *. ******************************************************************************. ******************************************************************************. *Tree :entries : entries *. *Entries : 10 : Total = 977 bytes File Size = 469 *. * : : Tree compression factor = 1.34 *. ******************************************************************************. *Br 0 :x : x/l *. *Entries : 10 : Total Size= 628 bytes File Size = 113 *. *Baskets : 1 : Basket Size= 32000 bytes Compression= 1.34 *. *............................................................................*. root [5] auto firstfile = static_cast<TFile *>(c.GetListOfFiles()->At(0));. root [6] firstfile->GetTitle(). (const char *) ""10entries.root?query"". ```. i.e. the file then stores `?query` as part of its name?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8741
https://github.com/root-project/root/pull/8741:28,reliability,doe,does,28,"Yeah that's terrible but it does not really impact this PR, I think? The chains that we construct with the `?query#` syntax here are internal to `TTreeProcessorMT` and not meant to be inspected by users.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8741
https://github.com/root-project/root/pull/8741:197,usability,user,users,197,"Yeah that's terrible but it does not really impact this PR, I think? The chains that we construct with the `?query#` syntax here are internal to `TTreeProcessorMT` and not meant to be inspected by users.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8741
https://github.com/root-project/root/pull/8741:11,deployability,build,build,11,@phsft-bot build please,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8741
https://github.com/root-project/root/issues/8742:75,deployability,instal,installed,75,@albert-github thanks for trying out the ROOT documentation. You need root installed to make the ROOT documentation because ROOT documentation uses a filter to create on the fly the picture for all code examples we expose and also to generate the graph of libraries needed by a given class. So to make ROOT do:. - `clone https://github.com/root-project/root.git`. - `mkdir rootbuild`. - `cd rootbuild`. - `cmake ../root`. - `make -j8`. - `source bin/thisroot.sh` // this defines the $ROOTSYS environment variables and more... - cd ../root/documentation/doxygen. - Edit Doxyfile to reduce the number of paths in the `INPUT` variable otherwise it will take a day. Keep `../../io/io` for instance . - `make -j8` // this will creat `~/rootdoc`. Open `~rootdoc/html/index.html`. . We have also [this page](https://root.cern/for_developers/doxygen/) on our web site which tells a bit the same. Thanks for pointing the wrong page. I will check.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:581,energy efficiency,reduc,reduce,581,@albert-github thanks for trying out the ROOT documentation. You need root installed to make the ROOT documentation because ROOT documentation uses a filter to create on the fly the picture for all code examples we expose and also to generate the graph of libraries needed by a given class. So to make ROOT do:. - `clone https://github.com/root-project/root.git`. - `mkdir rootbuild`. - `cd rootbuild`. - `cmake ../root`. - `make -j8`. - `source bin/thisroot.sh` // this defines the $ROOTSYS environment variables and more... - cd ../root/documentation/doxygen. - Edit Doxyfile to reduce the number of paths in the `INPUT` variable otherwise it will take a day. Keep `../../io/io` for instance . - `make -j8` // this will creat `~/rootdoc`. Open `~rootdoc/html/index.html`. . We have also [this page](https://root.cern/for_developers/doxygen/) on our web site which tells a bit the same. Thanks for pointing the wrong page. I will check.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:150,integrability,filter,filter,150,@albert-github thanks for trying out the ROOT documentation. You need root installed to make the ROOT documentation because ROOT documentation uses a filter to create on the fly the picture for all code examples we expose and also to generate the graph of libraries needed by a given class. So to make ROOT do:. - `clone https://github.com/root-project/root.git`. - `mkdir rootbuild`. - `cd rootbuild`. - `cmake ../root`. - `make -j8`. - `source bin/thisroot.sh` // this defines the $ROOTSYS environment variables and more... - cd ../root/documentation/doxygen. - Edit Doxyfile to reduce the number of paths in the `INPUT` variable otherwise it will take a day. Keep `../../io/io` for instance . - `make -j8` // this will creat `~/rootdoc`. Open `~rootdoc/html/index.html`. . We have also [this page](https://root.cern/for_developers/doxygen/) on our web site which tells a bit the same. Thanks for pointing the wrong page. I will check.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:504,modifiability,variab,variables,504,@albert-github thanks for trying out the ROOT documentation. You need root installed to make the ROOT documentation because ROOT documentation uses a filter to create on the fly the picture for all code examples we expose and also to generate the graph of libraries needed by a given class. So to make ROOT do:. - `clone https://github.com/root-project/root.git`. - `mkdir rootbuild`. - `cd rootbuild`. - `cmake ../root`. - `make -j8`. - `source bin/thisroot.sh` // this defines the $ROOTSYS environment variables and more... - cd ../root/documentation/doxygen. - Edit Doxyfile to reduce the number of paths in the `INPUT` variable otherwise it will take a day. Keep `../../io/io` for instance . - `make -j8` // this will creat `~/rootdoc`. Open `~rootdoc/html/index.html`. . We have also [this page](https://root.cern/for_developers/doxygen/) on our web site which tells a bit the same. Thanks for pointing the wrong page. I will check.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:623,modifiability,variab,variable,623,@albert-github thanks for trying out the ROOT documentation. You need root installed to make the ROOT documentation because ROOT documentation uses a filter to create on the fly the picture for all code examples we expose and also to generate the graph of libraries needed by a given class. So to make ROOT do:. - `clone https://github.com/root-project/root.git`. - `mkdir rootbuild`. - `cd rootbuild`. - `cmake ../root`. - `make -j8`. - `source bin/thisroot.sh` // this defines the $ROOTSYS environment variables and more... - cd ../root/documentation/doxygen. - Edit Doxyfile to reduce the number of paths in the `INPUT` variable otherwise it will take a day. Keep `../../io/io` for instance . - `make -j8` // this will creat `~/rootdoc`. Open `~rootdoc/html/index.html`. . We have also [this page](https://root.cern/for_developers/doxygen/) on our web site which tells a bit the same. Thanks for pointing the wrong page. I will check.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:616,safety,INPUT,INPUT,616,@albert-github thanks for trying out the ROOT documentation. You need root installed to make the ROOT documentation because ROOT documentation uses a filter to create on the fly the picture for all code examples we expose and also to generate the graph of libraries needed by a given class. So to make ROOT do:. - `clone https://github.com/root-project/root.git`. - `mkdir rootbuild`. - `cd rootbuild`. - `cmake ../root`. - `make -j8`. - `source bin/thisroot.sh` // this defines the $ROOTSYS environment variables and more... - cd ../root/documentation/doxygen. - Edit Doxyfile to reduce the number of paths in the `INPUT` variable otherwise it will take a day. Keep `../../io/io` for instance . - `make -j8` // this will creat `~/rootdoc`. Open `~rootdoc/html/index.html`. . We have also [this page](https://root.cern/for_developers/doxygen/) on our web site which tells a bit the same. Thanks for pointing the wrong page. I will check.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:215,security,expos,expose,215,@albert-github thanks for trying out the ROOT documentation. You need root installed to make the ROOT documentation because ROOT documentation uses a filter to create on the fly the picture for all code examples we expose and also to generate the graph of libraries needed by a given class. So to make ROOT do:. - `clone https://github.com/root-project/root.git`. - `mkdir rootbuild`. - `cd rootbuild`. - `cmake ../root`. - `make -j8`. - `source bin/thisroot.sh` // this defines the $ROOTSYS environment variables and more... - cd ../root/documentation/doxygen. - Edit Doxyfile to reduce the number of paths in the `INPUT` variable otherwise it will take a day. Keep `../../io/io` for instance . - `make -j8` // this will creat `~/rootdoc`. Open `~rootdoc/html/index.html`. . We have also [this page](https://root.cern/for_developers/doxygen/) on our web site which tells a bit the same. Thanks for pointing the wrong page. I will check.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:46,usability,document,documentation,46,@albert-github thanks for trying out the ROOT documentation. You need root installed to make the ROOT documentation because ROOT documentation uses a filter to create on the fly the picture for all code examples we expose and also to generate the graph of libraries needed by a given class. So to make ROOT do:. - `clone https://github.com/root-project/root.git`. - `mkdir rootbuild`. - `cd rootbuild`. - `cmake ../root`. - `make -j8`. - `source bin/thisroot.sh` // this defines the $ROOTSYS environment variables and more... - cd ../root/documentation/doxygen. - Edit Doxyfile to reduce the number of paths in the `INPUT` variable otherwise it will take a day. Keep `../../io/io` for instance . - `make -j8` // this will creat `~/rootdoc`. Open `~rootdoc/html/index.html`. . We have also [this page](https://root.cern/for_developers/doxygen/) on our web site which tells a bit the same. Thanks for pointing the wrong page. I will check.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:102,usability,document,documentation,102,@albert-github thanks for trying out the ROOT documentation. You need root installed to make the ROOT documentation because ROOT documentation uses a filter to create on the fly the picture for all code examples we expose and also to generate the graph of libraries needed by a given class. So to make ROOT do:. - `clone https://github.com/root-project/root.git`. - `mkdir rootbuild`. - `cd rootbuild`. - `cmake ../root`. - `make -j8`. - `source bin/thisroot.sh` // this defines the $ROOTSYS environment variables and more... - cd ../root/documentation/doxygen. - Edit Doxyfile to reduce the number of paths in the `INPUT` variable otherwise it will take a day. Keep `../../io/io` for instance . - `make -j8` // this will creat `~/rootdoc`. Open `~rootdoc/html/index.html`. . We have also [this page](https://root.cern/for_developers/doxygen/) on our web site which tells a bit the same. Thanks for pointing the wrong page. I will check.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:129,usability,document,documentation,129,@albert-github thanks for trying out the ROOT documentation. You need root installed to make the ROOT documentation because ROOT documentation uses a filter to create on the fly the picture for all code examples we expose and also to generate the graph of libraries needed by a given class. So to make ROOT do:. - `clone https://github.com/root-project/root.git`. - `mkdir rootbuild`. - `cd rootbuild`. - `cmake ../root`. - `make -j8`. - `source bin/thisroot.sh` // this defines the $ROOTSYS environment variables and more... - cd ../root/documentation/doxygen. - Edit Doxyfile to reduce the number of paths in the `INPUT` variable otherwise it will take a day. Keep `../../io/io` for instance . - `make -j8` // this will creat `~/rootdoc`. Open `~rootdoc/html/index.html`. . We have also [this page](https://root.cern/for_developers/doxygen/) on our web site which tells a bit the same. Thanks for pointing the wrong page. I will check.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:539,usability,document,documentation,539,@albert-github thanks for trying out the ROOT documentation. You need root installed to make the ROOT documentation because ROOT documentation uses a filter to create on the fly the picture for all code examples we expose and also to generate the graph of libraries needed by a given class. So to make ROOT do:. - `clone https://github.com/root-project/root.git`. - `mkdir rootbuild`. - `cd rootbuild`. - `cmake ../root`. - `make -j8`. - `source bin/thisroot.sh` // this defines the $ROOTSYS environment variables and more... - cd ../root/documentation/doxygen. - Edit Doxyfile to reduce the number of paths in the `INPUT` variable otherwise it will take a day. Keep `../../io/io` for instance . - `make -j8` // this will creat `~/rootdoc`. Open `~rootdoc/html/index.html`. . We have also [this page](https://root.cern/for_developers/doxygen/) on our web site which tells a bit the same. Thanks for pointing the wrong page. I will check.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:616,usability,INPUT,INPUT,616,@albert-github thanks for trying out the ROOT documentation. You need root installed to make the ROOT documentation because ROOT documentation uses a filter to create on the fly the picture for all code examples we expose and also to generate the graph of libraries needed by a given class. So to make ROOT do:. - `clone https://github.com/root-project/root.git`. - `mkdir rootbuild`. - `cd rootbuild`. - `cmake ../root`. - `make -j8`. - `source bin/thisroot.sh` // this defines the $ROOTSYS environment variables and more... - cd ../root/documentation/doxygen. - Edit Doxyfile to reduce the number of paths in the `INPUT` variable otherwise it will take a day. Keep `../../io/io` for instance . - `make -j8` // this will creat `~/rootdoc`. Open `~rootdoc/html/index.html`. . We have also [this page](https://root.cern/for_developers/doxygen/) on our web site which tells a bit the same. Thanks for pointing the wrong page. I will check.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:516,availability,error,errors,516,"@couet Thanks for the information. Is my interpretation correct that the documentation is created from an inplace position (i.e the `root/documentation/doxygen`) and directly written into the root directory of the user instead of a special build directory, in case this is the case this is, I think, not so nice and should be corrected. A big pity that a ROOT is required to just for generating the documentation. I'll give it a try... Looks like I'm already failing in an early stage on the VDT build as here I get errors about M_PI (and friends)like:. ```. In file included from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/sincos.h:27,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/sin.h:30,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/vdtMath.h:6,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/src/vdtMath_signatures.cc:2:. /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/vdtcore_common.h:40:25: error: ‚ÄòM_PI‚Äô was not declared in this scope. 40 | const double TWOPI = 2.*M_PI;. | ^~~~. ```. During the cmake build I saw:. ```. -- Looking for VDT. -- Could NOT find Vdt (missing: VDT_INCLUDE_DIR VDT_LIBRARY) (Required is at least version ""0.4""). -- VDT not found. Ensure that the installation of VDT is in the CMAKE_PREFIX_PATH. -- Switching ON 'builtin_vdt' option. ```. I'm using Cygwin with g++ (GCC) 10.2.0.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:1146,availability,error,error,1146,"@couet Thanks for the information. Is my interpretation correct that the documentation is created from an inplace position (i.e the `root/documentation/doxygen`) and directly written into the root directory of the user instead of a special build directory, in case this is the case this is, I think, not so nice and should be corrected. A big pity that a ROOT is required to just for generating the documentation. I'll give it a try... Looks like I'm already failing in an early stage on the VDT build as here I get errors about M_PI (and friends)like:. ```. In file included from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/sincos.h:27,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/sin.h:30,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/vdtMath.h:6,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/src/vdtMath_signatures.cc:2:. /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/vdtcore_common.h:40:25: error: ‚ÄòM_PI‚Äô was not declared in this scope. 40 | const double TWOPI = 2.*M_PI;. | ^~~~. ```. During the cmake build I saw:. ```. -- Looking for VDT. -- Could NOT find Vdt (missing: VDT_INCLUDE_DIR VDT_LIBRARY) (Required is at least version ""0.4""). -- VDT not found. Ensure that the installation of VDT is in the CMAKE_PREFIX_PATH. -- Switching ON 'builtin_vdt' option. ```. I'm using Cygwin with g++ (GCC) 10.2.0.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:240,deployability,build,build,240,"@couet Thanks for the information. Is my interpretation correct that the documentation is created from an inplace position (i.e the `root/documentation/doxygen`) and directly written into the root directory of the user instead of a special build directory, in case this is the case this is, I think, not so nice and should be corrected. A big pity that a ROOT is required to just for generating the documentation. I'll give it a try... Looks like I'm already failing in an early stage on the VDT build as here I get errors about M_PI (and friends)like:. ```. In file included from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/sincos.h:27,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/sin.h:30,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/vdtMath.h:6,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/src/vdtMath_signatures.cc:2:. /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/vdtcore_common.h:40:25: error: ‚ÄòM_PI‚Äô was not declared in this scope. 40 | const double TWOPI = 2.*M_PI;. | ^~~~. ```. During the cmake build I saw:. ```. -- Looking for VDT. -- Could NOT find Vdt (missing: VDT_INCLUDE_DIR VDT_LIBRARY) (Required is at least version ""0.4""). -- VDT not found. Ensure that the installation of VDT is in the CMAKE_PREFIX_PATH. -- Switching ON 'builtin_vdt' option. ```. I'm using Cygwin with g++ (GCC) 10.2.0.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:459,deployability,fail,failing,459,"@couet Thanks for the information. Is my interpretation correct that the documentation is created from an inplace position (i.e the `root/documentation/doxygen`) and directly written into the root directory of the user instead of a special build directory, in case this is the case this is, I think, not so nice and should be corrected. A big pity that a ROOT is required to just for generating the documentation. I'll give it a try... Looks like I'm already failing in an early stage on the VDT build as here I get errors about M_PI (and friends)like:. ```. In file included from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/sincos.h:27,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/sin.h:30,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/vdtMath.h:6,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/src/vdtMath_signatures.cc:2:. /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/vdtcore_common.h:40:25: error: ‚ÄòM_PI‚Äô was not declared in this scope. 40 | const double TWOPI = 2.*M_PI;. | ^~~~. ```. During the cmake build I saw:. ```. -- Looking for VDT. -- Could NOT find Vdt (missing: VDT_INCLUDE_DIR VDT_LIBRARY) (Required is at least version ""0.4""). -- VDT not found. Ensure that the installation of VDT is in the CMAKE_PREFIX_PATH. -- Switching ON 'builtin_vdt' option. ```. I'm using Cygwin with g++ (GCC) 10.2.0.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:479,deployability,stage,stage,479,"@couet Thanks for the information. Is my interpretation correct that the documentation is created from an inplace position (i.e the `root/documentation/doxygen`) and directly written into the root directory of the user instead of a special build directory, in case this is the case this is, I think, not so nice and should be corrected. A big pity that a ROOT is required to just for generating the documentation. I'll give it a try... Looks like I'm already failing in an early stage on the VDT build as here I get errors about M_PI (and friends)like:. ```. In file included from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/sincos.h:27,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/sin.h:30,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/vdtMath.h:6,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/src/vdtMath_signatures.cc:2:. /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/vdtcore_common.h:40:25: error: ‚ÄòM_PI‚Äô was not declared in this scope. 40 | const double TWOPI = 2.*M_PI;. | ^~~~. ```. During the cmake build I saw:. ```. -- Looking for VDT. -- Could NOT find Vdt (missing: VDT_INCLUDE_DIR VDT_LIBRARY) (Required is at least version ""0.4""). -- VDT not found. Ensure that the installation of VDT is in the CMAKE_PREFIX_PATH. -- Switching ON 'builtin_vdt' option. ```. I'm using Cygwin with g++ (GCC) 10.2.0.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:496,deployability,build,build,496,"@couet Thanks for the information. Is my interpretation correct that the documentation is created from an inplace position (i.e the `root/documentation/doxygen`) and directly written into the root directory of the user instead of a special build directory, in case this is the case this is, I think, not so nice and should be corrected. A big pity that a ROOT is required to just for generating the documentation. I'll give it a try... Looks like I'm already failing in an early stage on the VDT build as here I get errors about M_PI (and friends)like:. ```. In file included from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/sincos.h:27,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/sin.h:30,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/vdtMath.h:6,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/src/vdtMath_signatures.cc:2:. /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/vdtcore_common.h:40:25: error: ‚ÄòM_PI‚Äô was not declared in this scope. 40 | const double TWOPI = 2.*M_PI;. | ^~~~. ```. During the cmake build I saw:. ```. -- Looking for VDT. -- Could NOT find Vdt (missing: VDT_INCLUDE_DIR VDT_LIBRARY) (Required is at least version ""0.4""). -- VDT not found. Ensure that the installation of VDT is in the CMAKE_PREFIX_PATH. -- Switching ON 'builtin_vdt' option. ```. I'm using Cygwin with g++ (GCC) 10.2.0.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:1258,deployability,build,build,1258,"@couet Thanks for the information. Is my interpretation correct that the documentation is created from an inplace position (i.e the `root/documentation/doxygen`) and directly written into the root directory of the user instead of a special build directory, in case this is the case this is, I think, not so nice and should be corrected. A big pity that a ROOT is required to just for generating the documentation. I'll give it a try... Looks like I'm already failing in an early stage on the VDT build as here I get errors about M_PI (and friends)like:. ```. In file included from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/sincos.h:27,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/sin.h:30,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/vdtMath.h:6,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/src/vdtMath_signatures.cc:2:. /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/vdtcore_common.h:40:25: error: ‚ÄòM_PI‚Äô was not declared in this scope. 40 | const double TWOPI = 2.*M_PI;. | ^~~~. ```. During the cmake build I saw:. ```. -- Looking for VDT. -- Could NOT find Vdt (missing: VDT_INCLUDE_DIR VDT_LIBRARY) (Required is at least version ""0.4""). -- VDT not found. Ensure that the installation of VDT is in the CMAKE_PREFIX_PATH. -- Switching ON 'builtin_vdt' option. ```. I'm using Cygwin with g++ (GCC) 10.2.0.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:1380,deployability,version,version,1380,"@couet Thanks for the information. Is my interpretation correct that the documentation is created from an inplace position (i.e the `root/documentation/doxygen`) and directly written into the root directory of the user instead of a special build directory, in case this is the case this is, I think, not so nice and should be corrected. A big pity that a ROOT is required to just for generating the documentation. I'll give it a try... Looks like I'm already failing in an early stage on the VDT build as here I get errors about M_PI (and friends)like:. ```. In file included from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/sincos.h:27,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/sin.h:30,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/vdtMath.h:6,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/src/vdtMath_signatures.cc:2:. /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/vdtcore_common.h:40:25: error: ‚ÄòM_PI‚Äô was not declared in this scope. 40 | const double TWOPI = 2.*M_PI;. | ^~~~. ```. During the cmake build I saw:. ```. -- Looking for VDT. -- Could NOT find Vdt (missing: VDT_INCLUDE_DIR VDT_LIBRARY) (Required is at least version ""0.4""). -- VDT not found. Ensure that the installation of VDT is in the CMAKE_PREFIX_PATH. -- Switching ON 'builtin_vdt' option. ```. I'm using Cygwin with g++ (GCC) 10.2.0.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:1430,deployability,instal,installation,1430,"@couet Thanks for the information. Is my interpretation correct that the documentation is created from an inplace position (i.e the `root/documentation/doxygen`) and directly written into the root directory of the user instead of a special build directory, in case this is the case this is, I think, not so nice and should be corrected. A big pity that a ROOT is required to just for generating the documentation. I'll give it a try... Looks like I'm already failing in an early stage on the VDT build as here I get errors about M_PI (and friends)like:. ```. In file included from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/sincos.h:27,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/sin.h:30,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/vdtMath.h:6,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/src/vdtMath_signatures.cc:2:. /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/vdtcore_common.h:40:25: error: ‚ÄòM_PI‚Äô was not declared in this scope. 40 | const double TWOPI = 2.*M_PI;. | ^~~~. ```. During the cmake build I saw:. ```. -- Looking for VDT. -- Could NOT find Vdt (missing: VDT_INCLUDE_DIR VDT_LIBRARY) (Required is at least version ""0.4""). -- VDT not found. Ensure that the installation of VDT is in the CMAKE_PREFIX_PATH. -- Switching ON 'builtin_vdt' option. ```. I'm using Cygwin with g++ (GCC) 10.2.0.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:1380,integrability,version,version,1380,"@couet Thanks for the information. Is my interpretation correct that the documentation is created from an inplace position (i.e the `root/documentation/doxygen`) and directly written into the root directory of the user instead of a special build directory, in case this is the case this is, I think, not so nice and should be corrected. A big pity that a ROOT is required to just for generating the documentation. I'll give it a try... Looks like I'm already failing in an early stage on the VDT build as here I get errors about M_PI (and friends)like:. ```. In file included from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/sincos.h:27,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/sin.h:30,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/vdtMath.h:6,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/src/vdtMath_signatures.cc:2:. /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/vdtcore_common.h:40:25: error: ‚ÄòM_PI‚Äô was not declared in this scope. 40 | const double TWOPI = 2.*M_PI;. | ^~~~. ```. During the cmake build I saw:. ```. -- Looking for VDT. -- Could NOT find Vdt (missing: VDT_INCLUDE_DIR VDT_LIBRARY) (Required is at least version ""0.4""). -- VDT not found. Ensure that the installation of VDT is in the CMAKE_PREFIX_PATH. -- Switching ON 'builtin_vdt' option. ```. I'm using Cygwin with g++ (GCC) 10.2.0.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:1380,modifiability,version,version,1380,"@couet Thanks for the information. Is my interpretation correct that the documentation is created from an inplace position (i.e the `root/documentation/doxygen`) and directly written into the root directory of the user instead of a special build directory, in case this is the case this is, I think, not so nice and should be corrected. A big pity that a ROOT is required to just for generating the documentation. I'll give it a try... Looks like I'm already failing in an early stage on the VDT build as here I get errors about M_PI (and friends)like:. ```. In file included from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/sincos.h:27,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/sin.h:30,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/vdtMath.h:6,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/src/vdtMath_signatures.cc:2:. /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/vdtcore_common.h:40:25: error: ‚ÄòM_PI‚Äô was not declared in this scope. 40 | const double TWOPI = 2.*M_PI;. | ^~~~. ```. During the cmake build I saw:. ```. -- Looking for VDT. -- Could NOT find Vdt (missing: VDT_INCLUDE_DIR VDT_LIBRARY) (Required is at least version ""0.4""). -- VDT not found. Ensure that the installation of VDT is in the CMAKE_PREFIX_PATH. -- Switching ON 'builtin_vdt' option. ```. I'm using Cygwin with g++ (GCC) 10.2.0.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:516,performance,error,errors,516,"@couet Thanks for the information. Is my interpretation correct that the documentation is created from an inplace position (i.e the `root/documentation/doxygen`) and directly written into the root directory of the user instead of a special build directory, in case this is the case this is, I think, not so nice and should be corrected. A big pity that a ROOT is required to just for generating the documentation. I'll give it a try... Looks like I'm already failing in an early stage on the VDT build as here I get errors about M_PI (and friends)like:. ```. In file included from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/sincos.h:27,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/sin.h:30,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/vdtMath.h:6,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/src/vdtMath_signatures.cc:2:. /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/vdtcore_common.h:40:25: error: ‚ÄòM_PI‚Äô was not declared in this scope. 40 | const double TWOPI = 2.*M_PI;. | ^~~~. ```. During the cmake build I saw:. ```. -- Looking for VDT. -- Could NOT find Vdt (missing: VDT_INCLUDE_DIR VDT_LIBRARY) (Required is at least version ""0.4""). -- VDT not found. Ensure that the installation of VDT is in the CMAKE_PREFIX_PATH. -- Switching ON 'builtin_vdt' option. ```. I'm using Cygwin with g++ (GCC) 10.2.0.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:1146,performance,error,error,1146,"@couet Thanks for the information. Is my interpretation correct that the documentation is created from an inplace position (i.e the `root/documentation/doxygen`) and directly written into the root directory of the user instead of a special build directory, in case this is the case this is, I think, not so nice and should be corrected. A big pity that a ROOT is required to just for generating the documentation. I'll give it a try... Looks like I'm already failing in an early stage on the VDT build as here I get errors about M_PI (and friends)like:. ```. In file included from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/sincos.h:27,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/sin.h:30,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/vdtMath.h:6,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/src/vdtMath_signatures.cc:2:. /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/vdtcore_common.h:40:25: error: ‚ÄòM_PI‚Äô was not declared in this scope. 40 | const double TWOPI = 2.*M_PI;. | ^~~~. ```. During the cmake build I saw:. ```. -- Looking for VDT. -- Could NOT find Vdt (missing: VDT_INCLUDE_DIR VDT_LIBRARY) (Required is at least version ""0.4""). -- VDT not found. Ensure that the installation of VDT is in the CMAKE_PREFIX_PATH. -- Switching ON 'builtin_vdt' option. ```. I'm using Cygwin with g++ (GCC) 10.2.0.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:459,reliability,fail,failing,459,"@couet Thanks for the information. Is my interpretation correct that the documentation is created from an inplace position (i.e the `root/documentation/doxygen`) and directly written into the root directory of the user instead of a special build directory, in case this is the case this is, I think, not so nice and should be corrected. A big pity that a ROOT is required to just for generating the documentation. I'll give it a try... Looks like I'm already failing in an early stage on the VDT build as here I get errors about M_PI (and friends)like:. ```. In file included from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/sincos.h:27,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/sin.h:30,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/vdtMath.h:6,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/src/vdtMath_signatures.cc:2:. /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/vdtcore_common.h:40:25: error: ‚ÄòM_PI‚Äô was not declared in this scope. 40 | const double TWOPI = 2.*M_PI;. | ^~~~. ```. During the cmake build I saw:. ```. -- Looking for VDT. -- Could NOT find Vdt (missing: VDT_INCLUDE_DIR VDT_LIBRARY) (Required is at least version ""0.4""). -- VDT not found. Ensure that the installation of VDT is in the CMAKE_PREFIX_PATH. -- Switching ON 'builtin_vdt' option. ```. I'm using Cygwin with g++ (GCC) 10.2.0.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:516,safety,error,errors,516,"@couet Thanks for the information. Is my interpretation correct that the documentation is created from an inplace position (i.e the `root/documentation/doxygen`) and directly written into the root directory of the user instead of a special build directory, in case this is the case this is, I think, not so nice and should be corrected. A big pity that a ROOT is required to just for generating the documentation. I'll give it a try... Looks like I'm already failing in an early stage on the VDT build as here I get errors about M_PI (and friends)like:. ```. In file included from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/sincos.h:27,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/sin.h:30,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/vdtMath.h:6,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/src/vdtMath_signatures.cc:2:. /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/vdtcore_common.h:40:25: error: ‚ÄòM_PI‚Äô was not declared in this scope. 40 | const double TWOPI = 2.*M_PI;. | ^~~~. ```. During the cmake build I saw:. ```. -- Looking for VDT. -- Could NOT find Vdt (missing: VDT_INCLUDE_DIR VDT_LIBRARY) (Required is at least version ""0.4""). -- VDT not found. Ensure that the installation of VDT is in the CMAKE_PREFIX_PATH. -- Switching ON 'builtin_vdt' option. ```. I'm using Cygwin with g++ (GCC) 10.2.0.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:1146,safety,error,error,1146,"@couet Thanks for the information. Is my interpretation correct that the documentation is created from an inplace position (i.e the `root/documentation/doxygen`) and directly written into the root directory of the user instead of a special build directory, in case this is the case this is, I think, not so nice and should be corrected. A big pity that a ROOT is required to just for generating the documentation. I'll give it a try... Looks like I'm already failing in an early stage on the VDT build as here I get errors about M_PI (and friends)like:. ```. In file included from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/sincos.h:27,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/sin.h:30,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/vdtMath.h:6,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/src/vdtMath_signatures.cc:2:. /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/vdtcore_common.h:40:25: error: ‚ÄòM_PI‚Äô was not declared in this scope. 40 | const double TWOPI = 2.*M_PI;. | ^~~~. ```. During the cmake build I saw:. ```. -- Looking for VDT. -- Could NOT find Vdt (missing: VDT_INCLUDE_DIR VDT_LIBRARY) (Required is at least version ""0.4""). -- VDT not found. Ensure that the installation of VDT is in the CMAKE_PREFIX_PATH. -- Switching ON 'builtin_vdt' option. ```. I'm using Cygwin with g++ (GCC) 10.2.0.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:73,usability,document,documentation,73,"@couet Thanks for the information. Is my interpretation correct that the documentation is created from an inplace position (i.e the `root/documentation/doxygen`) and directly written into the root directory of the user instead of a special build directory, in case this is the case this is, I think, not so nice and should be corrected. A big pity that a ROOT is required to just for generating the documentation. I'll give it a try... Looks like I'm already failing in an early stage on the VDT build as here I get errors about M_PI (and friends)like:. ```. In file included from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/sincos.h:27,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/sin.h:30,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/vdtMath.h:6,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/src/vdtMath_signatures.cc:2:. /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/vdtcore_common.h:40:25: error: ‚ÄòM_PI‚Äô was not declared in this scope. 40 | const double TWOPI = 2.*M_PI;. | ^~~~. ```. During the cmake build I saw:. ```. -- Looking for VDT. -- Could NOT find Vdt (missing: VDT_INCLUDE_DIR VDT_LIBRARY) (Required is at least version ""0.4""). -- VDT not found. Ensure that the installation of VDT is in the CMAKE_PREFIX_PATH. -- Switching ON 'builtin_vdt' option. ```. I'm using Cygwin with g++ (GCC) 10.2.0.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:138,usability,document,documentation,138,"@couet Thanks for the information. Is my interpretation correct that the documentation is created from an inplace position (i.e the `root/documentation/doxygen`) and directly written into the root directory of the user instead of a special build directory, in case this is the case this is, I think, not so nice and should be corrected. A big pity that a ROOT is required to just for generating the documentation. I'll give it a try... Looks like I'm already failing in an early stage on the VDT build as here I get errors about M_PI (and friends)like:. ```. In file included from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/sincos.h:27,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/sin.h:30,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/vdtMath.h:6,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/src/vdtMath_signatures.cc:2:. /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/vdtcore_common.h:40:25: error: ‚ÄòM_PI‚Äô was not declared in this scope. 40 | const double TWOPI = 2.*M_PI;. | ^~~~. ```. During the cmake build I saw:. ```. -- Looking for VDT. -- Could NOT find Vdt (missing: VDT_INCLUDE_DIR VDT_LIBRARY) (Required is at least version ""0.4""). -- VDT not found. Ensure that the installation of VDT is in the CMAKE_PREFIX_PATH. -- Switching ON 'builtin_vdt' option. ```. I'm using Cygwin with g++ (GCC) 10.2.0.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:214,usability,user,user,214,"@couet Thanks for the information. Is my interpretation correct that the documentation is created from an inplace position (i.e the `root/documentation/doxygen`) and directly written into the root directory of the user instead of a special build directory, in case this is the case this is, I think, not so nice and should be corrected. A big pity that a ROOT is required to just for generating the documentation. I'll give it a try... Looks like I'm already failing in an early stage on the VDT build as here I get errors about M_PI (and friends)like:. ```. In file included from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/sincos.h:27,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/sin.h:30,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/vdtMath.h:6,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/src/vdtMath_signatures.cc:2:. /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/vdtcore_common.h:40:25: error: ‚ÄòM_PI‚Äô was not declared in this scope. 40 | const double TWOPI = 2.*M_PI;. | ^~~~. ```. During the cmake build I saw:. ```. -- Looking for VDT. -- Could NOT find Vdt (missing: VDT_INCLUDE_DIR VDT_LIBRARY) (Required is at least version ""0.4""). -- VDT not found. Ensure that the installation of VDT is in the CMAKE_PREFIX_PATH. -- Switching ON 'builtin_vdt' option. ```. I'm using Cygwin with g++ (GCC) 10.2.0.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:399,usability,document,documentation,399,"@couet Thanks for the information. Is my interpretation correct that the documentation is created from an inplace position (i.e the `root/documentation/doxygen`) and directly written into the root directory of the user instead of a special build directory, in case this is the case this is, I think, not so nice and should be corrected. A big pity that a ROOT is required to just for generating the documentation. I'll give it a try... Looks like I'm already failing in an early stage on the VDT build as here I get errors about M_PI (and friends)like:. ```. In file included from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/sincos.h:27,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/sin.h:30,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/vdtMath.h:6,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/src/vdtMath_signatures.cc:2:. /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/vdtcore_common.h:40:25: error: ‚ÄòM_PI‚Äô was not declared in this scope. 40 | const double TWOPI = 2.*M_PI;. | ^~~~. ```. During the cmake build I saw:. ```. -- Looking for VDT. -- Could NOT find Vdt (missing: VDT_INCLUDE_DIR VDT_LIBRARY) (Required is at least version ""0.4""). -- VDT not found. Ensure that the installation of VDT is in the CMAKE_PREFIX_PATH. -- Switching ON 'builtin_vdt' option. ```. I'm using Cygwin with g++ (GCC) 10.2.0.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:516,usability,error,errors,516,"@couet Thanks for the information. Is my interpretation correct that the documentation is created from an inplace position (i.e the `root/documentation/doxygen`) and directly written into the root directory of the user instead of a special build directory, in case this is the case this is, I think, not so nice and should be corrected. A big pity that a ROOT is required to just for generating the documentation. I'll give it a try... Looks like I'm already failing in an early stage on the VDT build as here I get errors about M_PI (and friends)like:. ```. In file included from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/sincos.h:27,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/sin.h:30,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/vdtMath.h:6,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/src/vdtMath_signatures.cc:2:. /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/vdtcore_common.h:40:25: error: ‚ÄòM_PI‚Äô was not declared in this scope. 40 | const double TWOPI = 2.*M_PI;. | ^~~~. ```. During the cmake build I saw:. ```. -- Looking for VDT. -- Could NOT find Vdt (missing: VDT_INCLUDE_DIR VDT_LIBRARY) (Required is at least version ""0.4""). -- VDT not found. Ensure that the installation of VDT is in the CMAKE_PREFIX_PATH. -- Switching ON 'builtin_vdt' option. ```. I'm using Cygwin with g++ (GCC) 10.2.0.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:1146,usability,error,error,1146,"@couet Thanks for the information. Is my interpretation correct that the documentation is created from an inplace position (i.e the `root/documentation/doxygen`) and directly written into the root directory of the user instead of a special build directory, in case this is the case this is, I think, not so nice and should be corrected. A big pity that a ROOT is required to just for generating the documentation. I'll give it a try... Looks like I'm already failing in an early stage on the VDT build as here I get errors about M_PI (and friends)like:. ```. In file included from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/sincos.h:27,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/sin.h:30,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/vdtMath.h:6,. from /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/src/vdtMath_signatures.cc:2:. /cygdrive/e/Programs/github_repositories/normal/root/build_normal/VDT-prefix/src/VDT/include/vdtcore_common.h:40:25: error: ‚ÄòM_PI‚Äô was not declared in this scope. 40 | const double TWOPI = 2.*M_PI;. | ^~~~. ```. During the cmake build I saw:. ```. -- Looking for VDT. -- Could NOT find Vdt (missing: VDT_INCLUDE_DIR VDT_LIBRARY) (Required is at least version ""0.4""). -- VDT not found. Ensure that the installation of VDT is in the CMAKE_PREFIX_PATH. -- Switching ON 'builtin_vdt' option. ```. I'm using Cygwin with g++ (GCC) 10.2.0.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:11,usability,document,documentation,11,- The ROOT documentation is a big stuff. We want all the pictures in the doc to be generated on the fly. We need ROOT for that. - The documentation is created in `~/rootdoc` when you type make in the in `$ROOTSYS/documentation/doxygen`. - Ah with Cygwin .. @bellenot may help. `loopdir.C` is fixed here https://github.com/root-project/root/pull/8749 Thanks,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:134,usability,document,documentation,134,- The ROOT documentation is a big stuff. We want all the pictures in the doc to be generated on the fly. We need ROOT for that. - The documentation is created in `~/rootdoc` when you type make in the in `$ROOTSYS/documentation/doxygen`. - Ah with Cygwin .. @bellenot may help. `loopdir.C` is fixed here https://github.com/root-project/root/pull/8749 Thanks,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:213,usability,document,documentation,213,- The ROOT documentation is a big stuff. We want all the pictures in the doc to be generated on the fly. We need ROOT for that. - The documentation is created in `~/rootdoc` when you type make in the in `$ROOTSYS/documentation/doxygen`. - Ah with Cygwin .. @bellenot may help. `loopdir.C` is fixed here https://github.com/root-project/root/pull/8749 Thanks,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:271,usability,help,help,271,- The ROOT documentation is a big stuff. We want all the pictures in the doc to be generated on the fly. We need ROOT for that. - The documentation is created in `~/rootdoc` when you type make in the in `$ROOTSYS/documentation/doxygen`. - Ah with Cygwin .. @bellenot may help. `loopdir.C` is fixed here https://github.com/root-project/root/pull/8749 Thanks,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:19,usability,support,supported,19,"FYI, Cygwin is not supported",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:22,deployability,build,build,22,@couet . In the CMake build log I see:. ```. -- Checking internet connectivity... -- Yes. ```. does this mean an internet connection is required? (would not be nice when just building the documentation).,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:28,deployability,log,log,28,@couet . In the CMake build log I see:. ```. -- Checking internet connectivity... -- Yes. ```. does this mean an internet connection is required? (would not be nice when just building the documentation).,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:175,deployability,build,building,175,@couet . In the CMake build log I see:. ```. -- Checking internet connectivity... -- Yes. ```. does this mean an internet connection is required? (would not be nice when just building the documentation).,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:95,reliability,doe,does,95,@couet . In the CMake build log I see:. ```. -- Checking internet connectivity... -- Yes. ```. does this mean an internet connection is required? (would not be nice when just building the documentation).,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:28,safety,log,log,28,@couet . In the CMake build log I see:. ```. -- Checking internet connectivity... -- Yes. ```. does this mean an internet connection is required? (would not be nice when just building the documentation).,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:28,security,log,log,28,@couet . In the CMake build log I see:. ```. -- Checking internet connectivity... -- Yes. ```. does this mean an internet connection is required? (would not be nice when just building the documentation).,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:28,testability,log,log,28,@couet . In the CMake build log I see:. ```. -- Checking internet connectivity... -- Yes. ```. does this mean an internet connection is required? (would not be nice when just building the documentation).,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:188,usability,document,documentation,188,@couet . In the CMake build log I see:. ```. -- Checking internet connectivity... -- Yes. ```. does this mean an internet connection is required? (would not be nice when just building the documentation).,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:25,deployability,build,build,25,"> @couet. > In the CMake build log I see:. > . > ```. > -- Checking internet connectivity... > -- Yes. > ```. > . > does this mean an internet connection is required? (would not be nice when just building the documentation). > . No, it's just disabling the external packages requiring internet access if there is no internet connection.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:31,deployability,log,log,31,"> @couet. > In the CMake build log I see:. > . > ```. > -- Checking internet connectivity... > -- Yes. > ```. > . > does this mean an internet connection is required? (would not be nice when just building the documentation). > . No, it's just disabling the external packages requiring internet access if there is no internet connection.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:196,deployability,build,building,196,"> @couet. > In the CMake build log I see:. > . > ```. > -- Checking internet connectivity... > -- Yes. > ```. > . > does this mean an internet connection is required? (would not be nice when just building the documentation). > . No, it's just disabling the external packages requiring internet access if there is no internet connection.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:266,modifiability,pac,packages,266,"> @couet. > In the CMake build log I see:. > . > ```. > -- Checking internet connectivity... > -- Yes. > ```. > . > does this mean an internet connection is required? (would not be nice when just building the documentation). > . No, it's just disabling the external packages requiring internet access if there is no internet connection.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:116,reliability,doe,does,116,"> @couet. > In the CMake build log I see:. > . > ```. > -- Checking internet connectivity... > -- Yes. > ```. > . > does this mean an internet connection is required? (would not be nice when just building the documentation). > . No, it's just disabling the external packages requiring internet access if there is no internet connection.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:31,safety,log,log,31,"> @couet. > In the CMake build log I see:. > . > ```. > -- Checking internet connectivity... > -- Yes. > ```. > . > does this mean an internet connection is required? (would not be nice when just building the documentation). > . No, it's just disabling the external packages requiring internet access if there is no internet connection.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:31,security,log,log,31,"> @couet. > In the CMake build log I see:. > . > ```. > -- Checking internet connectivity... > -- Yes. > ```. > . > does this mean an internet connection is required? (would not be nice when just building the documentation). > . No, it's just disabling the external packages requiring internet access if there is no internet connection.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:294,security,access,access,294,"> @couet. > In the CMake build log I see:. > . > ```. > -- Checking internet connectivity... > -- Yes. > ```. > . > does this mean an internet connection is required? (would not be nice when just building the documentation). > . No, it's just disabling the external packages requiring internet access if there is no internet connection.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:31,testability,log,log,31,"> @couet. > In the CMake build log I see:. > . > ```. > -- Checking internet connectivity... > -- Yes. > ```. > . > does this mean an internet connection is required? (would not be nice when just building the documentation). > . No, it's just disabling the external packages requiring internet access if there is no internet connection.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:209,usability,document,documentation,209,"> @couet. > In the CMake build log I see:. > . > ```. > -- Checking internet connectivity... > -- Yes. > ```. > . > does this mean an internet connection is required? (would not be nice when just building the documentation). > . No, it's just disabling the external packages requiring internet access if there is no internet connection.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:131,deployability,build,build,131,@bellenot Thanks for the information. Is it possible to disable the Internet connection with an option during the cmake (and later build) phase? I'll give Windows a try (not as nice as now nmake cannot run in parallel ...).,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:209,performance,parallel,parallel,209,@bellenot Thanks for the information. Is it possible to disable the Internet connection with an option during the cmake (and later build) phase? I'll give Windows a try (not as nice as now nmake cannot run in parallel ...).,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:95,deployability,build,build,95,"> Is it possible to disable the Internet connection with an option during the cmake (and later build) phase? Well, no, you can simply unplug (or disable) the internet connection. You can also use the `-Dminimal=ON` flag to reduce the number of packages",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:223,energy efficiency,reduc,reduce,223,"> Is it possible to disable the Internet connection with an option during the cmake (and later build) phase? Well, no, you can simply unplug (or disable) the internet connection. You can also use the `-Dminimal=ON` flag to reduce the number of packages",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:244,modifiability,pac,packages,244,"> Is it possible to disable the Internet connection with an option during the cmake (and later build) phase? Well, no, you can simply unplug (or disable) the internet connection. You can also use the `-Dminimal=ON` flag to reduce the number of packages",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:127,testability,simpl,simply,127,"> Is it possible to disable the Internet connection with an option during the cmake (and later build) phase? Well, no, you can simply unplug (or disable) the internet connection. You can also use the `-Dminimal=ON` flag to reduce the number of packages",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:127,usability,simpl,simply,127,"> Is it possible to disable the Internet connection with an option during the cmake (and later build) phase? Well, no, you can simply unplug (or disable) the internet connection. You can also use the `-Dminimal=ON` flag to reduce the number of packages",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:143,availability,error,errors,143,"> Well, no, you can simply unplug (or disable) the internet connection. Not really a good idea. I tried to run cmake for windows, but got some errors here as well, unfortunately stopping my attempts to build ROOT and especially to build the documentation.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:202,deployability,build,build,202,"> Well, no, you can simply unplug (or disable) the internet connection. Not really a good idea. I tried to run cmake for windows, but got some errors here as well, unfortunately stopping my attempts to build ROOT and especially to build the documentation.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:231,deployability,build,build,231,"> Well, no, you can simply unplug (or disable) the internet connection. Not really a good idea. I tried to run cmake for windows, but got some errors here as well, unfortunately stopping my attempts to build ROOT and especially to build the documentation.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:143,performance,error,errors,143,"> Well, no, you can simply unplug (or disable) the internet connection. Not really a good idea. I tried to run cmake for windows, but got some errors here as well, unfortunately stopping my attempts to build ROOT and especially to build the documentation.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:143,safety,error,errors,143,"> Well, no, you can simply unplug (or disable) the internet connection. Not really a good idea. I tried to run cmake for windows, but got some errors here as well, unfortunately stopping my attempts to build ROOT and especially to build the documentation.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:20,testability,simpl,simply,20,"> Well, no, you can simply unplug (or disable) the internet connection. Not really a good idea. I tried to run cmake for windows, but got some errors here as well, unfortunately stopping my attempts to build ROOT and especially to build the documentation.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:20,usability,simpl,simply,20,"> Well, no, you can simply unplug (or disable) the internet connection. Not really a good idea. I tried to run cmake for windows, but got some errors here as well, unfortunately stopping my attempts to build ROOT and especially to build the documentation.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:143,usability,error,errors,143,"> Well, no, you can simply unplug (or disable) the internet connection. Not really a good idea. I tried to run cmake for windows, but got some errors here as well, unfortunately stopping my attempts to build ROOT and especially to build the documentation.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:178,usability,stop,stopping,178,"> Well, no, you can simply unplug (or disable) the internet connection. Not really a good idea. I tried to run cmake for windows, but got some errors here as well, unfortunately stopping my attempts to build ROOT and especially to build the documentation.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:241,usability,document,documentation,241,"> Well, no, you can simply unplug (or disable) the internet connection. Not really a good idea. I tried to run cmake for windows, but got some errors here as well, unfortunately stopping my attempts to build ROOT and especially to build the documentation.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:80,availability,error,errors,80,"@albert-github BTW, why do you want to do it without internet? And what are the errors you got on Windows?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:80,performance,error,errors,80,"@albert-github BTW, why do you want to do it without internet? And what are the errors you got on Windows?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:80,safety,error,errors,80,"@albert-github BTW, why do you want to do it without internet? And what are the errors you got on Windows?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:80,usability,error,errors,80,"@albert-github BTW, why do you want to do it without internet? And what are the errors you got on Windows?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:52,deployability,instal,install,52,And ROOT only support 32 bit. See https://root.cern/install/#build-from-source,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:61,deployability,build,build-from-source,61,And ROOT only support 32 bit. See https://root.cern/install/#build-from-source,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:14,usability,support,support,14,And ROOT only support 32 bit. See https://root.cern/install/#build-from-source,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:13,deployability,build,build,13,You can also build the documentation for just the parts you need by modifying the Doxyfile. That way you don't have to build the whole thing but just the part(s) you want.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:119,deployability,build,build,119,You can also build the documentation for just the parts you need by modifying the Doxyfile. That way you don't have to build the whole thing but just the part(s) you want.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:68,security,modif,modifying,68,You can also build the documentation for just the parts you need by modifying the Doxyfile. That way you don't have to build the whole thing but just the part(s) you want.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:23,usability,document,documentation,23,You can also build the documentation for just the parts you need by modifying the Doxyfile. That way you don't have to build the whole thing but just the part(s) you want.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:70,deployability,build,build,70,"@AdvaitDhingra isn't that what @couet suggested by mentioning to just build the io part? But you still need to set environment variables like in the setting for: `EXAMPLE_PATH = $(DOXYGEN_EXAMPLE_PATH)` and you need to build the filter program`INPUT_FILTER = ./filter`. Furthermore I got from the discussion that some images needed to be generated, so ROOT has to be build...",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:219,deployability,build,build,219,"@AdvaitDhingra isn't that what @couet suggested by mentioning to just build the io part? But you still need to set environment variables like in the setting for: `EXAMPLE_PATH = $(DOXYGEN_EXAMPLE_PATH)` and you need to build the filter program`INPUT_FILTER = ./filter`. Furthermore I got from the discussion that some images needed to be generated, so ROOT has to be build...",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:367,deployability,build,build,367,"@AdvaitDhingra isn't that what @couet suggested by mentioning to just build the io part? But you still need to set environment variables like in the setting for: `EXAMPLE_PATH = $(DOXYGEN_EXAMPLE_PATH)` and you need to build the filter program`INPUT_FILTER = ./filter`. Furthermore I got from the discussion that some images needed to be generated, so ROOT has to be build...",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:229,integrability,filter,filter,229,"@AdvaitDhingra isn't that what @couet suggested by mentioning to just build the io part? But you still need to set environment variables like in the setting for: `EXAMPLE_PATH = $(DOXYGEN_EXAMPLE_PATH)` and you need to build the filter program`INPUT_FILTER = ./filter`. Furthermore I got from the discussion that some images needed to be generated, so ROOT has to be build...",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:261,integrability,filter,filter,261,"@AdvaitDhingra isn't that what @couet suggested by mentioning to just build the io part? But you still need to set environment variables like in the setting for: `EXAMPLE_PATH = $(DOXYGEN_EXAMPLE_PATH)` and you need to build the filter program`INPUT_FILTER = ./filter`. Furthermore I got from the discussion that some images needed to be generated, so ROOT has to be build...",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:127,modifiability,variab,variables,127,"@AdvaitDhingra isn't that what @couet suggested by mentioning to just build the io part? But you still need to set environment variables like in the setting for: `EXAMPLE_PATH = $(DOXYGEN_EXAMPLE_PATH)` and you need to build the filter program`INPUT_FILTER = ./filter`. Furthermore I got from the discussion that some images needed to be generated, so ROOT has to be build...",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:431,availability,error,error,431,@bellenot . Well just to build the documentation why should I connect to the Internet and especially don't know what is retrieved / send (this is of course always a concern). The problems in windows have to do with python usage. As I have a mixed system Windows / Cygwin this gives some problems. I now disabled the cygwin in the path and followed the instructions as provided. the cmake generation looks like not to give the word error anymore. Now building ...,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:25,deployability,build,build,25,@bellenot . Well just to build the documentation why should I connect to the Internet and especially don't know what is retrieved / send (this is of course always a concern). The problems in windows have to do with python usage. As I have a mixed system Windows / Cygwin this gives some problems. I now disabled the cygwin in the path and followed the instructions as provided. the cmake generation looks like not to give the word error anymore. Now building ...,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:450,deployability,build,building,450,@bellenot . Well just to build the documentation why should I connect to the Internet and especially don't know what is retrieved / send (this is of course always a concern). The problems in windows have to do with python usage. As I have a mixed system Windows / Cygwin this gives some problems. I now disabled the cygwin in the path and followed the instructions as provided. the cmake generation looks like not to give the word error anymore. Now building ...,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:165,modifiability,concern,concern,165,@bellenot . Well just to build the documentation why should I connect to the Internet and especially don't know what is retrieved / send (this is of course always a concern). The problems in windows have to do with python usage. As I have a mixed system Windows / Cygwin this gives some problems. I now disabled the cygwin in the path and followed the instructions as provided. the cmake generation looks like not to give the word error anymore. Now building ...,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:431,performance,error,error,431,@bellenot . Well just to build the documentation why should I connect to the Internet and especially don't know what is retrieved / send (this is of course always a concern). The problems in windows have to do with python usage. As I have a mixed system Windows / Cygwin this gives some problems. I now disabled the cygwin in the path and followed the instructions as provided. the cmake generation looks like not to give the word error anymore. Now building ...,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:431,safety,error,error,431,@bellenot . Well just to build the documentation why should I connect to the Internet and especially don't know what is retrieved / send (this is of course always a concern). The problems in windows have to do with python usage. As I have a mixed system Windows / Cygwin this gives some problems. I now disabled the cygwin in the path and followed the instructions as provided. the cmake generation looks like not to give the word error anymore. Now building ...,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:165,testability,concern,concern,165,@bellenot . Well just to build the documentation why should I connect to the Internet and especially don't know what is retrieved / send (this is of course always a concern). The problems in windows have to do with python usage. As I have a mixed system Windows / Cygwin this gives some problems. I now disabled the cygwin in the path and followed the instructions as provided. the cmake generation looks like not to give the word error anymore. Now building ...,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:35,usability,document,documentation,35,@bellenot . Well just to build the documentation why should I connect to the Internet and especially don't know what is retrieved / send (this is of course always a concern). The problems in windows have to do with python usage. As I have a mixed system Windows / Cygwin this gives some problems. I now disabled the cygwin in the path and followed the instructions as provided. the cmake generation looks like not to give the word error anymore. Now building ...,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:431,usability,error,error,431,@bellenot . Well just to build the documentation why should I connect to the Internet and especially don't know what is retrieved / send (this is of course always a concern). The problems in windows have to do with python usage. As I have a mixed system Windows / Cygwin this gives some problems. I now disabled the cygwin in the path and followed the instructions as provided. the cmake generation looks like not to give the word error anymore. Now building ...,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:93,deployability,build,build,93,@AdvaitDhingra as @albert-github said: that's what I was suggesting . And yes ROOT has to be build. The ROOT-Doxygen filter runs ROOT several hundred of times during the doc build.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:174,deployability,build,build,174,@AdvaitDhingra as @albert-github said: that's what I was suggesting . And yes ROOT has to be build. The ROOT-Doxygen filter runs ROOT several hundred of times during the doc build.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:117,integrability,filter,filter,117,@AdvaitDhingra as @albert-github said: that's what I was suggesting . And yes ROOT has to be build. The ROOT-Doxygen filter runs ROOT several hundred of times during the doc build.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:153,performance,time,times,153,@AdvaitDhingra as @albert-github said: that's what I was suggesting . And yes ROOT has to be build. The ROOT-Doxygen filter runs ROOT several hundred of times during the doc build.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:25,deployability,build,build,25,"@couet, @bellenot How to build the documentation under windows. The `Makefile` in `documentation/doxygen` looks to me like a typical *nix `Makefile` (and also seen the used commands like `./makehtmlfooter.sh` and as I now attempt the Windows version ...",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:242,deployability,version,version,242,"@couet, @bellenot How to build the documentation under windows. The `Makefile` in `documentation/doxygen` looks to me like a typical *nix `Makefile` (and also seen the used commands like `./makehtmlfooter.sh` and as I now attempt the Windows version ...",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:242,integrability,version,version,242,"@couet, @bellenot How to build the documentation under windows. The `Makefile` in `documentation/doxygen` looks to me like a typical *nix `Makefile` (and also seen the used commands like `./makehtmlfooter.sh` and as I now attempt the Windows version ...",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:242,modifiability,version,version,242,"@couet, @bellenot How to build the documentation under windows. The `Makefile` in `documentation/doxygen` looks to me like a typical *nix `Makefile` (and also seen the used commands like `./makehtmlfooter.sh` and as I now attempt the Windows version ...",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:35,usability,document,documentation,35,"@couet, @bellenot How to build the documentation under windows. The `Makefile` in `documentation/doxygen` looks to me like a typical *nix `Makefile` (and also seen the used commands like `./makehtmlfooter.sh` and as I now attempt the Windows version ...",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:83,usability,document,documentation,83,"@couet, @bellenot How to build the documentation under windows. The `Makefile` in `documentation/doxygen` looks to me like a typical *nix `Makefile` (and also seen the used commands like `./makehtmlfooter.sh` and as I now attempt the Windows version ...",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:173,usability,command,commands,173,"@couet, @bellenot How to build the documentation under windows. The `Makefile` in `documentation/doxygen` looks to me like a typical *nix `Makefile` (and also seen the used commands like `./makehtmlfooter.sh` and as I now attempt the Windows version ...",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:291,availability,down,download,291,@bellenot can confirm but I think we never built the documentation on Windows. We do not really need it. I never do it myself and I do not think he did it either. The documentation is built every night via a jenkins task. For people wanting a local version of the documentation we provide a download page https://root.cern/reference/ . Building the full ROOT documentation is very long... several hours. Nobody does that. Only the developers build some part of it locally . And I do not think any of us does it on Windows.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:249,deployability,version,version,249,@bellenot can confirm but I think we never built the documentation on Windows. We do not really need it. I never do it myself and I do not think he did it either. The documentation is built every night via a jenkins task. For people wanting a local version of the documentation we provide a download page https://root.cern/reference/ . Building the full ROOT documentation is very long... several hours. Nobody does that. Only the developers build some part of it locally . And I do not think any of us does it on Windows.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:336,deployability,Build,Building,336,@bellenot can confirm but I think we never built the documentation on Windows. We do not really need it. I never do it myself and I do not think he did it either. The documentation is built every night via a jenkins task. For people wanting a local version of the documentation we provide a download page https://root.cern/reference/ . Building the full ROOT documentation is very long... several hours. Nobody does that. Only the developers build some part of it locally . And I do not think any of us does it on Windows.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:442,deployability,build,build,442,@bellenot can confirm but I think we never built the documentation on Windows. We do not really need it. I never do it myself and I do not think he did it either. The documentation is built every night via a jenkins task. For people wanting a local version of the documentation we provide a download page https://root.cern/reference/ . Building the full ROOT documentation is very long... several hours. Nobody does that. Only the developers build some part of it locally . And I do not think any of us does it on Windows.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:249,integrability,version,version,249,@bellenot can confirm but I think we never built the documentation on Windows. We do not really need it. I never do it myself and I do not think he did it either. The documentation is built every night via a jenkins task. For people wanting a local version of the documentation we provide a download page https://root.cern/reference/ . Building the full ROOT documentation is very long... several hours. Nobody does that. Only the developers build some part of it locally . And I do not think any of us does it on Windows.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:249,modifiability,version,version,249,@bellenot can confirm but I think we never built the documentation on Windows. We do not really need it. I never do it myself and I do not think he did it either. The documentation is built every night via a jenkins task. For people wanting a local version of the documentation we provide a download page https://root.cern/reference/ . Building the full ROOT documentation is very long... several hours. Nobody does that. Only the developers build some part of it locally . And I do not think any of us does it on Windows.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:411,reliability,doe,does,411,@bellenot can confirm but I think we never built the documentation on Windows. We do not really need it. I never do it myself and I do not think he did it either. The documentation is built every night via a jenkins task. For people wanting a local version of the documentation we provide a download page https://root.cern/reference/ . Building the full ROOT documentation is very long... several hours. Nobody does that. Only the developers build some part of it locally . And I do not think any of us does it on Windows.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:503,reliability,doe,does,503,@bellenot can confirm but I think we never built the documentation on Windows. We do not really need it. I never do it myself and I do not think he did it either. The documentation is built every night via a jenkins task. For people wanting a local version of the documentation we provide a download page https://root.cern/reference/ . Building the full ROOT documentation is very long... several hours. Nobody does that. Only the developers build some part of it locally . And I do not think any of us does it on Windows.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:14,usability,confirm,confirm,14,@bellenot can confirm but I think we never built the documentation on Windows. We do not really need it. I never do it myself and I do not think he did it either. The documentation is built every night via a jenkins task. For people wanting a local version of the documentation we provide a download page https://root.cern/reference/ . Building the full ROOT documentation is very long... several hours. Nobody does that. Only the developers build some part of it locally . And I do not think any of us does it on Windows.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:53,usability,document,documentation,53,@bellenot can confirm but I think we never built the documentation on Windows. We do not really need it. I never do it myself and I do not think he did it either. The documentation is built every night via a jenkins task. For people wanting a local version of the documentation we provide a download page https://root.cern/reference/ . Building the full ROOT documentation is very long... several hours. Nobody does that. Only the developers build some part of it locally . And I do not think any of us does it on Windows.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:167,usability,document,documentation,167,@bellenot can confirm but I think we never built the documentation on Windows. We do not really need it. I never do it myself and I do not think he did it either. The documentation is built every night via a jenkins task. For people wanting a local version of the documentation we provide a download page https://root.cern/reference/ . Building the full ROOT documentation is very long... several hours. Nobody does that. Only the developers build some part of it locally . And I do not think any of us does it on Windows.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:264,usability,document,documentation,264,@bellenot can confirm but I think we never built the documentation on Windows. We do not really need it. I never do it myself and I do not think he did it either. The documentation is built every night via a jenkins task. For people wanting a local version of the documentation we provide a download page https://root.cern/reference/ . Building the full ROOT documentation is very long... several hours. Nobody does that. Only the developers build some part of it locally . And I do not think any of us does it on Windows.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:359,usability,document,documentation,359,@bellenot can confirm but I think we never built the documentation on Windows. We do not really need it. I never do it myself and I do not think he did it either. The documentation is built every night via a jenkins task. For people wanting a local version of the documentation we provide a download page https://root.cern/reference/ . Building the full ROOT documentation is very long... several hours. Nobody does that. Only the developers build some part of it locally . And I do not think any of us does it on Windows.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:35,deployability,build,build,35,Will be a hard task for me than to build and test the documentation. . As a side note did you ever think about using doxygen tag files? A project that extensively uses doxygen tag files for their project is the CGAL project (See https://www.cgal.org/ and https://doc.cgal.org/latest/Manual/packages.html). It is not easy to build and initially it takes also a complete doxygen run (to create a.o. the tag files) but later on it is possible to build individual manuals / subprojects. I don't know how feasible this would be for ROOT though.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:324,deployability,build,build,324,Will be a hard task for me than to build and test the documentation. . As a side note did you ever think about using doxygen tag files? A project that extensively uses doxygen tag files for their project is the CGAL project (See https://www.cgal.org/ and https://doc.cgal.org/latest/Manual/packages.html). It is not easy to build and initially it takes also a complete doxygen run (to create a.o. the tag files) but later on it is possible to build individual manuals / subprojects. I don't know how feasible this would be for ROOT though.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:443,deployability,build,build,443,Will be a hard task for me than to build and test the documentation. . As a side note did you ever think about using doxygen tag files? A project that extensively uses doxygen tag files for their project is the CGAL project (See https://www.cgal.org/ and https://doc.cgal.org/latest/Manual/packages.html). It is not easy to build and initially it takes also a complete doxygen run (to create a.o. the tag files) but later on it is possible to build individual manuals / subprojects. I don't know how feasible this would be for ROOT though.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:470,integrability,sub,subprojects,470,Will be a hard task for me than to build and test the documentation. . As a side note did you ever think about using doxygen tag files? A project that extensively uses doxygen tag files for their project is the CGAL project (See https://www.cgal.org/ and https://doc.cgal.org/latest/Manual/packages.html). It is not easy to build and initially it takes also a complete doxygen run (to create a.o. the tag files) but later on it is possible to build individual manuals / subprojects. I don't know how feasible this would be for ROOT though.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:151,modifiability,extens,extensively,151,Will be a hard task for me than to build and test the documentation. . As a side note did you ever think about using doxygen tag files? A project that extensively uses doxygen tag files for their project is the CGAL project (See https://www.cgal.org/ and https://doc.cgal.org/latest/Manual/packages.html). It is not easy to build and initially it takes also a complete doxygen run (to create a.o. the tag files) but later on it is possible to build individual manuals / subprojects. I don't know how feasible this would be for ROOT though.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:290,modifiability,pac,packages,290,Will be a hard task for me than to build and test the documentation. . As a side note did you ever think about using doxygen tag files? A project that extensively uses doxygen tag files for their project is the CGAL project (See https://www.cgal.org/ and https://doc.cgal.org/latest/Manual/packages.html). It is not easy to build and initially it takes also a complete doxygen run (to create a.o. the tag files) but later on it is possible to build individual manuals / subprojects. I don't know how feasible this would be for ROOT though.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:45,safety,test,test,45,Will be a hard task for me than to build and test the documentation. . As a side note did you ever think about using doxygen tag files? A project that extensively uses doxygen tag files for their project is the CGAL project (See https://www.cgal.org/ and https://doc.cgal.org/latest/Manual/packages.html). It is not easy to build and initially it takes also a complete doxygen run (to create a.o. the tag files) but later on it is possible to build individual manuals / subprojects. I don't know how feasible this would be for ROOT though.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:360,safety,compl,complete,360,Will be a hard task for me than to build and test the documentation. . As a side note did you ever think about using doxygen tag files? A project that extensively uses doxygen tag files for their project is the CGAL project (See https://www.cgal.org/ and https://doc.cgal.org/latest/Manual/packages.html). It is not easy to build and initially it takes also a complete doxygen run (to create a.o. the tag files) but later on it is possible to build individual manuals / subprojects. I don't know how feasible this would be for ROOT though.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:360,security,compl,complete,360,Will be a hard task for me than to build and test the documentation. . As a side note did you ever think about using doxygen tag files? A project that extensively uses doxygen tag files for their project is the CGAL project (See https://www.cgal.org/ and https://doc.cgal.org/latest/Manual/packages.html). It is not easy to build and initially it takes also a complete doxygen run (to create a.o. the tag files) but later on it is possible to build individual manuals / subprojects. I don't know how feasible this would be for ROOT though.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:45,testability,test,test,45,Will be a hard task for me than to build and test the documentation. . As a side note did you ever think about using doxygen tag files? A project that extensively uses doxygen tag files for their project is the CGAL project (See https://www.cgal.org/ and https://doc.cgal.org/latest/Manual/packages.html). It is not easy to build and initially it takes also a complete doxygen run (to create a.o. the tag files) but later on it is possible to build individual manuals / subprojects. I don't know how feasible this would be for ROOT though.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:54,usability,document,documentation,54,Will be a hard task for me than to build and test the documentation. . As a side note did you ever think about using doxygen tag files? A project that extensively uses doxygen tag files for their project is the CGAL project (See https://www.cgal.org/ and https://doc.cgal.org/latest/Manual/packages.html). It is not easy to build and initially it takes also a complete doxygen run (to create a.o. the tag files) but later on it is possible to build individual manuals / subprojects. I don't know how feasible this would be for ROOT though.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:56,deployability,build,building,56,"@albert-github sorry, never looked at the documentation building on Windows...",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:42,usability,document,documentation,42,"@albert-github sorry, never looked at the documentation building on Windows...",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:160,availability,down,download,160,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:175,availability,down,download,175,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:223,availability,down,download,223,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:349,availability,down,download,349,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:459,availability,Error,Error,459,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:0,deployability,Build,Building,0,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:21,deployability,version,version,21,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:68,deployability,Build,Building,68,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:360,deployability,log,log,360,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:368,deployability,updat,update,368,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:394,deployability,patch,patch,394,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:565,deployability,Releas,Release,565,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:603,deployability,fail,failed,603,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:639,deployability,upgrad,upgrade,639,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:649,deployability,build,build,649,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:786,deployability,log,log,786,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:822,deployability,log,log,822,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:869,deployability,Version,Version,869,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:21,integrability,version,version,21,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:427,integrability,configur,configure,427,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:555,integrability,configur,configure-Release,555,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:584,integrability,messag,message,584,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:774,integrability,configur,configure,774,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:869,integrability,Version,Version,869,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:584,interoperability,messag,message,584,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:21,modifiability,version,version,21,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:427,modifiability,configur,configure,427,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:555,modifiability,configur,configure-Release,555,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:639,modifiability,upgrad,upgrade,639,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:774,modifiability,configur,configure,774,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:869,modifiability,Version,Version,869,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:149,performance,Perform,Performing,149,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:416,performance,Perform,Performing,416,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:459,performance,Error,Error,459,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:603,reliability,fail,failed,603,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:360,safety,log,log,360,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:368,safety,updat,update,368,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:394,safety,patch,patch,394,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:459,safety,Error,Error,459,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:786,safety,log,log,786,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:822,safety,log,log,822,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:360,security,log,log,360,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:368,security,updat,update,368,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:394,security,patch,patch,394,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:427,security,configur,configure,427,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:555,security,configur,configure-Release,555,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:774,security,configur,configure,774,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:786,security,log,log,786,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:822,security,log,log,822,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:970,security,expir,expired,970,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:185,testability,verif,verify,185,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:360,testability,log,log,360,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:786,testability,log,log,786,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:822,testability,log,log,822,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:77,usability,Custom,Custom,77,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:149,usability,Perform,Performing,149,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:232,usability,command,command,232,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:416,usability,Perform,Performing,416,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:459,usability,Error,Error,459,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:595,usability,Command,Command,595,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:850,usability,Visual,Visual,850,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:952,usability,Visual,Visual,952,"Building the Windows version:. ```. Creating directories for 'TBB'. Building Custom Rule E:/Programs/github_repositories/normal/root/CMakeLists.txt. Performing download step (download, verify and extract) for 'TBB'. -- TBB download command succeeded. See also E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/T. BB-download-*.log. No update step for 'TBB'. No patch step for 'TBB'. Performing configure step for 'TBB'. CMake Error at E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-Release.cmake:49. (message):. Command failed: 1. 'devenv.exe' '/useenv' '/upgrade' 'build/vs2013/makefile.sln'. See also. E:/Programs/github_repositories/normal/root/build_windows/TBB-prefix/src/TBB-stamp/TBB-configure-*.log. ```. Might be due to (from the log files):. ```. Microsoft Visual Studio 2019 Version 16.9.4. Copyright (C) Microsoft Corp. All rights reserved. The license for Visual Studio has expired. The evaluation period for this product has ended. ```",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:72,deployability,version,version,72,"Well, yes, you need a proper license (free for the academic/open source version)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:72,integrability,version,version,72,"Well, yes, you need a proper license (free for the academic/open source version)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:72,modifiability,version,version,72,"Well, yes, you need a proper license (free for the academic/open source version)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:232,availability,down,down,232,"The TBB directory comes apparently from http://threadingbuildingblocks.org wanted confirmation by means of a telephone number. Wouldn't it anyway be good to provide http://threadingbuildingblocks.org and on my build directory I see down in the tar file in TBB-prefix that there is a vs2013 directory and nothing newer. So I probably have to see what I can do with the license without providing telephone numbers and alike ... Just as a side note:. On the page: https://software.intel.com/content/www/us/en/develop/articles/intel-oneapi-threading-building-blocks-system-requirements.html I see that there are newer (source) versions which also support: Microsoft* Visual C++ 14.2 (Microsoft* Visual Studio* 2019, Windows* OS only) so maybe there is a distribution yjat you could use as well.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:210,deployability,build,build,210,"The TBB directory comes apparently from http://threadingbuildingblocks.org wanted confirmation by means of a telephone number. Wouldn't it anyway be good to provide http://threadingbuildingblocks.org and on my build directory I see down in the tar file in TBB-prefix that there is a vs2013 directory and nothing newer. So I probably have to see what I can do with the license without providing telephone numbers and alike ... Just as a side note:. On the page: https://software.intel.com/content/www/us/en/develop/articles/intel-oneapi-threading-building-blocks-system-requirements.html I see that there are newer (source) versions which also support: Microsoft* Visual C++ 14.2 (Microsoft* Visual Studio* 2019, Windows* OS only) so maybe there is a distribution yjat you could use as well.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:546,deployability,build,building-blocks-system-requirements,546,"The TBB directory comes apparently from http://threadingbuildingblocks.org wanted confirmation by means of a telephone number. Wouldn't it anyway be good to provide http://threadingbuildingblocks.org and on my build directory I see down in the tar file in TBB-prefix that there is a vs2013 directory and nothing newer. So I probably have to see what I can do with the license without providing telephone numbers and alike ... Just as a side note:. On the page: https://software.intel.com/content/www/us/en/develop/articles/intel-oneapi-threading-building-blocks-system-requirements.html I see that there are newer (source) versions which also support: Microsoft* Visual C++ 14.2 (Microsoft* Visual Studio* 2019, Windows* OS only) so maybe there is a distribution yjat you could use as well.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:623,deployability,version,versions,623,"The TBB directory comes apparently from http://threadingbuildingblocks.org wanted confirmation by means of a telephone number. Wouldn't it anyway be good to provide http://threadingbuildingblocks.org and on my build directory I see down in the tar file in TBB-prefix that there is a vs2013 directory and nothing newer. So I probably have to see what I can do with the license without providing telephone numbers and alike ... Just as a side note:. On the page: https://software.intel.com/content/www/us/en/develop/articles/intel-oneapi-threading-building-blocks-system-requirements.html I see that there are newer (source) versions which also support: Microsoft* Visual C++ 14.2 (Microsoft* Visual Studio* 2019, Windows* OS only) so maybe there is a distribution yjat you could use as well.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:623,integrability,version,versions,623,"The TBB directory comes apparently from http://threadingbuildingblocks.org wanted confirmation by means of a telephone number. Wouldn't it anyway be good to provide http://threadingbuildingblocks.org and on my build directory I see down in the tar file in TBB-prefix that there is a vs2013 directory and nothing newer. So I probably have to see what I can do with the license without providing telephone numbers and alike ... Just as a side note:. On the page: https://software.intel.com/content/www/us/en/develop/articles/intel-oneapi-threading-building-blocks-system-requirements.html I see that there are newer (source) versions which also support: Microsoft* Visual C++ 14.2 (Microsoft* Visual Studio* 2019, Windows* OS only) so maybe there is a distribution yjat you could use as well.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:750,interoperability,distribut,distribution,750,"The TBB directory comes apparently from http://threadingbuildingblocks.org wanted confirmation by means of a telephone number. Wouldn't it anyway be good to provide http://threadingbuildingblocks.org and on my build directory I see down in the tar file in TBB-prefix that there is a vs2013 directory and nothing newer. So I probably have to see what I can do with the license without providing telephone numbers and alike ... Just as a side note:. On the page: https://software.intel.com/content/www/us/en/develop/articles/intel-oneapi-threading-building-blocks-system-requirements.html I see that there are newer (source) versions which also support: Microsoft* Visual C++ 14.2 (Microsoft* Visual Studio* 2019, Windows* OS only) so maybe there is a distribution yjat you could use as well.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:623,modifiability,version,versions,623,"The TBB directory comes apparently from http://threadingbuildingblocks.org wanted confirmation by means of a telephone number. Wouldn't it anyway be good to provide http://threadingbuildingblocks.org and on my build directory I see down in the tar file in TBB-prefix that there is a vs2013 directory and nothing newer. So I probably have to see what I can do with the license without providing telephone numbers and alike ... Just as a side note:. On the page: https://software.intel.com/content/www/us/en/develop/articles/intel-oneapi-threading-building-blocks-system-requirements.html I see that there are newer (source) versions which also support: Microsoft* Visual C++ 14.2 (Microsoft* Visual Studio* 2019, Windows* OS only) so maybe there is a distribution yjat you could use as well.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:488,performance,content,content,488,"The TBB directory comes apparently from http://threadingbuildingblocks.org wanted confirmation by means of a telephone number. Wouldn't it anyway be good to provide http://threadingbuildingblocks.org and on my build directory I see down in the tar file in TBB-prefix that there is a vs2013 directory and nothing newer. So I probably have to see what I can do with the license without providing telephone numbers and alike ... Just as a side note:. On the page: https://software.intel.com/content/www/us/en/develop/articles/intel-oneapi-threading-building-blocks-system-requirements.html I see that there are newer (source) versions which also support: Microsoft* Visual C++ 14.2 (Microsoft* Visual Studio* 2019, Windows* OS only) so maybe there is a distribution yjat you could use as well.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:82,usability,confirm,confirmation,82,"The TBB directory comes apparently from http://threadingbuildingblocks.org wanted confirmation by means of a telephone number. Wouldn't it anyway be good to provide http://threadingbuildingblocks.org and on my build directory I see down in the tar file in TBB-prefix that there is a vs2013 directory and nothing newer. So I probably have to see what I can do with the license without providing telephone numbers and alike ... Just as a side note:. On the page: https://software.intel.com/content/www/us/en/develop/articles/intel-oneapi-threading-building-blocks-system-requirements.html I see that there are newer (source) versions which also support: Microsoft* Visual C++ 14.2 (Microsoft* Visual Studio* 2019, Windows* OS only) so maybe there is a distribution yjat you could use as well.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:643,usability,support,support,643,"The TBB directory comes apparently from http://threadingbuildingblocks.org wanted confirmation by means of a telephone number. Wouldn't it anyway be good to provide http://threadingbuildingblocks.org and on my build directory I see down in the tar file in TBB-prefix that there is a vs2013 directory and nothing newer. So I probably have to see what I can do with the license without providing telephone numbers and alike ... Just as a side note:. On the page: https://software.intel.com/content/www/us/en/develop/articles/intel-oneapi-threading-building-blocks-system-requirements.html I see that there are newer (source) versions which also support: Microsoft* Visual C++ 14.2 (Microsoft* Visual Studio* 2019, Windows* OS only) so maybe there is a distribution yjat you could use as well.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:663,usability,Visual,Visual,663,"The TBB directory comes apparently from http://threadingbuildingblocks.org wanted confirmation by means of a telephone number. Wouldn't it anyway be good to provide http://threadingbuildingblocks.org and on my build directory I see down in the tar file in TBB-prefix that there is a vs2013 directory and nothing newer. So I probably have to see what I can do with the license without providing telephone numbers and alike ... Just as a side note:. On the page: https://software.intel.com/content/www/us/en/develop/articles/intel-oneapi-threading-building-blocks-system-requirements.html I see that there are newer (source) versions which also support: Microsoft* Visual C++ 14.2 (Microsoft* Visual Studio* 2019, Windows* OS only) so maybe there is a distribution yjat you could use as well.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:691,usability,Visual,Visual,691,"The TBB directory comes apparently from http://threadingbuildingblocks.org wanted confirmation by means of a telephone number. Wouldn't it anyway be good to provide http://threadingbuildingblocks.org and on my build directory I see down in the tar file in TBB-prefix that there is a vs2013 directory and nothing newer. So I probably have to see what I can do with the license without providing telephone numbers and alike ... Just as a side note:. On the page: https://software.intel.com/content/www/us/en/develop/articles/intel-oneapi-threading-building-blocks-system-requirements.html I see that there are newer (source) versions which also support: Microsoft* Visual C++ 14.2 (Microsoft* Visual Studio* 2019, Windows* OS only) so maybe there is a distribution yjat you could use as well.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:8,testability,simpl,simply,8,You can simply disable `imt` (`-Dimt=OFF`) and `builtin_tbb` (-D`builtin_tbb=OFF`).,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:8,usability,simpl,simply,8,You can simply disable `imt` (`-Dimt=OFF`) and `builtin_tbb` (-D`builtin_tbb=OFF`).,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:2790,availability,error,errors,2790,nui5:BOOL=ON. builtin_pcre:BOOL=ON. builtin_tbb:BOOL=OFF. builtin_unuran:BOOL=OFF. builtin_vc:BOOL=OFF. builtin_vdt:BOOL=OFF. builtin_veccore:BOOL=OFF. builtin_xrootd:BOOL=OFF. builtin_xxhash:BOOL=ON. builtin_zlib:BOOL=ON. builtin_zstd:BOOL=ON. ccache:BOOL=OFF. cefweb:BOOL=OFF. clad:BOOL=ON. clingtest:BOOL=OFF. cocoa:BOOL=OFF. compression_default:STRING=zlib. coverage:BOOL=OFF. cuda:BOOL=OFF. cudnn:BOOL=ON. cxxmodules:BOOL=OFF. daos:BOOL=OFF. dataframe:BOOL=ON. davix:BOOL=OFF. dcache:BOOL=OFF. dev:BOOL=OFF. distcc:BOOL=OFF. exceptions:BOOL=ON. fail-on-missing:BOOL=OFF. fcgi:BOOL=OFF. fftw3:BOOL=OFF. fitsio:BOOL=OFF. fortran:BOOL=OFF. gcctoolchain:PATH=. gdml:BOOL=ON. gfal:BOOL=OFF. gminimal:BOOL=OFF. gnuinstall:BOOL=OFF. gsl_shared:BOOL=OFF. gviz:BOOL=OFF. http:BOOL=ON. imt:BOOL=OFF. jemalloc:BOOL=OFF. libcxx:BOOL=OFF. macos_native:BOOL=OFF. mathmore:BOOL=OFF. memory_termination:BOOL=OFF. minimal:BOOL=OFF. minuit2:BOOL=ON. minuit2_mpi:BOOL=OFF. minuit2_omp:BOOL=OFF. mlp:BOOL=ON. monalisa:BOOL=OFF. mpi:BOOL=OFF. mysql:BOOL=OFF. nlohmann_json_DIR:PATH=nlohmann_json_DIR-NOTFOUND. odbc:BOOL=OFF. opengl:BOOL=ON. oracle:BOOL=OFF. pgsql:BOOL=OFF. pyroot:BOOL=OFF. pyroot_legacy:BOOL=OFF. pythia6:BOOL=OFF. pythia6_nolink:BOOL=OFF. pythia8:BOOL=OFF. qt5web:BOOL=OFF. r:BOOL=OFF. roofit:BOOL=ON. root7:BOOL=OFF. rootbench:BOOL=OFF. roottest:BOOL=OFF. rpath:BOOL=OFF. runtime_cxxmodules:BOOL=OFF. shadowpw:BOOL=OFF. shared:BOOL=ON. soversion:BOOL=OFF. spectrum:BOOL=ON. sqlite:BOOL=OFF. ssl:BOOL=OFF. tcmalloc:BOOL=OFF. test_distrdf_pyspark:BOOL=OFF. testing:BOOL=OFF. tmva:BOOL=ON. tmva-cpu:BOOL=OFF. tmva-gpu:BOOL=OFF. tmva-pymva:BOOL=OFF. tmva-rmva:BOOL=OFF. tmva-sofie:BOOL=OFF. unuran:BOOL=OFF. uring:BOOL=OFF. vc:BOOL=OFF. vdt:BOOL=OFF. veccore:BOOL=OFF. vecgeom:BOOL=OFF. webgui:BOOL=OFF. win_broken_tests:BOOL=OFF. winrtdebug:BOOL=OFF. x11:BOOL=OFF. xml:BOOL=OFF. xproofd:BOOL=OFF. xrootd:BOOL=OFF. xxHash_FOUND:BOOL=TRUE. ```. **Edit**: compilation build finished now without errors.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:1347,deployability,fail,fail-on-missing,1347,n:BOOL=OFF. asimage:BOOL=ON. asserts:BOOL=OFF. builtin_afterimage:BOOL=ON. builtin_cfitsio:BOOL=OFF. builtin_clang:BOOL=ON. builtin_cling:BOOL=ON. builtin_davix:BOOL=OFF. builtin_fftw3:BOOL=OFF. builtin_freetype:BOOL=ON. builtin_ftgl:BOOL=ON. builtin_gl2ps:BOOL=ON. builtin_glew:BOOL=ON. builtin_gsl:BOOL=OFF. builtin_llvm:BOOL=ON. builtin_lz4:BOOL=ON. builtin_lzma:BOOL=ON. builtin_nlohmannjson:BOOL=ON. builtin_openssl:BOOL=OFF. builtin_openui5:BOOL=ON. builtin_pcre:BOOL=ON. builtin_tbb:BOOL=OFF. builtin_unuran:BOOL=OFF. builtin_vc:BOOL=OFF. builtin_vdt:BOOL=OFF. builtin_veccore:BOOL=OFF. builtin_xrootd:BOOL=OFF. builtin_xxhash:BOOL=ON. builtin_zlib:BOOL=ON. builtin_zstd:BOOL=ON. ccache:BOOL=OFF. cefweb:BOOL=OFF. clad:BOOL=ON. clingtest:BOOL=OFF. cocoa:BOOL=OFF. compression_default:STRING=zlib. coverage:BOOL=OFF. cuda:BOOL=OFF. cudnn:BOOL=ON. cxxmodules:BOOL=OFF. daos:BOOL=OFF. dataframe:BOOL=ON. davix:BOOL=OFF. dcache:BOOL=OFF. dev:BOOL=OFF. distcc:BOOL=OFF. exceptions:BOOL=ON. fail-on-missing:BOOL=OFF. fcgi:BOOL=OFF. fftw3:BOOL=OFF. fitsio:BOOL=OFF. fortran:BOOL=OFF. gcctoolchain:PATH=. gdml:BOOL=ON. gfal:BOOL=OFF. gminimal:BOOL=OFF. gnuinstall:BOOL=OFF. gsl_shared:BOOL=OFF. gviz:BOOL=OFF. http:BOOL=ON. imt:BOOL=OFF. jemalloc:BOOL=OFF. libcxx:BOOL=OFF. macos_native:BOOL=OFF. mathmore:BOOL=OFF. memory_termination:BOOL=OFF. minimal:BOOL=OFF. minuit2:BOOL=ON. minuit2_mpi:BOOL=OFF. minuit2_omp:BOOL=OFF. mlp:BOOL=ON. monalisa:BOOL=OFF. mpi:BOOL=OFF. mysql:BOOL=OFF. nlohmann_json_DIR:PATH=nlohmann_json_DIR-NOTFOUND. odbc:BOOL=OFF. opengl:BOOL=ON. oracle:BOOL=OFF. pgsql:BOOL=OFF. pyroot:BOOL=OFF. pyroot_legacy:BOOL=OFF. pythia6:BOOL=OFF. pythia6_nolink:BOOL=OFF. pythia8:BOOL=OFF. qt5web:BOOL=OFF. r:BOOL=OFF. roofit:BOOL=ON. root7:BOOL=OFF. rootbench:BOOL=OFF. roottest:BOOL=OFF. rpath:BOOL=OFF. runtime_cxxmodules:BOOL=OFF. shadowpw:BOOL=OFF. shared:BOOL=ON. soversion:BOOL=OFF. spectrum:BOOL=ON. sqlite:BOOL=OFF. ssl:BOOL=OFF. tcmalloc:BOOL=OFF. test_distrdf_pyspark:BOOL=OFF.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:2763,deployability,build,build,2763,nui5:BOOL=ON. builtin_pcre:BOOL=ON. builtin_tbb:BOOL=OFF. builtin_unuran:BOOL=OFF. builtin_vc:BOOL=OFF. builtin_vdt:BOOL=OFF. builtin_veccore:BOOL=OFF. builtin_xrootd:BOOL=OFF. builtin_xxhash:BOOL=ON. builtin_zlib:BOOL=ON. builtin_zstd:BOOL=ON. ccache:BOOL=OFF. cefweb:BOOL=OFF. clad:BOOL=ON. clingtest:BOOL=OFF. cocoa:BOOL=OFF. compression_default:STRING=zlib. coverage:BOOL=OFF. cuda:BOOL=OFF. cudnn:BOOL=ON. cxxmodules:BOOL=OFF. daos:BOOL=OFF. dataframe:BOOL=ON. davix:BOOL=OFF. dcache:BOOL=OFF. dev:BOOL=OFF. distcc:BOOL=OFF. exceptions:BOOL=ON. fail-on-missing:BOOL=OFF. fcgi:BOOL=OFF. fftw3:BOOL=OFF. fitsio:BOOL=OFF. fortran:BOOL=OFF. gcctoolchain:PATH=. gdml:BOOL=ON. gfal:BOOL=OFF. gminimal:BOOL=OFF. gnuinstall:BOOL=OFF. gsl_shared:BOOL=OFF. gviz:BOOL=OFF. http:BOOL=ON. imt:BOOL=OFF. jemalloc:BOOL=OFF. libcxx:BOOL=OFF. macos_native:BOOL=OFF. mathmore:BOOL=OFF. memory_termination:BOOL=OFF. minimal:BOOL=OFF. minuit2:BOOL=ON. minuit2_mpi:BOOL=OFF. minuit2_omp:BOOL=OFF. mlp:BOOL=ON. monalisa:BOOL=OFF. mpi:BOOL=OFF. mysql:BOOL=OFF. nlohmann_json_DIR:PATH=nlohmann_json_DIR-NOTFOUND. odbc:BOOL=OFF. opengl:BOOL=ON. oracle:BOOL=OFF. pgsql:BOOL=OFF. pyroot:BOOL=OFF. pyroot_legacy:BOOL=OFF. pythia6:BOOL=OFF. pythia6_nolink:BOOL=OFF. pythia8:BOOL=OFF. qt5web:BOOL=OFF. r:BOOL=OFF. roofit:BOOL=ON. root7:BOOL=OFF. rootbench:BOOL=OFF. roottest:BOOL=OFF. rpath:BOOL=OFF. runtime_cxxmodules:BOOL=OFF. shadowpw:BOOL=OFF. shared:BOOL=ON. soversion:BOOL=OFF. spectrum:BOOL=ON. sqlite:BOOL=OFF. ssl:BOOL=OFF. tcmalloc:BOOL=OFF. test_distrdf_pyspark:BOOL=OFF. testing:BOOL=OFF. tmva:BOOL=ON. tmva-cpu:BOOL=OFF. tmva-gpu:BOOL=OFF. tmva-pymva:BOOL=OFF. tmva-rmva:BOOL=OFF. tmva-sofie:BOOL=OFF. unuran:BOOL=OFF. uring:BOOL=OFF. vc:BOOL=OFF. vdt:BOOL=OFF. veccore:BOOL=OFF. vecgeom:BOOL=OFF. webgui:BOOL=OFF. win_broken_tests:BOOL=OFF. winrtdebug:BOOL=OFF. x11:BOOL=OFF. xml:BOOL=OFF. xproofd:BOOL=OFF. xrootd:BOOL=OFF. xxHash_FOUND:BOOL=TRUE. ```. **Edit**: compilation build finished now without errors.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:2393,energy efficiency,cpu,cpu,2393,nui5:BOOL=ON. builtin_pcre:BOOL=ON. builtin_tbb:BOOL=OFF. builtin_unuran:BOOL=OFF. builtin_vc:BOOL=OFF. builtin_vdt:BOOL=OFF. builtin_veccore:BOOL=OFF. builtin_xrootd:BOOL=OFF. builtin_xxhash:BOOL=ON. builtin_zlib:BOOL=ON. builtin_zstd:BOOL=ON. ccache:BOOL=OFF. cefweb:BOOL=OFF. clad:BOOL=ON. clingtest:BOOL=OFF. cocoa:BOOL=OFF. compression_default:STRING=zlib. coverage:BOOL=OFF. cuda:BOOL=OFF. cudnn:BOOL=ON. cxxmodules:BOOL=OFF. daos:BOOL=OFF. dataframe:BOOL=ON. davix:BOOL=OFF. dcache:BOOL=OFF. dev:BOOL=OFF. distcc:BOOL=OFF. exceptions:BOOL=ON. fail-on-missing:BOOL=OFF. fcgi:BOOL=OFF. fftw3:BOOL=OFF. fitsio:BOOL=OFF. fortran:BOOL=OFF. gcctoolchain:PATH=. gdml:BOOL=ON. gfal:BOOL=OFF. gminimal:BOOL=OFF. gnuinstall:BOOL=OFF. gsl_shared:BOOL=OFF. gviz:BOOL=OFF. http:BOOL=ON. imt:BOOL=OFF. jemalloc:BOOL=OFF. libcxx:BOOL=OFF. macos_native:BOOL=OFF. mathmore:BOOL=OFF. memory_termination:BOOL=OFF. minimal:BOOL=OFF. minuit2:BOOL=ON. minuit2_mpi:BOOL=OFF. minuit2_omp:BOOL=OFF. mlp:BOOL=ON. monalisa:BOOL=OFF. mpi:BOOL=OFF. mysql:BOOL=OFF. nlohmann_json_DIR:PATH=nlohmann_json_DIR-NOTFOUND. odbc:BOOL=OFF. opengl:BOOL=ON. oracle:BOOL=OFF. pgsql:BOOL=OFF. pyroot:BOOL=OFF. pyroot_legacy:BOOL=OFF. pythia6:BOOL=OFF. pythia6_nolink:BOOL=OFF. pythia8:BOOL=OFF. qt5web:BOOL=OFF. r:BOOL=OFF. roofit:BOOL=ON. root7:BOOL=OFF. rootbench:BOOL=OFF. roottest:BOOL=OFF. rpath:BOOL=OFF. runtime_cxxmodules:BOOL=OFF. shadowpw:BOOL=OFF. shared:BOOL=ON. soversion:BOOL=OFF. spectrum:BOOL=ON. sqlite:BOOL=OFF. ssl:BOOL=OFF. tcmalloc:BOOL=OFF. test_distrdf_pyspark:BOOL=OFF. testing:BOOL=OFF. tmva:BOOL=ON. tmva-cpu:BOOL=OFF. tmva-gpu:BOOL=OFF. tmva-pymva:BOOL=OFF. tmva-rmva:BOOL=OFF. tmva-sofie:BOOL=OFF. unuran:BOOL=OFF. uring:BOOL=OFF. vc:BOOL=OFF. vdt:BOOL=OFF. veccore:BOOL=OFF. vecgeom:BOOL=OFF. webgui:BOOL=OFF. win_broken_tests:BOOL=OFF. winrtdebug:BOOL=OFF. x11:BOOL=OFF. xml:BOOL=OFF. xproofd:BOOL=OFF. xrootd:BOOL=OFF. xxHash_FOUND:BOOL=TRUE. ```. **Edit**: compilation build finished now without errors.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:2412,energy efficiency,gpu,gpu,2412,nui5:BOOL=ON. builtin_pcre:BOOL=ON. builtin_tbb:BOOL=OFF. builtin_unuran:BOOL=OFF. builtin_vc:BOOL=OFF. builtin_vdt:BOOL=OFF. builtin_veccore:BOOL=OFF. builtin_xrootd:BOOL=OFF. builtin_xxhash:BOOL=ON. builtin_zlib:BOOL=ON. builtin_zstd:BOOL=ON. ccache:BOOL=OFF. cefweb:BOOL=OFF. clad:BOOL=ON. clingtest:BOOL=OFF. cocoa:BOOL=OFF. compression_default:STRING=zlib. coverage:BOOL=OFF. cuda:BOOL=OFF. cudnn:BOOL=ON. cxxmodules:BOOL=OFF. daos:BOOL=OFF. dataframe:BOOL=ON. davix:BOOL=OFF. dcache:BOOL=OFF. dev:BOOL=OFF. distcc:BOOL=OFF. exceptions:BOOL=ON. fail-on-missing:BOOL=OFF. fcgi:BOOL=OFF. fftw3:BOOL=OFF. fitsio:BOOL=OFF. fortran:BOOL=OFF. gcctoolchain:PATH=. gdml:BOOL=ON. gfal:BOOL=OFF. gminimal:BOOL=OFF. gnuinstall:BOOL=OFF. gsl_shared:BOOL=OFF. gviz:BOOL=OFF. http:BOOL=ON. imt:BOOL=OFF. jemalloc:BOOL=OFF. libcxx:BOOL=OFF. macos_native:BOOL=OFF. mathmore:BOOL=OFF. memory_termination:BOOL=OFF. minimal:BOOL=OFF. minuit2:BOOL=ON. minuit2_mpi:BOOL=OFF. minuit2_omp:BOOL=OFF. mlp:BOOL=ON. monalisa:BOOL=OFF. mpi:BOOL=OFF. mysql:BOOL=OFF. nlohmann_json_DIR:PATH=nlohmann_json_DIR-NOTFOUND. odbc:BOOL=OFF. opengl:BOOL=ON. oracle:BOOL=OFF. pgsql:BOOL=OFF. pyroot:BOOL=OFF. pyroot_legacy:BOOL=OFF. pythia6:BOOL=OFF. pythia6_nolink:BOOL=OFF. pythia8:BOOL=OFF. qt5web:BOOL=OFF. r:BOOL=OFF. roofit:BOOL=ON. root7:BOOL=OFF. rootbench:BOOL=OFF. roottest:BOOL=OFF. rpath:BOOL=OFF. runtime_cxxmodules:BOOL=OFF. shadowpw:BOOL=OFF. shared:BOOL=ON. soversion:BOOL=OFF. spectrum:BOOL=ON. sqlite:BOOL=OFF. ssl:BOOL=OFF. tcmalloc:BOOL=OFF. test_distrdf_pyspark:BOOL=OFF. testing:BOOL=OFF. tmva:BOOL=ON. tmva-cpu:BOOL=OFF. tmva-gpu:BOOL=OFF. tmva-pymva:BOOL=OFF. tmva-rmva:BOOL=OFF. tmva-sofie:BOOL=OFF. unuran:BOOL=OFF. uring:BOOL=OFF. vc:BOOL=OFF. vdt:BOOL=OFF. veccore:BOOL=OFF. vecgeom:BOOL=OFF. webgui:BOOL=OFF. win_broken_tests:BOOL=OFF. winrtdebug:BOOL=OFF. x11:BOOL=OFF. xml:BOOL=OFF. xproofd:BOOL=OFF. xrootd:BOOL=OFF. xxHash_FOUND:BOOL=TRUE. ```. **Edit**: compilation build finished now without errors.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:227,integrability,sub,subpackages,227,"@bellenot . Thanks, I'm now trying it (runs already again for a while). Are there more of these high level options and are they documented somewhere? I ran `cmake -LA` and see a long list, with of course also settings from the subpackages used, but I think the bottom part gives, probably, the list:. ```. alien:BOOL=OFF. all:BOOL=OFF. arrow:BOOL=OFF. asan:BOOL=OFF. asimage:BOOL=ON. asserts:BOOL=OFF. builtin_afterimage:BOOL=ON. builtin_cfitsio:BOOL=OFF. builtin_clang:BOOL=ON. builtin_cling:BOOL=ON. builtin_davix:BOOL=OFF. builtin_fftw3:BOOL=OFF. builtin_freetype:BOOL=ON. builtin_ftgl:BOOL=ON. builtin_gl2ps:BOOL=ON. builtin_glew:BOOL=ON. builtin_gsl:BOOL=OFF. builtin_llvm:BOOL=ON. builtin_lz4:BOOL=ON. builtin_lzma:BOOL=ON. builtin_nlohmannjson:BOOL=ON. builtin_openssl:BOOL=OFF. builtin_openui5:BOOL=ON. builtin_pcre:BOOL=ON. builtin_tbb:BOOL=OFF. builtin_unuran:BOOL=OFF. builtin_vc:BOOL=OFF. builtin_vdt:BOOL=OFF. builtin_veccore:BOOL=OFF. builtin_xrootd:BOOL=OFF. builtin_xxhash:BOOL=ON. builtin_zlib:BOOL=ON. builtin_zstd:BOOL=ON. ccache:BOOL=OFF. cefweb:BOOL=OFF. clad:BOOL=ON. clingtest:BOOL=OFF. cocoa:BOOL=OFF. compression_default:STRING=zlib. coverage:BOOL=OFF. cuda:BOOL=OFF. cudnn:BOOL=ON. cxxmodules:BOOL=OFF. daos:BOOL=OFF. dataframe:BOOL=ON. davix:BOOL=OFF. dcache:BOOL=OFF. dev:BOOL=OFF. distcc:BOOL=OFF. exceptions:BOOL=ON. fail-on-missing:BOOL=OFF. fcgi:BOOL=OFF. fftw3:BOOL=OFF. fitsio:BOOL=OFF. fortran:BOOL=OFF. gcctoolchain:PATH=. gdml:BOOL=ON. gfal:BOOL=OFF. gminimal:BOOL=OFF. gnuinstall:BOOL=OFF. gsl_shared:BOOL=OFF. gviz:BOOL=OFF. http:BOOL=ON. imt:BOOL=OFF. jemalloc:BOOL=OFF. libcxx:BOOL=OFF. macos_native:BOOL=OFF. mathmore:BOOL=OFF. memory_termination:BOOL=OFF. minimal:BOOL=OFF. minuit2:BOOL=ON. minuit2_mpi:BOOL=OFF. minuit2_omp:BOOL=OFF. mlp:BOOL=ON. monalisa:BOOL=OFF. mpi:BOOL=OFF. mysql:BOOL=OFF. nlohmann_json_DIR:PATH=nlohmann_json_DIR-NOTFOUND. odbc:BOOL=OFF. opengl:BOOL=ON. oracle:BOOL=OFF. pgsql:BOOL=OFF. pyroot:BOOL=OFF. pyroot_legacy:BOOL=OFF. pyth",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:2221,interoperability,share,shared,2221,nui5:BOOL=ON. builtin_pcre:BOOL=ON. builtin_tbb:BOOL=OFF. builtin_unuran:BOOL=OFF. builtin_vc:BOOL=OFF. builtin_vdt:BOOL=OFF. builtin_veccore:BOOL=OFF. builtin_xrootd:BOOL=OFF. builtin_xxhash:BOOL=ON. builtin_zlib:BOOL=ON. builtin_zstd:BOOL=ON. ccache:BOOL=OFF. cefweb:BOOL=OFF. clad:BOOL=ON. clingtest:BOOL=OFF. cocoa:BOOL=OFF. compression_default:STRING=zlib. coverage:BOOL=OFF. cuda:BOOL=OFF. cudnn:BOOL=ON. cxxmodules:BOOL=OFF. daos:BOOL=OFF. dataframe:BOOL=ON. davix:BOOL=OFF. dcache:BOOL=OFF. dev:BOOL=OFF. distcc:BOOL=OFF. exceptions:BOOL=ON. fail-on-missing:BOOL=OFF. fcgi:BOOL=OFF. fftw3:BOOL=OFF. fitsio:BOOL=OFF. fortran:BOOL=OFF. gcctoolchain:PATH=. gdml:BOOL=ON. gfal:BOOL=OFF. gminimal:BOOL=OFF. gnuinstall:BOOL=OFF. gsl_shared:BOOL=OFF. gviz:BOOL=OFF. http:BOOL=ON. imt:BOOL=OFF. jemalloc:BOOL=OFF. libcxx:BOOL=OFF. macos_native:BOOL=OFF. mathmore:BOOL=OFF. memory_termination:BOOL=OFF. minimal:BOOL=OFF. minuit2:BOOL=ON. minuit2_mpi:BOOL=OFF. minuit2_omp:BOOL=OFF. mlp:BOOL=ON. monalisa:BOOL=OFF. mpi:BOOL=OFF. mysql:BOOL=OFF. nlohmann_json_DIR:PATH=nlohmann_json_DIR-NOTFOUND. odbc:BOOL=OFF. opengl:BOOL=ON. oracle:BOOL=OFF. pgsql:BOOL=OFF. pyroot:BOOL=OFF. pyroot_legacy:BOOL=OFF. pythia6:BOOL=OFF. pythia6_nolink:BOOL=OFF. pythia8:BOOL=OFF. qt5web:BOOL=OFF. r:BOOL=OFF. roofit:BOOL=ON. root7:BOOL=OFF. rootbench:BOOL=OFF. roottest:BOOL=OFF. rpath:BOOL=OFF. runtime_cxxmodules:BOOL=OFF. shadowpw:BOOL=OFF. shared:BOOL=ON. soversion:BOOL=OFF. spectrum:BOOL=ON. sqlite:BOOL=OFF. ssl:BOOL=OFF. tcmalloc:BOOL=OFF. test_distrdf_pyspark:BOOL=OFF. testing:BOOL=OFF. tmva:BOOL=ON. tmva-cpu:BOOL=OFF. tmva-gpu:BOOL=OFF. tmva-pymva:BOOL=OFF. tmva-rmva:BOOL=OFF. tmva-sofie:BOOL=OFF. unuran:BOOL=OFF. uring:BOOL=OFF. vc:BOOL=OFF. vdt:BOOL=OFF. veccore:BOOL=OFF. vecgeom:BOOL=OFF. webgui:BOOL=OFF. win_broken_tests:BOOL=OFF. winrtdebug:BOOL=OFF. x11:BOOL=OFF. xml:BOOL=OFF. xproofd:BOOL=OFF. xrootd:BOOL=OFF. xxHash_FOUND:BOOL=TRUE. ```. **Edit**: compilation build finished now without errors.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:2663,interoperability,xml,xml,2663,nui5:BOOL=ON. builtin_pcre:BOOL=ON. builtin_tbb:BOOL=OFF. builtin_unuran:BOOL=OFF. builtin_vc:BOOL=OFF. builtin_vdt:BOOL=OFF. builtin_veccore:BOOL=OFF. builtin_xrootd:BOOL=OFF. builtin_xxhash:BOOL=ON. builtin_zlib:BOOL=ON. builtin_zstd:BOOL=ON. ccache:BOOL=OFF. cefweb:BOOL=OFF. clad:BOOL=ON. clingtest:BOOL=OFF. cocoa:BOOL=OFF. compression_default:STRING=zlib. coverage:BOOL=OFF. cuda:BOOL=OFF. cudnn:BOOL=ON. cxxmodules:BOOL=OFF. daos:BOOL=OFF. dataframe:BOOL=ON. davix:BOOL=OFF. dcache:BOOL=OFF. dev:BOOL=OFF. distcc:BOOL=OFF. exceptions:BOOL=ON. fail-on-missing:BOOL=OFF. fcgi:BOOL=OFF. fftw3:BOOL=OFF. fitsio:BOOL=OFF. fortran:BOOL=OFF. gcctoolchain:PATH=. gdml:BOOL=ON. gfal:BOOL=OFF. gminimal:BOOL=OFF. gnuinstall:BOOL=OFF. gsl_shared:BOOL=OFF. gviz:BOOL=OFF. http:BOOL=ON. imt:BOOL=OFF. jemalloc:BOOL=OFF. libcxx:BOOL=OFF. macos_native:BOOL=OFF. mathmore:BOOL=OFF. memory_termination:BOOL=OFF. minimal:BOOL=OFF. minuit2:BOOL=ON. minuit2_mpi:BOOL=OFF. minuit2_omp:BOOL=OFF. mlp:BOOL=ON. monalisa:BOOL=OFF. mpi:BOOL=OFF. mysql:BOOL=OFF. nlohmann_json_DIR:PATH=nlohmann_json_DIR-NOTFOUND. odbc:BOOL=OFF. opengl:BOOL=ON. oracle:BOOL=OFF. pgsql:BOOL=OFF. pyroot:BOOL=OFF. pyroot_legacy:BOOL=OFF. pythia6:BOOL=OFF. pythia6_nolink:BOOL=OFF. pythia8:BOOL=OFF. qt5web:BOOL=OFF. r:BOOL=OFF. roofit:BOOL=ON. root7:BOOL=OFF. rootbench:BOOL=OFF. roottest:BOOL=OFF. rpath:BOOL=OFF. runtime_cxxmodules:BOOL=OFF. shadowpw:BOOL=OFF. shared:BOOL=ON. soversion:BOOL=OFF. spectrum:BOOL=ON. sqlite:BOOL=OFF. ssl:BOOL=OFF. tcmalloc:BOOL=OFF. test_distrdf_pyspark:BOOL=OFF. testing:BOOL=OFF. tmva:BOOL=ON. tmva-cpu:BOOL=OFF. tmva-gpu:BOOL=OFF. tmva-pymva:BOOL=OFF. tmva-rmva:BOOL=OFF. tmva-sofie:BOOL=OFF. unuran:BOOL=OFF. uring:BOOL=OFF. vc:BOOL=OFF. vdt:BOOL=OFF. veccore:BOOL=OFF. vecgeom:BOOL=OFF. webgui:BOOL=OFF. win_broken_tests:BOOL=OFF. winrtdebug:BOOL=OFF. x11:BOOL=OFF. xml:BOOL=OFF. xproofd:BOOL=OFF. xrootd:BOOL=OFF. xxHash_FOUND:BOOL=TRUE. ```. **Edit**: compilation build finished now without errors.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:2393,performance,cpu,cpu,2393,nui5:BOOL=ON. builtin_pcre:BOOL=ON. builtin_tbb:BOOL=OFF. builtin_unuran:BOOL=OFF. builtin_vc:BOOL=OFF. builtin_vdt:BOOL=OFF. builtin_veccore:BOOL=OFF. builtin_xrootd:BOOL=OFF. builtin_xxhash:BOOL=ON. builtin_zlib:BOOL=ON. builtin_zstd:BOOL=ON. ccache:BOOL=OFF. cefweb:BOOL=OFF. clad:BOOL=ON. clingtest:BOOL=OFF. cocoa:BOOL=OFF. compression_default:STRING=zlib. coverage:BOOL=OFF. cuda:BOOL=OFF. cudnn:BOOL=ON. cxxmodules:BOOL=OFF. daos:BOOL=OFF. dataframe:BOOL=ON. davix:BOOL=OFF. dcache:BOOL=OFF. dev:BOOL=OFF. distcc:BOOL=OFF. exceptions:BOOL=ON. fail-on-missing:BOOL=OFF. fcgi:BOOL=OFF. fftw3:BOOL=OFF. fitsio:BOOL=OFF. fortran:BOOL=OFF. gcctoolchain:PATH=. gdml:BOOL=ON. gfal:BOOL=OFF. gminimal:BOOL=OFF. gnuinstall:BOOL=OFF. gsl_shared:BOOL=OFF. gviz:BOOL=OFF. http:BOOL=ON. imt:BOOL=OFF. jemalloc:BOOL=OFF. libcxx:BOOL=OFF. macos_native:BOOL=OFF. mathmore:BOOL=OFF. memory_termination:BOOL=OFF. minimal:BOOL=OFF. minuit2:BOOL=ON. minuit2_mpi:BOOL=OFF. minuit2_omp:BOOL=OFF. mlp:BOOL=ON. monalisa:BOOL=OFF. mpi:BOOL=OFF. mysql:BOOL=OFF. nlohmann_json_DIR:PATH=nlohmann_json_DIR-NOTFOUND. odbc:BOOL=OFF. opengl:BOOL=ON. oracle:BOOL=OFF. pgsql:BOOL=OFF. pyroot:BOOL=OFF. pyroot_legacy:BOOL=OFF. pythia6:BOOL=OFF. pythia6_nolink:BOOL=OFF. pythia8:BOOL=OFF. qt5web:BOOL=OFF. r:BOOL=OFF. roofit:BOOL=ON. root7:BOOL=OFF. rootbench:BOOL=OFF. roottest:BOOL=OFF. rpath:BOOL=OFF. runtime_cxxmodules:BOOL=OFF. shadowpw:BOOL=OFF. shared:BOOL=ON. soversion:BOOL=OFF. spectrum:BOOL=ON. sqlite:BOOL=OFF. ssl:BOOL=OFF. tcmalloc:BOOL=OFF. test_distrdf_pyspark:BOOL=OFF. testing:BOOL=OFF. tmva:BOOL=ON. tmva-cpu:BOOL=OFF. tmva-gpu:BOOL=OFF. tmva-pymva:BOOL=OFF. tmva-rmva:BOOL=OFF. tmva-sofie:BOOL=OFF. unuran:BOOL=OFF. uring:BOOL=OFF. vc:BOOL=OFF. vdt:BOOL=OFF. veccore:BOOL=OFF. vecgeom:BOOL=OFF. webgui:BOOL=OFF. win_broken_tests:BOOL=OFF. winrtdebug:BOOL=OFF. x11:BOOL=OFF. xml:BOOL=OFF. xproofd:BOOL=OFF. xrootd:BOOL=OFF. xxHash_FOUND:BOOL=TRUE. ```. **Edit**: compilation build finished now without errors.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:2412,performance,gpu,gpu,2412,nui5:BOOL=ON. builtin_pcre:BOOL=ON. builtin_tbb:BOOL=OFF. builtin_unuran:BOOL=OFF. builtin_vc:BOOL=OFF. builtin_vdt:BOOL=OFF. builtin_veccore:BOOL=OFF. builtin_xrootd:BOOL=OFF. builtin_xxhash:BOOL=ON. builtin_zlib:BOOL=ON. builtin_zstd:BOOL=ON. ccache:BOOL=OFF. cefweb:BOOL=OFF. clad:BOOL=ON. clingtest:BOOL=OFF. cocoa:BOOL=OFF. compression_default:STRING=zlib. coverage:BOOL=OFF. cuda:BOOL=OFF. cudnn:BOOL=ON. cxxmodules:BOOL=OFF. daos:BOOL=OFF. dataframe:BOOL=ON. davix:BOOL=OFF. dcache:BOOL=OFF. dev:BOOL=OFF. distcc:BOOL=OFF. exceptions:BOOL=ON. fail-on-missing:BOOL=OFF. fcgi:BOOL=OFF. fftw3:BOOL=OFF. fitsio:BOOL=OFF. fortran:BOOL=OFF. gcctoolchain:PATH=. gdml:BOOL=ON. gfal:BOOL=OFF. gminimal:BOOL=OFF. gnuinstall:BOOL=OFF. gsl_shared:BOOL=OFF. gviz:BOOL=OFF. http:BOOL=ON. imt:BOOL=OFF. jemalloc:BOOL=OFF. libcxx:BOOL=OFF. macos_native:BOOL=OFF. mathmore:BOOL=OFF. memory_termination:BOOL=OFF. minimal:BOOL=OFF. minuit2:BOOL=ON. minuit2_mpi:BOOL=OFF. minuit2_omp:BOOL=OFF. mlp:BOOL=ON. monalisa:BOOL=OFF. mpi:BOOL=OFF. mysql:BOOL=OFF. nlohmann_json_DIR:PATH=nlohmann_json_DIR-NOTFOUND. odbc:BOOL=OFF. opengl:BOOL=ON. oracle:BOOL=OFF. pgsql:BOOL=OFF. pyroot:BOOL=OFF. pyroot_legacy:BOOL=OFF. pythia6:BOOL=OFF. pythia6_nolink:BOOL=OFF. pythia8:BOOL=OFF. qt5web:BOOL=OFF. r:BOOL=OFF. roofit:BOOL=ON. root7:BOOL=OFF. rootbench:BOOL=OFF. roottest:BOOL=OFF. rpath:BOOL=OFF. runtime_cxxmodules:BOOL=OFF. shadowpw:BOOL=OFF. shared:BOOL=ON. soversion:BOOL=OFF. spectrum:BOOL=ON. sqlite:BOOL=OFF. ssl:BOOL=OFF. tcmalloc:BOOL=OFF. test_distrdf_pyspark:BOOL=OFF. testing:BOOL=OFF. tmva:BOOL=ON. tmva-cpu:BOOL=OFF. tmva-gpu:BOOL=OFF. tmva-pymva:BOOL=OFF. tmva-rmva:BOOL=OFF. tmva-sofie:BOOL=OFF. unuran:BOOL=OFF. uring:BOOL=OFF. vc:BOOL=OFF. vdt:BOOL=OFF. veccore:BOOL=OFF. vecgeom:BOOL=OFF. webgui:BOOL=OFF. win_broken_tests:BOOL=OFF. winrtdebug:BOOL=OFF. x11:BOOL=OFF. xml:BOOL=OFF. xproofd:BOOL=OFF. xrootd:BOOL=OFF. xxHash_FOUND:BOOL=TRUE. ```. **Edit**: compilation build finished now without errors.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:2790,performance,error,errors,2790,nui5:BOOL=ON. builtin_pcre:BOOL=ON. builtin_tbb:BOOL=OFF. builtin_unuran:BOOL=OFF. builtin_vc:BOOL=OFF. builtin_vdt:BOOL=OFF. builtin_veccore:BOOL=OFF. builtin_xrootd:BOOL=OFF. builtin_xxhash:BOOL=ON. builtin_zlib:BOOL=ON. builtin_zstd:BOOL=ON. ccache:BOOL=OFF. cefweb:BOOL=OFF. clad:BOOL=ON. clingtest:BOOL=OFF. cocoa:BOOL=OFF. compression_default:STRING=zlib. coverage:BOOL=OFF. cuda:BOOL=OFF. cudnn:BOOL=ON. cxxmodules:BOOL=OFF. daos:BOOL=OFF. dataframe:BOOL=ON. davix:BOOL=OFF. dcache:BOOL=OFF. dev:BOOL=OFF. distcc:BOOL=OFF. exceptions:BOOL=ON. fail-on-missing:BOOL=OFF. fcgi:BOOL=OFF. fftw3:BOOL=OFF. fitsio:BOOL=OFF. fortran:BOOL=OFF. gcctoolchain:PATH=. gdml:BOOL=ON. gfal:BOOL=OFF. gminimal:BOOL=OFF. gnuinstall:BOOL=OFF. gsl_shared:BOOL=OFF. gviz:BOOL=OFF. http:BOOL=ON. imt:BOOL=OFF. jemalloc:BOOL=OFF. libcxx:BOOL=OFF. macos_native:BOOL=OFF. mathmore:BOOL=OFF. memory_termination:BOOL=OFF. minimal:BOOL=OFF. minuit2:BOOL=ON. minuit2_mpi:BOOL=OFF. minuit2_omp:BOOL=OFF. mlp:BOOL=ON. monalisa:BOOL=OFF. mpi:BOOL=OFF. mysql:BOOL=OFF. nlohmann_json_DIR:PATH=nlohmann_json_DIR-NOTFOUND. odbc:BOOL=OFF. opengl:BOOL=ON. oracle:BOOL=OFF. pgsql:BOOL=OFF. pyroot:BOOL=OFF. pyroot_legacy:BOOL=OFF. pythia6:BOOL=OFF. pythia6_nolink:BOOL=OFF. pythia8:BOOL=OFF. qt5web:BOOL=OFF. r:BOOL=OFF. roofit:BOOL=ON. root7:BOOL=OFF. rootbench:BOOL=OFF. roottest:BOOL=OFF. rpath:BOOL=OFF. runtime_cxxmodules:BOOL=OFF. shadowpw:BOOL=OFF. shared:BOOL=ON. soversion:BOOL=OFF. spectrum:BOOL=ON. sqlite:BOOL=OFF. ssl:BOOL=OFF. tcmalloc:BOOL=OFF. test_distrdf_pyspark:BOOL=OFF. testing:BOOL=OFF. tmva:BOOL=ON. tmva-cpu:BOOL=OFF. tmva-gpu:BOOL=OFF. tmva-pymva:BOOL=OFF. tmva-rmva:BOOL=OFF. tmva-sofie:BOOL=OFF. unuran:BOOL=OFF. uring:BOOL=OFF. vc:BOOL=OFF. vdt:BOOL=OFF. veccore:BOOL=OFF. vecgeom:BOOL=OFF. webgui:BOOL=OFF. win_broken_tests:BOOL=OFF. winrtdebug:BOOL=OFF. x11:BOOL=OFF. xml:BOOL=OFF. xproofd:BOOL=OFF. xrootd:BOOL=OFF. xxHash_FOUND:BOOL=TRUE. ```. **Edit**: compilation build finished now without errors.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:1347,reliability,fail,fail-on-missing,1347,n:BOOL=OFF. asimage:BOOL=ON. asserts:BOOL=OFF. builtin_afterimage:BOOL=ON. builtin_cfitsio:BOOL=OFF. builtin_clang:BOOL=ON. builtin_cling:BOOL=ON. builtin_davix:BOOL=OFF. builtin_fftw3:BOOL=OFF. builtin_freetype:BOOL=ON. builtin_ftgl:BOOL=ON. builtin_gl2ps:BOOL=ON. builtin_glew:BOOL=ON. builtin_gsl:BOOL=OFF. builtin_llvm:BOOL=ON. builtin_lz4:BOOL=ON. builtin_lzma:BOOL=ON. builtin_nlohmannjson:BOOL=ON. builtin_openssl:BOOL=OFF. builtin_openui5:BOOL=ON. builtin_pcre:BOOL=ON. builtin_tbb:BOOL=OFF. builtin_unuran:BOOL=OFF. builtin_vc:BOOL=OFF. builtin_vdt:BOOL=OFF. builtin_veccore:BOOL=OFF. builtin_xrootd:BOOL=OFF. builtin_xxhash:BOOL=ON. builtin_zlib:BOOL=ON. builtin_zstd:BOOL=ON. ccache:BOOL=OFF. cefweb:BOOL=OFF. clad:BOOL=ON. clingtest:BOOL=OFF. cocoa:BOOL=OFF. compression_default:STRING=zlib. coverage:BOOL=OFF. cuda:BOOL=OFF. cudnn:BOOL=ON. cxxmodules:BOOL=OFF. daos:BOOL=OFF. dataframe:BOOL=ON. davix:BOOL=OFF. dcache:BOOL=OFF. dev:BOOL=OFF. distcc:BOOL=OFF. exceptions:BOOL=ON. fail-on-missing:BOOL=OFF. fcgi:BOOL=OFF. fftw3:BOOL=OFF. fitsio:BOOL=OFF. fortran:BOOL=OFF. gcctoolchain:PATH=. gdml:BOOL=ON. gfal:BOOL=OFF. gminimal:BOOL=OFF. gnuinstall:BOOL=OFF. gsl_shared:BOOL=OFF. gviz:BOOL=OFF. http:BOOL=ON. imt:BOOL=OFF. jemalloc:BOOL=OFF. libcxx:BOOL=OFF. macos_native:BOOL=OFF. mathmore:BOOL=OFF. memory_termination:BOOL=OFF. minimal:BOOL=OFF. minuit2:BOOL=ON. minuit2_mpi:BOOL=OFF. minuit2_omp:BOOL=OFF. mlp:BOOL=ON. monalisa:BOOL=OFF. mpi:BOOL=OFF. mysql:BOOL=OFF. nlohmann_json_DIR:PATH=nlohmann_json_DIR-NOTFOUND. odbc:BOOL=OFF. opengl:BOOL=ON. oracle:BOOL=OFF. pgsql:BOOL=OFF. pyroot:BOOL=OFF. pyroot_legacy:BOOL=OFF. pythia6:BOOL=OFF. pythia6_nolink:BOOL=OFF. pythia8:BOOL=OFF. qt5web:BOOL=OFF. r:BOOL=OFF. roofit:BOOL=ON. root7:BOOL=OFF. rootbench:BOOL=OFF. roottest:BOOL=OFF. rpath:BOOL=OFF. runtime_cxxmodules:BOOL=OFF. shadowpw:BOOL=OFF. shared:BOOL=ON. soversion:BOOL=OFF. spectrum:BOOL=ON. sqlite:BOOL=OFF. ssl:BOOL=OFF. tcmalloc:BOOL=OFF. test_distrdf_pyspark:BOOL=OFF.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:1327,safety,except,exceptions,1327,FF. arrow:BOOL=OFF. asan:BOOL=OFF. asimage:BOOL=ON. asserts:BOOL=OFF. builtin_afterimage:BOOL=ON. builtin_cfitsio:BOOL=OFF. builtin_clang:BOOL=ON. builtin_cling:BOOL=ON. builtin_davix:BOOL=OFF. builtin_fftw3:BOOL=OFF. builtin_freetype:BOOL=ON. builtin_ftgl:BOOL=ON. builtin_gl2ps:BOOL=ON. builtin_glew:BOOL=ON. builtin_gsl:BOOL=OFF. builtin_llvm:BOOL=ON. builtin_lz4:BOOL=ON. builtin_lzma:BOOL=ON. builtin_nlohmannjson:BOOL=ON. builtin_openssl:BOOL=OFF. builtin_openui5:BOOL=ON. builtin_pcre:BOOL=ON. builtin_tbb:BOOL=OFF. builtin_unuran:BOOL=OFF. builtin_vc:BOOL=OFF. builtin_vdt:BOOL=OFF. builtin_veccore:BOOL=OFF. builtin_xrootd:BOOL=OFF. builtin_xxhash:BOOL=ON. builtin_zlib:BOOL=ON. builtin_zstd:BOOL=ON. ccache:BOOL=OFF. cefweb:BOOL=OFF. clad:BOOL=ON. clingtest:BOOL=OFF. cocoa:BOOL=OFF. compression_default:STRING=zlib. coverage:BOOL=OFF. cuda:BOOL=OFF. cudnn:BOOL=ON. cxxmodules:BOOL=OFF. daos:BOOL=OFF. dataframe:BOOL=ON. davix:BOOL=OFF. dcache:BOOL=OFF. dev:BOOL=OFF. distcc:BOOL=OFF. exceptions:BOOL=ON. fail-on-missing:BOOL=OFF. fcgi:BOOL=OFF. fftw3:BOOL=OFF. fitsio:BOOL=OFF. fortran:BOOL=OFF. gcctoolchain:PATH=. gdml:BOOL=ON. gfal:BOOL=OFF. gminimal:BOOL=OFF. gnuinstall:BOOL=OFF. gsl_shared:BOOL=OFF. gviz:BOOL=OFF. http:BOOL=ON. imt:BOOL=OFF. jemalloc:BOOL=OFF. libcxx:BOOL=OFF. macos_native:BOOL=OFF. mathmore:BOOL=OFF. memory_termination:BOOL=OFF. minimal:BOOL=OFF. minuit2:BOOL=ON. minuit2_mpi:BOOL=OFF. minuit2_omp:BOOL=OFF. mlp:BOOL=ON. monalisa:BOOL=OFF. mpi:BOOL=OFF. mysql:BOOL=OFF. nlohmann_json_DIR:PATH=nlohmann_json_DIR-NOTFOUND. odbc:BOOL=OFF. opengl:BOOL=ON. oracle:BOOL=OFF. pgsql:BOOL=OFF. pyroot:BOOL=OFF. pyroot_legacy:BOOL=OFF. pythia6:BOOL=OFF. pythia6_nolink:BOOL=OFF. pythia8:BOOL=OFF. qt5web:BOOL=OFF. r:BOOL=OFF. roofit:BOOL=ON. root7:BOOL=OFF. rootbench:BOOL=OFF. roottest:BOOL=OFF. rpath:BOOL=OFF. runtime_cxxmodules:BOOL=OFF. shadowpw:BOOL=OFF. shared:BOOL=ON. soversion:BOOL=OFF. spectrum:BOOL=ON. sqlite:BOOL=OFF. ssl:BOOL=OFF. tcmalloc:BOOL=OFF. test_di,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:2356,safety,test,testing,2356,nui5:BOOL=ON. builtin_pcre:BOOL=ON. builtin_tbb:BOOL=OFF. builtin_unuran:BOOL=OFF. builtin_vc:BOOL=OFF. builtin_vdt:BOOL=OFF. builtin_veccore:BOOL=OFF. builtin_xrootd:BOOL=OFF. builtin_xxhash:BOOL=ON. builtin_zlib:BOOL=ON. builtin_zstd:BOOL=ON. ccache:BOOL=OFF. cefweb:BOOL=OFF. clad:BOOL=ON. clingtest:BOOL=OFF. cocoa:BOOL=OFF. compression_default:STRING=zlib. coverage:BOOL=OFF. cuda:BOOL=OFF. cudnn:BOOL=ON. cxxmodules:BOOL=OFF. daos:BOOL=OFF. dataframe:BOOL=ON. davix:BOOL=OFF. dcache:BOOL=OFF. dev:BOOL=OFF. distcc:BOOL=OFF. exceptions:BOOL=ON. fail-on-missing:BOOL=OFF. fcgi:BOOL=OFF. fftw3:BOOL=OFF. fitsio:BOOL=OFF. fortran:BOOL=OFF. gcctoolchain:PATH=. gdml:BOOL=ON. gfal:BOOL=OFF. gminimal:BOOL=OFF. gnuinstall:BOOL=OFF. gsl_shared:BOOL=OFF. gviz:BOOL=OFF. http:BOOL=ON. imt:BOOL=OFF. jemalloc:BOOL=OFF. libcxx:BOOL=OFF. macos_native:BOOL=OFF. mathmore:BOOL=OFF. memory_termination:BOOL=OFF. minimal:BOOL=OFF. minuit2:BOOL=ON. minuit2_mpi:BOOL=OFF. minuit2_omp:BOOL=OFF. mlp:BOOL=ON. monalisa:BOOL=OFF. mpi:BOOL=OFF. mysql:BOOL=OFF. nlohmann_json_DIR:PATH=nlohmann_json_DIR-NOTFOUND. odbc:BOOL=OFF. opengl:BOOL=ON. oracle:BOOL=OFF. pgsql:BOOL=OFF. pyroot:BOOL=OFF. pyroot_legacy:BOOL=OFF. pythia6:BOOL=OFF. pythia6_nolink:BOOL=OFF. pythia8:BOOL=OFF. qt5web:BOOL=OFF. r:BOOL=OFF. roofit:BOOL=ON. root7:BOOL=OFF. rootbench:BOOL=OFF. roottest:BOOL=OFF. rpath:BOOL=OFF. runtime_cxxmodules:BOOL=OFF. shadowpw:BOOL=OFF. shared:BOOL=ON. soversion:BOOL=OFF. spectrum:BOOL=ON. sqlite:BOOL=OFF. ssl:BOOL=OFF. tcmalloc:BOOL=OFF. test_distrdf_pyspark:BOOL=OFF. testing:BOOL=OFF. tmva:BOOL=ON. tmva-cpu:BOOL=OFF. tmva-gpu:BOOL=OFF. tmva-pymva:BOOL=OFF. tmva-rmva:BOOL=OFF. tmva-sofie:BOOL=OFF. unuran:BOOL=OFF. uring:BOOL=OFF. vc:BOOL=OFF. vdt:BOOL=OFF. veccore:BOOL=OFF. vecgeom:BOOL=OFF. webgui:BOOL=OFF. win_broken_tests:BOOL=OFF. winrtdebug:BOOL=OFF. x11:BOOL=OFF. xml:BOOL=OFF. xproofd:BOOL=OFF. xrootd:BOOL=OFF. xxHash_FOUND:BOOL=TRUE. ```. **Edit**: compilation build finished now without errors.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:2790,safety,error,errors,2790,nui5:BOOL=ON. builtin_pcre:BOOL=ON. builtin_tbb:BOOL=OFF. builtin_unuran:BOOL=OFF. builtin_vc:BOOL=OFF. builtin_vdt:BOOL=OFF. builtin_veccore:BOOL=OFF. builtin_xrootd:BOOL=OFF. builtin_xxhash:BOOL=ON. builtin_zlib:BOOL=ON. builtin_zstd:BOOL=ON. ccache:BOOL=OFF. cefweb:BOOL=OFF. clad:BOOL=ON. clingtest:BOOL=OFF. cocoa:BOOL=OFF. compression_default:STRING=zlib. coverage:BOOL=OFF. cuda:BOOL=OFF. cudnn:BOOL=ON. cxxmodules:BOOL=OFF. daos:BOOL=OFF. dataframe:BOOL=ON. davix:BOOL=OFF. dcache:BOOL=OFF. dev:BOOL=OFF. distcc:BOOL=OFF. exceptions:BOOL=ON. fail-on-missing:BOOL=OFF. fcgi:BOOL=OFF. fftw3:BOOL=OFF. fitsio:BOOL=OFF. fortran:BOOL=OFF. gcctoolchain:PATH=. gdml:BOOL=ON. gfal:BOOL=OFF. gminimal:BOOL=OFF. gnuinstall:BOOL=OFF. gsl_shared:BOOL=OFF. gviz:BOOL=OFF. http:BOOL=ON. imt:BOOL=OFF. jemalloc:BOOL=OFF. libcxx:BOOL=OFF. macos_native:BOOL=OFF. mathmore:BOOL=OFF. memory_termination:BOOL=OFF. minimal:BOOL=OFF. minuit2:BOOL=ON. minuit2_mpi:BOOL=OFF. minuit2_omp:BOOL=OFF. mlp:BOOL=ON. monalisa:BOOL=OFF. mpi:BOOL=OFF. mysql:BOOL=OFF. nlohmann_json_DIR:PATH=nlohmann_json_DIR-NOTFOUND. odbc:BOOL=OFF. opengl:BOOL=ON. oracle:BOOL=OFF. pgsql:BOOL=OFF. pyroot:BOOL=OFF. pyroot_legacy:BOOL=OFF. pythia6:BOOL=OFF. pythia6_nolink:BOOL=OFF. pythia8:BOOL=OFF. qt5web:BOOL=OFF. r:BOOL=OFF. roofit:BOOL=ON. root7:BOOL=OFF. rootbench:BOOL=OFF. roottest:BOOL=OFF. rpath:BOOL=OFF. runtime_cxxmodules:BOOL=OFF. shadowpw:BOOL=OFF. shared:BOOL=ON. soversion:BOOL=OFF. spectrum:BOOL=ON. sqlite:BOOL=OFF. ssl:BOOL=OFF. tcmalloc:BOOL=OFF. test_distrdf_pyspark:BOOL=OFF. testing:BOOL=OFF. tmva:BOOL=ON. tmva-cpu:BOOL=OFF. tmva-gpu:BOOL=OFF. tmva-pymva:BOOL=OFF. tmva-rmva:BOOL=OFF. tmva-sofie:BOOL=OFF. unuran:BOOL=OFF. uring:BOOL=OFF. vc:BOOL=OFF. vdt:BOOL=OFF. veccore:BOOL=OFF. vecgeom:BOOL=OFF. webgui:BOOL=OFF. win_broken_tests:BOOL=OFF. winrtdebug:BOOL=OFF. x11:BOOL=OFF. xml:BOOL=OFF. xproofd:BOOL=OFF. xrootd:BOOL=OFF. xxHash_FOUND:BOOL=TRUE. ```. **Edit**: compilation build finished now without errors.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:2292,security,ssl,ssl,2292,nui5:BOOL=ON. builtin_pcre:BOOL=ON. builtin_tbb:BOOL=OFF. builtin_unuran:BOOL=OFF. builtin_vc:BOOL=OFF. builtin_vdt:BOOL=OFF. builtin_veccore:BOOL=OFF. builtin_xrootd:BOOL=OFF. builtin_xxhash:BOOL=ON. builtin_zlib:BOOL=ON. builtin_zstd:BOOL=ON. ccache:BOOL=OFF. cefweb:BOOL=OFF. clad:BOOL=ON. clingtest:BOOL=OFF. cocoa:BOOL=OFF. compression_default:STRING=zlib. coverage:BOOL=OFF. cuda:BOOL=OFF. cudnn:BOOL=ON. cxxmodules:BOOL=OFF. daos:BOOL=OFF. dataframe:BOOL=ON. davix:BOOL=OFF. dcache:BOOL=OFF. dev:BOOL=OFF. distcc:BOOL=OFF. exceptions:BOOL=ON. fail-on-missing:BOOL=OFF. fcgi:BOOL=OFF. fftw3:BOOL=OFF. fitsio:BOOL=OFF. fortran:BOOL=OFF. gcctoolchain:PATH=. gdml:BOOL=ON. gfal:BOOL=OFF. gminimal:BOOL=OFF. gnuinstall:BOOL=OFF. gsl_shared:BOOL=OFF. gviz:BOOL=OFF. http:BOOL=ON. imt:BOOL=OFF. jemalloc:BOOL=OFF. libcxx:BOOL=OFF. macos_native:BOOL=OFF. mathmore:BOOL=OFF. memory_termination:BOOL=OFF. minimal:BOOL=OFF. minuit2:BOOL=ON. minuit2_mpi:BOOL=OFF. minuit2_omp:BOOL=OFF. mlp:BOOL=ON. monalisa:BOOL=OFF. mpi:BOOL=OFF. mysql:BOOL=OFF. nlohmann_json_DIR:PATH=nlohmann_json_DIR-NOTFOUND. odbc:BOOL=OFF. opengl:BOOL=ON. oracle:BOOL=OFF. pgsql:BOOL=OFF. pyroot:BOOL=OFF. pyroot_legacy:BOOL=OFF. pythia6:BOOL=OFF. pythia6_nolink:BOOL=OFF. pythia8:BOOL=OFF. qt5web:BOOL=OFF. r:BOOL=OFF. roofit:BOOL=ON. root7:BOOL=OFF. rootbench:BOOL=OFF. roottest:BOOL=OFF. rpath:BOOL=OFF. runtime_cxxmodules:BOOL=OFF. shadowpw:BOOL=OFF. shared:BOOL=ON. soversion:BOOL=OFF. spectrum:BOOL=ON. sqlite:BOOL=OFF. ssl:BOOL=OFF. tcmalloc:BOOL=OFF. test_distrdf_pyspark:BOOL=OFF. testing:BOOL=OFF. tmva:BOOL=ON. tmva-cpu:BOOL=OFF. tmva-gpu:BOOL=OFF. tmva-pymva:BOOL=OFF. tmva-rmva:BOOL=OFF. tmva-sofie:BOOL=OFF. unuran:BOOL=OFF. uring:BOOL=OFF. vc:BOOL=OFF. vdt:BOOL=OFF. veccore:BOOL=OFF. vecgeom:BOOL=OFF. webgui:BOOL=OFF. win_broken_tests:BOOL=OFF. winrtdebug:BOOL=OFF. x11:BOOL=OFF. xml:BOOL=OFF. xproofd:BOOL=OFF. xrootd:BOOL=OFF. xxHash_FOUND:BOOL=TRUE. ```. **Edit**: compilation build finished now without errors.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:384,testability,assert,asserts,384,"@bellenot . Thanks, I'm now trying it (runs already again for a while). Are there more of these high level options and are they documented somewhere? I ran `cmake -LA` and see a long list, with of course also settings from the subpackages used, but I think the bottom part gives, probably, the list:. ```. alien:BOOL=OFF. all:BOOL=OFF. arrow:BOOL=OFF. asan:BOOL=OFF. asimage:BOOL=ON. asserts:BOOL=OFF. builtin_afterimage:BOOL=ON. builtin_cfitsio:BOOL=OFF. builtin_clang:BOOL=ON. builtin_cling:BOOL=ON. builtin_davix:BOOL=OFF. builtin_fftw3:BOOL=OFF. builtin_freetype:BOOL=ON. builtin_ftgl:BOOL=ON. builtin_gl2ps:BOOL=ON. builtin_glew:BOOL=ON. builtin_gsl:BOOL=OFF. builtin_llvm:BOOL=ON. builtin_lz4:BOOL=ON. builtin_lzma:BOOL=ON. builtin_nlohmannjson:BOOL=ON. builtin_openssl:BOOL=OFF. builtin_openui5:BOOL=ON. builtin_pcre:BOOL=ON. builtin_tbb:BOOL=OFF. builtin_unuran:BOOL=OFF. builtin_vc:BOOL=OFF. builtin_vdt:BOOL=OFF. builtin_veccore:BOOL=OFF. builtin_xrootd:BOOL=OFF. builtin_xxhash:BOOL=ON. builtin_zlib:BOOL=ON. builtin_zstd:BOOL=ON. ccache:BOOL=OFF. cefweb:BOOL=OFF. clad:BOOL=ON. clingtest:BOOL=OFF. cocoa:BOOL=OFF. compression_default:STRING=zlib. coverage:BOOL=OFF. cuda:BOOL=OFF. cudnn:BOOL=ON. cxxmodules:BOOL=OFF. daos:BOOL=OFF. dataframe:BOOL=ON. davix:BOOL=OFF. dcache:BOOL=OFF. dev:BOOL=OFF. distcc:BOOL=OFF. exceptions:BOOL=ON. fail-on-missing:BOOL=OFF. fcgi:BOOL=OFF. fftw3:BOOL=OFF. fitsio:BOOL=OFF. fortran:BOOL=OFF. gcctoolchain:PATH=. gdml:BOOL=ON. gfal:BOOL=OFF. gminimal:BOOL=OFF. gnuinstall:BOOL=OFF. gsl_shared:BOOL=OFF. gviz:BOOL=OFF. http:BOOL=ON. imt:BOOL=OFF. jemalloc:BOOL=OFF. libcxx:BOOL=OFF. macos_native:BOOL=OFF. mathmore:BOOL=OFF. memory_termination:BOOL=OFF. minimal:BOOL=OFF. minuit2:BOOL=ON. minuit2_mpi:BOOL=OFF. minuit2_omp:BOOL=OFF. mlp:BOOL=ON. monalisa:BOOL=OFF. mpi:BOOL=OFF. mysql:BOOL=OFF. nlohmann_json_DIR:PATH=nlohmann_json_DIR-NOTFOUND. odbc:BOOL=OFF. opengl:BOOL=ON. oracle:BOOL=OFF. pgsql:BOOL=OFF. pyroot:BOOL=OFF. pyroot_legacy:BOOL=OFF. pyth",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:1159,testability,coverag,coverage,1159,"-LA` and see a long list, with of course also settings from the subpackages used, but I think the bottom part gives, probably, the list:. ```. alien:BOOL=OFF. all:BOOL=OFF. arrow:BOOL=OFF. asan:BOOL=OFF. asimage:BOOL=ON. asserts:BOOL=OFF. builtin_afterimage:BOOL=ON. builtin_cfitsio:BOOL=OFF. builtin_clang:BOOL=ON. builtin_cling:BOOL=ON. builtin_davix:BOOL=OFF. builtin_fftw3:BOOL=OFF. builtin_freetype:BOOL=ON. builtin_ftgl:BOOL=ON. builtin_gl2ps:BOOL=ON. builtin_glew:BOOL=ON. builtin_gsl:BOOL=OFF. builtin_llvm:BOOL=ON. builtin_lz4:BOOL=ON. builtin_lzma:BOOL=ON. builtin_nlohmannjson:BOOL=ON. builtin_openssl:BOOL=OFF. builtin_openui5:BOOL=ON. builtin_pcre:BOOL=ON. builtin_tbb:BOOL=OFF. builtin_unuran:BOOL=OFF. builtin_vc:BOOL=OFF. builtin_vdt:BOOL=OFF. builtin_veccore:BOOL=OFF. builtin_xrootd:BOOL=OFF. builtin_xxhash:BOOL=ON. builtin_zlib:BOOL=ON. builtin_zstd:BOOL=ON. ccache:BOOL=OFF. cefweb:BOOL=OFF. clad:BOOL=ON. clingtest:BOOL=OFF. cocoa:BOOL=OFF. compression_default:STRING=zlib. coverage:BOOL=OFF. cuda:BOOL=OFF. cudnn:BOOL=ON. cxxmodules:BOOL=OFF. daos:BOOL=OFF. dataframe:BOOL=ON. davix:BOOL=OFF. dcache:BOOL=OFF. dev:BOOL=OFF. distcc:BOOL=OFF. exceptions:BOOL=ON. fail-on-missing:BOOL=OFF. fcgi:BOOL=OFF. fftw3:BOOL=OFF. fitsio:BOOL=OFF. fortran:BOOL=OFF. gcctoolchain:PATH=. gdml:BOOL=ON. gfal:BOOL=OFF. gminimal:BOOL=OFF. gnuinstall:BOOL=OFF. gsl_shared:BOOL=OFF. gviz:BOOL=OFF. http:BOOL=ON. imt:BOOL=OFF. jemalloc:BOOL=OFF. libcxx:BOOL=OFF. macos_native:BOOL=OFF. mathmore:BOOL=OFF. memory_termination:BOOL=OFF. minimal:BOOL=OFF. minuit2:BOOL=ON. minuit2_mpi:BOOL=OFF. minuit2_omp:BOOL=OFF. mlp:BOOL=ON. monalisa:BOOL=OFF. mpi:BOOL=OFF. mysql:BOOL=OFF. nlohmann_json_DIR:PATH=nlohmann_json_DIR-NOTFOUND. odbc:BOOL=OFF. opengl:BOOL=ON. oracle:BOOL=OFF. pgsql:BOOL=OFF. pyroot:BOOL=OFF. pyroot_legacy:BOOL=OFF. pythia6:BOOL=OFF. pythia6_nolink:BOOL=OFF. pythia8:BOOL=OFF. qt5web:BOOL=OFF. r:BOOL=OFF. roofit:BOOL=ON. root7:BOOL=OFF. rootbench:BOOL=OFF. roottest:BOOL=OFF. rpath:",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:2356,testability,test,testing,2356,nui5:BOOL=ON. builtin_pcre:BOOL=ON. builtin_tbb:BOOL=OFF. builtin_unuran:BOOL=OFF. builtin_vc:BOOL=OFF. builtin_vdt:BOOL=OFF. builtin_veccore:BOOL=OFF. builtin_xrootd:BOOL=OFF. builtin_xxhash:BOOL=ON. builtin_zlib:BOOL=ON. builtin_zstd:BOOL=ON. ccache:BOOL=OFF. cefweb:BOOL=OFF. clad:BOOL=ON. clingtest:BOOL=OFF. cocoa:BOOL=OFF. compression_default:STRING=zlib. coverage:BOOL=OFF. cuda:BOOL=OFF. cudnn:BOOL=ON. cxxmodules:BOOL=OFF. daos:BOOL=OFF. dataframe:BOOL=ON. davix:BOOL=OFF. dcache:BOOL=OFF. dev:BOOL=OFF. distcc:BOOL=OFF. exceptions:BOOL=ON. fail-on-missing:BOOL=OFF. fcgi:BOOL=OFF. fftw3:BOOL=OFF. fitsio:BOOL=OFF. fortran:BOOL=OFF. gcctoolchain:PATH=. gdml:BOOL=ON. gfal:BOOL=OFF. gminimal:BOOL=OFF. gnuinstall:BOOL=OFF. gsl_shared:BOOL=OFF. gviz:BOOL=OFF. http:BOOL=ON. imt:BOOL=OFF. jemalloc:BOOL=OFF. libcxx:BOOL=OFF. macos_native:BOOL=OFF. mathmore:BOOL=OFF. memory_termination:BOOL=OFF. minimal:BOOL=OFF. minuit2:BOOL=ON. minuit2_mpi:BOOL=OFF. minuit2_omp:BOOL=OFF. mlp:BOOL=ON. monalisa:BOOL=OFF. mpi:BOOL=OFF. mysql:BOOL=OFF. nlohmann_json_DIR:PATH=nlohmann_json_DIR-NOTFOUND. odbc:BOOL=OFF. opengl:BOOL=ON. oracle:BOOL=OFF. pgsql:BOOL=OFF. pyroot:BOOL=OFF. pyroot_legacy:BOOL=OFF. pythia6:BOOL=OFF. pythia6_nolink:BOOL=OFF. pythia8:BOOL=OFF. qt5web:BOOL=OFF. r:BOOL=OFF. roofit:BOOL=ON. root7:BOOL=OFF. rootbench:BOOL=OFF. roottest:BOOL=OFF. rpath:BOOL=OFF. runtime_cxxmodules:BOOL=OFF. shadowpw:BOOL=OFF. shared:BOOL=ON. soversion:BOOL=OFF. spectrum:BOOL=ON. sqlite:BOOL=OFF. ssl:BOOL=OFF. tcmalloc:BOOL=OFF. test_distrdf_pyspark:BOOL=OFF. testing:BOOL=OFF. tmva:BOOL=ON. tmva-cpu:BOOL=OFF. tmva-gpu:BOOL=OFF. tmva-pymva:BOOL=OFF. tmva-rmva:BOOL=OFF. tmva-sofie:BOOL=OFF. unuran:BOOL=OFF. uring:BOOL=OFF. vc:BOOL=OFF. vdt:BOOL=OFF. veccore:BOOL=OFF. vecgeom:BOOL=OFF. webgui:BOOL=OFF. win_broken_tests:BOOL=OFF. winrtdebug:BOOL=OFF. x11:BOOL=OFF. xml:BOOL=OFF. xproofd:BOOL=OFF. xrootd:BOOL=OFF. xxHash_FOUND:BOOL=TRUE. ```. **Edit**: compilation build finished now without errors.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:128,usability,document,documented,128,"@bellenot . Thanks, I'm now trying it (runs already again for a while). Are there more of these high level options and are they documented somewhere? I ran `cmake -LA` and see a long list, with of course also settings from the subpackages used, but I think the bottom part gives, probably, the list:. ```. alien:BOOL=OFF. all:BOOL=OFF. arrow:BOOL=OFF. asan:BOOL=OFF. asimage:BOOL=ON. asserts:BOOL=OFF. builtin_afterimage:BOOL=ON. builtin_cfitsio:BOOL=OFF. builtin_clang:BOOL=ON. builtin_cling:BOOL=ON. builtin_davix:BOOL=OFF. builtin_fftw3:BOOL=OFF. builtin_freetype:BOOL=ON. builtin_ftgl:BOOL=ON. builtin_gl2ps:BOOL=ON. builtin_glew:BOOL=ON. builtin_gsl:BOOL=OFF. builtin_llvm:BOOL=ON. builtin_lz4:BOOL=ON. builtin_lzma:BOOL=ON. builtin_nlohmannjson:BOOL=ON. builtin_openssl:BOOL=OFF. builtin_openui5:BOOL=ON. builtin_pcre:BOOL=ON. builtin_tbb:BOOL=OFF. builtin_unuran:BOOL=OFF. builtin_vc:BOOL=OFF. builtin_vdt:BOOL=OFF. builtin_veccore:BOOL=OFF. builtin_xrootd:BOOL=OFF. builtin_xxhash:BOOL=ON. builtin_zlib:BOOL=ON. builtin_zstd:BOOL=ON. ccache:BOOL=OFF. cefweb:BOOL=OFF. clad:BOOL=ON. clingtest:BOOL=OFF. cocoa:BOOL=OFF. compression_default:STRING=zlib. coverage:BOOL=OFF. cuda:BOOL=OFF. cudnn:BOOL=ON. cxxmodules:BOOL=OFF. daos:BOOL=OFF. dataframe:BOOL=ON. davix:BOOL=OFF. dcache:BOOL=OFF. dev:BOOL=OFF. distcc:BOOL=OFF. exceptions:BOOL=ON. fail-on-missing:BOOL=OFF. fcgi:BOOL=OFF. fftw3:BOOL=OFF. fitsio:BOOL=OFF. fortran:BOOL=OFF. gcctoolchain:PATH=. gdml:BOOL=ON. gfal:BOOL=OFF. gminimal:BOOL=OFF. gnuinstall:BOOL=OFF. gsl_shared:BOOL=OFF. gviz:BOOL=OFF. http:BOOL=ON. imt:BOOL=OFF. jemalloc:BOOL=OFF. libcxx:BOOL=OFF. macos_native:BOOL=OFF. mathmore:BOOL=OFF. memory_termination:BOOL=OFF. minimal:BOOL=OFF. minuit2:BOOL=ON. minuit2_mpi:BOOL=OFF. minuit2_omp:BOOL=OFF. mlp:BOOL=ON. monalisa:BOOL=OFF. mpi:BOOL=OFF. mysql:BOOL=OFF. nlohmann_json_DIR:PATH=nlohmann_json_DIR-NOTFOUND. odbc:BOOL=OFF. opengl:BOOL=ON. oracle:BOOL=OFF. pgsql:BOOL=OFF. pyroot:BOOL=OFF. pyroot_legacy:BOOL=OFF. pyth",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:1699,usability,minim,minimal,1699,=ON. builtin_lzma:BOOL=ON. builtin_nlohmannjson:BOOL=ON. builtin_openssl:BOOL=OFF. builtin_openui5:BOOL=ON. builtin_pcre:BOOL=ON. builtin_tbb:BOOL=OFF. builtin_unuran:BOOL=OFF. builtin_vc:BOOL=OFF. builtin_vdt:BOOL=OFF. builtin_veccore:BOOL=OFF. builtin_xrootd:BOOL=OFF. builtin_xxhash:BOOL=ON. builtin_zlib:BOOL=ON. builtin_zstd:BOOL=ON. ccache:BOOL=OFF. cefweb:BOOL=OFF. clad:BOOL=ON. clingtest:BOOL=OFF. cocoa:BOOL=OFF. compression_default:STRING=zlib. coverage:BOOL=OFF. cuda:BOOL=OFF. cudnn:BOOL=ON. cxxmodules:BOOL=OFF. daos:BOOL=OFF. dataframe:BOOL=ON. davix:BOOL=OFF. dcache:BOOL=OFF. dev:BOOL=OFF. distcc:BOOL=OFF. exceptions:BOOL=ON. fail-on-missing:BOOL=OFF. fcgi:BOOL=OFF. fftw3:BOOL=OFF. fitsio:BOOL=OFF. fortran:BOOL=OFF. gcctoolchain:PATH=. gdml:BOOL=ON. gfal:BOOL=OFF. gminimal:BOOL=OFF. gnuinstall:BOOL=OFF. gsl_shared:BOOL=OFF. gviz:BOOL=OFF. http:BOOL=ON. imt:BOOL=OFF. jemalloc:BOOL=OFF. libcxx:BOOL=OFF. macos_native:BOOL=OFF. mathmore:BOOL=OFF. memory_termination:BOOL=OFF. minimal:BOOL=OFF. minuit2:BOOL=ON. minuit2_mpi:BOOL=OFF. minuit2_omp:BOOL=OFF. mlp:BOOL=ON. monalisa:BOOL=OFF. mpi:BOOL=OFF. mysql:BOOL=OFF. nlohmann_json_DIR:PATH=nlohmann_json_DIR-NOTFOUND. odbc:BOOL=OFF. opengl:BOOL=ON. oracle:BOOL=OFF. pgsql:BOOL=OFF. pyroot:BOOL=OFF. pyroot_legacy:BOOL=OFF. pythia6:BOOL=OFF. pythia6_nolink:BOOL=OFF. pythia8:BOOL=OFF. qt5web:BOOL=OFF. r:BOOL=OFF. roofit:BOOL=ON. root7:BOOL=OFF. rootbench:BOOL=OFF. roottest:BOOL=OFF. rpath:BOOL=OFF. runtime_cxxmodules:BOOL=OFF. shadowpw:BOOL=OFF. shared:BOOL=ON. soversion:BOOL=OFF. spectrum:BOOL=ON. sqlite:BOOL=OFF. ssl:BOOL=OFF. tcmalloc:BOOL=OFF. test_distrdf_pyspark:BOOL=OFF. testing:BOOL=OFF. tmva:BOOL=ON. tmva-cpu:BOOL=OFF. tmva-gpu:BOOL=OFF. tmva-pymva:BOOL=OFF. tmva-rmva:BOOL=OFF. tmva-sofie:BOOL=OFF. unuran:BOOL=OFF. uring:BOOL=OFF. vc:BOOL=OFF. vdt:BOOL=OFF. veccore:BOOL=OFF. vecgeom:BOOL=OFF. webgui:BOOL=OFF. win_broken_tests:BOOL=OFF. winrtdebug:BOOL=OFF. x11:BOOL=OFF. xml:BOOL=OFF. xproofd:BOOL=OFF. xrootd:B,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:2790,usability,error,errors,2790,nui5:BOOL=ON. builtin_pcre:BOOL=ON. builtin_tbb:BOOL=OFF. builtin_unuran:BOOL=OFF. builtin_vc:BOOL=OFF. builtin_vdt:BOOL=OFF. builtin_veccore:BOOL=OFF. builtin_xrootd:BOOL=OFF. builtin_xxhash:BOOL=ON. builtin_zlib:BOOL=ON. builtin_zstd:BOOL=ON. ccache:BOOL=OFF. cefweb:BOOL=OFF. clad:BOOL=ON. clingtest:BOOL=OFF. cocoa:BOOL=OFF. compression_default:STRING=zlib. coverage:BOOL=OFF. cuda:BOOL=OFF. cudnn:BOOL=ON. cxxmodules:BOOL=OFF. daos:BOOL=OFF. dataframe:BOOL=ON. davix:BOOL=OFF. dcache:BOOL=OFF. dev:BOOL=OFF. distcc:BOOL=OFF. exceptions:BOOL=ON. fail-on-missing:BOOL=OFF. fcgi:BOOL=OFF. fftw3:BOOL=OFF. fitsio:BOOL=OFF. fortran:BOOL=OFF. gcctoolchain:PATH=. gdml:BOOL=ON. gfal:BOOL=OFF. gminimal:BOOL=OFF. gnuinstall:BOOL=OFF. gsl_shared:BOOL=OFF. gviz:BOOL=OFF. http:BOOL=ON. imt:BOOL=OFF. jemalloc:BOOL=OFF. libcxx:BOOL=OFF. macos_native:BOOL=OFF. mathmore:BOOL=OFF. memory_termination:BOOL=OFF. minimal:BOOL=OFF. minuit2:BOOL=ON. minuit2_mpi:BOOL=OFF. minuit2_omp:BOOL=OFF. mlp:BOOL=ON. monalisa:BOOL=OFF. mpi:BOOL=OFF. mysql:BOOL=OFF. nlohmann_json_DIR:PATH=nlohmann_json_DIR-NOTFOUND. odbc:BOOL=OFF. opengl:BOOL=ON. oracle:BOOL=OFF. pgsql:BOOL=OFF. pyroot:BOOL=OFF. pyroot_legacy:BOOL=OFF. pythia6:BOOL=OFF. pythia6_nolink:BOOL=OFF. pythia8:BOOL=OFF. qt5web:BOOL=OFF. r:BOOL=OFF. roofit:BOOL=ON. root7:BOOL=OFF. rootbench:BOOL=OFF. roottest:BOOL=OFF. rpath:BOOL=OFF. runtime_cxxmodules:BOOL=OFF. shadowpw:BOOL=OFF. shared:BOOL=ON. soversion:BOOL=OFF. spectrum:BOOL=ON. sqlite:BOOL=OFF. ssl:BOOL=OFF. tcmalloc:BOOL=OFF. test_distrdf_pyspark:BOOL=OFF. testing:BOOL=OFF. tmva:BOOL=ON. tmva-cpu:BOOL=OFF. tmva-gpu:BOOL=OFF. tmva-pymva:BOOL=OFF. tmva-rmva:BOOL=OFF. tmva-sofie:BOOL=OFF. unuran:BOOL=OFF. uring:BOOL=OFF. vc:BOOL=OFF. vdt:BOOL=OFF. veccore:BOOL=OFF. vecgeom:BOOL=OFF. webgui:BOOL=OFF. win_broken_tests:BOOL=OFF. winrtdebug:BOOL=OFF. x11:BOOL=OFF. xml:BOOL=OFF. xproofd:BOOL=OFF. xrootd:BOOL=OFF. xxHash_FOUND:BOOL=TRUE. ```. **Edit**: compilation build finished now without errors.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:120,deployability,instal,install,120,> Are there more of these high level options and are they documented somewhere? Some of them are. See https://root.cern/install/build_from_source/.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:58,usability,document,documented,58,> Are there more of these high level options and are they documented somewhere? Some of them are. See https://root.cern/install/build_from_source/.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:54,deployability,build,build,54,"@bellenot . Thanks I found them under the header ""All build options"" (https://root.cern/install/build_from_source/#all-build-options) and here are 2 versions one for version 6.22 and 6.24. (One of my next steps will be to try to build the documentation under Windows ..., though will take a while before I can start with this)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:88,deployability,instal,install,88,"@bellenot . Thanks I found them under the header ""All build options"" (https://root.cern/install/build_from_source/#all-build-options) and here are 2 versions one for version 6.22 and 6.24. (One of my next steps will be to try to build the documentation under Windows ..., though will take a while before I can start with this)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:119,deployability,build,build-options,119,"@bellenot . Thanks I found them under the header ""All build options"" (https://root.cern/install/build_from_source/#all-build-options) and here are 2 versions one for version 6.22 and 6.24. (One of my next steps will be to try to build the documentation under Windows ..., though will take a while before I can start with this)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:149,deployability,version,versions,149,"@bellenot . Thanks I found them under the header ""All build options"" (https://root.cern/install/build_from_source/#all-build-options) and here are 2 versions one for version 6.22 and 6.24. (One of my next steps will be to try to build the documentation under Windows ..., though will take a while before I can start with this)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:166,deployability,version,version,166,"@bellenot . Thanks I found them under the header ""All build options"" (https://root.cern/install/build_from_source/#all-build-options) and here are 2 versions one for version 6.22 and 6.24. (One of my next steps will be to try to build the documentation under Windows ..., though will take a while before I can start with this)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:229,deployability,build,build,229,"@bellenot . Thanks I found them under the header ""All build options"" (https://root.cern/install/build_from_source/#all-build-options) and here are 2 versions one for version 6.22 and 6.24. (One of my next steps will be to try to build the documentation under Windows ..., though will take a while before I can start with this)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:149,integrability,version,versions,149,"@bellenot . Thanks I found them under the header ""All build options"" (https://root.cern/install/build_from_source/#all-build-options) and here are 2 versions one for version 6.22 and 6.24. (One of my next steps will be to try to build the documentation under Windows ..., though will take a while before I can start with this)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:166,integrability,version,version,166,"@bellenot . Thanks I found them under the header ""All build options"" (https://root.cern/install/build_from_source/#all-build-options) and here are 2 versions one for version 6.22 and 6.24. (One of my next steps will be to try to build the documentation under Windows ..., though will take a while before I can start with this)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:149,modifiability,version,versions,149,"@bellenot . Thanks I found them under the header ""All build options"" (https://root.cern/install/build_from_source/#all-build-options) and here are 2 versions one for version 6.22 and 6.24. (One of my next steps will be to try to build the documentation under Windows ..., though will take a while before I can start with this)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:166,modifiability,version,version,166,"@bellenot . Thanks I found them under the header ""All build options"" (https://root.cern/install/build_from_source/#all-build-options) and here are 2 versions one for version 6.22 and 6.24. (One of my next steps will be to try to build the documentation under Windows ..., though will take a while before I can start with this)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:239,usability,document,documentation,239,"@bellenot . Thanks I found them under the header ""All build options"" (https://root.cern/install/build_from_source/#all-build-options) and here are 2 versions one for version 6.22 and 6.24. (One of my next steps will be to try to build the documentation under Windows ..., though will take a while before I can start with this)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:41,deployability,build,build,41,"> One of my next steps will be to try to build the documentation under Windows ..., though will take a while before I can start with this. I never tried this. Good luck!",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:51,usability,document,documentation,51,"> One of my next steps will be to try to build the documentation under Windows ..., though will take a while before I can start with this. I never tried this. Good luck!",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:195,availability,down,downloaded,195,> As a side note did you ever think about using doxygen tag files? . ROOT documentation is included in many experiments' documentations. We provide the tag files for them . The page files can be downloaded (for various versions of the documentation)[ from this page](https://root.cern/reference/).,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:219,deployability,version,versions,219,> As a side note did you ever think about using doxygen tag files? . ROOT documentation is included in many experiments' documentations. We provide the tag files for them . The page files can be downloaded (for various versions of the documentation)[ from this page](https://root.cern/reference/).,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:219,integrability,version,versions,219,> As a side note did you ever think about using doxygen tag files? . ROOT documentation is included in many experiments' documentations. We provide the tag files for them . The page files can be downloaded (for various versions of the documentation)[ from this page](https://root.cern/reference/).,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:219,modifiability,version,versions,219,> As a side note did you ever think about using doxygen tag files? . ROOT documentation is included in many experiments' documentations. We provide the tag files for them . The page files can be downloaded (for various versions of the documentation)[ from this page](https://root.cern/reference/).,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:74,usability,document,documentation,74,> As a side note did you ever think about using doxygen tag files? . ROOT documentation is included in many experiments' documentations. We provide the tag files for them . The page files can be downloaded (for various versions of the documentation)[ from this page](https://root.cern/reference/).,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:121,usability,document,documentations,121,> As a side note did you ever think about using doxygen tag files? . ROOT documentation is included in many experiments' documentations. We provide the tag files for them . The page files can be downloaded (for various versions of the documentation)[ from this page](https://root.cern/reference/).,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:235,usability,document,documentation,235,> As a side note did you ever think about using doxygen tag files? . ROOT documentation is included in many experiments' documentations. We provide the tag files for them . The page files can be downloaded (for various versions of the documentation)[ from this page](https://root.cern/reference/).,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:24,availability,servic,service,24,"@couet . That is a nice service to the users. I was thinking about the usage of tag files for ""internal"" use where the build of the documentation would be split in different parts (""sub-projects"") so that each part generates its own tag file that can be used by the other sub-projects. This could make the building of the documentation a bit more modular and it would give the opportunity to re-build just one sub-project (could be useful during development). I don't know how feasible this would be for the ROOT project though.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:24,deployability,servic,service,24,"@couet . That is a nice service to the users. I was thinking about the usage of tag files for ""internal"" use where the build of the documentation would be split in different parts (""sub-projects"") so that each part generates its own tag file that can be used by the other sub-projects. This could make the building of the documentation a bit more modular and it would give the opportunity to re-build just one sub-project (could be useful during development). I don't know how feasible this would be for the ROOT project though.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:119,deployability,build,build,119,"@couet . That is a nice service to the users. I was thinking about the usage of tag files for ""internal"" use where the build of the documentation would be split in different parts (""sub-projects"") so that each part generates its own tag file that can be used by the other sub-projects. This could make the building of the documentation a bit more modular and it would give the opportunity to re-build just one sub-project (could be useful during development). I don't know how feasible this would be for the ROOT project though.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:306,deployability,build,building,306,"@couet . That is a nice service to the users. I was thinking about the usage of tag files for ""internal"" use where the build of the documentation would be split in different parts (""sub-projects"") so that each part generates its own tag file that can be used by the other sub-projects. This could make the building of the documentation a bit more modular and it would give the opportunity to re-build just one sub-project (could be useful during development). I don't know how feasible this would be for the ROOT project though.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:347,deployability,modul,modular,347,"@couet . That is a nice service to the users. I was thinking about the usage of tag files for ""internal"" use where the build of the documentation would be split in different parts (""sub-projects"") so that each part generates its own tag file that can be used by the other sub-projects. This could make the building of the documentation a bit more modular and it would give the opportunity to re-build just one sub-project (could be useful during development). I don't know how feasible this would be for the ROOT project though.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:395,deployability,build,build,395,"@couet . That is a nice service to the users. I was thinking about the usage of tag files for ""internal"" use where the build of the documentation would be split in different parts (""sub-projects"") so that each part generates its own tag file that can be used by the other sub-projects. This could make the building of the documentation a bit more modular and it would give the opportunity to re-build just one sub-project (could be useful during development). I don't know how feasible this would be for the ROOT project though.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:24,integrability,servic,service,24,"@couet . That is a nice service to the users. I was thinking about the usage of tag files for ""internal"" use where the build of the documentation would be split in different parts (""sub-projects"") so that each part generates its own tag file that can be used by the other sub-projects. This could make the building of the documentation a bit more modular and it would give the opportunity to re-build just one sub-project (could be useful during development). I don't know how feasible this would be for the ROOT project though.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:182,integrability,sub,sub-projects,182,"@couet . That is a nice service to the users. I was thinking about the usage of tag files for ""internal"" use where the build of the documentation would be split in different parts (""sub-projects"") so that each part generates its own tag file that can be used by the other sub-projects. This could make the building of the documentation a bit more modular and it would give the opportunity to re-build just one sub-project (could be useful during development). I don't know how feasible this would be for the ROOT project though.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:272,integrability,sub,sub-projects,272,"@couet . That is a nice service to the users. I was thinking about the usage of tag files for ""internal"" use where the build of the documentation would be split in different parts (""sub-projects"") so that each part generates its own tag file that can be used by the other sub-projects. This could make the building of the documentation a bit more modular and it would give the opportunity to re-build just one sub-project (could be useful during development). I don't know how feasible this would be for the ROOT project though.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:347,integrability,modular,modular,347,"@couet . That is a nice service to the users. I was thinking about the usage of tag files for ""internal"" use where the build of the documentation would be split in different parts (""sub-projects"") so that each part generates its own tag file that can be used by the other sub-projects. This could make the building of the documentation a bit more modular and it would give the opportunity to re-build just one sub-project (could be useful during development). I don't know how feasible this would be for the ROOT project though.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:410,integrability,sub,sub-project,410,"@couet . That is a nice service to the users. I was thinking about the usage of tag files for ""internal"" use where the build of the documentation would be split in different parts (""sub-projects"") so that each part generates its own tag file that can be used by the other sub-projects. This could make the building of the documentation a bit more modular and it would give the opportunity to re-build just one sub-project (could be useful during development). I don't know how feasible this would be for the ROOT project though.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:24,modifiability,servic,service,24,"@couet . That is a nice service to the users. I was thinking about the usage of tag files for ""internal"" use where the build of the documentation would be split in different parts (""sub-projects"") so that each part generates its own tag file that can be used by the other sub-projects. This could make the building of the documentation a bit more modular and it would give the opportunity to re-build just one sub-project (could be useful during development). I don't know how feasible this would be for the ROOT project though.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:347,modifiability,modul,modular,347,"@couet . That is a nice service to the users. I was thinking about the usage of tag files for ""internal"" use where the build of the documentation would be split in different parts (""sub-projects"") so that each part generates its own tag file that can be used by the other sub-projects. This could make the building of the documentation a bit more modular and it would give the opportunity to re-build just one sub-project (could be useful during development). I don't know how feasible this would be for the ROOT project though.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:347,safety,modul,modular,347,"@couet . That is a nice service to the users. I was thinking about the usage of tag files for ""internal"" use where the build of the documentation would be split in different parts (""sub-projects"") so that each part generates its own tag file that can be used by the other sub-projects. This could make the building of the documentation a bit more modular and it would give the opportunity to re-build just one sub-project (could be useful during development). I don't know how feasible this would be for the ROOT project though.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:347,testability,modula,modular,347,"@couet . That is a nice service to the users. I was thinking about the usage of tag files for ""internal"" use where the build of the documentation would be split in different parts (""sub-projects"") so that each part generates its own tag file that can be used by the other sub-projects. This could make the building of the documentation a bit more modular and it would give the opportunity to re-build just one sub-project (could be useful during development). I don't know how feasible this would be for the ROOT project though.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:39,usability,user,users,39,"@couet . That is a nice service to the users. I was thinking about the usage of tag files for ""internal"" use where the build of the documentation would be split in different parts (""sub-projects"") so that each part generates its own tag file that can be used by the other sub-projects. This could make the building of the documentation a bit more modular and it would give the opportunity to re-build just one sub-project (could be useful during development). I don't know how feasible this would be for the ROOT project though.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:132,usability,document,documentation,132,"@couet . That is a nice service to the users. I was thinking about the usage of tag files for ""internal"" use where the build of the documentation would be split in different parts (""sub-projects"") so that each part generates its own tag file that can be used by the other sub-projects. This could make the building of the documentation a bit more modular and it would give the opportunity to re-build just one sub-project (could be useful during development). I don't know how feasible this would be for the ROOT project though.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:322,usability,document,documentation,322,"@couet . That is a nice service to the users. I was thinking about the usage of tag files for ""internal"" use where the build of the documentation would be split in different parts (""sub-projects"") so that each part generates its own tag file that can be used by the other sub-projects. This could make the building of the documentation a bit more modular and it would give the opportunity to re-build just one sub-project (could be useful during development). I don't know how feasible this would be for the ROOT project though.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:234,availability,avail,available,234,"@albert-github . That's a nice idea, and could be something to think about. I think we will anyway to build the whole documentation every night because:. - the various modules refer to each other a lot,. - we want to run all the code available in the doc. We have test suites to validate de software but the documentation is surely an extra one. - now it takes ""only"" a night so it is still doable to build it daily. If at some point the building becomes unmanageable the approach you suggest might be a solution.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:102,deployability,build,build,102,"@albert-github . That's a nice idea, and could be something to think about. I think we will anyway to build the whole documentation every night because:. - the various modules refer to each other a lot,. - we want to run all the code available in the doc. We have test suites to validate de software but the documentation is surely an extra one. - now it takes ""only"" a night so it is still doable to build it daily. If at some point the building becomes unmanageable the approach you suggest might be a solution.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:168,deployability,modul,modules,168,"@albert-github . That's a nice idea, and could be something to think about. I think we will anyway to build the whole documentation every night because:. - the various modules refer to each other a lot,. - we want to run all the code available in the doc. We have test suites to validate de software but the documentation is surely an extra one. - now it takes ""only"" a night so it is still doable to build it daily. If at some point the building becomes unmanageable the approach you suggest might be a solution.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:401,deployability,build,build,401,"@albert-github . That's a nice idea, and could be something to think about. I think we will anyway to build the whole documentation every night because:. - the various modules refer to each other a lot,. - we want to run all the code available in the doc. We have test suites to validate de software but the documentation is surely an extra one. - now it takes ""only"" a night so it is still doable to build it daily. If at some point the building becomes unmanageable the approach you suggest might be a solution.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:438,deployability,build,building,438,"@albert-github . That's a nice idea, and could be something to think about. I think we will anyway to build the whole documentation every night because:. - the various modules refer to each other a lot,. - we want to run all the code available in the doc. We have test suites to validate de software but the documentation is surely an extra one. - now it takes ""only"" a night so it is still doable to build it daily. If at some point the building becomes unmanageable the approach you suggest might be a solution.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:168,modifiability,modul,modules,168,"@albert-github . That's a nice idea, and could be something to think about. I think we will anyway to build the whole documentation every night because:. - the various modules refer to each other a lot,. - we want to run all the code available in the doc. We have test suites to validate de software but the documentation is surely an extra one. - now it takes ""only"" a night so it is still doable to build it daily. If at some point the building becomes unmanageable the approach you suggest might be a solution.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:234,reliability,availab,available,234,"@albert-github . That's a nice idea, and could be something to think about. I think we will anyway to build the whole documentation every night because:. - the various modules refer to each other a lot,. - we want to run all the code available in the doc. We have test suites to validate de software but the documentation is surely an extra one. - now it takes ""only"" a night so it is still doable to build it daily. If at some point the building becomes unmanageable the approach you suggest might be a solution.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:168,safety,modul,modules,168,"@albert-github . That's a nice idea, and could be something to think about. I think we will anyway to build the whole documentation every night because:. - the various modules refer to each other a lot,. - we want to run all the code available in the doc. We have test suites to validate de software but the documentation is surely an extra one. - now it takes ""only"" a night so it is still doable to build it daily. If at some point the building becomes unmanageable the approach you suggest might be a solution.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:234,safety,avail,available,234,"@albert-github . That's a nice idea, and could be something to think about. I think we will anyway to build the whole documentation every night because:. - the various modules refer to each other a lot,. - we want to run all the code available in the doc. We have test suites to validate de software but the documentation is surely an extra one. - now it takes ""only"" a night so it is still doable to build it daily. If at some point the building becomes unmanageable the approach you suggest might be a solution.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:264,safety,test,test,264,"@albert-github . That's a nice idea, and could be something to think about. I think we will anyway to build the whole documentation every night because:. - the various modules refer to each other a lot,. - we want to run all the code available in the doc. We have test suites to validate de software but the documentation is surely an extra one. - now it takes ""only"" a night so it is still doable to build it daily. If at some point the building becomes unmanageable the approach you suggest might be a solution.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:279,safety,valid,validate,279,"@albert-github . That's a nice idea, and could be something to think about. I think we will anyway to build the whole documentation every night because:. - the various modules refer to each other a lot,. - we want to run all the code available in the doc. We have test suites to validate de software but the documentation is surely an extra one. - now it takes ""only"" a night so it is still doable to build it daily. If at some point the building becomes unmanageable the approach you suggest might be a solution.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:234,security,availab,available,234,"@albert-github . That's a nice idea, and could be something to think about. I think we will anyway to build the whole documentation every night because:. - the various modules refer to each other a lot,. - we want to run all the code available in the doc. We have test suites to validate de software but the documentation is surely an extra one. - now it takes ""only"" a night so it is still doable to build it daily. If at some point the building becomes unmanageable the approach you suggest might be a solution.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:279,security,validat,validate,279,"@albert-github . That's a nice idea, and could be something to think about. I think we will anyway to build the whole documentation every night because:. - the various modules refer to each other a lot,. - we want to run all the code available in the doc. We have test suites to validate de software but the documentation is surely an extra one. - now it takes ""only"" a night so it is still doable to build it daily. If at some point the building becomes unmanageable the approach you suggest might be a solution.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:264,testability,test,test,264,"@albert-github . That's a nice idea, and could be something to think about. I think we will anyway to build the whole documentation every night because:. - the various modules refer to each other a lot,. - we want to run all the code available in the doc. We have test suites to validate de software but the documentation is surely an extra one. - now it takes ""only"" a night so it is still doable to build it daily. If at some point the building becomes unmanageable the approach you suggest might be a solution.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:118,usability,document,documentation,118,"@albert-github . That's a nice idea, and could be something to think about. I think we will anyway to build the whole documentation every night because:. - the various modules refer to each other a lot,. - we want to run all the code available in the doc. We have test suites to validate de software but the documentation is surely an extra one. - now it takes ""only"" a night so it is still doable to build it daily. If at some point the building becomes unmanageable the approach you suggest might be a solution.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:308,usability,document,documentation,308,"@albert-github . That's a nice idea, and could be something to think about. I think we will anyway to build the whole documentation every night because:. - the various modules refer to each other a lot,. - we want to run all the code available in the doc. We have test suites to validate de software but the documentation is surely an extra one. - now it takes ""only"" a night so it is still doable to build it daily. If at some point the building becomes unmanageable the approach you suggest might be a solution.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:23,deployability,build,build,23,"The complete overnight build is certainly necessary and preferably from scratch (so no residuals are left over). Advantage of the tag files might be that some sub-projects can be build in parallel. A reasonable new feature is the setting `NUM_PROC_THREADS`, so it might still be a bit buggy but it would definitely be of interest to see how ROOT performs here and whether or not the output is the same for larger project, so doxygen could be improved as well.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:179,deployability,build,build,179,"The complete overnight build is certainly necessary and preferably from scratch (so no residuals are left over). Advantage of the tag files might be that some sub-projects can be build in parallel. A reasonable new feature is the setting `NUM_PROC_THREADS`, so it might still be a bit buggy but it would definitely be of interest to see how ROOT performs here and whether or not the output is the same for larger project, so doxygen could be improved as well.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:159,integrability,sub,sub-projects,159,"The complete overnight build is certainly necessary and preferably from scratch (so no residuals are left over). Advantage of the tag files might be that some sub-projects can be build in parallel. A reasonable new feature is the setting `NUM_PROC_THREADS`, so it might still be a bit buggy but it would definitely be of interest to see how ROOT performs here and whether or not the output is the same for larger project, so doxygen could be improved as well.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:188,performance,parallel,parallel,188,"The complete overnight build is certainly necessary and preferably from scratch (so no residuals are left over). Advantage of the tag files might be that some sub-projects can be build in parallel. A reasonable new feature is the setting `NUM_PROC_THREADS`, so it might still be a bit buggy but it would definitely be of interest to see how ROOT performs here and whether or not the output is the same for larger project, so doxygen could be improved as well.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:346,performance,perform,performs,346,"The complete overnight build is certainly necessary and preferably from scratch (so no residuals are left over). Advantage of the tag files might be that some sub-projects can be build in parallel. A reasonable new feature is the setting `NUM_PROC_THREADS`, so it might still be a bit buggy but it would definitely be of interest to see how ROOT performs here and whether or not the output is the same for larger project, so doxygen could be improved as well.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:4,safety,compl,complete,4,"The complete overnight build is certainly necessary and preferably from scratch (so no residuals are left over). Advantage of the tag files might be that some sub-projects can be build in parallel. A reasonable new feature is the setting `NUM_PROC_THREADS`, so it might still be a bit buggy but it would definitely be of interest to see how ROOT performs here and whether or not the output is the same for larger project, so doxygen could be improved as well.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:4,security,compl,complete,4,"The complete overnight build is certainly necessary and preferably from scratch (so no residuals are left over). Advantage of the tag files might be that some sub-projects can be build in parallel. A reasonable new feature is the setting `NUM_PROC_THREADS`, so it might still be a bit buggy but it would definitely be of interest to see how ROOT performs here and whether or not the output is the same for larger project, so doxygen could be improved as well.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:56,usability,prefer,preferably,56,"The complete overnight build is certainly necessary and preferably from scratch (so no residuals are left over). Advantage of the tag files might be that some sub-projects can be build in parallel. A reasonable new feature is the setting `NUM_PROC_THREADS`, so it might still be a bit buggy but it would definitely be of interest to see how ROOT performs here and whether or not the output is the same for larger project, so doxygen could be improved as well.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:346,usability,perform,performs,346,"The complete overnight build is certainly necessary and preferably from scratch (so no residuals are left over). Advantage of the tag files might be that some sub-projects can be build in parallel. A reasonable new feature is the setting `NUM_PROC_THREADS`, so it might still be a bit buggy but it would definitely be of interest to see how ROOT performs here and whether or not the output is the same for larger project, so doxygen could be improved as well.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8742:24,usability,close,closed,24,I guess this one can be closed now.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8742
https://github.com/root-project/root/issues/8744:9,testability,understand,understand,9,I do not understand exactly the bug being reportd. Do you @pcanal ?,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8744
https://github.com/root-project/root/issues/8744:396,availability,Error,Error,396,"This snippet:. . ```. #include ""TFile.h"". #include ""TTree.h"". void nested(). {. auto f = new TFile(""nestedclones.root"");. TTree* TopTree = (TTree*)f->Get(""TopTree"");. TopTree->Draw(""Objects.SecondaryVertexArray.TrackIndices.index"");. }. ```. fails with. ```. Warning in <TTreeFormula::DefinedVariable>: TClonesArray object was not retrievable for Objects.SecondaryVertexArray.TrackIndices.index! Error in <TTreeFormula::Compile>: Bad numerical expression : ""Objects.SecondaryVertexArray.TrackIndices.index"". Info in <TSelectorDraw::AbortProcess>: Variable compilation failed: {Objects.SecondaryVertexArray.TrackIndices.index,}. ```. If I open a TBrowser and then double click on the relevant leaf, it draws ""Empty"" and says:. `Warning in <TSelectorDraw::ProcessFillObject>: Not implemented for TClonesArray`. Funnily, if I then call again from the prompt:. `TopTree->Draw(""Objects.SecondaryVertexArray.TrackIndices.index"");`. then it is drawn correctly and no warning is raised. ![image](https://github.com/root-project/root/assets/10653970/fbfe2dda-8e5f-42eb-94ca-8a51d9ce1de6). So I guess that Axel was suggesting that, once this bug is fixed, this snippet should go back into roottest to prevent future regressions.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8744
https://github.com/root-project/root/issues/8744:242,deployability,fail,fails,242,"This snippet:. . ```. #include ""TFile.h"". #include ""TTree.h"". void nested(). {. auto f = new TFile(""nestedclones.root"");. TTree* TopTree = (TTree*)f->Get(""TopTree"");. TopTree->Draw(""Objects.SecondaryVertexArray.TrackIndices.index"");. }. ```. fails with. ```. Warning in <TTreeFormula::DefinedVariable>: TClonesArray object was not retrievable for Objects.SecondaryVertexArray.TrackIndices.index! Error in <TTreeFormula::Compile>: Bad numerical expression : ""Objects.SecondaryVertexArray.TrackIndices.index"". Info in <TSelectorDraw::AbortProcess>: Variable compilation failed: {Objects.SecondaryVertexArray.TrackIndices.index,}. ```. If I open a TBrowser and then double click on the relevant leaf, it draws ""Empty"" and says:. `Warning in <TSelectorDraw::ProcessFillObject>: Not implemented for TClonesArray`. Funnily, if I then call again from the prompt:. `TopTree->Draw(""Objects.SecondaryVertexArray.TrackIndices.index"");`. then it is drawn correctly and no warning is raised. ![image](https://github.com/root-project/root/assets/10653970/fbfe2dda-8e5f-42eb-94ca-8a51d9ce1de6). So I guess that Axel was suggesting that, once this bug is fixed, this snippet should go back into roottest to prevent future regressions.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8744
https://github.com/root-project/root/issues/8744:568,deployability,fail,failed,568,"This snippet:. . ```. #include ""TFile.h"". #include ""TTree.h"". void nested(). {. auto f = new TFile(""nestedclones.root"");. TTree* TopTree = (TTree*)f->Get(""TopTree"");. TopTree->Draw(""Objects.SecondaryVertexArray.TrackIndices.index"");. }. ```. fails with. ```. Warning in <TTreeFormula::DefinedVariable>: TClonesArray object was not retrievable for Objects.SecondaryVertexArray.TrackIndices.index! Error in <TTreeFormula::Compile>: Bad numerical expression : ""Objects.SecondaryVertexArray.TrackIndices.index"". Info in <TSelectorDraw::AbortProcess>: Variable compilation failed: {Objects.SecondaryVertexArray.TrackIndices.index,}. ```. If I open a TBrowser and then double click on the relevant leaf, it draws ""Empty"" and says:. `Warning in <TSelectorDraw::ProcessFillObject>: Not implemented for TClonesArray`. Funnily, if I then call again from the prompt:. `TopTree->Draw(""Objects.SecondaryVertexArray.TrackIndices.index"");`. then it is drawn correctly and no warning is raised. ![image](https://github.com/root-project/root/assets/10653970/fbfe2dda-8e5f-42eb-94ca-8a51d9ce1de6). So I guess that Axel was suggesting that, once this bug is fixed, this snippet should go back into roottest to prevent future regressions.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8744
https://github.com/root-project/root/issues/8744:176,energy efficiency,Draw,Draw,176,"This snippet:. . ```. #include ""TFile.h"". #include ""TTree.h"". void nested(). {. auto f = new TFile(""nestedclones.root"");. TTree* TopTree = (TTree*)f->Get(""TopTree"");. TopTree->Draw(""Objects.SecondaryVertexArray.TrackIndices.index"");. }. ```. fails with. ```. Warning in <TTreeFormula::DefinedVariable>: TClonesArray object was not retrievable for Objects.SecondaryVertexArray.TrackIndices.index! Error in <TTreeFormula::Compile>: Bad numerical expression : ""Objects.SecondaryVertexArray.TrackIndices.index"". Info in <TSelectorDraw::AbortProcess>: Variable compilation failed: {Objects.SecondaryVertexArray.TrackIndices.index,}. ```. If I open a TBrowser and then double click on the relevant leaf, it draws ""Empty"" and says:. `Warning in <TSelectorDraw::ProcessFillObject>: Not implemented for TClonesArray`. Funnily, if I then call again from the prompt:. `TopTree->Draw(""Objects.SecondaryVertexArray.TrackIndices.index"");`. then it is drawn correctly and no warning is raised. ![image](https://github.com/root-project/root/assets/10653970/fbfe2dda-8e5f-42eb-94ca-8a51d9ce1de6). So I guess that Axel was suggesting that, once this bug is fixed, this snippet should go back into roottest to prevent future regressions.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8744
https://github.com/root-project/root/issues/8744:701,energy efficiency,draw,draws,701,"This snippet:. . ```. #include ""TFile.h"". #include ""TTree.h"". void nested(). {. auto f = new TFile(""nestedclones.root"");. TTree* TopTree = (TTree*)f->Get(""TopTree"");. TopTree->Draw(""Objects.SecondaryVertexArray.TrackIndices.index"");. }. ```. fails with. ```. Warning in <TTreeFormula::DefinedVariable>: TClonesArray object was not retrievable for Objects.SecondaryVertexArray.TrackIndices.index! Error in <TTreeFormula::Compile>: Bad numerical expression : ""Objects.SecondaryVertexArray.TrackIndices.index"". Info in <TSelectorDraw::AbortProcess>: Variable compilation failed: {Objects.SecondaryVertexArray.TrackIndices.index,}. ```. If I open a TBrowser and then double click on the relevant leaf, it draws ""Empty"" and says:. `Warning in <TSelectorDraw::ProcessFillObject>: Not implemented for TClonesArray`. Funnily, if I then call again from the prompt:. `TopTree->Draw(""Objects.SecondaryVertexArray.TrackIndices.index"");`. then it is drawn correctly and no warning is raised. ![image](https://github.com/root-project/root/assets/10653970/fbfe2dda-8e5f-42eb-94ca-8a51d9ce1de6). So I guess that Axel was suggesting that, once this bug is fixed, this snippet should go back into roottest to prevent future regressions.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8744
https://github.com/root-project/root/issues/8744:867,energy efficiency,Draw,Draw,867,"This snippet:. . ```. #include ""TFile.h"". #include ""TTree.h"". void nested(). {. auto f = new TFile(""nestedclones.root"");. TTree* TopTree = (TTree*)f->Get(""TopTree"");. TopTree->Draw(""Objects.SecondaryVertexArray.TrackIndices.index"");. }. ```. fails with. ```. Warning in <TTreeFormula::DefinedVariable>: TClonesArray object was not retrievable for Objects.SecondaryVertexArray.TrackIndices.index! Error in <TTreeFormula::Compile>: Bad numerical expression : ""Objects.SecondaryVertexArray.TrackIndices.index"". Info in <TSelectorDraw::AbortProcess>: Variable compilation failed: {Objects.SecondaryVertexArray.TrackIndices.index,}. ```. If I open a TBrowser and then double click on the relevant leaf, it draws ""Empty"" and says:. `Warning in <TSelectorDraw::ProcessFillObject>: Not implemented for TClonesArray`. Funnily, if I then call again from the prompt:. `TopTree->Draw(""Objects.SecondaryVertexArray.TrackIndices.index"");`. then it is drawn correctly and no warning is raised. ![image](https://github.com/root-project/root/assets/10653970/fbfe2dda-8e5f-42eb-94ca-8a51d9ce1de6). So I guess that Axel was suggesting that, once this bug is fixed, this snippet should go back into roottest to prevent future regressions.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8744
https://github.com/root-project/root/issues/8744:937,energy efficiency,draw,drawn,937,"This snippet:. . ```. #include ""TFile.h"". #include ""TTree.h"". void nested(). {. auto f = new TFile(""nestedclones.root"");. TTree* TopTree = (TTree*)f->Get(""TopTree"");. TopTree->Draw(""Objects.SecondaryVertexArray.TrackIndices.index"");. }. ```. fails with. ```. Warning in <TTreeFormula::DefinedVariable>: TClonesArray object was not retrievable for Objects.SecondaryVertexArray.TrackIndices.index! Error in <TTreeFormula::Compile>: Bad numerical expression : ""Objects.SecondaryVertexArray.TrackIndices.index"". Info in <TSelectorDraw::AbortProcess>: Variable compilation failed: {Objects.SecondaryVertexArray.TrackIndices.index,}. ```. If I open a TBrowser and then double click on the relevant leaf, it draws ""Empty"" and says:. `Warning in <TSelectorDraw::ProcessFillObject>: Not implemented for TClonesArray`. Funnily, if I then call again from the prompt:. `TopTree->Draw(""Objects.SecondaryVertexArray.TrackIndices.index"");`. then it is drawn correctly and no warning is raised. ![image](https://github.com/root-project/root/assets/10653970/fbfe2dda-8e5f-42eb-94ca-8a51d9ce1de6). So I guess that Axel was suggesting that, once this bug is fixed, this snippet should go back into roottest to prevent future regressions.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8744
https://github.com/root-project/root/issues/8744:547,modifiability,Variab,Variable,547,"This snippet:. . ```. #include ""TFile.h"". #include ""TTree.h"". void nested(). {. auto f = new TFile(""nestedclones.root"");. TTree* TopTree = (TTree*)f->Get(""TopTree"");. TopTree->Draw(""Objects.SecondaryVertexArray.TrackIndices.index"");. }. ```. fails with. ```. Warning in <TTreeFormula::DefinedVariable>: TClonesArray object was not retrievable for Objects.SecondaryVertexArray.TrackIndices.index! Error in <TTreeFormula::Compile>: Bad numerical expression : ""Objects.SecondaryVertexArray.TrackIndices.index"". Info in <TSelectorDraw::AbortProcess>: Variable compilation failed: {Objects.SecondaryVertexArray.TrackIndices.index,}. ```. If I open a TBrowser and then double click on the relevant leaf, it draws ""Empty"" and says:. `Warning in <TSelectorDraw::ProcessFillObject>: Not implemented for TClonesArray`. Funnily, if I then call again from the prompt:. `TopTree->Draw(""Objects.SecondaryVertexArray.TrackIndices.index"");`. then it is drawn correctly and no warning is raised. ![image](https://github.com/root-project/root/assets/10653970/fbfe2dda-8e5f-42eb-94ca-8a51d9ce1de6). So I guess that Axel was suggesting that, once this bug is fixed, this snippet should go back into roottest to prevent future regressions.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8744
https://github.com/root-project/root/issues/8744:396,performance,Error,Error,396,"This snippet:. . ```. #include ""TFile.h"". #include ""TTree.h"". void nested(). {. auto f = new TFile(""nestedclones.root"");. TTree* TopTree = (TTree*)f->Get(""TopTree"");. TopTree->Draw(""Objects.SecondaryVertexArray.TrackIndices.index"");. }. ```. fails with. ```. Warning in <TTreeFormula::DefinedVariable>: TClonesArray object was not retrievable for Objects.SecondaryVertexArray.TrackIndices.index! Error in <TTreeFormula::Compile>: Bad numerical expression : ""Objects.SecondaryVertexArray.TrackIndices.index"". Info in <TSelectorDraw::AbortProcess>: Variable compilation failed: {Objects.SecondaryVertexArray.TrackIndices.index,}. ```. If I open a TBrowser and then double click on the relevant leaf, it draws ""Empty"" and says:. `Warning in <TSelectorDraw::ProcessFillObject>: Not implemented for TClonesArray`. Funnily, if I then call again from the prompt:. `TopTree->Draw(""Objects.SecondaryVertexArray.TrackIndices.index"");`. then it is drawn correctly and no warning is raised. ![image](https://github.com/root-project/root/assets/10653970/fbfe2dda-8e5f-42eb-94ca-8a51d9ce1de6). So I guess that Axel was suggesting that, once this bug is fixed, this snippet should go back into roottest to prevent future regressions.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8744
https://github.com/root-project/root/issues/8744:242,reliability,fail,fails,242,"This snippet:. . ```. #include ""TFile.h"". #include ""TTree.h"". void nested(). {. auto f = new TFile(""nestedclones.root"");. TTree* TopTree = (TTree*)f->Get(""TopTree"");. TopTree->Draw(""Objects.SecondaryVertexArray.TrackIndices.index"");. }. ```. fails with. ```. Warning in <TTreeFormula::DefinedVariable>: TClonesArray object was not retrievable for Objects.SecondaryVertexArray.TrackIndices.index! Error in <TTreeFormula::Compile>: Bad numerical expression : ""Objects.SecondaryVertexArray.TrackIndices.index"". Info in <TSelectorDraw::AbortProcess>: Variable compilation failed: {Objects.SecondaryVertexArray.TrackIndices.index,}. ```. If I open a TBrowser and then double click on the relevant leaf, it draws ""Empty"" and says:. `Warning in <TSelectorDraw::ProcessFillObject>: Not implemented for TClonesArray`. Funnily, if I then call again from the prompt:. `TopTree->Draw(""Objects.SecondaryVertexArray.TrackIndices.index"");`. then it is drawn correctly and no warning is raised. ![image](https://github.com/root-project/root/assets/10653970/fbfe2dda-8e5f-42eb-94ca-8a51d9ce1de6). So I guess that Axel was suggesting that, once this bug is fixed, this snippet should go back into roottest to prevent future regressions.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8744
https://github.com/root-project/root/issues/8744:568,reliability,fail,failed,568,"This snippet:. . ```. #include ""TFile.h"". #include ""TTree.h"". void nested(). {. auto f = new TFile(""nestedclones.root"");. TTree* TopTree = (TTree*)f->Get(""TopTree"");. TopTree->Draw(""Objects.SecondaryVertexArray.TrackIndices.index"");. }. ```. fails with. ```. Warning in <TTreeFormula::DefinedVariable>: TClonesArray object was not retrievable for Objects.SecondaryVertexArray.TrackIndices.index! Error in <TTreeFormula::Compile>: Bad numerical expression : ""Objects.SecondaryVertexArray.TrackIndices.index"". Info in <TSelectorDraw::AbortProcess>: Variable compilation failed: {Objects.SecondaryVertexArray.TrackIndices.index,}. ```. If I open a TBrowser and then double click on the relevant leaf, it draws ""Empty"" and says:. `Warning in <TSelectorDraw::ProcessFillObject>: Not implemented for TClonesArray`. Funnily, if I then call again from the prompt:. `TopTree->Draw(""Objects.SecondaryVertexArray.TrackIndices.index"");`. then it is drawn correctly and no warning is raised. ![image](https://github.com/root-project/root/assets/10653970/fbfe2dda-8e5f-42eb-94ca-8a51d9ce1de6). So I guess that Axel was suggesting that, once this bug is fixed, this snippet should go back into roottest to prevent future regressions.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8744
https://github.com/root-project/root/issues/8744:396,safety,Error,Error,396,"This snippet:. . ```. #include ""TFile.h"". #include ""TTree.h"". void nested(). {. auto f = new TFile(""nestedclones.root"");. TTree* TopTree = (TTree*)f->Get(""TopTree"");. TopTree->Draw(""Objects.SecondaryVertexArray.TrackIndices.index"");. }. ```. fails with. ```. Warning in <TTreeFormula::DefinedVariable>: TClonesArray object was not retrievable for Objects.SecondaryVertexArray.TrackIndices.index! Error in <TTreeFormula::Compile>: Bad numerical expression : ""Objects.SecondaryVertexArray.TrackIndices.index"". Info in <TSelectorDraw::AbortProcess>: Variable compilation failed: {Objects.SecondaryVertexArray.TrackIndices.index,}. ```. If I open a TBrowser and then double click on the relevant leaf, it draws ""Empty"" and says:. `Warning in <TSelectorDraw::ProcessFillObject>: Not implemented for TClonesArray`. Funnily, if I then call again from the prompt:. `TopTree->Draw(""Objects.SecondaryVertexArray.TrackIndices.index"");`. then it is drawn correctly and no warning is raised. ![image](https://github.com/root-project/root/assets/10653970/fbfe2dda-8e5f-42eb-94ca-8a51d9ce1de6). So I guess that Axel was suggesting that, once this bug is fixed, this snippet should go back into roottest to prevent future regressions.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8744
https://github.com/root-project/root/issues/8744:1191,safety,prevent,prevent,1191,"This snippet:. . ```. #include ""TFile.h"". #include ""TTree.h"". void nested(). {. auto f = new TFile(""nestedclones.root"");. TTree* TopTree = (TTree*)f->Get(""TopTree"");. TopTree->Draw(""Objects.SecondaryVertexArray.TrackIndices.index"");. }. ```. fails with. ```. Warning in <TTreeFormula::DefinedVariable>: TClonesArray object was not retrievable for Objects.SecondaryVertexArray.TrackIndices.index! Error in <TTreeFormula::Compile>: Bad numerical expression : ""Objects.SecondaryVertexArray.TrackIndices.index"". Info in <TSelectorDraw::AbortProcess>: Variable compilation failed: {Objects.SecondaryVertexArray.TrackIndices.index,}. ```. If I open a TBrowser and then double click on the relevant leaf, it draws ""Empty"" and says:. `Warning in <TSelectorDraw::ProcessFillObject>: Not implemented for TClonesArray`. Funnily, if I then call again from the prompt:. `TopTree->Draw(""Objects.SecondaryVertexArray.TrackIndices.index"");`. then it is drawn correctly and no warning is raised. ![image](https://github.com/root-project/root/assets/10653970/fbfe2dda-8e5f-42eb-94ca-8a51d9ce1de6). So I guess that Axel was suggesting that, once this bug is fixed, this snippet should go back into roottest to prevent future regressions.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8744
https://github.com/root-project/root/issues/8744:1191,security,preven,prevent,1191,"This snippet:. . ```. #include ""TFile.h"". #include ""TTree.h"". void nested(). {. auto f = new TFile(""nestedclones.root"");. TTree* TopTree = (TTree*)f->Get(""TopTree"");. TopTree->Draw(""Objects.SecondaryVertexArray.TrackIndices.index"");. }. ```. fails with. ```. Warning in <TTreeFormula::DefinedVariable>: TClonesArray object was not retrievable for Objects.SecondaryVertexArray.TrackIndices.index! Error in <TTreeFormula::Compile>: Bad numerical expression : ""Objects.SecondaryVertexArray.TrackIndices.index"". Info in <TSelectorDraw::AbortProcess>: Variable compilation failed: {Objects.SecondaryVertexArray.TrackIndices.index,}. ```. If I open a TBrowser and then double click on the relevant leaf, it draws ""Empty"" and says:. `Warning in <TSelectorDraw::ProcessFillObject>: Not implemented for TClonesArray`. Funnily, if I then call again from the prompt:. `TopTree->Draw(""Objects.SecondaryVertexArray.TrackIndices.index"");`. then it is drawn correctly and no warning is raised. ![image](https://github.com/root-project/root/assets/10653970/fbfe2dda-8e5f-42eb-94ca-8a51d9ce1de6). So I guess that Axel was suggesting that, once this bug is fixed, this snippet should go back into roottest to prevent future regressions.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8744
https://github.com/root-project/root/issues/8744:1206,testability,regress,regressions,1206,"This snippet:. . ```. #include ""TFile.h"". #include ""TTree.h"". void nested(). {. auto f = new TFile(""nestedclones.root"");. TTree* TopTree = (TTree*)f->Get(""TopTree"");. TopTree->Draw(""Objects.SecondaryVertexArray.TrackIndices.index"");. }. ```. fails with. ```. Warning in <TTreeFormula::DefinedVariable>: TClonesArray object was not retrievable for Objects.SecondaryVertexArray.TrackIndices.index! Error in <TTreeFormula::Compile>: Bad numerical expression : ""Objects.SecondaryVertexArray.TrackIndices.index"". Info in <TSelectorDraw::AbortProcess>: Variable compilation failed: {Objects.SecondaryVertexArray.TrackIndices.index,}. ```. If I open a TBrowser and then double click on the relevant leaf, it draws ""Empty"" and says:. `Warning in <TSelectorDraw::ProcessFillObject>: Not implemented for TClonesArray`. Funnily, if I then call again from the prompt:. `TopTree->Draw(""Objects.SecondaryVertexArray.TrackIndices.index"");`. then it is drawn correctly and no warning is raised. ![image](https://github.com/root-project/root/assets/10653970/fbfe2dda-8e5f-42eb-94ca-8a51d9ce1de6). So I guess that Axel was suggesting that, once this bug is fixed, this snippet should go back into roottest to prevent future regressions.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8744
https://github.com/root-project/root/issues/8744:396,usability,Error,Error,396,"This snippet:. . ```. #include ""TFile.h"". #include ""TTree.h"". void nested(). {. auto f = new TFile(""nestedclones.root"");. TTree* TopTree = (TTree*)f->Get(""TopTree"");. TopTree->Draw(""Objects.SecondaryVertexArray.TrackIndices.index"");. }. ```. fails with. ```. Warning in <TTreeFormula::DefinedVariable>: TClonesArray object was not retrievable for Objects.SecondaryVertexArray.TrackIndices.index! Error in <TTreeFormula::Compile>: Bad numerical expression : ""Objects.SecondaryVertexArray.TrackIndices.index"". Info in <TSelectorDraw::AbortProcess>: Variable compilation failed: {Objects.SecondaryVertexArray.TrackIndices.index,}. ```. If I open a TBrowser and then double click on the relevant leaf, it draws ""Empty"" and says:. `Warning in <TSelectorDraw::ProcessFillObject>: Not implemented for TClonesArray`. Funnily, if I then call again from the prompt:. `TopTree->Draw(""Objects.SecondaryVertexArray.TrackIndices.index"");`. then it is drawn correctly and no warning is raised. ![image](https://github.com/root-project/root/assets/10653970/fbfe2dda-8e5f-42eb-94ca-8a51d9ce1de6). So I guess that Axel was suggesting that, once this bug is fixed, this snippet should go back into roottest to prevent future regressions.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8744
https://github.com/root-project/root/issues/8745:289,deployability,modul,module,289,"To add some more information after some investigation with @vepadulano: when doing `ROOT.Detail.MyStruct`, the `Detail` namespace that PyROOT returns is the one in the global namespace, i.e. `Detail`, not `ROOT::Detail`. This happens because, when doing a lookup from Python in the `ROOT` module, the global namespace is inspected first, and only if that lookup fails the `ROOT` namespace is tried. But since `Detail` already exists in the global namespace, `ROOT::Detail` is never returned - and `Detail` does not contain any `MyStruct`. Bottom line: the PyROOT lookup works as expected. That being said, we found a weird behavior with the `Detail` namespace, which can be reproduced from Python and also from C++ in the ROOT prompt:. ```. root [0] auto c = TClass::GetClass(""Detail""). (TClass *) @0x7fffd0908fc0. root [1] namespace Detail { int i; }. root.exe: /home/etejedor/root/fork/root/interpreter/llvm/src/include/llvm/Support/Casting.h:105: static bool llvm::isa_impl_cl<To, const From*>::doit(const From*) [with To = clang::TagType; From = clang::Type]: Assertion `Val && ""isa<> used on a null pointer""' failed. ```. If we look up the `Detail` namespace first and then define something else in it, there is a crash. I will open another ticket to follow this up.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8745
https://github.com/root-project/root/issues/8745:362,deployability,fail,fails,362,"To add some more information after some investigation with @vepadulano: when doing `ROOT.Detail.MyStruct`, the `Detail` namespace that PyROOT returns is the one in the global namespace, i.e. `Detail`, not `ROOT::Detail`. This happens because, when doing a lookup from Python in the `ROOT` module, the global namespace is inspected first, and only if that lookup fails the `ROOT` namespace is tried. But since `Detail` already exists in the global namespace, `ROOT::Detail` is never returned - and `Detail` does not contain any `MyStruct`. Bottom line: the PyROOT lookup works as expected. That being said, we found a weird behavior with the `Detail` namespace, which can be reproduced from Python and also from C++ in the ROOT prompt:. ```. root [0] auto c = TClass::GetClass(""Detail""). (TClass *) @0x7fffd0908fc0. root [1] namespace Detail { int i; }. root.exe: /home/etejedor/root/fork/root/interpreter/llvm/src/include/llvm/Support/Casting.h:105: static bool llvm::isa_impl_cl<To, const From*>::doit(const From*) [with To = clang::TagType; From = clang::Type]: Assertion `Val && ""isa<> used on a null pointer""' failed. ```. If we look up the `Detail` namespace first and then define something else in it, there is a crash. I will open another ticket to follow this up.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8745
https://github.com/root-project/root/issues/8745:515,deployability,contain,contain,515,"To add some more information after some investigation with @vepadulano: when doing `ROOT.Detail.MyStruct`, the `Detail` namespace that PyROOT returns is the one in the global namespace, i.e. `Detail`, not `ROOT::Detail`. This happens because, when doing a lookup from Python in the `ROOT` module, the global namespace is inspected first, and only if that lookup fails the `ROOT` namespace is tried. But since `Detail` already exists in the global namespace, `ROOT::Detail` is never returned - and `Detail` does not contain any `MyStruct`. Bottom line: the PyROOT lookup works as expected. That being said, we found a weird behavior with the `Detail` namespace, which can be reproduced from Python and also from C++ in the ROOT prompt:. ```. root [0] auto c = TClass::GetClass(""Detail""). (TClass *) @0x7fffd0908fc0. root [1] namespace Detail { int i; }. root.exe: /home/etejedor/root/fork/root/interpreter/llvm/src/include/llvm/Support/Casting.h:105: static bool llvm::isa_impl_cl<To, const From*>::doit(const From*) [with To = clang::TagType; From = clang::Type]: Assertion `Val && ""isa<> used on a null pointer""' failed. ```. If we look up the `Detail` namespace first and then define something else in it, there is a crash. I will open another ticket to follow this up.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8745
https://github.com/root-project/root/issues/8745:1114,deployability,fail,failed,1114,"To add some more information after some investigation with @vepadulano: when doing `ROOT.Detail.MyStruct`, the `Detail` namespace that PyROOT returns is the one in the global namespace, i.e. `Detail`, not `ROOT::Detail`. This happens because, when doing a lookup from Python in the `ROOT` module, the global namespace is inspected first, and only if that lookup fails the `ROOT` namespace is tried. But since `Detail` already exists in the global namespace, `ROOT::Detail` is never returned - and `Detail` does not contain any `MyStruct`. Bottom line: the PyROOT lookup works as expected. That being said, we found a weird behavior with the `Detail` namespace, which can be reproduced from Python and also from C++ in the ROOT prompt:. ```. root [0] auto c = TClass::GetClass(""Detail""). (TClass *) @0x7fffd0908fc0. root [1] namespace Detail { int i; }. root.exe: /home/etejedor/root/fork/root/interpreter/llvm/src/include/llvm/Support/Casting.h:105: static bool llvm::isa_impl_cl<To, const From*>::doit(const From*) [with To = clang::TagType; From = clang::Type]: Assertion `Val && ""isa<> used on a null pointer""' failed. ```. If we look up the `Detail` namespace first and then define something else in it, there is a crash. I will open another ticket to follow this up.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8745
https://github.com/root-project/root/issues/8745:289,modifiability,modul,module,289,"To add some more information after some investigation with @vepadulano: when doing `ROOT.Detail.MyStruct`, the `Detail` namespace that PyROOT returns is the one in the global namespace, i.e. `Detail`, not `ROOT::Detail`. This happens because, when doing a lookup from Python in the `ROOT` module, the global namespace is inspected first, and only if that lookup fails the `ROOT` namespace is tried. But since `Detail` already exists in the global namespace, `ROOT::Detail` is never returned - and `Detail` does not contain any `MyStruct`. Bottom line: the PyROOT lookup works as expected. That being said, we found a weird behavior with the `Detail` namespace, which can be reproduced from Python and also from C++ in the ROOT prompt:. ```. root [0] auto c = TClass::GetClass(""Detail""). (TClass *) @0x7fffd0908fc0. root [1] namespace Detail { int i; }. root.exe: /home/etejedor/root/fork/root/interpreter/llvm/src/include/llvm/Support/Casting.h:105: static bool llvm::isa_impl_cl<To, const From*>::doit(const From*) [with To = clang::TagType; From = clang::Type]: Assertion `Val && ""isa<> used on a null pointer""' failed. ```. If we look up the `Detail` namespace first and then define something else in it, there is a crash. I will open another ticket to follow this up.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8745
https://github.com/root-project/root/issues/8745:362,reliability,fail,fails,362,"To add some more information after some investigation with @vepadulano: when doing `ROOT.Detail.MyStruct`, the `Detail` namespace that PyROOT returns is the one in the global namespace, i.e. `Detail`, not `ROOT::Detail`. This happens because, when doing a lookup from Python in the `ROOT` module, the global namespace is inspected first, and only if that lookup fails the `ROOT` namespace is tried. But since `Detail` already exists in the global namespace, `ROOT::Detail` is never returned - and `Detail` does not contain any `MyStruct`. Bottom line: the PyROOT lookup works as expected. That being said, we found a weird behavior with the `Detail` namespace, which can be reproduced from Python and also from C++ in the ROOT prompt:. ```. root [0] auto c = TClass::GetClass(""Detail""). (TClass *) @0x7fffd0908fc0. root [1] namespace Detail { int i; }. root.exe: /home/etejedor/root/fork/root/interpreter/llvm/src/include/llvm/Support/Casting.h:105: static bool llvm::isa_impl_cl<To, const From*>::doit(const From*) [with To = clang::TagType; From = clang::Type]: Assertion `Val && ""isa<> used on a null pointer""' failed. ```. If we look up the `Detail` namespace first and then define something else in it, there is a crash. I will open another ticket to follow this up.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8745
https://github.com/root-project/root/issues/8745:506,reliability,doe,does,506,"To add some more information after some investigation with @vepadulano: when doing `ROOT.Detail.MyStruct`, the `Detail` namespace that PyROOT returns is the one in the global namespace, i.e. `Detail`, not `ROOT::Detail`. This happens because, when doing a lookup from Python in the `ROOT` module, the global namespace is inspected first, and only if that lookup fails the `ROOT` namespace is tried. But since `Detail` already exists in the global namespace, `ROOT::Detail` is never returned - and `Detail` does not contain any `MyStruct`. Bottom line: the PyROOT lookup works as expected. That being said, we found a weird behavior with the `Detail` namespace, which can be reproduced from Python and also from C++ in the ROOT prompt:. ```. root [0] auto c = TClass::GetClass(""Detail""). (TClass *) @0x7fffd0908fc0. root [1] namespace Detail { int i; }. root.exe: /home/etejedor/root/fork/root/interpreter/llvm/src/include/llvm/Support/Casting.h:105: static bool llvm::isa_impl_cl<To, const From*>::doit(const From*) [with To = clang::TagType; From = clang::Type]: Assertion `Val && ""isa<> used on a null pointer""' failed. ```. If we look up the `Detail` namespace first and then define something else in it, there is a crash. I will open another ticket to follow this up.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8745
https://github.com/root-project/root/issues/8745:1114,reliability,fail,failed,1114,"To add some more information after some investigation with @vepadulano: when doing `ROOT.Detail.MyStruct`, the `Detail` namespace that PyROOT returns is the one in the global namespace, i.e. `Detail`, not `ROOT::Detail`. This happens because, when doing a lookup from Python in the `ROOT` module, the global namespace is inspected first, and only if that lookup fails the `ROOT` namespace is tried. But since `Detail` already exists in the global namespace, `ROOT::Detail` is never returned - and `Detail` does not contain any `MyStruct`. Bottom line: the PyROOT lookup works as expected. That being said, we found a weird behavior with the `Detail` namespace, which can be reproduced from Python and also from C++ in the ROOT prompt:. ```. root [0] auto c = TClass::GetClass(""Detail""). (TClass *) @0x7fffd0908fc0. root [1] namespace Detail { int i; }. root.exe: /home/etejedor/root/fork/root/interpreter/llvm/src/include/llvm/Support/Casting.h:105: static bool llvm::isa_impl_cl<To, const From*>::doit(const From*) [with To = clang::TagType; From = clang::Type]: Assertion `Val && ""isa<> used on a null pointer""' failed. ```. If we look up the `Detail` namespace first and then define something else in it, there is a crash. I will open another ticket to follow this up.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8745
https://github.com/root-project/root/issues/8745:289,safety,modul,module,289,"To add some more information after some investigation with @vepadulano: when doing `ROOT.Detail.MyStruct`, the `Detail` namespace that PyROOT returns is the one in the global namespace, i.e. `Detail`, not `ROOT::Detail`. This happens because, when doing a lookup from Python in the `ROOT` module, the global namespace is inspected first, and only if that lookup fails the `ROOT` namespace is tried. But since `Detail` already exists in the global namespace, `ROOT::Detail` is never returned - and `Detail` does not contain any `MyStruct`. Bottom line: the PyROOT lookup works as expected. That being said, we found a weird behavior with the `Detail` namespace, which can be reproduced from Python and also from C++ in the ROOT prompt:. ```. root [0] auto c = TClass::GetClass(""Detail""). (TClass *) @0x7fffd0908fc0. root [1] namespace Detail { int i; }. root.exe: /home/etejedor/root/fork/root/interpreter/llvm/src/include/llvm/Support/Casting.h:105: static bool llvm::isa_impl_cl<To, const From*>::doit(const From*) [with To = clang::TagType; From = clang::Type]: Assertion `Val && ""isa<> used on a null pointer""' failed. ```. If we look up the `Detail` namespace first and then define something else in it, there is a crash. I will open another ticket to follow this up.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8745
https://github.com/root-project/root/issues/8745:1064,testability,Assert,Assertion,1064,"To add some more information after some investigation with @vepadulano: when doing `ROOT.Detail.MyStruct`, the `Detail` namespace that PyROOT returns is the one in the global namespace, i.e. `Detail`, not `ROOT::Detail`. This happens because, when doing a lookup from Python in the `ROOT` module, the global namespace is inspected first, and only if that lookup fails the `ROOT` namespace is tried. But since `Detail` already exists in the global namespace, `ROOT::Detail` is never returned - and `Detail` does not contain any `MyStruct`. Bottom line: the PyROOT lookup works as expected. That being said, we found a weird behavior with the `Detail` namespace, which can be reproduced from Python and also from C++ in the ROOT prompt:. ```. root [0] auto c = TClass::GetClass(""Detail""). (TClass *) @0x7fffd0908fc0. root [1] namespace Detail { int i; }. root.exe: /home/etejedor/root/fork/root/interpreter/llvm/src/include/llvm/Support/Casting.h:105: static bool llvm::isa_impl_cl<To, const From*>::doit(const From*) [with To = clang::TagType; From = clang::Type]: Assertion `Val && ""isa<> used on a null pointer""' failed. ```. If we look up the `Detail` namespace first and then define something else in it, there is a crash. I will open another ticket to follow this up.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8745
https://github.com/root-project/root/issues/8745:623,usability,behavi,behavior,623,"To add some more information after some investigation with @vepadulano: when doing `ROOT.Detail.MyStruct`, the `Detail` namespace that PyROOT returns is the one in the global namespace, i.e. `Detail`, not `ROOT::Detail`. This happens because, when doing a lookup from Python in the `ROOT` module, the global namespace is inspected first, and only if that lookup fails the `ROOT` namespace is tried. But since `Detail` already exists in the global namespace, `ROOT::Detail` is never returned - and `Detail` does not contain any `MyStruct`. Bottom line: the PyROOT lookup works as expected. That being said, we found a weird behavior with the `Detail` namespace, which can be reproduced from Python and also from C++ in the ROOT prompt:. ```. root [0] auto c = TClass::GetClass(""Detail""). (TClass *) @0x7fffd0908fc0. root [1] namespace Detail { int i; }. root.exe: /home/etejedor/root/fork/root/interpreter/llvm/src/include/llvm/Support/Casting.h:105: static bool llvm::isa_impl_cl<To, const From*>::doit(const From*) [with To = clang::TagType; From = clang::Type]: Assertion `Val && ""isa<> used on a null pointer""' failed. ```. If we look up the `Detail` namespace first and then define something else in it, there is a crash. I will open another ticket to follow this up.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8745
https://github.com/root-project/root/issues/8745:927,usability,Support,Support,927,"To add some more information after some investigation with @vepadulano: when doing `ROOT.Detail.MyStruct`, the `Detail` namespace that PyROOT returns is the one in the global namespace, i.e. `Detail`, not `ROOT::Detail`. This happens because, when doing a lookup from Python in the `ROOT` module, the global namespace is inspected first, and only if that lookup fails the `ROOT` namespace is tried. But since `Detail` already exists in the global namespace, `ROOT::Detail` is never returned - and `Detail` does not contain any `MyStruct`. Bottom line: the PyROOT lookup works as expected. That being said, we found a weird behavior with the `Detail` namespace, which can be reproduced from Python and also from C++ in the ROOT prompt:. ```. root [0] auto c = TClass::GetClass(""Detail""). (TClass *) @0x7fffd0908fc0. root [1] namespace Detail { int i; }. root.exe: /home/etejedor/root/fork/root/interpreter/llvm/src/include/llvm/Support/Casting.h:105: static bool llvm::isa_impl_cl<To, const From*>::doit(const From*) [with To = clang::TagType; From = clang::Type]: Assertion `Val && ""isa<> used on a null pointer""' failed. ```. If we look up the `Detail` namespace first and then define something else in it, there is a crash. I will open another ticket to follow this up.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8745
https://github.com/root-project/root/pull/8746:11,deployability,build,build,11,@phsft-bot build.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8746
https://github.com/root-project/root/pull/8746:5,deployability,patch,patch,5,This patch on a M1 machine is working. The ‚Äúperiodic‚Äù example does not crash anymore.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8746
https://github.com/root-project/root/pull/8746:62,reliability,doe,does,62,This patch on a M1 machine is working. The ‚Äúperiodic‚Äù example does not crash anymore.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8746
https://github.com/root-project/root/pull/8746:5,safety,patch,patch,5,This patch on a M1 machine is working. The ‚Äúperiodic‚Äù example does not crash anymore.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8746
https://github.com/root-project/root/pull/8746:5,security,patch,patch,5,This patch on a M1 machine is working. The ‚Äúperiodic‚Äù example does not crash anymore.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8746
https://github.com/root-project/root/pull/8747:10,deployability,fail,failing,10,"About the failing tests, they now fail because they check for `kEntryNotFound` after an event loop finishes normally. In `TTreeReaderBasic.ZeroEntryRange` and `TTreeReaderBasic.InvertedEntryRange` the following logic is present (the comments also come from the current test code):. ```cpp. // Read beyond end:. EXPECT_FALSE(tr.Next());. // As the TTree only has up to entry 19, 20 is kEntryNotFound:. EXPECT_EQ(TTreeReader::kEntryNotFound, tr.GetEntryStatus());. ```. Similarly, `TTreeReaderBasic.EntryListBeyondEnd` expects `kEntryNotFound` after the last `r.Next()` when a `TEntryList` is present. Also in that case, I would argue `kEntryBeyondEnd` would be more correct.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8747
https://github.com/root-project/root/pull/8747:34,deployability,fail,fail,34,"About the failing tests, they now fail because they check for `kEntryNotFound` after an event loop finishes normally. In `TTreeReaderBasic.ZeroEntryRange` and `TTreeReaderBasic.InvertedEntryRange` the following logic is present (the comments also come from the current test code):. ```cpp. // Read beyond end:. EXPECT_FALSE(tr.Next());. // As the TTree only has up to entry 19, 20 is kEntryNotFound:. EXPECT_EQ(TTreeReader::kEntryNotFound, tr.GetEntryStatus());. ```. Similarly, `TTreeReaderBasic.EntryListBeyondEnd` expects `kEntryNotFound` after the last `r.Next()` when a `TEntryList` is present. Also in that case, I would argue `kEntryBeyondEnd` would be more correct.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8747
https://github.com/root-project/root/pull/8747:211,deployability,log,logic,211,"About the failing tests, they now fail because they check for `kEntryNotFound` after an event loop finishes normally. In `TTreeReaderBasic.ZeroEntryRange` and `TTreeReaderBasic.InvertedEntryRange` the following logic is present (the comments also come from the current test code):. ```cpp. // Read beyond end:. EXPECT_FALSE(tr.Next());. // As the TTree only has up to entry 19, 20 is kEntryNotFound:. EXPECT_EQ(TTreeReader::kEntryNotFound, tr.GetEntryStatus());. ```. Similarly, `TTreeReaderBasic.EntryListBeyondEnd` expects `kEntryNotFound` after the last `r.Next()` when a `TEntryList` is present. Also in that case, I would argue `kEntryBeyondEnd` would be more correct.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8747
https://github.com/root-project/root/pull/8747:261,energy efficiency,current,current,261,"About the failing tests, they now fail because they check for `kEntryNotFound` after an event loop finishes normally. In `TTreeReaderBasic.ZeroEntryRange` and `TTreeReaderBasic.InvertedEntryRange` the following logic is present (the comments also come from the current test code):. ```cpp. // Read beyond end:. EXPECT_FALSE(tr.Next());. // As the TTree only has up to entry 19, 20 is kEntryNotFound:. EXPECT_EQ(TTreeReader::kEntryNotFound, tr.GetEntryStatus());. ```. Similarly, `TTreeReaderBasic.EntryListBeyondEnd` expects `kEntryNotFound` after the last `r.Next()` when a `TEntryList` is present. Also in that case, I would argue `kEntryBeyondEnd` would be more correct.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8747
https://github.com/root-project/root/pull/8747:88,integrability,event,event,88,"About the failing tests, they now fail because they check for `kEntryNotFound` after an event loop finishes normally. In `TTreeReaderBasic.ZeroEntryRange` and `TTreeReaderBasic.InvertedEntryRange` the following logic is present (the comments also come from the current test code):. ```cpp. // Read beyond end:. EXPECT_FALSE(tr.Next());. // As the TTree only has up to entry 19, 20 is kEntryNotFound:. EXPECT_EQ(TTreeReader::kEntryNotFound, tr.GetEntryStatus());. ```. Similarly, `TTreeReaderBasic.EntryListBeyondEnd` expects `kEntryNotFound` after the last `r.Next()` when a `TEntryList` is present. Also in that case, I would argue `kEntryBeyondEnd` would be more correct.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8747
https://github.com/root-project/root/pull/8747:10,reliability,fail,failing,10,"About the failing tests, they now fail because they check for `kEntryNotFound` after an event loop finishes normally. In `TTreeReaderBasic.ZeroEntryRange` and `TTreeReaderBasic.InvertedEntryRange` the following logic is present (the comments also come from the current test code):. ```cpp. // Read beyond end:. EXPECT_FALSE(tr.Next());. // As the TTree only has up to entry 19, 20 is kEntryNotFound:. EXPECT_EQ(TTreeReader::kEntryNotFound, tr.GetEntryStatus());. ```. Similarly, `TTreeReaderBasic.EntryListBeyondEnd` expects `kEntryNotFound` after the last `r.Next()` when a `TEntryList` is present. Also in that case, I would argue `kEntryBeyondEnd` would be more correct.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8747
https://github.com/root-project/root/pull/8747:34,reliability,fail,fail,34,"About the failing tests, they now fail because they check for `kEntryNotFound` after an event loop finishes normally. In `TTreeReaderBasic.ZeroEntryRange` and `TTreeReaderBasic.InvertedEntryRange` the following logic is present (the comments also come from the current test code):. ```cpp. // Read beyond end:. EXPECT_FALSE(tr.Next());. // As the TTree only has up to entry 19, 20 is kEntryNotFound:. EXPECT_EQ(TTreeReader::kEntryNotFound, tr.GetEntryStatus());. ```. Similarly, `TTreeReaderBasic.EntryListBeyondEnd` expects `kEntryNotFound` after the last `r.Next()` when a `TEntryList` is present. Also in that case, I would argue `kEntryBeyondEnd` would be more correct.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8747
https://github.com/root-project/root/pull/8747:18,safety,test,tests,18,"About the failing tests, they now fail because they check for `kEntryNotFound` after an event loop finishes normally. In `TTreeReaderBasic.ZeroEntryRange` and `TTreeReaderBasic.InvertedEntryRange` the following logic is present (the comments also come from the current test code):. ```cpp. // Read beyond end:. EXPECT_FALSE(tr.Next());. // As the TTree only has up to entry 19, 20 is kEntryNotFound:. EXPECT_EQ(TTreeReader::kEntryNotFound, tr.GetEntryStatus());. ```. Similarly, `TTreeReaderBasic.EntryListBeyondEnd` expects `kEntryNotFound` after the last `r.Next()` when a `TEntryList` is present. Also in that case, I would argue `kEntryBeyondEnd` would be more correct.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8747
https://github.com/root-project/root/pull/8747:211,safety,log,logic,211,"About the failing tests, they now fail because they check for `kEntryNotFound` after an event loop finishes normally. In `TTreeReaderBasic.ZeroEntryRange` and `TTreeReaderBasic.InvertedEntryRange` the following logic is present (the comments also come from the current test code):. ```cpp. // Read beyond end:. EXPECT_FALSE(tr.Next());. // As the TTree only has up to entry 19, 20 is kEntryNotFound:. EXPECT_EQ(TTreeReader::kEntryNotFound, tr.GetEntryStatus());. ```. Similarly, `TTreeReaderBasic.EntryListBeyondEnd` expects `kEntryNotFound` after the last `r.Next()` when a `TEntryList` is present. Also in that case, I would argue `kEntryBeyondEnd` would be more correct.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8747
https://github.com/root-project/root/pull/8747:269,safety,test,test,269,"About the failing tests, they now fail because they check for `kEntryNotFound` after an event loop finishes normally. In `TTreeReaderBasic.ZeroEntryRange` and `TTreeReaderBasic.InvertedEntryRange` the following logic is present (the comments also come from the current test code):. ```cpp. // Read beyond end:. EXPECT_FALSE(tr.Next());. // As the TTree only has up to entry 19, 20 is kEntryNotFound:. EXPECT_EQ(TTreeReader::kEntryNotFound, tr.GetEntryStatus());. ```. Similarly, `TTreeReaderBasic.EntryListBeyondEnd` expects `kEntryNotFound` after the last `r.Next()` when a `TEntryList` is present. Also in that case, I would argue `kEntryBeyondEnd` would be more correct.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8747
https://github.com/root-project/root/pull/8747:211,security,log,logic,211,"About the failing tests, they now fail because they check for `kEntryNotFound` after an event loop finishes normally. In `TTreeReaderBasic.ZeroEntryRange` and `TTreeReaderBasic.InvertedEntryRange` the following logic is present (the comments also come from the current test code):. ```cpp. // Read beyond end:. EXPECT_FALSE(tr.Next());. // As the TTree only has up to entry 19, 20 is kEntryNotFound:. EXPECT_EQ(TTreeReader::kEntryNotFound, tr.GetEntryStatus());. ```. Similarly, `TTreeReaderBasic.EntryListBeyondEnd` expects `kEntryNotFound` after the last `r.Next()` when a `TEntryList` is present. Also in that case, I would argue `kEntryBeyondEnd` would be more correct.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8747
https://github.com/root-project/root/pull/8747:18,testability,test,tests,18,"About the failing tests, they now fail because they check for `kEntryNotFound` after an event loop finishes normally. In `TTreeReaderBasic.ZeroEntryRange` and `TTreeReaderBasic.InvertedEntryRange` the following logic is present (the comments also come from the current test code):. ```cpp. // Read beyond end:. EXPECT_FALSE(tr.Next());. // As the TTree only has up to entry 19, 20 is kEntryNotFound:. EXPECT_EQ(TTreeReader::kEntryNotFound, tr.GetEntryStatus());. ```. Similarly, `TTreeReaderBasic.EntryListBeyondEnd` expects `kEntryNotFound` after the last `r.Next()` when a `TEntryList` is present. Also in that case, I would argue `kEntryBeyondEnd` would be more correct.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8747
https://github.com/root-project/root/pull/8747:211,testability,log,logic,211,"About the failing tests, they now fail because they check for `kEntryNotFound` after an event loop finishes normally. In `TTreeReaderBasic.ZeroEntryRange` and `TTreeReaderBasic.InvertedEntryRange` the following logic is present (the comments also come from the current test code):. ```cpp. // Read beyond end:. EXPECT_FALSE(tr.Next());. // As the TTree only has up to entry 19, 20 is kEntryNotFound:. EXPECT_EQ(TTreeReader::kEntryNotFound, tr.GetEntryStatus());. ```. Similarly, `TTreeReaderBasic.EntryListBeyondEnd` expects `kEntryNotFound` after the last `r.Next()` when a `TEntryList` is present. Also in that case, I would argue `kEntryBeyondEnd` would be more correct.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8747
https://github.com/root-project/root/pull/8747:269,testability,test,test,269,"About the failing tests, they now fail because they check for `kEntryNotFound` after an event loop finishes normally. In `TTreeReaderBasic.ZeroEntryRange` and `TTreeReaderBasic.InvertedEntryRange` the following logic is present (the comments also come from the current test code):. ```cpp. // Read beyond end:. EXPECT_FALSE(tr.Next());. // As the TTree only has up to entry 19, 20 is kEntryNotFound:. EXPECT_EQ(TTreeReader::kEntryNotFound, tr.GetEntryStatus());. ```. Similarly, `TTreeReaderBasic.EntryListBeyondEnd` expects `kEntryNotFound` after the last `r.Next()` when a `TEntryList` is present. Also in that case, I would argue `kEntryBeyondEnd` would be more correct.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8747
https://github.com/root-project/root/pull/8747:22,availability,ping,ping,22,"@pcanal @Axel-Naumann ping, what do you think of this patch? (EDIT: without it, it's unclear to me how to check whether the TTreeReader loop completed successfully: sometimes it exits with `kEntryBeyondEnd`, sometimes with `kEntryNotFound` even if everything went fine)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8747
https://github.com/root-project/root/pull/8747:54,deployability,patch,patch,54,"@pcanal @Axel-Naumann ping, what do you think of this patch? (EDIT: without it, it's unclear to me how to check whether the TTreeReader loop completed successfully: sometimes it exits with `kEntryBeyondEnd`, sometimes with `kEntryNotFound` even if everything went fine)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8747
https://github.com/root-project/root/pull/8747:54,safety,patch,patch,54,"@pcanal @Axel-Naumann ping, what do you think of this patch? (EDIT: without it, it's unclear to me how to check whether the TTreeReader loop completed successfully: sometimes it exits with `kEntryBeyondEnd`, sometimes with `kEntryNotFound` even if everything went fine)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8747
https://github.com/root-project/root/pull/8747:141,safety,compl,completed,141,"@pcanal @Axel-Naumann ping, what do you think of this patch? (EDIT: without it, it's unclear to me how to check whether the TTreeReader loop completed successfully: sometimes it exits with `kEntryBeyondEnd`, sometimes with `kEntryNotFound` even if everything went fine)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8747
https://github.com/root-project/root/pull/8747:54,security,patch,patch,54,"@pcanal @Axel-Naumann ping, what do you think of this patch? (EDIT: without it, it's unclear to me how to check whether the TTreeReader loop completed successfully: sometimes it exits with `kEntryBeyondEnd`, sometimes with `kEntryNotFound` even if everything went fine)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8747
https://github.com/root-project/root/pull/8747:141,security,compl,completed,141,"@pcanal @Axel-Naumann ping, what do you think of this patch? (EDIT: without it, it's unclear to me how to check whether the TTreeReader loop completed successfully: sometimes it exits with `kEntryBeyondEnd`, sometimes with `kEntryNotFound` even if everything went fine)",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8747
https://github.com/root-project/root/pull/8751:769,availability,servic,service,769,"<img src=""https://www.deepcode.ai/icons/green_check.svg"" width= ""50px"" align= ""left""/> Congratulations :tada:. DeepCode [analyzed](https://www.deepcode.ai/app/gh/root-project/root/350f5360909f8db1309a0b8500a4582a05b30b95/root-project/root/fbbf2e554f6f29efaa87c9c61e5870f33006dde9/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) your code in 3.451 seconds and we found no issues. Enjoy a moment of no bugs :sunny:. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/350f5360909f8db1309a0b8500a4582a05b30b95/root-project/root/fbbf2e554f6f29efaa87c9c61e5870f33006dde9/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8751
https://github.com/root-project/root/pull/8751:769,deployability,servic,service,769,"<img src=""https://www.deepcode.ai/icons/green_check.svg"" width= ""50px"" align= ""left""/> Congratulations :tada:. DeepCode [analyzed](https://www.deepcode.ai/app/gh/root-project/root/350f5360909f8db1309a0b8500a4582a05b30b95/root-project/root/fbbf2e554f6f29efaa87c9c61e5870f33006dde9/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) your code in 3.451 seconds and we found no issues. Enjoy a moment of no bugs :sunny:. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/350f5360909f8db1309a0b8500a4582a05b30b95/root-project/root/fbbf2e554f6f29efaa87c9c61e5870f33006dde9/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8751
https://github.com/root-project/root/pull/8751:781,deployability,API,API,781,"<img src=""https://www.deepcode.ai/icons/green_check.svg"" width= ""50px"" align= ""left""/> Congratulations :tada:. DeepCode [analyzed](https://www.deepcode.ai/app/gh/root-project/root/350f5360909f8db1309a0b8500a4582a05b30b95/root-project/root/fbbf2e554f6f29efaa87c9c61e5870f33006dde9/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) your code in 3.451 seconds and we found no issues. Enjoy a moment of no bugs :sunny:. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/350f5360909f8db1309a0b8500a4582a05b30b95/root-project/root/fbbf2e554f6f29efaa87c9c61e5870f33006dde9/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8751
https://github.com/root-project/root/pull/8751:769,integrability,servic,service,769,"<img src=""https://www.deepcode.ai/icons/green_check.svg"" width= ""50px"" align= ""left""/> Congratulations :tada:. DeepCode [analyzed](https://www.deepcode.ai/app/gh/root-project/root/350f5360909f8db1309a0b8500a4582a05b30b95/root-project/root/fbbf2e554f6f29efaa87c9c61e5870f33006dde9/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) your code in 3.451 seconds and we found no issues. Enjoy a moment of no bugs :sunny:. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/350f5360909f8db1309a0b8500a4582a05b30b95/root-project/root/fbbf2e554f6f29efaa87c9c61e5870f33006dde9/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8751
https://github.com/root-project/root/pull/8751:781,integrability,API,API,781,"<img src=""https://www.deepcode.ai/icons/green_check.svg"" width= ""50px"" align= ""left""/> Congratulations :tada:. DeepCode [analyzed](https://www.deepcode.ai/app/gh/root-project/root/350f5360909f8db1309a0b8500a4582a05b30b95/root-project/root/fbbf2e554f6f29efaa87c9c61e5870f33006dde9/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) your code in 3.451 seconds and we found no issues. Enjoy a moment of no bugs :sunny:. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/350f5360909f8db1309a0b8500a4582a05b30b95/root-project/root/fbbf2e554f6f29efaa87c9c61e5870f33006dde9/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8751
https://github.com/root-project/root/pull/8751:781,interoperability,API,API,781,"<img src=""https://www.deepcode.ai/icons/green_check.svg"" width= ""50px"" align= ""left""/> Congratulations :tada:. DeepCode [analyzed](https://www.deepcode.ai/app/gh/root-project/root/350f5360909f8db1309a0b8500a4582a05b30b95/root-project/root/fbbf2e554f6f29efaa87c9c61e5870f33006dde9/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) your code in 3.451 seconds and we found no issues. Enjoy a moment of no bugs :sunny:. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/350f5360909f8db1309a0b8500a4582a05b30b95/root-project/root/fbbf2e554f6f29efaa87c9c61e5870f33006dde9/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8751
https://github.com/root-project/root/pull/8751:971,interoperability,plug,plugins,971,"<img src=""https://www.deepcode.ai/icons/green_check.svg"" width= ""50px"" align= ""left""/> Congratulations :tada:. DeepCode [analyzed](https://www.deepcode.ai/app/gh/root-project/root/350f5360909f8db1309a0b8500a4582a05b30b95/root-project/root/fbbf2e554f6f29efaa87c9c61e5870f33006dde9/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) your code in 3.451 seconds and we found no issues. Enjoy a moment of no bugs :sunny:. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/350f5360909f8db1309a0b8500a4582a05b30b95/root-project/root/fbbf2e554f6f29efaa87c9c61e5870f33006dde9/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8751
https://github.com/root-project/root/pull/8751:1043,interoperability,plug,plugin,1043,"<img src=""https://www.deepcode.ai/icons/green_check.svg"" width= ""50px"" align= ""left""/> Congratulations :tada:. DeepCode [analyzed](https://www.deepcode.ai/app/gh/root-project/root/350f5360909f8db1309a0b8500a4582a05b30b95/root-project/root/fbbf2e554f6f29efaa87c9c61e5870f33006dde9/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) your code in 3.451 seconds and we found no issues. Enjoy a moment of no bugs :sunny:. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/350f5360909f8db1309a0b8500a4582a05b30b95/root-project/root/fbbf2e554f6f29efaa87c9c61e5870f33006dde9/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8751
https://github.com/root-project/root/pull/8751:1094,interoperability,plug,plugin,1094,"<img src=""https://www.deepcode.ai/icons/green_check.svg"" width= ""50px"" align= ""left""/> Congratulations :tada:. DeepCode [analyzed](https://www.deepcode.ai/app/gh/root-project/root/350f5360909f8db1309a0b8500a4582a05b30b95/root-project/root/fbbf2e554f6f29efaa87c9c61e5870f33006dde9/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) your code in 3.451 seconds and we found no issues. Enjoy a moment of no bugs :sunny:. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/350f5360909f8db1309a0b8500a4582a05b30b95/root-project/root/fbbf2e554f6f29efaa87c9c61e5870f33006dde9/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8751
https://github.com/root-project/root/pull/8751:769,modifiability,servic,service,769,"<img src=""https://www.deepcode.ai/icons/green_check.svg"" width= ""50px"" align= ""left""/> Congratulations :tada:. DeepCode [analyzed](https://www.deepcode.ai/app/gh/root-project/root/350f5360909f8db1309a0b8500a4582a05b30b95/root-project/root/fbbf2e554f6f29efaa87c9c61e5870f33006dde9/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) your code in 3.451 seconds and we found no issues. Enjoy a moment of no bugs :sunny:. #### üëâ View analysis in [**DeepCode‚Äôs Dashboard**](https://www.deepcode.ai/app/gh/root-project/root/350f5360909f8db1309a0b8500a4582a05b30b95/root-project/root/fbbf2e554f6f29efaa87c9c61e5870f33006dde9/pr/_/%2F/code/?utm_source=gh_review&c=0&w=0&i=0&) | [_Configure the bot_](https://www.deepcode.ai/app/gh/?ownerconfig=root-project). #### üëâ The DeepCode service and API will be deprecated in August, 2021. [Here](https://www.deepcode.ai/blog/migrate-to-snyk-code) is the information how to migrate. Thank you for using DeepCode üôè ‚ù§Ô∏è ! If you are using our plugins, you might be interested in their successors: [Snyk's JetBrains plugin](https://snyk.co/udpkq) and [Snyk's VS Code plugin](https://snyk.co/udpkr).",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8751
https://github.com/root-project/root/pull/8751:83,reliability,doe,does,83,I don't think the LGTM warning is justified here: the new `__init__` function just does it's job of forwarding the return value of the original function (now renamed to `_init`).,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8751
https://github.com/root-project/root/pull/8751:61,integrability,messag,message,61,Recent force push was just to fix a typo in the first commit message.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8751
https://github.com/root-project/root/pull/8751:61,interoperability,messag,message,61,Recent force push was just to fix a typo in the first commit message.,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8751
https://github.com/root-project/root/pull/8754:88,safety,test,testing,88,"It's actually an _array_ ds, not the arrow ds :smile: it's a simple datasource used for testing",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8754
https://github.com/root-project/root/pull/8754:61,testability,simpl,simple,61,"It's actually an _array_ ds, not the arrow ds :smile: it's a simple datasource used for testing",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8754
https://github.com/root-project/root/pull/8754:88,testability,test,testing,88,"It's actually an _array_ ds, not the arrow ds :smile: it's a simple datasource used for testing",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8754
https://github.com/root-project/root/pull/8754:61,usability,simpl,simple,61,"It's actually an _array_ ds, not the arrow ds :smile: it's a simple datasource used for testing",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8754
https://github.com/root-project/root/pull/8754:15,interoperability,conflict,conflicts,15,Rebased to fix conflicts,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8754
https://github.com/root-project/root/pull/8755:101,reliability,doe,does,101,"Yes I think the intention was to somehow special case the return type of a Snapshot, but it actually does not need any special casing (you can treat it as any other RResultPtr and show the computation graph for the branch it belongs to) -- plus it was doing it the special-casing wrong anyway.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8755
https://github.com/root-project/root/pull/8757:33,safety,test,test,33,"Thanks, good point! I will add a test.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8757
https://github.com/root-project/root/pull/8757:33,testability,test,test,33,"Thanks, good point! I will add a test.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/pull/8757
https://github.com/root-project/root/issues/8758:58,integrability,repositor,repository,58,"Hello @guitargeek @couet , I'd like to contribute to this repository. Can you guide me in going about this issue?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:58,interoperability,repositor,repository,58,"Hello @guitargeek @couet , I'd like to contribute to this repository. Can you guide me in going about this issue?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:78,usability,guid,guide,78,"Hello @guitargeek @couet , I'd like to contribute to this repository. Can you guide me in going about this issue?",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:63,usability,help,help,63,Sir I'm having a little issue with setting up the code can you help out? @guitargeek @couet,MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:288,availability,avail,available,288,"Hi @virenvarma007, thanks for reaching out to us about this issue! Fortunately, you don't necessarily have to compile and set up ROOT yourself to work on this issue :) All you need it a working ROOT installation on your system. Which Linux distribution are you using? Most likely root is available as a package for your distribution, for example on Ubuntu you can just do `sudo apt install root` to install it. For more information on how to install ROOT also on different platforms, please take a look also at https://root.cern/install/. Once you have ROOT installed, you can download one of the roostats tutorials that you want to translate (maybe pick a shorter one in the beginning, like [rs101_limitexample](https://github.com/root-project/root/blob/master/tutorials/roostats/rs101_limitexample.C). Remember you just need the tutorial file, not the whole ROOT repository. To try out the tutorial, you can run it with the root interpreter by typing `root rs101_limitexample.C` in the command line. You will get a graph as a result. To translate the tutorial, you need to create a `rs101_limitexample.py` file where you exactly recreate the tutorial in pyROOT, such that one can run it with `python rs101_limitexample.py` and get the same graph than with the C++ version. When you have translated your first tutorial, you can make a pull request here with the new python file in the same directory as the C++ tutorials. You can find more information of ROOT and especially pyROOT for your usecase in the ROOT users manual: https://root.cern/manual/. If you have any questions that are not answered in this manual, please feel free to ask! Good luck and thanks for your effort already! Jonas.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:577,availability,down,download,577,"Hi @virenvarma007, thanks for reaching out to us about this issue! Fortunately, you don't necessarily have to compile and set up ROOT yourself to work on this issue :) All you need it a working ROOT installation on your system. Which Linux distribution are you using? Most likely root is available as a package for your distribution, for example on Ubuntu you can just do `sudo apt install root` to install it. For more information on how to install ROOT also on different platforms, please take a look also at https://root.cern/install/. Once you have ROOT installed, you can download one of the roostats tutorials that you want to translate (maybe pick a shorter one in the beginning, like [rs101_limitexample](https://github.com/root-project/root/blob/master/tutorials/roostats/rs101_limitexample.C). Remember you just need the tutorial file, not the whole ROOT repository. To try out the tutorial, you can run it with the root interpreter by typing `root rs101_limitexample.C` in the command line. You will get a graph as a result. To translate the tutorial, you need to create a `rs101_limitexample.py` file where you exactly recreate the tutorial in pyROOT, such that one can run it with `python rs101_limitexample.py` and get the same graph than with the C++ version. When you have translated your first tutorial, you can make a pull request here with the new python file in the same directory as the C++ tutorials. You can find more information of ROOT and especially pyROOT for your usecase in the ROOT users manual: https://root.cern/manual/. If you have any questions that are not answered in this manual, please feel free to ask! Good luck and thanks for your effort already! Jonas.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:199,deployability,instal,installation,199,"Hi @virenvarma007, thanks for reaching out to us about this issue! Fortunately, you don't necessarily have to compile and set up ROOT yourself to work on this issue :) All you need it a working ROOT installation on your system. Which Linux distribution are you using? Most likely root is available as a package for your distribution, for example on Ubuntu you can just do `sudo apt install root` to install it. For more information on how to install ROOT also on different platforms, please take a look also at https://root.cern/install/. Once you have ROOT installed, you can download one of the roostats tutorials that you want to translate (maybe pick a shorter one in the beginning, like [rs101_limitexample](https://github.com/root-project/root/blob/master/tutorials/roostats/rs101_limitexample.C). Remember you just need the tutorial file, not the whole ROOT repository. To try out the tutorial, you can run it with the root interpreter by typing `root rs101_limitexample.C` in the command line. You will get a graph as a result. To translate the tutorial, you need to create a `rs101_limitexample.py` file where you exactly recreate the tutorial in pyROOT, such that one can run it with `python rs101_limitexample.py` and get the same graph than with the C++ version. When you have translated your first tutorial, you can make a pull request here with the new python file in the same directory as the C++ tutorials. You can find more information of ROOT and especially pyROOT for your usecase in the ROOT users manual: https://root.cern/manual/. If you have any questions that are not answered in this manual, please feel free to ask! Good luck and thanks for your effort already! Jonas.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:382,deployability,instal,install,382,"Hi @virenvarma007, thanks for reaching out to us about this issue! Fortunately, you don't necessarily have to compile and set up ROOT yourself to work on this issue :) All you need it a working ROOT installation on your system. Which Linux distribution are you using? Most likely root is available as a package for your distribution, for example on Ubuntu you can just do `sudo apt install root` to install it. For more information on how to install ROOT also on different platforms, please take a look also at https://root.cern/install/. Once you have ROOT installed, you can download one of the roostats tutorials that you want to translate (maybe pick a shorter one in the beginning, like [rs101_limitexample](https://github.com/root-project/root/blob/master/tutorials/roostats/rs101_limitexample.C). Remember you just need the tutorial file, not the whole ROOT repository. To try out the tutorial, you can run it with the root interpreter by typing `root rs101_limitexample.C` in the command line. You will get a graph as a result. To translate the tutorial, you need to create a `rs101_limitexample.py` file where you exactly recreate the tutorial in pyROOT, such that one can run it with `python rs101_limitexample.py` and get the same graph than with the C++ version. When you have translated your first tutorial, you can make a pull request here with the new python file in the same directory as the C++ tutorials. You can find more information of ROOT and especially pyROOT for your usecase in the ROOT users manual: https://root.cern/manual/. If you have any questions that are not answered in this manual, please feel free to ask! Good luck and thanks for your effort already! Jonas.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:399,deployability,instal,install,399,"Hi @virenvarma007, thanks for reaching out to us about this issue! Fortunately, you don't necessarily have to compile and set up ROOT yourself to work on this issue :) All you need it a working ROOT installation on your system. Which Linux distribution are you using? Most likely root is available as a package for your distribution, for example on Ubuntu you can just do `sudo apt install root` to install it. For more information on how to install ROOT also on different platforms, please take a look also at https://root.cern/install/. Once you have ROOT installed, you can download one of the roostats tutorials that you want to translate (maybe pick a shorter one in the beginning, like [rs101_limitexample](https://github.com/root-project/root/blob/master/tutorials/roostats/rs101_limitexample.C). Remember you just need the tutorial file, not the whole ROOT repository. To try out the tutorial, you can run it with the root interpreter by typing `root rs101_limitexample.C` in the command line. You will get a graph as a result. To translate the tutorial, you need to create a `rs101_limitexample.py` file where you exactly recreate the tutorial in pyROOT, such that one can run it with `python rs101_limitexample.py` and get the same graph than with the C++ version. When you have translated your first tutorial, you can make a pull request here with the new python file in the same directory as the C++ tutorials. You can find more information of ROOT and especially pyROOT for your usecase in the ROOT users manual: https://root.cern/manual/. If you have any questions that are not answered in this manual, please feel free to ask! Good luck and thanks for your effort already! Jonas.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:442,deployability,instal,install,442,"Hi @virenvarma007, thanks for reaching out to us about this issue! Fortunately, you don't necessarily have to compile and set up ROOT yourself to work on this issue :) All you need it a working ROOT installation on your system. Which Linux distribution are you using? Most likely root is available as a package for your distribution, for example on Ubuntu you can just do `sudo apt install root` to install it. For more information on how to install ROOT also on different platforms, please take a look also at https://root.cern/install/. Once you have ROOT installed, you can download one of the roostats tutorials that you want to translate (maybe pick a shorter one in the beginning, like [rs101_limitexample](https://github.com/root-project/root/blob/master/tutorials/roostats/rs101_limitexample.C). Remember you just need the tutorial file, not the whole ROOT repository. To try out the tutorial, you can run it with the root interpreter by typing `root rs101_limitexample.C` in the command line. You will get a graph as a result. To translate the tutorial, you need to create a `rs101_limitexample.py` file where you exactly recreate the tutorial in pyROOT, such that one can run it with `python rs101_limitexample.py` and get the same graph than with the C++ version. When you have translated your first tutorial, you can make a pull request here with the new python file in the same directory as the C++ tutorials. You can find more information of ROOT and especially pyROOT for your usecase in the ROOT users manual: https://root.cern/manual/. If you have any questions that are not answered in this manual, please feel free to ask! Good luck and thanks for your effort already! Jonas.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:529,deployability,instal,install,529,"Hi @virenvarma007, thanks for reaching out to us about this issue! Fortunately, you don't necessarily have to compile and set up ROOT yourself to work on this issue :) All you need it a working ROOT installation on your system. Which Linux distribution are you using? Most likely root is available as a package for your distribution, for example on Ubuntu you can just do `sudo apt install root` to install it. For more information on how to install ROOT also on different platforms, please take a look also at https://root.cern/install/. Once you have ROOT installed, you can download one of the roostats tutorials that you want to translate (maybe pick a shorter one in the beginning, like [rs101_limitexample](https://github.com/root-project/root/blob/master/tutorials/roostats/rs101_limitexample.C). Remember you just need the tutorial file, not the whole ROOT repository. To try out the tutorial, you can run it with the root interpreter by typing `root rs101_limitexample.C` in the command line. You will get a graph as a result. To translate the tutorial, you need to create a `rs101_limitexample.py` file where you exactly recreate the tutorial in pyROOT, such that one can run it with `python rs101_limitexample.py` and get the same graph than with the C++ version. When you have translated your first tutorial, you can make a pull request here with the new python file in the same directory as the C++ tutorials. You can find more information of ROOT and especially pyROOT for your usecase in the ROOT users manual: https://root.cern/manual/. If you have any questions that are not answered in this manual, please feel free to ask! Good luck and thanks for your effort already! Jonas.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:558,deployability,instal,installed,558,"Hi @virenvarma007, thanks for reaching out to us about this issue! Fortunately, you don't necessarily have to compile and set up ROOT yourself to work on this issue :) All you need it a working ROOT installation on your system. Which Linux distribution are you using? Most likely root is available as a package for your distribution, for example on Ubuntu you can just do `sudo apt install root` to install it. For more information on how to install ROOT also on different platforms, please take a look also at https://root.cern/install/. Once you have ROOT installed, you can download one of the roostats tutorials that you want to translate (maybe pick a shorter one in the beginning, like [rs101_limitexample](https://github.com/root-project/root/blob/master/tutorials/roostats/rs101_limitexample.C). Remember you just need the tutorial file, not the whole ROOT repository. To try out the tutorial, you can run it with the root interpreter by typing `root rs101_limitexample.C` in the command line. You will get a graph as a result. To translate the tutorial, you need to create a `rs101_limitexample.py` file where you exactly recreate the tutorial in pyROOT, such that one can run it with `python rs101_limitexample.py` and get the same graph than with the C++ version. When you have translated your first tutorial, you can make a pull request here with the new python file in the same directory as the C++ tutorials. You can find more information of ROOT and especially pyROOT for your usecase in the ROOT users manual: https://root.cern/manual/. If you have any questions that are not answered in this manual, please feel free to ask! Good luck and thanks for your effort already! Jonas.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:1266,deployability,version,version,1266,"Hi @virenvarma007, thanks for reaching out to us about this issue! Fortunately, you don't necessarily have to compile and set up ROOT yourself to work on this issue :) All you need it a working ROOT installation on your system. Which Linux distribution are you using? Most likely root is available as a package for your distribution, for example on Ubuntu you can just do `sudo apt install root` to install it. For more information on how to install ROOT also on different platforms, please take a look also at https://root.cern/install/. Once you have ROOT installed, you can download one of the roostats tutorials that you want to translate (maybe pick a shorter one in the beginning, like [rs101_limitexample](https://github.com/root-project/root/blob/master/tutorials/roostats/rs101_limitexample.C). Remember you just need the tutorial file, not the whole ROOT repository. To try out the tutorial, you can run it with the root interpreter by typing `root rs101_limitexample.C` in the command line. You will get a graph as a result. To translate the tutorial, you need to create a `rs101_limitexample.py` file where you exactly recreate the tutorial in pyROOT, such that one can run it with `python rs101_limitexample.py` and get the same graph than with the C++ version. When you have translated your first tutorial, you can make a pull request here with the new python file in the same directory as the C++ tutorials. You can find more information of ROOT and especially pyROOT for your usecase in the ROOT users manual: https://root.cern/manual/. If you have any questions that are not answered in this manual, please feel free to ask! Good luck and thanks for your effort already! Jonas.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:633,integrability,translat,translate,633,"Hi @virenvarma007, thanks for reaching out to us about this issue! Fortunately, you don't necessarily have to compile and set up ROOT yourself to work on this issue :) All you need it a working ROOT installation on your system. Which Linux distribution are you using? Most likely root is available as a package for your distribution, for example on Ubuntu you can just do `sudo apt install root` to install it. For more information on how to install ROOT also on different platforms, please take a look also at https://root.cern/install/. Once you have ROOT installed, you can download one of the roostats tutorials that you want to translate (maybe pick a shorter one in the beginning, like [rs101_limitexample](https://github.com/root-project/root/blob/master/tutorials/roostats/rs101_limitexample.C). Remember you just need the tutorial file, not the whole ROOT repository. To try out the tutorial, you can run it with the root interpreter by typing `root rs101_limitexample.C` in the command line. You will get a graph as a result. To translate the tutorial, you need to create a `rs101_limitexample.py` file where you exactly recreate the tutorial in pyROOT, such that one can run it with `python rs101_limitexample.py` and get the same graph than with the C++ version. When you have translated your first tutorial, you can make a pull request here with the new python file in the same directory as the C++ tutorials. You can find more information of ROOT and especially pyROOT for your usecase in the ROOT users manual: https://root.cern/manual/. If you have any questions that are not answered in this manual, please feel free to ask! Good luck and thanks for your effort already! Jonas.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:865,integrability,repositor,repository,865,"Hi @virenvarma007, thanks for reaching out to us about this issue! Fortunately, you don't necessarily have to compile and set up ROOT yourself to work on this issue :) All you need it a working ROOT installation on your system. Which Linux distribution are you using? Most likely root is available as a package for your distribution, for example on Ubuntu you can just do `sudo apt install root` to install it. For more information on how to install ROOT also on different platforms, please take a look also at https://root.cern/install/. Once you have ROOT installed, you can download one of the roostats tutorials that you want to translate (maybe pick a shorter one in the beginning, like [rs101_limitexample](https://github.com/root-project/root/blob/master/tutorials/roostats/rs101_limitexample.C). Remember you just need the tutorial file, not the whole ROOT repository. To try out the tutorial, you can run it with the root interpreter by typing `root rs101_limitexample.C` in the command line. You will get a graph as a result. To translate the tutorial, you need to create a `rs101_limitexample.py` file where you exactly recreate the tutorial in pyROOT, such that one can run it with `python rs101_limitexample.py` and get the same graph than with the C++ version. When you have translated your first tutorial, you can make a pull request here with the new python file in the same directory as the C++ tutorials. You can find more information of ROOT and especially pyROOT for your usecase in the ROOT users manual: https://root.cern/manual/. If you have any questions that are not answered in this manual, please feel free to ask! Good luck and thanks for your effort already! Jonas.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:1039,integrability,translat,translate,1039,"Hi @virenvarma007, thanks for reaching out to us about this issue! Fortunately, you don't necessarily have to compile and set up ROOT yourself to work on this issue :) All you need it a working ROOT installation on your system. Which Linux distribution are you using? Most likely root is available as a package for your distribution, for example on Ubuntu you can just do `sudo apt install root` to install it. For more information on how to install ROOT also on different platforms, please take a look also at https://root.cern/install/. Once you have ROOT installed, you can download one of the roostats tutorials that you want to translate (maybe pick a shorter one in the beginning, like [rs101_limitexample](https://github.com/root-project/root/blob/master/tutorials/roostats/rs101_limitexample.C). Remember you just need the tutorial file, not the whole ROOT repository. To try out the tutorial, you can run it with the root interpreter by typing `root rs101_limitexample.C` in the command line. You will get a graph as a result. To translate the tutorial, you need to create a `rs101_limitexample.py` file where you exactly recreate the tutorial in pyROOT, such that one can run it with `python rs101_limitexample.py` and get the same graph than with the C++ version. When you have translated your first tutorial, you can make a pull request here with the new python file in the same directory as the C++ tutorials. You can find more information of ROOT and especially pyROOT for your usecase in the ROOT users manual: https://root.cern/manual/. If you have any questions that are not answered in this manual, please feel free to ask! Good luck and thanks for your effort already! Jonas.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:1266,integrability,version,version,1266,"Hi @virenvarma007, thanks for reaching out to us about this issue! Fortunately, you don't necessarily have to compile and set up ROOT yourself to work on this issue :) All you need it a working ROOT installation on your system. Which Linux distribution are you using? Most likely root is available as a package for your distribution, for example on Ubuntu you can just do `sudo apt install root` to install it. For more information on how to install ROOT also on different platforms, please take a look also at https://root.cern/install/. Once you have ROOT installed, you can download one of the roostats tutorials that you want to translate (maybe pick a shorter one in the beginning, like [rs101_limitexample](https://github.com/root-project/root/blob/master/tutorials/roostats/rs101_limitexample.C). Remember you just need the tutorial file, not the whole ROOT repository. To try out the tutorial, you can run it with the root interpreter by typing `root rs101_limitexample.C` in the command line. You will get a graph as a result. To translate the tutorial, you need to create a `rs101_limitexample.py` file where you exactly recreate the tutorial in pyROOT, such that one can run it with `python rs101_limitexample.py` and get the same graph than with the C++ version. When you have translated your first tutorial, you can make a pull request here with the new python file in the same directory as the C++ tutorials. You can find more information of ROOT and especially pyROOT for your usecase in the ROOT users manual: https://root.cern/manual/. If you have any questions that are not answered in this manual, please feel free to ask! Good luck and thanks for your effort already! Jonas.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:1289,integrability,translat,translated,1289,"Hi @virenvarma007, thanks for reaching out to us about this issue! Fortunately, you don't necessarily have to compile and set up ROOT yourself to work on this issue :) All you need it a working ROOT installation on your system. Which Linux distribution are you using? Most likely root is available as a package for your distribution, for example on Ubuntu you can just do `sudo apt install root` to install it. For more information on how to install ROOT also on different platforms, please take a look also at https://root.cern/install/. Once you have ROOT installed, you can download one of the roostats tutorials that you want to translate (maybe pick a shorter one in the beginning, like [rs101_limitexample](https://github.com/root-project/root/blob/master/tutorials/roostats/rs101_limitexample.C). Remember you just need the tutorial file, not the whole ROOT repository. To try out the tutorial, you can run it with the root interpreter by typing `root rs101_limitexample.C` in the command line. You will get a graph as a result. To translate the tutorial, you need to create a `rs101_limitexample.py` file where you exactly recreate the tutorial in pyROOT, such that one can run it with `python rs101_limitexample.py` and get the same graph than with the C++ version. When you have translated your first tutorial, you can make a pull request here with the new python file in the same directory as the C++ tutorials. You can find more information of ROOT and especially pyROOT for your usecase in the ROOT users manual: https://root.cern/manual/. If you have any questions that are not answered in this manual, please feel free to ask! Good luck and thanks for your effort already! Jonas.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:240,interoperability,distribut,distribution,240,"Hi @virenvarma007, thanks for reaching out to us about this issue! Fortunately, you don't necessarily have to compile and set up ROOT yourself to work on this issue :) All you need it a working ROOT installation on your system. Which Linux distribution are you using? Most likely root is available as a package for your distribution, for example on Ubuntu you can just do `sudo apt install root` to install it. For more information on how to install ROOT also on different platforms, please take a look also at https://root.cern/install/. Once you have ROOT installed, you can download one of the roostats tutorials that you want to translate (maybe pick a shorter one in the beginning, like [rs101_limitexample](https://github.com/root-project/root/blob/master/tutorials/roostats/rs101_limitexample.C). Remember you just need the tutorial file, not the whole ROOT repository. To try out the tutorial, you can run it with the root interpreter by typing `root rs101_limitexample.C` in the command line. You will get a graph as a result. To translate the tutorial, you need to create a `rs101_limitexample.py` file where you exactly recreate the tutorial in pyROOT, such that one can run it with `python rs101_limitexample.py` and get the same graph than with the C++ version. When you have translated your first tutorial, you can make a pull request here with the new python file in the same directory as the C++ tutorials. You can find more information of ROOT and especially pyROOT for your usecase in the ROOT users manual: https://root.cern/manual/. If you have any questions that are not answered in this manual, please feel free to ask! Good luck and thanks for your effort already! Jonas.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:320,interoperability,distribut,distribution,320,"Hi @virenvarma007, thanks for reaching out to us about this issue! Fortunately, you don't necessarily have to compile and set up ROOT yourself to work on this issue :) All you need it a working ROOT installation on your system. Which Linux distribution are you using? Most likely root is available as a package for your distribution, for example on Ubuntu you can just do `sudo apt install root` to install it. For more information on how to install ROOT also on different platforms, please take a look also at https://root.cern/install/. Once you have ROOT installed, you can download one of the roostats tutorials that you want to translate (maybe pick a shorter one in the beginning, like [rs101_limitexample](https://github.com/root-project/root/blob/master/tutorials/roostats/rs101_limitexample.C). Remember you just need the tutorial file, not the whole ROOT repository. To try out the tutorial, you can run it with the root interpreter by typing `root rs101_limitexample.C` in the command line. You will get a graph as a result. To translate the tutorial, you need to create a `rs101_limitexample.py` file where you exactly recreate the tutorial in pyROOT, such that one can run it with `python rs101_limitexample.py` and get the same graph than with the C++ version. When you have translated your first tutorial, you can make a pull request here with the new python file in the same directory as the C++ tutorials. You can find more information of ROOT and especially pyROOT for your usecase in the ROOT users manual: https://root.cern/manual/. If you have any questions that are not answered in this manual, please feel free to ask! Good luck and thanks for your effort already! Jonas.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:473,interoperability,platform,platforms,473,"Hi @virenvarma007, thanks for reaching out to us about this issue! Fortunately, you don't necessarily have to compile and set up ROOT yourself to work on this issue :) All you need it a working ROOT installation on your system. Which Linux distribution are you using? Most likely root is available as a package for your distribution, for example on Ubuntu you can just do `sudo apt install root` to install it. For more information on how to install ROOT also on different platforms, please take a look also at https://root.cern/install/. Once you have ROOT installed, you can download one of the roostats tutorials that you want to translate (maybe pick a shorter one in the beginning, like [rs101_limitexample](https://github.com/root-project/root/blob/master/tutorials/roostats/rs101_limitexample.C). Remember you just need the tutorial file, not the whole ROOT repository. To try out the tutorial, you can run it with the root interpreter by typing `root rs101_limitexample.C` in the command line. You will get a graph as a result. To translate the tutorial, you need to create a `rs101_limitexample.py` file where you exactly recreate the tutorial in pyROOT, such that one can run it with `python rs101_limitexample.py` and get the same graph than with the C++ version. When you have translated your first tutorial, you can make a pull request here with the new python file in the same directory as the C++ tutorials. You can find more information of ROOT and especially pyROOT for your usecase in the ROOT users manual: https://root.cern/manual/. If you have any questions that are not answered in this manual, please feel free to ask! Good luck and thanks for your effort already! Jonas.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:633,interoperability,translat,translate,633,"Hi @virenvarma007, thanks for reaching out to us about this issue! Fortunately, you don't necessarily have to compile and set up ROOT yourself to work on this issue :) All you need it a working ROOT installation on your system. Which Linux distribution are you using? Most likely root is available as a package for your distribution, for example on Ubuntu you can just do `sudo apt install root` to install it. For more information on how to install ROOT also on different platforms, please take a look also at https://root.cern/install/. Once you have ROOT installed, you can download one of the roostats tutorials that you want to translate (maybe pick a shorter one in the beginning, like [rs101_limitexample](https://github.com/root-project/root/blob/master/tutorials/roostats/rs101_limitexample.C). Remember you just need the tutorial file, not the whole ROOT repository. To try out the tutorial, you can run it with the root interpreter by typing `root rs101_limitexample.C` in the command line. You will get a graph as a result. To translate the tutorial, you need to create a `rs101_limitexample.py` file where you exactly recreate the tutorial in pyROOT, such that one can run it with `python rs101_limitexample.py` and get the same graph than with the C++ version. When you have translated your first tutorial, you can make a pull request here with the new python file in the same directory as the C++ tutorials. You can find more information of ROOT and especially pyROOT for your usecase in the ROOT users manual: https://root.cern/manual/. If you have any questions that are not answered in this manual, please feel free to ask! Good luck and thanks for your effort already! Jonas.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:865,interoperability,repositor,repository,865,"Hi @virenvarma007, thanks for reaching out to us about this issue! Fortunately, you don't necessarily have to compile and set up ROOT yourself to work on this issue :) All you need it a working ROOT installation on your system. Which Linux distribution are you using? Most likely root is available as a package for your distribution, for example on Ubuntu you can just do `sudo apt install root` to install it. For more information on how to install ROOT also on different platforms, please take a look also at https://root.cern/install/. Once you have ROOT installed, you can download one of the roostats tutorials that you want to translate (maybe pick a shorter one in the beginning, like [rs101_limitexample](https://github.com/root-project/root/blob/master/tutorials/roostats/rs101_limitexample.C). Remember you just need the tutorial file, not the whole ROOT repository. To try out the tutorial, you can run it with the root interpreter by typing `root rs101_limitexample.C` in the command line. You will get a graph as a result. To translate the tutorial, you need to create a `rs101_limitexample.py` file where you exactly recreate the tutorial in pyROOT, such that one can run it with `python rs101_limitexample.py` and get the same graph than with the C++ version. When you have translated your first tutorial, you can make a pull request here with the new python file in the same directory as the C++ tutorials. You can find more information of ROOT and especially pyROOT for your usecase in the ROOT users manual: https://root.cern/manual/. If you have any questions that are not answered in this manual, please feel free to ask! Good luck and thanks for your effort already! Jonas.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:1039,interoperability,translat,translate,1039,"Hi @virenvarma007, thanks for reaching out to us about this issue! Fortunately, you don't necessarily have to compile and set up ROOT yourself to work on this issue :) All you need it a working ROOT installation on your system. Which Linux distribution are you using? Most likely root is available as a package for your distribution, for example on Ubuntu you can just do `sudo apt install root` to install it. For more information on how to install ROOT also on different platforms, please take a look also at https://root.cern/install/. Once you have ROOT installed, you can download one of the roostats tutorials that you want to translate (maybe pick a shorter one in the beginning, like [rs101_limitexample](https://github.com/root-project/root/blob/master/tutorials/roostats/rs101_limitexample.C). Remember you just need the tutorial file, not the whole ROOT repository. To try out the tutorial, you can run it with the root interpreter by typing `root rs101_limitexample.C` in the command line. You will get a graph as a result. To translate the tutorial, you need to create a `rs101_limitexample.py` file where you exactly recreate the tutorial in pyROOT, such that one can run it with `python rs101_limitexample.py` and get the same graph than with the C++ version. When you have translated your first tutorial, you can make a pull request here with the new python file in the same directory as the C++ tutorials. You can find more information of ROOT and especially pyROOT for your usecase in the ROOT users manual: https://root.cern/manual/. If you have any questions that are not answered in this manual, please feel free to ask! Good luck and thanks for your effort already! Jonas.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:1289,interoperability,translat,translated,1289,"Hi @virenvarma007, thanks for reaching out to us about this issue! Fortunately, you don't necessarily have to compile and set up ROOT yourself to work on this issue :) All you need it a working ROOT installation on your system. Which Linux distribution are you using? Most likely root is available as a package for your distribution, for example on Ubuntu you can just do `sudo apt install root` to install it. For more information on how to install ROOT also on different platforms, please take a look also at https://root.cern/install/. Once you have ROOT installed, you can download one of the roostats tutorials that you want to translate (maybe pick a shorter one in the beginning, like [rs101_limitexample](https://github.com/root-project/root/blob/master/tutorials/roostats/rs101_limitexample.C). Remember you just need the tutorial file, not the whole ROOT repository. To try out the tutorial, you can run it with the root interpreter by typing `root rs101_limitexample.C` in the command line. You will get a graph as a result. To translate the tutorial, you need to create a `rs101_limitexample.py` file where you exactly recreate the tutorial in pyROOT, such that one can run it with `python rs101_limitexample.py` and get the same graph than with the C++ version. When you have translated your first tutorial, you can make a pull request here with the new python file in the same directory as the C++ tutorials. You can find more information of ROOT and especially pyROOT for your usecase in the ROOT users manual: https://root.cern/manual/. If you have any questions that are not answered in this manual, please feel free to ask! Good luck and thanks for your effort already! Jonas.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:303,modifiability,pac,package,303,"Hi @virenvarma007, thanks for reaching out to us about this issue! Fortunately, you don't necessarily have to compile and set up ROOT yourself to work on this issue :) All you need it a working ROOT installation on your system. Which Linux distribution are you using? Most likely root is available as a package for your distribution, for example on Ubuntu you can just do `sudo apt install root` to install it. For more information on how to install ROOT also on different platforms, please take a look also at https://root.cern/install/. Once you have ROOT installed, you can download one of the roostats tutorials that you want to translate (maybe pick a shorter one in the beginning, like [rs101_limitexample](https://github.com/root-project/root/blob/master/tutorials/roostats/rs101_limitexample.C). Remember you just need the tutorial file, not the whole ROOT repository. To try out the tutorial, you can run it with the root interpreter by typing `root rs101_limitexample.C` in the command line. You will get a graph as a result. To translate the tutorial, you need to create a `rs101_limitexample.py` file where you exactly recreate the tutorial in pyROOT, such that one can run it with `python rs101_limitexample.py` and get the same graph than with the C++ version. When you have translated your first tutorial, you can make a pull request here with the new python file in the same directory as the C++ tutorials. You can find more information of ROOT and especially pyROOT for your usecase in the ROOT users manual: https://root.cern/manual/. If you have any questions that are not answered in this manual, please feel free to ask! Good luck and thanks for your effort already! Jonas.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:1266,modifiability,version,version,1266,"Hi @virenvarma007, thanks for reaching out to us about this issue! Fortunately, you don't necessarily have to compile and set up ROOT yourself to work on this issue :) All you need it a working ROOT installation on your system. Which Linux distribution are you using? Most likely root is available as a package for your distribution, for example on Ubuntu you can just do `sudo apt install root` to install it. For more information on how to install ROOT also on different platforms, please take a look also at https://root.cern/install/. Once you have ROOT installed, you can download one of the roostats tutorials that you want to translate (maybe pick a shorter one in the beginning, like [rs101_limitexample](https://github.com/root-project/root/blob/master/tutorials/roostats/rs101_limitexample.C). Remember you just need the tutorial file, not the whole ROOT repository. To try out the tutorial, you can run it with the root interpreter by typing `root rs101_limitexample.C` in the command line. You will get a graph as a result. To translate the tutorial, you need to create a `rs101_limitexample.py` file where you exactly recreate the tutorial in pyROOT, such that one can run it with `python rs101_limitexample.py` and get the same graph than with the C++ version. When you have translated your first tutorial, you can make a pull request here with the new python file in the same directory as the C++ tutorials. You can find more information of ROOT and especially pyROOT for your usecase in the ROOT users manual: https://root.cern/manual/. If you have any questions that are not answered in this manual, please feel free to ask! Good luck and thanks for your effort already! Jonas.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:288,reliability,availab,available,288,"Hi @virenvarma007, thanks for reaching out to us about this issue! Fortunately, you don't necessarily have to compile and set up ROOT yourself to work on this issue :) All you need it a working ROOT installation on your system. Which Linux distribution are you using? Most likely root is available as a package for your distribution, for example on Ubuntu you can just do `sudo apt install root` to install it. For more information on how to install ROOT also on different platforms, please take a look also at https://root.cern/install/. Once you have ROOT installed, you can download one of the roostats tutorials that you want to translate (maybe pick a shorter one in the beginning, like [rs101_limitexample](https://github.com/root-project/root/blob/master/tutorials/roostats/rs101_limitexample.C). Remember you just need the tutorial file, not the whole ROOT repository. To try out the tutorial, you can run it with the root interpreter by typing `root rs101_limitexample.C` in the command line. You will get a graph as a result. To translate the tutorial, you need to create a `rs101_limitexample.py` file where you exactly recreate the tutorial in pyROOT, such that one can run it with `python rs101_limitexample.py` and get the same graph than with the C++ version. When you have translated your first tutorial, you can make a pull request here with the new python file in the same directory as the C++ tutorials. You can find more information of ROOT and especially pyROOT for your usecase in the ROOT users manual: https://root.cern/manual/. If you have any questions that are not answered in this manual, please feel free to ask! Good luck and thanks for your effort already! Jonas.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:288,safety,avail,available,288,"Hi @virenvarma007, thanks for reaching out to us about this issue! Fortunately, you don't necessarily have to compile and set up ROOT yourself to work on this issue :) All you need it a working ROOT installation on your system. Which Linux distribution are you using? Most likely root is available as a package for your distribution, for example on Ubuntu you can just do `sudo apt install root` to install it. For more information on how to install ROOT also on different platforms, please take a look also at https://root.cern/install/. Once you have ROOT installed, you can download one of the roostats tutorials that you want to translate (maybe pick a shorter one in the beginning, like [rs101_limitexample](https://github.com/root-project/root/blob/master/tutorials/roostats/rs101_limitexample.C). Remember you just need the tutorial file, not the whole ROOT repository. To try out the tutorial, you can run it with the root interpreter by typing `root rs101_limitexample.C` in the command line. You will get a graph as a result. To translate the tutorial, you need to create a `rs101_limitexample.py` file where you exactly recreate the tutorial in pyROOT, such that one can run it with `python rs101_limitexample.py` and get the same graph than with the C++ version. When you have translated your first tutorial, you can make a pull request here with the new python file in the same directory as the C++ tutorials. You can find more information of ROOT and especially pyROOT for your usecase in the ROOT users manual: https://root.cern/manual/. If you have any questions that are not answered in this manual, please feel free to ask! Good luck and thanks for your effort already! Jonas.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:804,safety,Reme,Remember,804,"Hi @virenvarma007, thanks for reaching out to us about this issue! Fortunately, you don't necessarily have to compile and set up ROOT yourself to work on this issue :) All you need it a working ROOT installation on your system. Which Linux distribution are you using? Most likely root is available as a package for your distribution, for example on Ubuntu you can just do `sudo apt install root` to install it. For more information on how to install ROOT also on different platforms, please take a look also at https://root.cern/install/. Once you have ROOT installed, you can download one of the roostats tutorials that you want to translate (maybe pick a shorter one in the beginning, like [rs101_limitexample](https://github.com/root-project/root/blob/master/tutorials/roostats/rs101_limitexample.C). Remember you just need the tutorial file, not the whole ROOT repository. To try out the tutorial, you can run it with the root interpreter by typing `root rs101_limitexample.C` in the command line. You will get a graph as a result. To translate the tutorial, you need to create a `rs101_limitexample.py` file where you exactly recreate the tutorial in pyROOT, such that one can run it with `python rs101_limitexample.py` and get the same graph than with the C++ version. When you have translated your first tutorial, you can make a pull request here with the new python file in the same directory as the C++ tutorials. You can find more information of ROOT and especially pyROOT for your usecase in the ROOT users manual: https://root.cern/manual/. If you have any questions that are not answered in this manual, please feel free to ask! Good luck and thanks for your effort already! Jonas.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:288,security,availab,available,288,"Hi @virenvarma007, thanks for reaching out to us about this issue! Fortunately, you don't necessarily have to compile and set up ROOT yourself to work on this issue :) All you need it a working ROOT installation on your system. Which Linux distribution are you using? Most likely root is available as a package for your distribution, for example on Ubuntu you can just do `sudo apt install root` to install it. For more information on how to install ROOT also on different platforms, please take a look also at https://root.cern/install/. Once you have ROOT installed, you can download one of the roostats tutorials that you want to translate (maybe pick a shorter one in the beginning, like [rs101_limitexample](https://github.com/root-project/root/blob/master/tutorials/roostats/rs101_limitexample.C). Remember you just need the tutorial file, not the whole ROOT repository. To try out the tutorial, you can run it with the root interpreter by typing `root rs101_limitexample.C` in the command line. You will get a graph as a result. To translate the tutorial, you need to create a `rs101_limitexample.py` file where you exactly recreate the tutorial in pyROOT, such that one can run it with `python rs101_limitexample.py` and get the same graph than with the C++ version. When you have translated your first tutorial, you can make a pull request here with the new python file in the same directory as the C++ tutorials. You can find more information of ROOT and especially pyROOT for your usecase in the ROOT users manual: https://root.cern/manual/. If you have any questions that are not answered in this manual, please feel free to ask! Good luck and thanks for your effort already! Jonas.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:378,security,apt,apt,378,"Hi @virenvarma007, thanks for reaching out to us about this issue! Fortunately, you don't necessarily have to compile and set up ROOT yourself to work on this issue :) All you need it a working ROOT installation on your system. Which Linux distribution are you using? Most likely root is available as a package for your distribution, for example on Ubuntu you can just do `sudo apt install root` to install it. For more information on how to install ROOT also on different platforms, please take a look also at https://root.cern/install/. Once you have ROOT installed, you can download one of the roostats tutorials that you want to translate (maybe pick a shorter one in the beginning, like [rs101_limitexample](https://github.com/root-project/root/blob/master/tutorials/roostats/rs101_limitexample.C). Remember you just need the tutorial file, not the whole ROOT repository. To try out the tutorial, you can run it with the root interpreter by typing `root rs101_limitexample.C` in the command line. You will get a graph as a result. To translate the tutorial, you need to create a `rs101_limitexample.py` file where you exactly recreate the tutorial in pyROOT, such that one can run it with `python rs101_limitexample.py` and get the same graph than with the C++ version. When you have translated your first tutorial, you can make a pull request here with the new python file in the same directory as the C++ tutorials. You can find more information of ROOT and especially pyROOT for your usecase in the ROOT users manual: https://root.cern/manual/. If you have any questions that are not answered in this manual, please feel free to ask! Good luck and thanks for your effort already! Jonas.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:988,usability,command,command,988,"Hi @virenvarma007, thanks for reaching out to us about this issue! Fortunately, you don't necessarily have to compile and set up ROOT yourself to work on this issue :) All you need it a working ROOT installation on your system. Which Linux distribution are you using? Most likely root is available as a package for your distribution, for example on Ubuntu you can just do `sudo apt install root` to install it. For more information on how to install ROOT also on different platforms, please take a look also at https://root.cern/install/. Once you have ROOT installed, you can download one of the roostats tutorials that you want to translate (maybe pick a shorter one in the beginning, like [rs101_limitexample](https://github.com/root-project/root/blob/master/tutorials/roostats/rs101_limitexample.C). Remember you just need the tutorial file, not the whole ROOT repository. To try out the tutorial, you can run it with the root interpreter by typing `root rs101_limitexample.C` in the command line. You will get a graph as a result. To translate the tutorial, you need to create a `rs101_limitexample.py` file where you exactly recreate the tutorial in pyROOT, such that one can run it with `python rs101_limitexample.py` and get the same graph than with the C++ version. When you have translated your first tutorial, you can make a pull request here with the new python file in the same directory as the C++ tutorials. You can find more information of ROOT and especially pyROOT for your usecase in the ROOT users manual: https://root.cern/manual/. If you have any questions that are not answered in this manual, please feel free to ask! Good luck and thanks for your effort already! Jonas.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:1512,usability,user,users,1512,"Hi @virenvarma007, thanks for reaching out to us about this issue! Fortunately, you don't necessarily have to compile and set up ROOT yourself to work on this issue :) All you need it a working ROOT installation on your system. Which Linux distribution are you using? Most likely root is available as a package for your distribution, for example on Ubuntu you can just do `sudo apt install root` to install it. For more information on how to install ROOT also on different platforms, please take a look also at https://root.cern/install/. Once you have ROOT installed, you can download one of the roostats tutorials that you want to translate (maybe pick a shorter one in the beginning, like [rs101_limitexample](https://github.com/root-project/root/blob/master/tutorials/roostats/rs101_limitexample.C). Remember you just need the tutorial file, not the whole ROOT repository. To try out the tutorial, you can run it with the root interpreter by typing `root rs101_limitexample.C` in the command line. You will get a graph as a result. To translate the tutorial, you need to create a `rs101_limitexample.py` file where you exactly recreate the tutorial in pyROOT, such that one can run it with `python rs101_limitexample.py` and get the same graph than with the C++ version. When you have translated your first tutorial, you can make a pull request here with the new python file in the same directory as the C++ tutorials. You can find more information of ROOT and especially pyROOT for your usecase in the ROOT users manual: https://root.cern/manual/. If you have any questions that are not answered in this manual, please feel free to ask! Good luck and thanks for your effort already! Jonas.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:87,deployability,instal,installed,87,"Thanks you so much for the details Sir, I'm using Ubuntu 18.04 and I already have root installed in pc. I'll start with the tutorial and work on the Issue right away.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:156,availability,error,error,156,"Hello @guitargeek Sir, while Installing Root to my Ubuntu VM 18.04, during the building of the file ""cmake --build . -- -j3"" the build stops and shows this error every time at the 49% of the build. I have tried to start the process multiple times and even increase space in my Oracle VM but nothing seemed to work. CMake Error at /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-RelWithDebInfo.cmake:16 (message):. Command failed: 2. '/usr/bin/make' 'install'. See also. /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-*.log. CMakeFiles/AFTERIMAGE.dir/build.make:73: recipe for target 'AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install' failed. make[2]: *** [AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install] Error 1. CMakeFiles/Makefile2:280: recipe for target 'CMakeFiles/AFTERIMAGE.dir/all' failed. make[1]: *** [CMakeFiles/AFTERIMAGE.dir/all] Error 2. make[1]: *** Waiting for unfinished jobs.... [ 49%] Built target move_artifacts. [ 49%] Built target clang-tblgen. Makefile:151: recipe for target 'all' failed. make: *** [all] Error 2.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:321,availability,Error,Error,321,"Hello @guitargeek Sir, while Installing Root to my Ubuntu VM 18.04, during the building of the file ""cmake --build . -- -j3"" the build stops and shows this error every time at the 49% of the build. I have tried to start the process multiple times and even increase space in my Oracle VM but nothing seemed to work. CMake Error at /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-RelWithDebInfo.cmake:16 (message):. Command failed: 2. '/usr/bin/make' 'install'. See also. /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-*.log. CMakeFiles/AFTERIMAGE.dir/build.make:73: recipe for target 'AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install' failed. make[2]: *** [AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install] Error 1. CMakeFiles/Makefile2:280: recipe for target 'CMakeFiles/AFTERIMAGE.dir/all' failed. make[1]: *** [CMakeFiles/AFTERIMAGE.dir/all] Error 2. make[1]: *** Waiting for unfinished jobs.... [ 49%] Built target move_artifacts. [ 49%] Built target clang-tblgen. Makefile:151: recipe for target 'all' failed. make: *** [all] Error 2.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:799,availability,Error,Error,799,"Hello @guitargeek Sir, while Installing Root to my Ubuntu VM 18.04, during the building of the file ""cmake --build . -- -j3"" the build stops and shows this error every time at the 49% of the build. I have tried to start the process multiple times and even increase space in my Oracle VM but nothing seemed to work. CMake Error at /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-RelWithDebInfo.cmake:16 (message):. Command failed: 2. '/usr/bin/make' 'install'. See also. /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-*.log. CMakeFiles/AFTERIMAGE.dir/build.make:73: recipe for target 'AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install' failed. make[2]: *** [AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install] Error 1. CMakeFiles/Makefile2:280: recipe for target 'CMakeFiles/AFTERIMAGE.dir/all' failed. make[1]: *** [CMakeFiles/AFTERIMAGE.dir/all] Error 2. make[1]: *** Waiting for unfinished jobs.... [ 49%] Built target move_artifacts. [ 49%] Built target clang-tblgen. Makefile:151: recipe for target 'all' failed. make: *** [all] Error 2.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:937,availability,Error,Error,937,"Hello @guitargeek Sir, while Installing Root to my Ubuntu VM 18.04, during the building of the file ""cmake --build . -- -j3"" the build stops and shows this error every time at the 49% of the build. I have tried to start the process multiple times and even increase space in my Oracle VM but nothing seemed to work. CMake Error at /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-RelWithDebInfo.cmake:16 (message):. Command failed: 2. '/usr/bin/make' 'install'. See also. /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-*.log. CMakeFiles/AFTERIMAGE.dir/build.make:73: recipe for target 'AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install' failed. make[2]: *** [AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install] Error 1. CMakeFiles/Makefile2:280: recipe for target 'CMakeFiles/AFTERIMAGE.dir/all' failed. make[1]: *** [CMakeFiles/AFTERIMAGE.dir/all] Error 2. make[1]: *** Waiting for unfinished jobs.... [ 49%] Built target move_artifacts. [ 49%] Built target clang-tblgen. Makefile:151: recipe for target 'all' failed. make: *** [all] Error 2.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:1123,availability,Error,Error,1123,"Hello @guitargeek Sir, while Installing Root to my Ubuntu VM 18.04, during the building of the file ""cmake --build . -- -j3"" the build stops and shows this error every time at the 49% of the build. I have tried to start the process multiple times and even increase space in my Oracle VM but nothing seemed to work. CMake Error at /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-RelWithDebInfo.cmake:16 (message):. Command failed: 2. '/usr/bin/make' 'install'. See also. /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-*.log. CMakeFiles/AFTERIMAGE.dir/build.make:73: recipe for target 'AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install' failed. make[2]: *** [AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install] Error 1. CMakeFiles/Makefile2:280: recipe for target 'CMakeFiles/AFTERIMAGE.dir/all' failed. make[1]: *** [CMakeFiles/AFTERIMAGE.dir/all] Error 2. make[1]: *** Waiting for unfinished jobs.... [ 49%] Built target move_artifacts. [ 49%] Built target clang-tblgen. Makefile:151: recipe for target 'all' failed. make: *** [all] Error 2.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:29,deployability,Instal,Installing,29,"Hello @guitargeek Sir, while Installing Root to my Ubuntu VM 18.04, during the building of the file ""cmake --build . -- -j3"" the build stops and shows this error every time at the 49% of the build. I have tried to start the process multiple times and even increase space in my Oracle VM but nothing seemed to work. CMake Error at /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-RelWithDebInfo.cmake:16 (message):. Command failed: 2. '/usr/bin/make' 'install'. See also. /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-*.log. CMakeFiles/AFTERIMAGE.dir/build.make:73: recipe for target 'AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install' failed. make[2]: *** [AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install] Error 1. CMakeFiles/Makefile2:280: recipe for target 'CMakeFiles/AFTERIMAGE.dir/all' failed. make[1]: *** [CMakeFiles/AFTERIMAGE.dir/all] Error 2. make[1]: *** Waiting for unfinished jobs.... [ 49%] Built target move_artifacts. [ 49%] Built target clang-tblgen. Makefile:151: recipe for target 'all' failed. make: *** [all] Error 2.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:79,deployability,build,building,79,"Hello @guitargeek Sir, while Installing Root to my Ubuntu VM 18.04, during the building of the file ""cmake --build . -- -j3"" the build stops and shows this error every time at the 49% of the build. I have tried to start the process multiple times and even increase space in my Oracle VM but nothing seemed to work. CMake Error at /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-RelWithDebInfo.cmake:16 (message):. Command failed: 2. '/usr/bin/make' 'install'. See also. /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-*.log. CMakeFiles/AFTERIMAGE.dir/build.make:73: recipe for target 'AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install' failed. make[2]: *** [AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install] Error 1. CMakeFiles/Makefile2:280: recipe for target 'CMakeFiles/AFTERIMAGE.dir/all' failed. make[1]: *** [CMakeFiles/AFTERIMAGE.dir/all] Error 2. make[1]: *** Waiting for unfinished jobs.... [ 49%] Built target move_artifacts. [ 49%] Built target clang-tblgen. Makefile:151: recipe for target 'all' failed. make: *** [all] Error 2.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:109,deployability,build,build,109,"Hello @guitargeek Sir, while Installing Root to my Ubuntu VM 18.04, during the building of the file ""cmake --build . -- -j3"" the build stops and shows this error every time at the 49% of the build. I have tried to start the process multiple times and even increase space in my Oracle VM but nothing seemed to work. CMake Error at /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-RelWithDebInfo.cmake:16 (message):. Command failed: 2. '/usr/bin/make' 'install'. See also. /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-*.log. CMakeFiles/AFTERIMAGE.dir/build.make:73: recipe for target 'AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install' failed. make[2]: *** [AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install] Error 1. CMakeFiles/Makefile2:280: recipe for target 'CMakeFiles/AFTERIMAGE.dir/all' failed. make[1]: *** [CMakeFiles/AFTERIMAGE.dir/all] Error 2. make[1]: *** Waiting for unfinished jobs.... [ 49%] Built target move_artifacts. [ 49%] Built target clang-tblgen. Makefile:151: recipe for target 'all' failed. make: *** [all] Error 2.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:129,deployability,build,build,129,"Hello @guitargeek Sir, while Installing Root to my Ubuntu VM 18.04, during the building of the file ""cmake --build . -- -j3"" the build stops and shows this error every time at the 49% of the build. I have tried to start the process multiple times and even increase space in my Oracle VM but nothing seemed to work. CMake Error at /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-RelWithDebInfo.cmake:16 (message):. Command failed: 2. '/usr/bin/make' 'install'. See also. /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-*.log. CMakeFiles/AFTERIMAGE.dir/build.make:73: recipe for target 'AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install' failed. make[2]: *** [AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install] Error 1. CMakeFiles/Makefile2:280: recipe for target 'CMakeFiles/AFTERIMAGE.dir/all' failed. make[1]: *** [CMakeFiles/AFTERIMAGE.dir/all] Error 2. make[1]: *** Waiting for unfinished jobs.... [ 49%] Built target move_artifacts. [ 49%] Built target clang-tblgen. Makefile:151: recipe for target 'all' failed. make: *** [all] Error 2.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:191,deployability,build,build,191,"Hello @guitargeek Sir, while Installing Root to my Ubuntu VM 18.04, during the building of the file ""cmake --build . -- -j3"" the build stops and shows this error every time at the 49% of the build. I have tried to start the process multiple times and even increase space in my Oracle VM but nothing seemed to work. CMake Error at /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-RelWithDebInfo.cmake:16 (message):. Command failed: 2. '/usr/bin/make' 'install'. See also. /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-*.log. CMakeFiles/AFTERIMAGE.dir/build.make:73: recipe for target 'AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install' failed. make[2]: *** [AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install] Error 1. CMakeFiles/Makefile2:280: recipe for target 'CMakeFiles/AFTERIMAGE.dir/all' failed. make[1]: *** [CMakeFiles/AFTERIMAGE.dir/all] Error 2. make[1]: *** Waiting for unfinished jobs.... [ 49%] Built target move_artifacts. [ 49%] Built target clang-tblgen. Makefile:151: recipe for target 'all' failed. make: *** [all] Error 2.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:407,deployability,instal,install-RelWithDebInfo,407,"Hello @guitargeek Sir, while Installing Root to my Ubuntu VM 18.04, during the building of the file ""cmake --build . -- -j3"" the build stops and shows this error every time at the 49% of the build. I have tried to start the process multiple times and even increase space in my Oracle VM but nothing seemed to work. CMake Error at /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-RelWithDebInfo.cmake:16 (message):. Command failed: 2. '/usr/bin/make' 'install'. See also. /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-*.log. CMakeFiles/AFTERIMAGE.dir/build.make:73: recipe for target 'AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install' failed. make[2]: *** [AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install] Error 1. CMakeFiles/Makefile2:280: recipe for target 'CMakeFiles/AFTERIMAGE.dir/all' failed. make[1]: *** [CMakeFiles/AFTERIMAGE.dir/all] Error 2. make[1]: *** Waiting for unfinished jobs.... [ 49%] Built target move_artifacts. [ 49%] Built target clang-tblgen. Makefile:151: recipe for target 'all' failed. make: *** [all] Error 2.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:459,deployability,fail,failed,459,"Hello @guitargeek Sir, while Installing Root to my Ubuntu VM 18.04, during the building of the file ""cmake --build . -- -j3"" the build stops and shows this error every time at the 49% of the build. I have tried to start the process multiple times and even increase space in my Oracle VM but nothing seemed to work. CMake Error at /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-RelWithDebInfo.cmake:16 (message):. Command failed: 2. '/usr/bin/make' 'install'. See also. /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-*.log. CMakeFiles/AFTERIMAGE.dir/build.make:73: recipe for target 'AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install' failed. make[2]: *** [AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install] Error 1. CMakeFiles/Makefile2:280: recipe for target 'CMakeFiles/AFTERIMAGE.dir/all' failed. make[1]: *** [CMakeFiles/AFTERIMAGE.dir/all] Error 2. make[1]: *** Waiting for unfinished jobs.... [ 49%] Built target move_artifacts. [ 49%] Built target clang-tblgen. Makefile:151: recipe for target 'all' failed. make: *** [all] Error 2.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:487,deployability,instal,install,487,"Hello @guitargeek Sir, while Installing Root to my Ubuntu VM 18.04, during the building of the file ""cmake --build . -- -j3"" the build stops and shows this error every time at the 49% of the build. I have tried to start the process multiple times and even increase space in my Oracle VM but nothing seemed to work. CMake Error at /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-RelWithDebInfo.cmake:16 (message):. Command failed: 2. '/usr/bin/make' 'install'. See also. /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-*.log. CMakeFiles/AFTERIMAGE.dir/build.make:73: recipe for target 'AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install' failed. make[2]: *** [AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install] Error 1. CMakeFiles/Makefile2:280: recipe for target 'CMakeFiles/AFTERIMAGE.dir/all' failed. make[1]: *** [CMakeFiles/AFTERIMAGE.dir/all] Error 2. make[1]: *** Waiting for unfinished jobs.... [ 49%] Built target move_artifacts. [ 49%] Built target clang-tblgen. Makefile:151: recipe for target 'all' failed. make: *** [all] Error 2.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:584,deployability,instal,install,584,"Hello @guitargeek Sir, while Installing Root to my Ubuntu VM 18.04, during the building of the file ""cmake --build . -- -j3"" the build stops and shows this error every time at the 49% of the build. I have tried to start the process multiple times and even increase space in my Oracle VM but nothing seemed to work. CMake Error at /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-RelWithDebInfo.cmake:16 (message):. Command failed: 2. '/usr/bin/make' 'install'. See also. /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-*.log. CMakeFiles/AFTERIMAGE.dir/build.make:73: recipe for target 'AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install' failed. make[2]: *** [AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install] Error 1. CMakeFiles/Makefile2:280: recipe for target 'CMakeFiles/AFTERIMAGE.dir/all' failed. make[1]: *** [CMakeFiles/AFTERIMAGE.dir/all] Error 2. make[1]: *** Waiting for unfinished jobs.... [ 49%] Built target move_artifacts. [ 49%] Built target clang-tblgen. Makefile:151: recipe for target 'all' failed. make: *** [all] Error 2.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:594,deployability,log,log,594,"Hello @guitargeek Sir, while Installing Root to my Ubuntu VM 18.04, during the building of the file ""cmake --build . -- -j3"" the build stops and shows this error every time at the 49% of the build. I have tried to start the process multiple times and even increase space in my Oracle VM but nothing seemed to work. CMake Error at /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-RelWithDebInfo.cmake:16 (message):. Command failed: 2. '/usr/bin/make' 'install'. See also. /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-*.log. CMakeFiles/AFTERIMAGE.dir/build.make:73: recipe for target 'AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install' failed. make[2]: *** [AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install] Error 1. CMakeFiles/Makefile2:280: recipe for target 'CMakeFiles/AFTERIMAGE.dir/all' failed. make[1]: *** [CMakeFiles/AFTERIMAGE.dir/all] Error 2. make[1]: *** Waiting for unfinished jobs.... [ 49%] Built target move_artifacts. [ 49%] Built target clang-tblgen. Makefile:151: recipe for target 'all' failed. make: *** [all] Error 2.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:625,deployability,build,build,625,"Hello @guitargeek Sir, while Installing Root to my Ubuntu VM 18.04, during the building of the file ""cmake --build . -- -j3"" the build stops and shows this error every time at the 49% of the build. I have tried to start the process multiple times and even increase space in my Oracle VM but nothing seemed to work. CMake Error at /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-RelWithDebInfo.cmake:16 (message):. Command failed: 2. '/usr/bin/make' 'install'. See also. /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-*.log. CMakeFiles/AFTERIMAGE.dir/build.make:73: recipe for target 'AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install' failed. make[2]: *** [AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install] Error 1. CMakeFiles/Makefile2:280: recipe for target 'CMakeFiles/AFTERIMAGE.dir/all' failed. make[1]: *** [CMakeFiles/AFTERIMAGE.dir/all] Error 2. make[1]: *** Waiting for unfinished jobs.... [ 49%] Built target move_artifacts. [ 49%] Built target clang-tblgen. Makefile:151: recipe for target 'all' failed. make: *** [all] Error 2.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:709,deployability,instal,install,709,"Hello @guitargeek Sir, while Installing Root to my Ubuntu VM 18.04, during the building of the file ""cmake --build . -- -j3"" the build stops and shows this error every time at the 49% of the build. I have tried to start the process multiple times and even increase space in my Oracle VM but nothing seemed to work. CMake Error at /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-RelWithDebInfo.cmake:16 (message):. Command failed: 2. '/usr/bin/make' 'install'. See also. /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-*.log. CMakeFiles/AFTERIMAGE.dir/build.make:73: recipe for target 'AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install' failed. make[2]: *** [AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install] Error 1. CMakeFiles/Makefile2:280: recipe for target 'CMakeFiles/AFTERIMAGE.dir/all' failed. make[1]: *** [CMakeFiles/AFTERIMAGE.dir/all] Error 2. make[1]: *** Waiting for unfinished jobs.... [ 49%] Built target move_artifacts. [ 49%] Built target clang-tblgen. Makefile:151: recipe for target 'all' failed. make: *** [all] Error 2.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:718,deployability,fail,failed,718,"Hello @guitargeek Sir, while Installing Root to my Ubuntu VM 18.04, during the building of the file ""cmake --build . -- -j3"" the build stops and shows this error every time at the 49% of the build. I have tried to start the process multiple times and even increase space in my Oracle VM but nothing seemed to work. CMake Error at /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-RelWithDebInfo.cmake:16 (message):. Command failed: 2. '/usr/bin/make' 'install'. See also. /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-*.log. CMakeFiles/AFTERIMAGE.dir/build.make:73: recipe for target 'AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install' failed. make[2]: *** [AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install] Error 1. CMakeFiles/Makefile2:280: recipe for target 'CMakeFiles/AFTERIMAGE.dir/all' failed. make[1]: *** [CMakeFiles/AFTERIMAGE.dir/all] Error 2. make[1]: *** Waiting for unfinished jobs.... [ 49%] Built target move_artifacts. [ 49%] Built target clang-tblgen. Makefile:151: recipe for target 'all' failed. make: *** [all] Error 2.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:790,deployability,instal,install,790,"Hello @guitargeek Sir, while Installing Root to my Ubuntu VM 18.04, during the building of the file ""cmake --build . -- -j3"" the build stops and shows this error every time at the 49% of the build. I have tried to start the process multiple times and even increase space in my Oracle VM but nothing seemed to work. CMake Error at /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-RelWithDebInfo.cmake:16 (message):. Command failed: 2. '/usr/bin/make' 'install'. See also. /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-*.log. CMakeFiles/AFTERIMAGE.dir/build.make:73: recipe for target 'AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install' failed. make[2]: *** [AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install] Error 1. CMakeFiles/Makefile2:280: recipe for target 'CMakeFiles/AFTERIMAGE.dir/all' failed. make[1]: *** [CMakeFiles/AFTERIMAGE.dir/all] Error 2. make[1]: *** Waiting for unfinished jobs.... [ 49%] Built target move_artifacts. [ 49%] Built target clang-tblgen. Makefile:151: recipe for target 'all' failed. make: *** [all] Error 2.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:884,deployability,fail,failed,884,"Hello @guitargeek Sir, while Installing Root to my Ubuntu VM 18.04, during the building of the file ""cmake --build . -- -j3"" the build stops and shows this error every time at the 49% of the build. I have tried to start the process multiple times and even increase space in my Oracle VM but nothing seemed to work. CMake Error at /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-RelWithDebInfo.cmake:16 (message):. Command failed: 2. '/usr/bin/make' 'install'. See also. /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-*.log. CMakeFiles/AFTERIMAGE.dir/build.make:73: recipe for target 'AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install' failed. make[2]: *** [AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install] Error 1. CMakeFiles/Makefile2:280: recipe for target 'CMakeFiles/AFTERIMAGE.dir/all' failed. make[1]: *** [CMakeFiles/AFTERIMAGE.dir/all] Error 2. make[1]: *** Waiting for unfinished jobs.... [ 49%] Built target move_artifacts. [ 49%] Built target clang-tblgen. Makefile:151: recipe for target 'all' failed. make: *** [all] Error 2.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:1099,deployability,fail,failed,1099,"Hello @guitargeek Sir, while Installing Root to my Ubuntu VM 18.04, during the building of the file ""cmake --build . -- -j3"" the build stops and shows this error every time at the 49% of the build. I have tried to start the process multiple times and even increase space in my Oracle VM but nothing seemed to work. CMake Error at /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-RelWithDebInfo.cmake:16 (message):. Command failed: 2. '/usr/bin/make' 'install'. See also. /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-*.log. CMakeFiles/AFTERIMAGE.dir/build.make:73: recipe for target 'AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install' failed. make[2]: *** [AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install] Error 1. CMakeFiles/Makefile2:280: recipe for target 'CMakeFiles/AFTERIMAGE.dir/all' failed. make[1]: *** [CMakeFiles/AFTERIMAGE.dir/all] Error 2. make[1]: *** Waiting for unfinished jobs.... [ 49%] Built target move_artifacts. [ 49%] Built target clang-tblgen. Makefile:151: recipe for target 'all' failed. make: *** [all] Error 2.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:440,integrability,messag,message,440,"Hello @guitargeek Sir, while Installing Root to my Ubuntu VM 18.04, during the building of the file ""cmake --build . -- -j3"" the build stops and shows this error every time at the 49% of the build. I have tried to start the process multiple times and even increase space in my Oracle VM but nothing seemed to work. CMake Error at /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-RelWithDebInfo.cmake:16 (message):. Command failed: 2. '/usr/bin/make' 'install'. See also. /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-*.log. CMakeFiles/AFTERIMAGE.dir/build.make:73: recipe for target 'AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install' failed. make[2]: *** [AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install] Error 1. CMakeFiles/Makefile2:280: recipe for target 'CMakeFiles/AFTERIMAGE.dir/all' failed. make[1]: *** [CMakeFiles/AFTERIMAGE.dir/all] Error 2. make[1]: *** Waiting for unfinished jobs.... [ 49%] Built target move_artifacts. [ 49%] Built target clang-tblgen. Makefile:151: recipe for target 'all' failed. make: *** [all] Error 2.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:440,interoperability,messag,message,440,"Hello @guitargeek Sir, while Installing Root to my Ubuntu VM 18.04, during the building of the file ""cmake --build . -- -j3"" the build stops and shows this error every time at the 49% of the build. I have tried to start the process multiple times and even increase space in my Oracle VM but nothing seemed to work. CMake Error at /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-RelWithDebInfo.cmake:16 (message):. Command failed: 2. '/usr/bin/make' 'install'. See also. /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-*.log. CMakeFiles/AFTERIMAGE.dir/build.make:73: recipe for target 'AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install' failed. make[2]: *** [AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install] Error 1. CMakeFiles/Makefile2:280: recipe for target 'CMakeFiles/AFTERIMAGE.dir/all' failed. make[1]: *** [CMakeFiles/AFTERIMAGE.dir/all] Error 2. make[1]: *** Waiting for unfinished jobs.... [ 49%] Built target move_artifacts. [ 49%] Built target clang-tblgen. Makefile:151: recipe for target 'all' failed. make: *** [all] Error 2.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:156,performance,error,error,156,"Hello @guitargeek Sir, while Installing Root to my Ubuntu VM 18.04, during the building of the file ""cmake --build . -- -j3"" the build stops and shows this error every time at the 49% of the build. I have tried to start the process multiple times and even increase space in my Oracle VM but nothing seemed to work. CMake Error at /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-RelWithDebInfo.cmake:16 (message):. Command failed: 2. '/usr/bin/make' 'install'. See also. /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-*.log. CMakeFiles/AFTERIMAGE.dir/build.make:73: recipe for target 'AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install' failed. make[2]: *** [AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install] Error 1. CMakeFiles/Makefile2:280: recipe for target 'CMakeFiles/AFTERIMAGE.dir/all' failed. make[1]: *** [CMakeFiles/AFTERIMAGE.dir/all] Error 2. make[1]: *** Waiting for unfinished jobs.... [ 49%] Built target move_artifacts. [ 49%] Built target clang-tblgen. Makefile:151: recipe for target 'all' failed. make: *** [all] Error 2.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:168,performance,time,time,168,"Hello @guitargeek Sir, while Installing Root to my Ubuntu VM 18.04, during the building of the file ""cmake --build . -- -j3"" the build stops and shows this error every time at the 49% of the build. I have tried to start the process multiple times and even increase space in my Oracle VM but nothing seemed to work. CMake Error at /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-RelWithDebInfo.cmake:16 (message):. Command failed: 2. '/usr/bin/make' 'install'. See also. /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-*.log. CMakeFiles/AFTERIMAGE.dir/build.make:73: recipe for target 'AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install' failed. make[2]: *** [AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install] Error 1. CMakeFiles/Makefile2:280: recipe for target 'CMakeFiles/AFTERIMAGE.dir/all' failed. make[1]: *** [CMakeFiles/AFTERIMAGE.dir/all] Error 2. make[1]: *** Waiting for unfinished jobs.... [ 49%] Built target move_artifacts. [ 49%] Built target clang-tblgen. Makefile:151: recipe for target 'all' failed. make: *** [all] Error 2.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:241,performance,time,times,241,"Hello @guitargeek Sir, while Installing Root to my Ubuntu VM 18.04, during the building of the file ""cmake --build . -- -j3"" the build stops and shows this error every time at the 49% of the build. I have tried to start the process multiple times and even increase space in my Oracle VM but nothing seemed to work. CMake Error at /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-RelWithDebInfo.cmake:16 (message):. Command failed: 2. '/usr/bin/make' 'install'. See also. /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-*.log. CMakeFiles/AFTERIMAGE.dir/build.make:73: recipe for target 'AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install' failed. make[2]: *** [AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install] Error 1. CMakeFiles/Makefile2:280: recipe for target 'CMakeFiles/AFTERIMAGE.dir/all' failed. make[1]: *** [CMakeFiles/AFTERIMAGE.dir/all] Error 2. make[1]: *** Waiting for unfinished jobs.... [ 49%] Built target move_artifacts. [ 49%] Built target clang-tblgen. Makefile:151: recipe for target 'all' failed. make: *** [all] Error 2.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:321,performance,Error,Error,321,"Hello @guitargeek Sir, while Installing Root to my Ubuntu VM 18.04, during the building of the file ""cmake --build . -- -j3"" the build stops and shows this error every time at the 49% of the build. I have tried to start the process multiple times and even increase space in my Oracle VM but nothing seemed to work. CMake Error at /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-RelWithDebInfo.cmake:16 (message):. Command failed: 2. '/usr/bin/make' 'install'. See also. /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-*.log. CMakeFiles/AFTERIMAGE.dir/build.make:73: recipe for target 'AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install' failed. make[2]: *** [AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install] Error 1. CMakeFiles/Makefile2:280: recipe for target 'CMakeFiles/AFTERIMAGE.dir/all' failed. make[1]: *** [CMakeFiles/AFTERIMAGE.dir/all] Error 2. make[1]: *** Waiting for unfinished jobs.... [ 49%] Built target move_artifacts. [ 49%] Built target clang-tblgen. Makefile:151: recipe for target 'all' failed. make: *** [all] Error 2.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:799,performance,Error,Error,799,"Hello @guitargeek Sir, while Installing Root to my Ubuntu VM 18.04, during the building of the file ""cmake --build . -- -j3"" the build stops and shows this error every time at the 49% of the build. I have tried to start the process multiple times and even increase space in my Oracle VM but nothing seemed to work. CMake Error at /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-RelWithDebInfo.cmake:16 (message):. Command failed: 2. '/usr/bin/make' 'install'. See also. /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-*.log. CMakeFiles/AFTERIMAGE.dir/build.make:73: recipe for target 'AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install' failed. make[2]: *** [AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install] Error 1. CMakeFiles/Makefile2:280: recipe for target 'CMakeFiles/AFTERIMAGE.dir/all' failed. make[1]: *** [CMakeFiles/AFTERIMAGE.dir/all] Error 2. make[1]: *** Waiting for unfinished jobs.... [ 49%] Built target move_artifacts. [ 49%] Built target clang-tblgen. Makefile:151: recipe for target 'all' failed. make: *** [all] Error 2.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:937,performance,Error,Error,937,"Hello @guitargeek Sir, while Installing Root to my Ubuntu VM 18.04, during the building of the file ""cmake --build . -- -j3"" the build stops and shows this error every time at the 49% of the build. I have tried to start the process multiple times and even increase space in my Oracle VM but nothing seemed to work. CMake Error at /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-RelWithDebInfo.cmake:16 (message):. Command failed: 2. '/usr/bin/make' 'install'. See also. /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-*.log. CMakeFiles/AFTERIMAGE.dir/build.make:73: recipe for target 'AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install' failed. make[2]: *** [AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install] Error 1. CMakeFiles/Makefile2:280: recipe for target 'CMakeFiles/AFTERIMAGE.dir/all' failed. make[1]: *** [CMakeFiles/AFTERIMAGE.dir/all] Error 2. make[1]: *** Waiting for unfinished jobs.... [ 49%] Built target move_artifacts. [ 49%] Built target clang-tblgen. Makefile:151: recipe for target 'all' failed. make: *** [all] Error 2.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:1123,performance,Error,Error,1123,"Hello @guitargeek Sir, while Installing Root to my Ubuntu VM 18.04, during the building of the file ""cmake --build . -- -j3"" the build stops and shows this error every time at the 49% of the build. I have tried to start the process multiple times and even increase space in my Oracle VM but nothing seemed to work. CMake Error at /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-RelWithDebInfo.cmake:16 (message):. Command failed: 2. '/usr/bin/make' 'install'. See also. /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-*.log. CMakeFiles/AFTERIMAGE.dir/build.make:73: recipe for target 'AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install' failed. make[2]: *** [AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install] Error 1. CMakeFiles/Makefile2:280: recipe for target 'CMakeFiles/AFTERIMAGE.dir/all' failed. make[1]: *** [CMakeFiles/AFTERIMAGE.dir/all] Error 2. make[1]: *** Waiting for unfinished jobs.... [ 49%] Built target move_artifacts. [ 49%] Built target clang-tblgen. Makefile:151: recipe for target 'all' failed. make: *** [all] Error 2.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:459,reliability,fail,failed,459,"Hello @guitargeek Sir, while Installing Root to my Ubuntu VM 18.04, during the building of the file ""cmake --build . -- -j3"" the build stops and shows this error every time at the 49% of the build. I have tried to start the process multiple times and even increase space in my Oracle VM but nothing seemed to work. CMake Error at /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-RelWithDebInfo.cmake:16 (message):. Command failed: 2. '/usr/bin/make' 'install'. See also. /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-*.log. CMakeFiles/AFTERIMAGE.dir/build.make:73: recipe for target 'AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install' failed. make[2]: *** [AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install] Error 1. CMakeFiles/Makefile2:280: recipe for target 'CMakeFiles/AFTERIMAGE.dir/all' failed. make[1]: *** [CMakeFiles/AFTERIMAGE.dir/all] Error 2. make[1]: *** Waiting for unfinished jobs.... [ 49%] Built target move_artifacts. [ 49%] Built target clang-tblgen. Makefile:151: recipe for target 'all' failed. make: *** [all] Error 2.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:718,reliability,fail,failed,718,"Hello @guitargeek Sir, while Installing Root to my Ubuntu VM 18.04, during the building of the file ""cmake --build . -- -j3"" the build stops and shows this error every time at the 49% of the build. I have tried to start the process multiple times and even increase space in my Oracle VM but nothing seemed to work. CMake Error at /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-RelWithDebInfo.cmake:16 (message):. Command failed: 2. '/usr/bin/make' 'install'. See also. /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-*.log. CMakeFiles/AFTERIMAGE.dir/build.make:73: recipe for target 'AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install' failed. make[2]: *** [AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install] Error 1. CMakeFiles/Makefile2:280: recipe for target 'CMakeFiles/AFTERIMAGE.dir/all' failed. make[1]: *** [CMakeFiles/AFTERIMAGE.dir/all] Error 2. make[1]: *** Waiting for unfinished jobs.... [ 49%] Built target move_artifacts. [ 49%] Built target clang-tblgen. Makefile:151: recipe for target 'all' failed. make: *** [all] Error 2.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:884,reliability,fail,failed,884,"Hello @guitargeek Sir, while Installing Root to my Ubuntu VM 18.04, during the building of the file ""cmake --build . -- -j3"" the build stops and shows this error every time at the 49% of the build. I have tried to start the process multiple times and even increase space in my Oracle VM but nothing seemed to work. CMake Error at /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-RelWithDebInfo.cmake:16 (message):. Command failed: 2. '/usr/bin/make' 'install'. See also. /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-*.log. CMakeFiles/AFTERIMAGE.dir/build.make:73: recipe for target 'AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install' failed. make[2]: *** [AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install] Error 1. CMakeFiles/Makefile2:280: recipe for target 'CMakeFiles/AFTERIMAGE.dir/all' failed. make[1]: *** [CMakeFiles/AFTERIMAGE.dir/all] Error 2. make[1]: *** Waiting for unfinished jobs.... [ 49%] Built target move_artifacts. [ 49%] Built target clang-tblgen. Makefile:151: recipe for target 'all' failed. make: *** [all] Error 2.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:1099,reliability,fail,failed,1099,"Hello @guitargeek Sir, while Installing Root to my Ubuntu VM 18.04, during the building of the file ""cmake --build . -- -j3"" the build stops and shows this error every time at the 49% of the build. I have tried to start the process multiple times and even increase space in my Oracle VM but nothing seemed to work. CMake Error at /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-RelWithDebInfo.cmake:16 (message):. Command failed: 2. '/usr/bin/make' 'install'. See also. /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-*.log. CMakeFiles/AFTERIMAGE.dir/build.make:73: recipe for target 'AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install' failed. make[2]: *** [AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install] Error 1. CMakeFiles/Makefile2:280: recipe for target 'CMakeFiles/AFTERIMAGE.dir/all' failed. make[1]: *** [CMakeFiles/AFTERIMAGE.dir/all] Error 2. make[1]: *** Waiting for unfinished jobs.... [ 49%] Built target move_artifacts. [ 49%] Built target clang-tblgen. Makefile:151: recipe for target 'all' failed. make: *** [all] Error 2.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:156,safety,error,error,156,"Hello @guitargeek Sir, while Installing Root to my Ubuntu VM 18.04, during the building of the file ""cmake --build . -- -j3"" the build stops and shows this error every time at the 49% of the build. I have tried to start the process multiple times and even increase space in my Oracle VM but nothing seemed to work. CMake Error at /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-RelWithDebInfo.cmake:16 (message):. Command failed: 2. '/usr/bin/make' 'install'. See also. /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-*.log. CMakeFiles/AFTERIMAGE.dir/build.make:73: recipe for target 'AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install' failed. make[2]: *** [AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install] Error 1. CMakeFiles/Makefile2:280: recipe for target 'CMakeFiles/AFTERIMAGE.dir/all' failed. make[1]: *** [CMakeFiles/AFTERIMAGE.dir/all] Error 2. make[1]: *** Waiting for unfinished jobs.... [ 49%] Built target move_artifacts. [ 49%] Built target clang-tblgen. Makefile:151: recipe for target 'all' failed. make: *** [all] Error 2.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:321,safety,Error,Error,321,"Hello @guitargeek Sir, while Installing Root to my Ubuntu VM 18.04, during the building of the file ""cmake --build . -- -j3"" the build stops and shows this error every time at the 49% of the build. I have tried to start the process multiple times and even increase space in my Oracle VM but nothing seemed to work. CMake Error at /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-RelWithDebInfo.cmake:16 (message):. Command failed: 2. '/usr/bin/make' 'install'. See also. /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-*.log. CMakeFiles/AFTERIMAGE.dir/build.make:73: recipe for target 'AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install' failed. make[2]: *** [AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install] Error 1. CMakeFiles/Makefile2:280: recipe for target 'CMakeFiles/AFTERIMAGE.dir/all' failed. make[1]: *** [CMakeFiles/AFTERIMAGE.dir/all] Error 2. make[1]: *** Waiting for unfinished jobs.... [ 49%] Built target move_artifacts. [ 49%] Built target clang-tblgen. Makefile:151: recipe for target 'all' failed. make: *** [all] Error 2.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:594,safety,log,log,594,"Hello @guitargeek Sir, while Installing Root to my Ubuntu VM 18.04, during the building of the file ""cmake --build . -- -j3"" the build stops and shows this error every time at the 49% of the build. I have tried to start the process multiple times and even increase space in my Oracle VM but nothing seemed to work. CMake Error at /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-RelWithDebInfo.cmake:16 (message):. Command failed: 2. '/usr/bin/make' 'install'. See also. /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-*.log. CMakeFiles/AFTERIMAGE.dir/build.make:73: recipe for target 'AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install' failed. make[2]: *** [AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install] Error 1. CMakeFiles/Makefile2:280: recipe for target 'CMakeFiles/AFTERIMAGE.dir/all' failed. make[1]: *** [CMakeFiles/AFTERIMAGE.dir/all] Error 2. make[1]: *** Waiting for unfinished jobs.... [ 49%] Built target move_artifacts. [ 49%] Built target clang-tblgen. Makefile:151: recipe for target 'all' failed. make: *** [all] Error 2.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:799,safety,Error,Error,799,"Hello @guitargeek Sir, while Installing Root to my Ubuntu VM 18.04, during the building of the file ""cmake --build . -- -j3"" the build stops and shows this error every time at the 49% of the build. I have tried to start the process multiple times and even increase space in my Oracle VM but nothing seemed to work. CMake Error at /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-RelWithDebInfo.cmake:16 (message):. Command failed: 2. '/usr/bin/make' 'install'. See also. /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-*.log. CMakeFiles/AFTERIMAGE.dir/build.make:73: recipe for target 'AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install' failed. make[2]: *** [AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install] Error 1. CMakeFiles/Makefile2:280: recipe for target 'CMakeFiles/AFTERIMAGE.dir/all' failed. make[1]: *** [CMakeFiles/AFTERIMAGE.dir/all] Error 2. make[1]: *** Waiting for unfinished jobs.... [ 49%] Built target move_artifacts. [ 49%] Built target clang-tblgen. Makefile:151: recipe for target 'all' failed. make: *** [all] Error 2.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:937,safety,Error,Error,937,"Hello @guitargeek Sir, while Installing Root to my Ubuntu VM 18.04, during the building of the file ""cmake --build . -- -j3"" the build stops and shows this error every time at the 49% of the build. I have tried to start the process multiple times and even increase space in my Oracle VM but nothing seemed to work. CMake Error at /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-RelWithDebInfo.cmake:16 (message):. Command failed: 2. '/usr/bin/make' 'install'. See also. /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-*.log. CMakeFiles/AFTERIMAGE.dir/build.make:73: recipe for target 'AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install' failed. make[2]: *** [AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install] Error 1. CMakeFiles/Makefile2:280: recipe for target 'CMakeFiles/AFTERIMAGE.dir/all' failed. make[1]: *** [CMakeFiles/AFTERIMAGE.dir/all] Error 2. make[1]: *** Waiting for unfinished jobs.... [ 49%] Built target move_artifacts. [ 49%] Built target clang-tblgen. Makefile:151: recipe for target 'all' failed. make: *** [all] Error 2.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:1123,safety,Error,Error,1123,"Hello @guitargeek Sir, while Installing Root to my Ubuntu VM 18.04, during the building of the file ""cmake --build . -- -j3"" the build stops and shows this error every time at the 49% of the build. I have tried to start the process multiple times and even increase space in my Oracle VM but nothing seemed to work. CMake Error at /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-RelWithDebInfo.cmake:16 (message):. Command failed: 2. '/usr/bin/make' 'install'. See also. /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-*.log. CMakeFiles/AFTERIMAGE.dir/build.make:73: recipe for target 'AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install' failed. make[2]: *** [AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install] Error 1. CMakeFiles/Makefile2:280: recipe for target 'CMakeFiles/AFTERIMAGE.dir/all' failed. make[1]: *** [CMakeFiles/AFTERIMAGE.dir/all] Error 2. make[1]: *** Waiting for unfinished jobs.... [ 49%] Built target move_artifacts. [ 49%] Built target clang-tblgen. Makefile:151: recipe for target 'all' failed. make: *** [all] Error 2.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:594,security,log,log,594,"Hello @guitargeek Sir, while Installing Root to my Ubuntu VM 18.04, during the building of the file ""cmake --build . -- -j3"" the build stops and shows this error every time at the 49% of the build. I have tried to start the process multiple times and even increase space in my Oracle VM but nothing seemed to work. CMake Error at /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-RelWithDebInfo.cmake:16 (message):. Command failed: 2. '/usr/bin/make' 'install'. See also. /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-*.log. CMakeFiles/AFTERIMAGE.dir/build.make:73: recipe for target 'AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install' failed. make[2]: *** [AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install] Error 1. CMakeFiles/Makefile2:280: recipe for target 'CMakeFiles/AFTERIMAGE.dir/all' failed. make[1]: *** [CMakeFiles/AFTERIMAGE.dir/all] Error 2. make[1]: *** Waiting for unfinished jobs.... [ 49%] Built target move_artifacts. [ 49%] Built target clang-tblgen. Makefile:151: recipe for target 'all' failed. make: *** [all] Error 2.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:594,testability,log,log,594,"Hello @guitargeek Sir, while Installing Root to my Ubuntu VM 18.04, during the building of the file ""cmake --build . -- -j3"" the build stops and shows this error every time at the 49% of the build. I have tried to start the process multiple times and even increase space in my Oracle VM but nothing seemed to work. CMake Error at /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-RelWithDebInfo.cmake:16 (message):. Command failed: 2. '/usr/bin/make' 'install'. See also. /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-*.log. CMakeFiles/AFTERIMAGE.dir/build.make:73: recipe for target 'AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install' failed. make[2]: *** [AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install] Error 1. CMakeFiles/Makefile2:280: recipe for target 'CMakeFiles/AFTERIMAGE.dir/all' failed. make[1]: *** [CMakeFiles/AFTERIMAGE.dir/all] Error 2. make[1]: *** Waiting for unfinished jobs.... [ 49%] Built target move_artifacts. [ 49%] Built target clang-tblgen. Makefile:151: recipe for target 'all' failed. make: *** [all] Error 2.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:135,usability,stop,stops,135,"Hello @guitargeek Sir, while Installing Root to my Ubuntu VM 18.04, during the building of the file ""cmake --build . -- -j3"" the build stops and shows this error every time at the 49% of the build. I have tried to start the process multiple times and even increase space in my Oracle VM but nothing seemed to work. CMake Error at /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-RelWithDebInfo.cmake:16 (message):. Command failed: 2. '/usr/bin/make' 'install'. See also. /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-*.log. CMakeFiles/AFTERIMAGE.dir/build.make:73: recipe for target 'AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install' failed. make[2]: *** [AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install] Error 1. CMakeFiles/Makefile2:280: recipe for target 'CMakeFiles/AFTERIMAGE.dir/all' failed. make[1]: *** [CMakeFiles/AFTERIMAGE.dir/all] Error 2. make[1]: *** Waiting for unfinished jobs.... [ 49%] Built target move_artifacts. [ 49%] Built target clang-tblgen. Makefile:151: recipe for target 'all' failed. make: *** [all] Error 2.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:156,usability,error,error,156,"Hello @guitargeek Sir, while Installing Root to my Ubuntu VM 18.04, during the building of the file ""cmake --build . -- -j3"" the build stops and shows this error every time at the 49% of the build. I have tried to start the process multiple times and even increase space in my Oracle VM but nothing seemed to work. CMake Error at /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-RelWithDebInfo.cmake:16 (message):. Command failed: 2. '/usr/bin/make' 'install'. See also. /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-*.log. CMakeFiles/AFTERIMAGE.dir/build.make:73: recipe for target 'AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install' failed. make[2]: *** [AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install] Error 1. CMakeFiles/Makefile2:280: recipe for target 'CMakeFiles/AFTERIMAGE.dir/all' failed. make[1]: *** [CMakeFiles/AFTERIMAGE.dir/all] Error 2. make[1]: *** Waiting for unfinished jobs.... [ 49%] Built target move_artifacts. [ 49%] Built target clang-tblgen. Makefile:151: recipe for target 'all' failed. make: *** [all] Error 2.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
https://github.com/root-project/root/issues/8758:321,usability,Error,Error,321,"Hello @guitargeek Sir, while Installing Root to my Ubuntu VM 18.04, during the building of the file ""cmake --build . -- -j3"" the build stops and shows this error every time at the 49% of the build. I have tried to start the process multiple times and even increase space in my Oracle VM but nothing seemed to work. CMake Error at /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-RelWithDebInfo.cmake:16 (message):. Command failed: 2. '/usr/bin/make' 'install'. See also. /home/viren/Root CERN/root/AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install-*.log. CMakeFiles/AFTERIMAGE.dir/build.make:73: recipe for target 'AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install' failed. make[2]: *** [AFTERIMAGE-prefix/src/AFTERIMAGE-stamp/AFTERIMAGE-install] Error 1. CMakeFiles/Makefile2:280: recipe for target 'CMakeFiles/AFTERIMAGE.dir/all' failed. make[1]: *** [CMakeFiles/AFTERIMAGE.dir/all] Error 2. make[1]: *** Waiting for unfinished jobs.... [ 49%] Built target move_artifacts. [ 49%] Built target clang-tblgen. Makefile:151: recipe for target 'all' failed. make: *** [all] Error 2.",MatchSource.ISSUE_COMMENT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/issues/8758
